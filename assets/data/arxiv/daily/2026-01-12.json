{
  "date": "2026-01-12",
  "fetched_at": "2026-01-12T06:11:15.711649+00:00",
  "categories": [
    "cs.CV",
    "cs.CL",
    "cs.LG",
    "cs.AI",
    "cs.RO"
  ],
  "total_count": 390,
  "papers": [
    {
      "id": "2601.06025v1",
      "title": "Manifold limit for the training of shallow graph convolutional neural networks",
      "abstract": "We study the discrete-to-continuum consistency of the training of shallow graph convolutional neural networks (GCNNs) on proximity graphs of sampled point clouds under a manifold assumption. Graph convolution is defined spectrally via the graph Laplacian, whose low-frequency spectrum approximates that of the Laplace-Beltrami operator of the underlying smooth manifold, and shallow GCNNs of possibly infinite width are linear functionals on the space of measures on the parameter space. From this functional-analytic perspective, graph signals are seen as spatial discretizations of functions on the manifold, which leads to a natural notion of training data consistent across graph resolutions. To enable convergence results, the continuum parameter space is chosen as a weakly compact product of unit balls, with Sobolev regularity imposed on the output weight and bias, but not on the convolutional parameter. The corresponding discrete parameter spaces inherit the corresponding spectral decay, and are additionally restricted by a frequency cutoff adapted to the informative spectral window of the graph Laplacians. Under these assumptions, we prove $Γ$-convergence of regularized empirical risk minimization functionals and corresponding convergence of their global minimizers, in the sense of weak convergence of the parameter measures and uniform convergence of the functions over compact sets. This provides a formalization of mesh and sample independence for the training of such networks.",
      "authors": [
        "Johanna Tengler",
        "Christoph Brune",
        "José A. Iglesias"
      ],
      "published": "2026-01-09T18:59:20Z",
      "categories": [
        "stat.ML",
        "cs.LG",
        "math.FA",
        "math.OC"
      ],
      "primary_category": "stat.ML",
      "abs_url": "https://arxiv.org/abs/2601.06025v1",
      "pdf_url": "https://arxiv.org/pdf/2601.06025v1.pdf"
    },
    {
      "id": "2601.06022v1",
      "title": "AdaFuse: Adaptive Ensemble Decoding with Test-Time Scaling for LLMs",
      "abstract": "Large language models (LLMs) exhibit complementary strengths arising from differences in pretraining data, model architectures, and decoding behaviors. Inference-time ensembling provides a practical way to combine these capabilities without retraining. However, existing ensemble approaches suffer from fundamental limitations. Most rely on fixed fusion granularity, which lacks the flexibility required for mid-generation adaptation and fails to adapt to different generation characteristics across tasks. To address these challenges, we propose AdaFuse, an adaptive ensemble decoding framework that dynamically selects semantically appropriate fusion units during generation. Rather than committing to a fixed granularity, AdaFuse adjusts fusion behavior on the fly based on the decoding context, with words serving as basic building blocks for alignment. To be specific, we introduce an uncertainty-based criterion to decide whether to apply ensembling at each decoding step. Under confident decoding states, the model continues generation directly. In less certain states, AdaFuse invokes a diversity-aware scaling strategy to explore alternative candidate continuations and inform ensemble decisions. This design establishes a synergistic interaction between adaptive ensembling and test-time scaling, where ensemble decisions guide targeted exploration, and the resulting diversity in turn strengthens ensemble quality. Experiments on open-domain question answering, arithmetic reasoning, and machine translation demonstrate that AdaFuse consistently outperforms strong ensemble baselines, achieving an average relative improvement of 6.88%. The code is available at https://github.com/CCM0111/AdaFuse.",
      "authors": [
        "Chengming Cui",
        "Tianxin Wei",
        "Ziyi Chen",
        "Ruizhong Qiu",
        "Zhichen Zeng",
        "Zhining Liu",
        "Xuying Ning",
        "Duo Zhou",
        "Jingrui He"
      ],
      "published": "2026-01-09T18:58:22Z",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "abs_url": "https://arxiv.org/abs/2601.06022v1",
      "pdf_url": "https://arxiv.org/pdf/2601.06022v1.pdf"
    },
    {
      "id": "2601.06021v1",
      "title": "Chaining the Evidence: Robust Reinforcement Learning for Deep Search Agents with Citation-Aware Rubric Rewards",
      "abstract": "Reinforcement learning (RL) has emerged as a critical technique for enhancing LLM-based deep search agents. However, existing approaches primarily rely on binary outcome rewards, which fail to capture the comprehensiveness and factuality of agents' reasoning process, and often lead to undesirable behaviors such as shortcut exploitation and hallucinations. To address these limitations, we propose \\textbf{Citation-aware Rubric Rewards (CaRR)}, a fine-grained reward framework for deep search agents that emphasizes reasoning comprehensiveness, factual grounding, and evidence connectivity. CaRR decomposes complex questions into verifiable single-hop rubrics and requires agents to satisfy these rubrics by explicitly identifying hidden entities, supporting them with correct citations, and constructing complete evidence chains that link to the predicted answer. We further introduce \\textbf{Citation-aware Group Relative Policy Optimization (C-GRPO)}, which combines CaRR and outcome rewards for training robust deep search agents. Experiments show that C-GRPO consistently outperforms standard outcome-based RL baselines across multiple deep search benchmarks. Our analysis also validates that C-GRPO effectively discourages shortcut exploitation, promotes comprehensive, evidence-grounded reasoning, and exhibits strong generalization to open-ended deep research tasks. Our code and data are available at https://github.com/THUDM/CaRR.",
      "authors": [
        "Jiajie Zhang",
        "Xin Lv",
        "Ling Feng",
        "Lei Hou",
        "Juanzi Li"
      ],
      "published": "2026-01-09T18:57:53Z",
      "categories": [
        "cs.CL"
      ],
      "primary_category": "cs.CL",
      "abs_url": "https://arxiv.org/abs/2601.06021v1",
      "pdf_url": "https://arxiv.org/pdf/2601.06021v1.pdf"
    },
    {
      "id": "2601.06016v1",
      "title": "LookAroundNet: Extending Temporal Context with Transformers for Clinically Viable EEG Seizure Detection",
      "abstract": "Automated seizure detection from electroencephalography (EEG) remains difficult due to the large variability of seizure dynamics across patients, recording conditions, and clinical settings. We introduce LookAroundNet, a transformer-based seizure detector that uses a wider temporal window of EEG data to model seizure activity. The seizure detector incorporates EEG signals before and after the segment of interest, reflecting how clinicians use surrounding context when interpreting EEG recordings. We evaluate the proposed method on multiple EEG datasets spanning diverse clinical environments, patient populations, and recording modalities, including routine clinical EEG and long-term ambulatory recordings, in order to study performance across varying data distributions. The evaluation includes publicly available datasets as well as a large proprietary collection of home EEG recordings, providing complementary views of controlled clinical data and unconstrained home-monitoring conditions. Our results show that LookAroundNet achieves strong performance across datasets, generalizes well to previously unseen recording conditions, and operates with computational costs compatible with real-world clinical deployment. The results indicate that extended temporal context, increased training data diversity, and model ensembling are key factors for improving performance. This work contributes to moving automatic seizure detection models toward clinically viable solutions.",
      "authors": [
        "Þór Sverrisson",
        "Steinn Guðmundsson"
      ],
      "published": "2026-01-09T18:52:24Z",
      "categories": [
        "cs.LG"
      ],
      "primary_category": "cs.LG",
      "abs_url": "https://arxiv.org/abs/2601.06016v1",
      "pdf_url": "https://arxiv.org/pdf/2601.06016v1.pdf"
    },
    {
      "id": "2601.06009v1",
      "title": "Detecting Stochasticity in Discrete Signals via Nonparametric Excursion Theorem",
      "abstract": "We develop a practical framework for distinguishing diffusive stochastic processes from deterministic signals using only a single discrete time series. Our approach is based on classical excursion and crossing theorems for continuous semimartingales, which correlates number $N_\\varepsilon$ of excursions of magnitude at least $\\varepsilon$ with the quadratic variation $[X]_T$ of the process. The scaling law holds universally for all continuous semimartingales with finite quadratic variation, including general Ito diffusions with nonlinear or state-dependent volatility, but fails sharply for deterministic systems -- thereby providing a theoretically-certfied method of distinguishing between these dynamics, as opposed to the subjective entropy or recurrence based state of the art methods. We construct a robust data-driven diffusion test. The method compares the empirical excursion counts against the theoretical expectation. The resulting ratio $K(\\varepsilon)=N_{\\varepsilon}^{\\mathrm{emp}}/N_{\\varepsilon}^{\\mathrm{theory}}$ is then summarized by a log-log slope deviation measuring the $\\varepsilon^{-2}$ law that provides a classification into diffusion-like or not. We demonstrate the method on canonical stochastic systems, some periodic and chaotic maps and systems with additive white noise, as well as the stochastic Duffing system. The approach is nonparametric, model-free, and relies only on the universal small-scale structure of continuous semimartingales.",
      "authors": [
        "Sunia Tanweer",
        "Firas A. Khasawneh"
      ],
      "published": "2026-01-09T18:47:57Z",
      "categories": [
        "stat.ML",
        "cs.LG",
        "eess.SP",
        "math.PR",
        "stat.AP"
      ],
      "primary_category": "stat.ML",
      "abs_url": "https://arxiv.org/abs/2601.06009v1",
      "pdf_url": "https://arxiv.org/pdf/2601.06009v1.pdf"
    },
    {
      "id": "2601.06007v1",
      "title": "Don't Break the Cache: An Evaluation of Prompt Caching for Long-Horizon Agentic Tasks",
      "abstract": "Recent advancements in Large Language Model (LLM) agents have enabled complex multi-turn agentic tasks requiring extensive tool calling, where conversations can span dozens of API calls with increasingly large context windows. However, although major LLM providers offer prompt caching to reduce cost and latency, its benefits for agentic workloads remain underexplored in the research literature. To our knowledge, no prior work quantifies these cost savings or compares caching strategies for multi-turn agentic tasks. We present a comprehensive evaluation of prompt caching across three major LLM providers (OpenAI, Anthropic, and Google) and compare three caching strategies, including full context caching, system prompt only caching, and caching that excludes dynamic tool results. We evaluate on DeepResearchBench, a multi-turn agentic benchmark where agents autonomously execute real-world web search tool calls to answer complex research questions, measuring both API cost and time to first token (TTFT) across over 500 agent sessions with 10,000-token system prompts. Our results demonstrate that prompt caching reduces API costs by 45-80% and improves time to first token by 13-31% across providers. We find that strategic prompt cache block control, such as placing dynamic content at the end of the system prompt, avoiding dynamic traditional function calling, and excluding dynamic tool results, provides more consistent benefits than naive full-context caching, which can paradoxically increase latency. Our analysis reveals nuanced variations in caching behavior across providers, and we provide practical guidance for implementing prompt caching in production agentic systems.",
      "authors": [
        "Elias Lumer",
        "Faheem Nizar",
        "Akshaya Jangiti",
        "Kevin Frank",
        "Anmol Gulati",
        "Mandar Phadate",
        "Vamse Kumar Subbiah"
      ],
      "published": "2026-01-09T18:41:57Z",
      "categories": [
        "cs.CL"
      ],
      "primary_category": "cs.CL",
      "abs_url": "https://arxiv.org/abs/2601.06007v1",
      "pdf_url": "https://arxiv.org/pdf/2601.06007v1.pdf"
    },
    {
      "id": "2601.06002v1",
      "title": "The Molecular Structure of Thought: Mapping the Topology of Long Chain-of-Thought Reasoning",
      "abstract": "Large language models (LLMs) often fail to learn effective long chain-of-thought (Long CoT) reasoning from human or non-Long-CoT LLMs imitation. To understand this, we propose that effective and learnable Long CoT trajectories feature stable molecular-like structures in unified view, which are formed by three interaction types: Deep-Reasoning (covalent-like), Self-Reflection (hydrogen-bond-like), and Self-Exploration (van der Waals-like). Analysis of distilled trajectories reveals these structures emerge from Long CoT fine-tuning, not keyword imitation. We introduce Effective Semantic Isomers and show that only bonds promoting fast entropy convergence support stable Long CoT learning, while structural competition impairs training. Drawing on these findings, we present Mole-Syn, a distribution-transfer-graph method that guides synthesis of effective Long CoT structures, boosting performance and RL stability across benchmarks.",
      "authors": [
        "Qiguang Chen",
        "Yantao Du",
        "Ziniu Li",
        "Jinhao Liu",
        "Songyao Duan",
        "Jiarui Guo",
        "Minghao Liu",
        "Jiaheng Liu",
        "Tong Yang",
        "Ge Zhang",
        "Libo Qin",
        "Wanxiang Che",
        "Wenhao Huang"
      ],
      "published": "2026-01-09T18:39:01Z",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "abs_url": "https://arxiv.org/abs/2601.06002v1",
      "pdf_url": "https://arxiv.org/pdf/2601.06002v1.pdf"
    },
    {
      "id": "2601.05991v1",
      "title": "Open-Vocabulary 3D Instruction Ambiguity Detection",
      "abstract": "In safety-critical domains, linguistic ambiguity can have severe consequences; a vague command like \"Pass me the vial\" in a surgical setting could lead to catastrophic errors. Yet, most embodied AI research overlooks this, assuming instructions are clear and focusing on execution rather than confirmation. To address this critical safety gap, we are the first to define Open-Vocabulary 3D Instruction Ambiguity Detection, a fundamental new task where a model must determine if a command has a single, unambiguous meaning within a given 3D scene. To support this research, we build Ambi3D, the large-scale benchmark for this task, featuring over 700 diverse 3D scenes and around 22k instructions. Our analysis reveals a surprising limitation: state-of-the-art 3D Large Language Models (LLMs) struggle to reliably determine if an instruction is ambiguous. To address this challenge, we propose AmbiVer, a two-stage framework that collects explicit visual evidence from multiple views and uses it to guide an vision-language model (VLM) in judging instruction ambiguity. Extensive experiments demonstrate the challenge of our task and the effectiveness of AmbiVer, paving the way for safer and more trustworthy embodied AI. Code and dataset available at https://jiayuding031020.github.io/ambi3d/.",
      "authors": [
        "Jiayu Ding",
        "Haoran Tang",
        "Ge Li"
      ],
      "published": "2026-01-09T18:17:11Z",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "abs_url": "https://arxiv.org/abs/2601.05991v1",
      "pdf_url": "https://arxiv.org/pdf/2601.05991v1.pdf"
    },
    {
      "id": "2601.05988v1",
      "title": "CyberGFM: Graph Foundation Models for Lateral Movement Detection in Enterprise Networks",
      "abstract": "Representing networks as a graph and training a link prediction model using benign connections is an effective method of anomaly-based intrusion detection. Existing works using this technique have shown great success using temporal graph neural networks and skip-gram-based approaches on random walks. However, random walk-based approaches are unable to incorporate rich edge data, while the GNN-based approaches require large amounts of memory to train. In this work, we propose extending the original insight from random walk-based skip-grams--that random walks through a graph are analogous to sentences in a corpus--to the more modern transformer-based foundation models. Using language models that take advantage of GPU optimizations, we can quickly train a graph foundation model to predict missing tokens in random walks through a network of computers. The graph foundation model is then finetuned for link prediction and used as a network anomaly detector. This new approach allows us to combine the efficiency of random walk-based methods and the rich semantic representation of deep learning methods. This system, which we call CyberGFM, achieved state-of-the-art results on three widely used network anomaly detection datasets, delivering a up to 2$\\times$ improvement in average precision. We found that CyberGFM outperforms all prior works in unsupervised link prediction for network anomaly detection, using the same number of parameters, and with equal or better efficiency than the previous best approaches.",
      "authors": [
        "Isaiah J. King",
        "Bernardo Trindade",
        "Benjamin Bowman",
        "H. Howie Huang"
      ],
      "published": "2026-01-09T18:08:47Z",
      "categories": [
        "cs.CR",
        "cs.LG"
      ],
      "primary_category": "cs.CR",
      "abs_url": "https://arxiv.org/abs/2601.05988v1",
      "pdf_url": "https://arxiv.org/pdf/2601.05988v1.pdf"
    },
    {
      "id": "2601.05986v1",
      "title": "Deepfake detectors are DUMB: A benchmark to assess adversarial training robustness under transferability constraints",
      "abstract": "Deepfake detection systems deployed in real-world environments are subject to adversaries capable of crafting imperceptible perturbations that degrade model performance. While adversarial training is a widely adopted defense, its effectiveness under realistic conditions -- where attackers operate with limited knowledge and mismatched data distributions - remains underexplored. In this work, we extend the DUMB -- Dataset soUrces, Model architecture and Balance - and DUMBer methodology to deepfake detection. We evaluate detectors robustness against adversarial attacks under transferability constraints and cross-dataset configuration to extract real-world insights. Our study spans five state-of-the-art detectors (RECCE, SRM, XCeption, UCF, SPSL), three attacks (PGD, FGSM, FPBA), and two datasets (FaceForensics++ and Celeb-DF-V2). We analyze both attacker and defender perspectives mapping results to mismatch scenarios. Experiments show that adversarial training strategies reinforce robustness in the in-distribution cases but can also degrade it under cross-dataset configuration depending on the strategy adopted. These findings highlight the need for case-aware defense strategies in real-world applications exposed to adversarial attacks.",
      "authors": [
        "Adrian Serrano",
        "Erwan Umlil",
        "Ronan Thomas"
      ],
      "published": "2026-01-09T18:06:19Z",
      "categories": [
        "cs.CV",
        "cs.CR"
      ],
      "primary_category": "cs.CV",
      "abs_url": "https://arxiv.org/abs/2601.05986v1",
      "pdf_url": "https://arxiv.org/pdf/2601.05986v1.pdf"
    },
    {
      "id": "2601.05984v1",
      "title": "Community-Based Model Sharing and Generalisation: Anomaly Detection in IoT Temperature Sensor Networks",
      "abstract": "The rapid deployment of Internet of Things (IoT) devices has led to large-scale sensor networks that monitor environmental and urban phenomena in real time. Communities of Interest (CoIs) provide a promising paradigm for organising heterogeneous IoT sensor networks by grouping devices with similar operational and environmental characteristics. This work presents an anomaly detection framework based on the CoI paradigm by grouping sensors into communities using a fused similarity matrix that incorporates temporal correlations via Spearman coefficients, spatial proximity using Gaussian distance decay, and elevation similarities. For each community, representative stations based on the best silhouette are selected and three autoencoder architectures (BiLSTM, LSTM, and MLP) are trained using Bayesian hyperparameter optimization with expanding window cross-validation and tested on stations from the same cluster and the best representative stations of other clusters. The models are trained on normal temperature patterns of the data and anomalies are detected through reconstruction error analysis. Experimental results show a robust within-community performance across the evaluated configurations, while variations across communities are observed. Overall, the results support the applicability of community-based model sharing in reducing computational overhead and to analyse model generalisability across IoT sensor networks.",
      "authors": [
        "Sahibzada Saadoon Hammad",
        "Joaquín Huerta Guijarro",
        "Francisco Ramos",
        "Michael Gould Carlson",
        "Sergio Trilles Oliver"
      ],
      "published": "2026-01-09T18:05:57Z",
      "categories": [
        "cs.LG"
      ],
      "primary_category": "cs.LG",
      "abs_url": "https://arxiv.org/abs/2601.05984v1",
      "pdf_url": "https://arxiv.org/pdf/2601.05984v1.pdf"
    },
    {
      "id": "2601.05981v1",
      "title": "Adaptive Conditional Contrast-Agnostic Deformable Image Registration with Uncertainty Estimation",
      "abstract": "Deformable multi-contrast image registration is a challenging yet crucial task due to the complex, non-linear intensity relationships across different imaging contrasts. Conventional registration methods typically rely on iterative optimization of the deformation field, which is time-consuming. Although recent learning-based approaches enable fast and accurate registration during inference, their generalizability remains limited to the specific contrasts observed during training. In this work, we propose an adaptive conditional contrast-agnostic deformable image registration framework (AC-CAR) based on a random convolution-based contrast augmentation scheme. AC-CAR can generalize to arbitrary imaging contrasts without observing them during training. To encourage contrast-invariant feature learning, we propose an adaptive conditional feature modulator (ACFM) that adaptively modulates the features and the contrast-invariant latent regularization to enforce the consistency of the learned feature across different imaging contrasts. Additionally, we enable our framework to provide contrast-agnostic registration uncertainty by integrating a variance network that leverages the contrast-agnostic registration encoder to improve the trustworthiness and reliability of AC-CAR. Experimental results demonstrate that AC-CAR outperforms baseline methods in registration accuracy and exhibits superior generalization to unseen imaging contrasts. Code is available at https://github.com/Yinsong0510/AC-CAR.",
      "authors": [
        "Yinsong Wang",
        "Xinzhe Luo",
        "Siyi Du",
        "Chen Qin"
      ],
      "published": "2026-01-09T18:00:49Z",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "abs_url": "https://arxiv.org/abs/2601.05981v1",
      "pdf_url": "https://arxiv.org/pdf/2601.05981v1.pdf"
    },
    {
      "id": "2601.05978v1",
      "title": "AWaRe-SAC: Proactive Slice Admission Control under Weather-Induced Capacity Uncertainty",
      "abstract": "As emerging applications demand higher throughput and lower latencies, operators are increasingly deploying millimeter-wave (mmWave) links within x-haul transport networks, spanning fronthaul, midhaul, and backhaul segments. However, the inherent susceptibility of mmWave frequencies to weather-related attenuation, particularly rain fading, complicates the maintenance of stringent Quality of Service (QoS) requirements. This creates a critical challenge: making admission decisions under uncertainty regarding future network capacity. To address this, we develop a proactive slice admission control framework for mmWave x-haul networks subject to rain-induced fluctuations. Our objective is to improve network performance, ensure QoS, and optimize revenue, thereby surpassing the limitations of standard reactive approaches. The proposed framework integrates a deep learning predictor of future network conditions with a proactive Q-learning-based slice admission control mechanism. We validate our solution using real-world data from a mmWave x-haul deployment in a dense urban area, incorporating realistic models of link capacity attenuation and dynamic slice demands. Extensive evaluations demonstrate that our proactive solution achieves 2-3x higher long-term average revenue under dynamic link conditions, providing a scalable and resilient framework for adaptive admission control.",
      "authors": [
        "Dror Jacoby",
        "Yanzhi Li",
        "Shuyue Yu",
        "Nicola Di Cicco",
        "Hagit Messer",
        "Gil Zussman",
        "Igor Kadota"
      ],
      "published": "2026-01-09T17:53:09Z",
      "categories": [
        "cs.NI",
        "cs.LG"
      ],
      "primary_category": "cs.NI",
      "abs_url": "https://arxiv.org/abs/2601.05978v1",
      "pdf_url": "https://arxiv.org/pdf/2601.05978v1.pdf"
    },
    {
      "id": "2601.05975v1",
      "title": "DeePM: Regime-Robust Deep Learning for Systematic Macro Portfolio Management",
      "abstract": "We propose DeePM (Deep Portfolio Manager), a structured deep-learning macro portfolio manager trained end-to-end to maximize a robust, risk-adjusted utility. DeePM addresses three fundamental challenges in financial learning: (1) it resolves the asynchronous \"ragged filtration\" problem via a Directed Delay (Causal Sieve) mechanism that prioritizes causal impulse-response learning over information freshness; (2) it combats low signal-to-noise ratios via a Macroeconomic Graph Prior, regularizing cross-asset dependence according to economic first principles; and (3) it optimizes a distributionally robust objective where a smooth worst-window penalty serves as a differentiable proxy for Entropic Value-at-Risk (EVaR) - a window-robust utility encouraging strong performance in the most adverse historical subperiods. In large-scale backtests from 2010-2025 on 50 diversified futures with highly realistic transaction costs, DeePM attains net risk-adjusted returns that are roughly twice those of classical trend-following strategies and passive benchmarks, solely using daily closing prices. Furthermore, DeePM improves upon the state-of-the-art Momentum Transformer architecture by roughly fifty percent. The model demonstrates structural resilience across the 2010s \"CTA (Commodity Trading Advisor) Winter\" and the post-2020 volatility regime shift, maintaining consistent performance through the pandemic, inflation shocks, and the subsequent higher-for-longer environment. Ablation studies confirm that strictly lagged cross-sectional attention, graph prior, principled treatment of transaction costs, and robust minimax optimization are the primary drivers of this generalization capability.",
      "authors": [
        "Kieran Wood",
        "Stephen J. Roberts",
        "Stefan Zohren"
      ],
      "published": "2026-01-09T17:47:32Z",
      "categories": [
        "q-fin.TR",
        "cs.LG",
        "stat.ML"
      ],
      "primary_category": "q-fin.TR",
      "abs_url": "https://arxiv.org/abs/2601.05975v1",
      "pdf_url": "https://arxiv.org/pdf/2601.05975v1.pdf"
    },
    {
      "id": "2601.05966v1",
      "title": "VideoAR: Autoregressive Video Generation via Next-Frame & Scale Prediction",
      "abstract": "Recent advances in video generation have been dominated by diffusion and flow-matching models, which produce high-quality results but remain computationally intensive and difficult to scale. In this work, we introduce VideoAR, the first large-scale Visual Autoregressive (VAR) framework for video generation that combines multi-scale next-frame prediction with autoregressive modeling. VideoAR disentangles spatial and temporal dependencies by integrating intra-frame VAR modeling with causal next-frame prediction, supported by a 3D multi-scale tokenizer that efficiently encodes spatio-temporal dynamics. To improve long-term consistency, we propose Multi-scale Temporal RoPE, Cross-Frame Error Correction, and Random Frame Mask, which collectively mitigate error propagation and stabilize temporal coherence. Our multi-stage pretraining pipeline progressively aligns spatial and temporal learning across increasing resolutions and durations. Empirically, VideoAR achieves new state-of-the-art results among autoregressive models, improving FVD on UCF-101 from 99.5 to 88.6 while reducing inference steps by over 10x, and reaching a VBench score of 81.74-competitive with diffusion-based models an order of magnitude larger. These results demonstrate that VideoAR narrows the performance gap between autoregressive and diffusion paradigms, offering a scalable, efficient, and temporally consistent foundation for future video generation research.",
      "authors": [
        "Longbin Ji",
        "Xiaoxiong Liu",
        "Junyuan Shang",
        "Shuohuan Wang",
        "Yu Sun",
        "Hua Wu",
        "Haifeng Wang"
      ],
      "published": "2026-01-09T17:34:59Z",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "abs_url": "https://arxiv.org/abs/2601.05966v1",
      "pdf_url": "https://arxiv.org/pdf/2601.05966v1.pdf"
    },
    {
      "id": "2601.05960v1",
      "title": "Distilling Feedback into Memory-as-a-Tool",
      "abstract": "We propose a framework that amortizes the cost of inference-time reasoning by converting transient critiques into retrievable guidelines, through a file-based memory system and agent-controlled tool calls. We evaluate this method on the Rubric Feedback Bench, a novel dataset for rubric-based learning. Experiments demonstrate that our augmented LLMs rapidly match the performance of test-time refinement pipelines while drastically reducing inference cost.",
      "authors": [
        "Víctor Gallego"
      ],
      "published": "2026-01-09T17:26:52Z",
      "categories": [
        "cs.CL"
      ],
      "primary_category": "cs.CL",
      "abs_url": "https://arxiv.org/abs/2601.05960v1",
      "pdf_url": "https://arxiv.org/pdf/2601.05960v1.pdf"
    },
    {
      "id": "2601.05956v1",
      "title": "On the Robustness of Age for Learning-Based Wireless Scheduling in Unknown Environments",
      "abstract": "The constrained combinatorial multi-armed bandit model has been widely employed to solve problems in wireless networking and related areas, including the problem of wireless scheduling for throughput optimization under unknown channel conditions. Most work in this area uses an algorithm design strategy that combines a bandit learning algorithm with the virtual queue technique to track the throughput constraint violation. These algorithms seek to minimize the virtual queue length in their algorithm design. However, in networks where channel conditions change abruptly, the resulting constraints may become infeasible, leading to unbounded growth in virtual queue lengths. In this paper, we make the key observation that the dynamics of the head-of-line age, i.e. the age of the oldest packet in the virtual queue, make it more robust when used in algorithm design compared to the virtual queue length. We therefore design a learning-based scheduling policy that uses the head-of-line age in place of the virtual queue length. We show that our policy matches state-of-the-art performance under i.i.d. network conditions. Crucially, we also show that the system remains stable even under abrupt changes in channel conditions and can rapidly recover from periods of constraint infeasibility.",
      "authors": [
        "Juaren Steiger",
        "Bin Li"
      ],
      "published": "2026-01-09T17:15:17Z",
      "categories": [
        "cs.LG"
      ],
      "primary_category": "cs.LG",
      "abs_url": "https://arxiv.org/abs/2601.05956v1",
      "pdf_url": "https://arxiv.org/pdf/2601.05956v1.pdf"
    },
    {
      "id": "2601.05946v1",
      "title": "A Critical Examination of Active Learning Workflows in Materials Science",
      "abstract": "Active learning (AL) plays a critical role in materials science, enabling applications such as the construction of machine-learning interatomic potentials for atomistic simulations and the operation of self-driving laboratories. Despite its widespread use, the reliability and effectiveness of AL workflows depend on implicit design assumptions that are rarely examined systematically. Here, we critically assess AL workflows deployed in materials science and investigate how key design choices, such as surrogate models, sampling strategies, uncertainty quantification and evaluation metrics, relate to their performance. By identifying common pitfalls and discussing practical mitigation strategies, we provide guidance to practitioners for the efficient design, assessment, and interpretation of AL workflows in materials science.",
      "authors": [
        "Akhil S. Nair",
        "Lucas Foppa"
      ],
      "published": "2026-01-09T17:01:56Z",
      "categories": [
        "cond-mat.mtrl-sci",
        "cs.LG"
      ],
      "primary_category": "cond-mat.mtrl-sci",
      "abs_url": "https://arxiv.org/abs/2601.05946v1",
      "pdf_url": "https://arxiv.org/pdf/2601.05946v1.pdf"
    },
    {
      "id": "2601.05942v1",
      "title": "WaveRNet: Wavelet-Guided Frequency Learning for Multi-Source Domain-Generalized Retinal Vessel Segmentation",
      "abstract": "Domain-generalized retinal vessel segmentation is critical for automated ophthalmic diagnosis, yet faces significant challenges from domain shift induced by non-uniform illumination and varying contrast, compounded by the difficulty of preserving fine vessel structures. While the Segment Anything Model (SAM) exhibits remarkable zero-shot capabilities, existing SAM-based methods rely on simple adapter fine-tuning while overlooking frequency-domain information that encodes domain-invariant features, resulting in degraded generalization under illumination and contrast variations. Furthermore, SAM's direct upsampling inevitably loses fine vessel details. To address these limitations, we propose WaveRNet, a wavelet-guided frequency learning framework for robust multi-source domain-generalized retinal vessel segmentation. Specifically, we devise a Spectral-guided Domain Modulator (SDM) that integrates wavelet decomposition with learnable domain tokens, enabling the separation of illumination-robust low-frequency structures from high-frequency vessel boundaries while facilitating domain-specific feature generation. Furthermore, we introduce a Frequency-Adaptive Domain Fusion (FADF) module that performs intelligent test-time domain selection through wavelet-based frequency similarity and soft-weighted fusion. Finally, we present a Hierarchical Mask-Prompt Refiner (HMPR) that overcomes SAM's upsampling limitation through coarse-to-fine refinement with long-range dependency modeling. Extensive experiments under the Leave-One-Domain-Out protocol on four public retinal datasets demonstrate that WaveRNet achieves state-of-the-art generalization performance. The source code is available at https://github.com/Chanchan-Wang/WaveRNet.",
      "authors": [
        "Chanchan Wang",
        "Yuanfang Wang",
        "Qing Xu",
        "Guanxin Chen"
      ],
      "published": "2026-01-09T16:58:29Z",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "abs_url": "https://arxiv.org/abs/2601.05942v1",
      "pdf_url": "https://arxiv.org/pdf/2601.05942v1.pdf"
    },
    {
      "id": "2601.05939v1",
      "title": "Context-Aware Decoding for Faithful Vision-Language Generation",
      "abstract": "Hallucinations, generating responses inconsistent with the visual input, remain a critical limitation of large vision-language models (LVLMs), especially in open-ended tasks such as image captioning and visual reasoning. In this work, we probe the layer-wise generation dynamics that drive hallucinations and propose a training-free mitigation strategy. Employing the Logit Lens, we examine how LVLMs construct next-token distributions across decoder layers, uncovering a pronounced commitment-depth gap: truthful tokens accumulate probability mass on their final candidates earlier than hallucinatory ones. Drawing on this discovery, we introduce Context Embedding Injection (CEI), a lightweight method that harnesses the hidden state of the last input token-the context embedding-as a grounding signal to maintain visual fidelity throughout decoding and curb hallucinations. Evaluated on the CHAIR, AMBER, and MMHal-Bench benchmarks (with a maximum token length of 512), CEI outperforms state-of-the-art baselines across three LVLMs, with its dynamic variant yielding the lowest overall hallucination rates. By integrating novel mechanistic insights with a scalable intervention, this work advances the mitigation of hallucinations in LVLMs.",
      "authors": [
        "Mehrdad Fazli",
        "Bowen Wei",
        "Ziwei Zhu"
      ],
      "published": "2026-01-09T16:50:57Z",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "abs_url": "https://arxiv.org/abs/2601.05939v1",
      "pdf_url": "https://arxiv.org/pdf/2601.05939v1.pdf"
    },
    {
      "id": "2601.05937v1",
      "title": "Performance of a Deep Learning-Based Segmentation Model for Pancreatic Tumors on Public Endoscopic Ultrasound Datasets",
      "abstract": "Background: Pancreatic cancer is one of the most aggressive cancers, with poor survival rates. Endoscopic ultrasound (EUS) is a key diagnostic modality, but its effectiveness is constrained by operator subjectivity. This study evaluates a Vision Transformer-based deep learning segmentation model for pancreatic tumors. Methods: A segmentation model using the USFM framework with a Vision Transformer backbone was trained and validated with 17,367 EUS images (from two public datasets) in 5-fold cross-validation. The model was tested on an independent dataset of 350 EUS images from another public dataset, manually segmented by radiologists. Preprocessing included grayscale conversion, cropping, and resizing to 512x512 pixels. Metrics included Dice similarity coefficient (DSC), intersection over union (IoU), sensitivity, specificity, and accuracy. Results: In 5-fold cross-validation, the model achieved a mean DSC of 0.651 +/- 0.738, IoU of 0.579 +/- 0.658, sensitivity of 69.8%, specificity of 98.8%, and accuracy of 97.5%. For the external validation set, the model achieved a DSC of 0.657 (95% CI: 0.634-0.769), IoU of 0.614 (95% CI: 0.590-0.689), sensitivity of 71.8%, and specificity of 97.7%. Results were consistent, but 9.7% of cases exhibited erroneous multiple predictions. Conclusions: The Vision Transformer-based model demonstrated strong performance for pancreatic tumor segmentation in EUS images. However, dataset heterogeneity and limited external validation highlight the need for further refinement, standardization, and prospective studies.",
      "authors": [
        "Pankaj Gupta",
        "Priya Mudgil",
        "Niharika Dutta",
        "Kartik Bose",
        "Nitish Kumar",
        "Anupam Kumar",
        "Jimil Shah",
        "Vaneet Jearth",
        "Jayanta Samanta",
        "Vishal Sharma",
        "Harshal Mandavdhare",
        "Surinder Rana",
        "Saroj K Sinha",
        "Usha Dutta"
      ],
      "published": "2026-01-09T16:48:50Z",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "abs_url": "https://arxiv.org/abs/2601.05937v1",
      "pdf_url": "https://arxiv.org/pdf/2601.05937v1.pdf"
    },
    {
      "id": "2601.05930v1",
      "title": "Can We Predict Before Executing Machine Learning Agents?",
      "abstract": "Autonomous machine learning agents have revolutionized scientific discovery, yet they remain constrained by a Generate-Execute-Feedback paradigm. Previous approaches suffer from a severe Execution Bottleneck, as hypothesis evaluation relies strictly on expensive physical execution. To bypass these physical constraints, we internalize execution priors to substitute costly runtime checks with instantaneous predictive reasoning, drawing inspiration from World Models. In this work, we formalize the task of Data-centric Solution Preference and construct a comprehensive corpus of 18,438 pairwise comparisons. We demonstrate that LLMs exhibit significant predictive capabilities when primed with a Verified Data Analysis Report, achieving 61.5% accuracy and robust confidence calibration. Finally, we instantiate this framework in FOREAGENT, an agent that employs a Predict-then-Verify loop, achieving a 6x acceleration in convergence while surpassing execution-based baselines by +6%. Our code and dataset will be publicly available soon at https://github.com/zjunlp/predict-before-execute.",
      "authors": [
        "Jingsheng Zheng",
        "Jintian Zhang",
        "Yujie Luo",
        "Yuren Mao",
        "Yunjun Gao",
        "Lun Du",
        "Huajun Chen",
        "Ningyu Zhang"
      ],
      "published": "2026-01-09T16:44:17Z",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG",
        "cs.MA"
      ],
      "primary_category": "cs.CL",
      "abs_url": "https://arxiv.org/abs/2601.05930v1",
      "pdf_url": "https://arxiv.org/pdf/2601.05930v1.pdf"
    },
    {
      "id": "2601.05929v1",
      "title": "Prophet as a Repro ducible Forecasting Framework: A Methodological Guide for Business and Financial Analytics",
      "abstract": "Reproducibility remains a persistent challenge in forecasting research and practice, particularly in business and financial analytics where forecasts inform high-stakes decisions. Traditional forecasting methods, while theoretically interpretable, often require extensive manual tuning and are difficult to replicate in proprietary environments. Machine learning approaches offer predictive flexibility but introduce challenges related to interpretability, stochastic training procedures, and cross-environment reproducibility. This paper examines Prophet, an open-source forecasting framework developed by Meta, as a reproducibility-enabling solution that balances interpretability, standardized workflows, and accessibility. Rather than proposing a new algorithm, this study evaluates how Prophet's additive structure, open-source implementation, and standardized workflow contribute to transparent and replicable forecasting practice. Using publicly available financial and retail datasets, we compare Prophet's performance and interpretability with multiple ARIMA specifications (auto-selected, manually specified, and seasonal variants) and Random Forest under a controlled and fully documented experimental design. This multi-model comparison provides a robust assessment of Prophet's relative performance and reproducibility advantages. Through concrete Python examples, we demonstrate how Prophet facilitates efficient forecasting workflows and integration with analytical pipelines. The study positions Prophet within the broader context of reproducible research. It highlights Prophet's role as a methodological building block that supports verification, auditability, and methodological rigor. This work provides researchers and practitioners with a practical reference framework for reproducible forecasting in Python-based research workflows.",
      "authors": [
        "Sidney Shapiro",
        "Burhanuddin Panvelwala"
      ],
      "published": "2026-01-09T16:43:28Z",
      "categories": [
        "cs.LG"
      ],
      "primary_category": "cs.LG",
      "abs_url": "https://arxiv.org/abs/2601.05929v1",
      "pdf_url": "https://arxiv.org/pdf/2601.05929v1.pdf"
    },
    {
      "id": "2601.05927v1",
      "title": "Adapting Vision Transformers to Ultra-High Resolution Semantic Segmentation with Relay Tokens",
      "abstract": "Current approaches for segmenting ultra high resolution images either slide a window, thereby discarding global context, or downsample and lose fine detail. We propose a simple yet effective method that brings explicit multi scale reasoning to vision transformers, simultaneously preserving local details and global awareness. Concretely, we process each image in parallel at a local scale (high resolution, small crops) and a global scale (low resolution, large crops), and aggregate and propagate features between the two branches with a small set of learnable relay tokens. The design plugs directly into standard transformer backbones (eg ViT and Swin) and adds fewer than 2 % parameters. Extensive experiments on three ultra high resolution segmentation benchmarks, Archaeoscape, URUR, and Gleason, and on the conventional Cityscapes dataset show consistent gains, with up to 15 % relative mIoU improvement. Code and pretrained models are available at https://archaeoscape.ai/work/relay-tokens/ .",
      "authors": [
        "Yohann Perron",
        "Vladyslav Sydorov",
        "Christophe Pottier",
        "Loic Landrieu"
      ],
      "published": "2026-01-09T16:41:08Z",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "abs_url": "https://arxiv.org/abs/2601.05927v1",
      "pdf_url": "https://arxiv.org/pdf/2601.05927v1.pdf"
    },
    {
      "id": "2601.05923v1",
      "title": "Cedalion Tutorial: A Python-based framework for comprehensive analysis of multimodal fNIRS & DOT from the lab to the everyday world",
      "abstract": "Functional near-infrared spectroscopy (fNIRS) and diffuse optical tomography (DOT) are rapidly evolving toward wearable, multimodal, and data-driven, AI-supported neuroimaging in the everyday world. However, current analytical tools are fragmented across platforms, limiting reproducibility, interoperability, and integration with modern machine learning (ML) workflows. Cedalion is a Python-based open-source framework designed to unify advanced model-based and data-driven analysis of multimodal fNIRS and DOT data within a reproducible, extensible, and community-driven environment. Cedalion integrates forward modelling, photogrammetric optode co-registration, signal processing, GLM Analysis, DOT image reconstruction, and ML-based data-driven methods within a single standardized architecture based on the Python ecosystem. It adheres to SNIRF and BIDS standards, supports cloud-executable Jupyter notebooks, and provides containerized workflows for scalable, fully reproducible analysis pipelines that can be provided alongside original research publications. Cedalion connects established optical-neuroimaging pipelines with ML frameworks such as scikit-learn and PyTorch, enabling seamless multimodal fusion with EEG, MEG, and physiological data. It implements validated algorithms for signal-quality assessment, motion correction, GLM modelling, and DOT reconstruction, complemented by modules for simulation, data augmentation, and multimodal physiology analysis. Automated documentation links each method to its source publication, and continuous-integration testing ensures robustness. This tutorial paper provides seven fully executable notebooks that demonstrate core features. Cedalion offers an open, transparent, and community extensible foundation that supports reproducible, scalable, cloud- and ML-ready fNIRS/DOT workflows for laboratory-based and real-world neuroimaging.",
      "authors": [
        "E. Middell",
        "L. Carlton",
        "S. Moradi",
        "T. Codina",
        "T. Fischer",
        "J. Cutler",
        "S. Kelley",
        "J. Behrendt",
        "T. Dissanayake",
        "N. Harmening",
        "M. A. Yücel",
        "D. A. Boas",
        "A. von Lühmann"
      ],
      "published": "2026-01-09T16:37:48Z",
      "categories": [
        "eess.SP",
        "cs.AI",
        "cs.LG",
        "eess.IV",
        "q-bio.QM"
      ],
      "primary_category": "eess.SP",
      "abs_url": "https://arxiv.org/abs/2601.05923v1",
      "pdf_url": "https://arxiv.org/pdf/2601.05923v1.pdf"
    },
    {
      "id": "2601.05918v1",
      "title": "Agentic LLMs as Powerful Deanonymizers: Re-identification of Participants in the Anthropic Interviewer Dataset",
      "abstract": "On December 4, 2025, Anthropic released Anthropic Interviewer, an AI tool for running qualitative interviews at scale, along with a public dataset of 1,250 interviews with professionals, including 125 scientists, about their use of AI for research. Focusing on the scientist subset, I show that widely available LLMs with web search and agentic capabilities can link six out of twenty-four interviews to specific scientific works, recovering associated authors and, in some cases, uniquely identifying the interviewees. My contribution is to show that modern LLM-based agents make such re-identification attacks easy and low-effort: off-the-shelf tools can, with a few natural-language prompts, search the web, cross-reference details, and propose likely matches, effectively lowering the technical barrier. Existing safeguards can be bypassed by breaking down the re-identification into benign tasks. I outline the attack at a high level, discuss implications for releasing rich qualitative data in the age of LLM agents, and propose mitigation recommendations and open problems. I have notified Anthropic of my findings.",
      "authors": [
        "Tianshi Li"
      ],
      "published": "2026-01-09T16:32:33Z",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.CY"
      ],
      "primary_category": "cs.CR",
      "abs_url": "https://arxiv.org/abs/2601.05918v1",
      "pdf_url": "https://arxiv.org/pdf/2601.05918v1.pdf"
    },
    {
      "id": "2601.05913v1",
      "title": "Distilling Lightweight Domain Experts from Large ML Models by Identifying Relevant Subspaces",
      "abstract": "Knowledge distillation involves transferring the predictive capabilities of large, high-performing AI models (teachers) to smaller models (students) that can operate in environments with limited computing power. In this paper, we address the scenario in which only a few classes and their associated intermediate concepts are relevant to distill. This scenario is common in practice, yet few existing distillation methods explicitly focus on the relevant subtask. To address this gap, we introduce 'SubDistill', a new distillation algorithm with improved numerical properties that only distills the relevant components of the teacher model at each layer. Experiments on CIFAR-100 and ImageNet with Convolutional and Transformer models demonstrate that SubDistill outperforms existing layer-wise distillation techniques on a representative set of subtasks. Our benchmark evaluations are complemented by Explainable AI analyses showing that our distilled student models more closely match the decision structure of the original teacher model.",
      "authors": [
        "Pattarawat Chormai",
        "Ali Hashemi",
        "Klaus-Robert Müller",
        "Grégoire Montavon"
      ],
      "published": "2026-01-09T16:28:55Z",
      "categories": [
        "cs.LG"
      ],
      "primary_category": "cs.LG",
      "abs_url": "https://arxiv.org/abs/2601.05913v1",
      "pdf_url": "https://arxiv.org/pdf/2601.05913v1.pdf"
    },
    {
      "id": "2601.05911v1",
      "title": "Pantagruel: Unified Self-Supervised Encoders for French Text and Speech",
      "abstract": "We release Pantagruel models, a new family of self-supervised encoder models for French text and speech. Instead of predicting modality-tailored targets such as textual tokens or speech units, Pantagruel learns contextualized target representations in the feature space, allowing modality-specific encoders to capture linguistic and acoustic regularities more effectively. Separate models are pre-trained on large-scale French corpora, including Wikipedia, OSCAR and CroissantLLM for text, together with MultilingualLibriSpeech, LeBenchmark, and INA-100k for speech. INA-100k is a newly introduced 100,000-hour corpus of French audio derived from the archives of the Institut National de l'Audiovisuel (INA), the national repository of French radio and television broadcasts, providing highly diverse audio data. We evaluate Pantagruel across a broad range of downstream tasks spanning both modalities, including those from the standard French benchmarks such as FLUE or LeBenchmark. Across these tasks, Pantagruel models show competitive or superior performance compared to strong French baselines such as CamemBERT, FlauBERT, and LeBenchmark2.0, while maintaining a shared architecture that can seamlessly handle either speech or text inputs. These results confirm the effectiveness of feature-space self-supervised objectives for French representation learning and highlight Pantagruel as a robust foundation for multimodal speech-text understanding.",
      "authors": [
        "Phuong-Hang Le",
        "Valentin Pelloin",
        "Arnault Chatelain",
        "Maryem Bouziane",
        "Mohammed Ghennai",
        "Qianwen Guan",
        "Kirill Milintsevich",
        "Salima Mdhaffar",
        "Aidan Mannion",
        "Nils Defauw",
        "Shuyue Gu",
        "Alexandre Audibert",
        "Marco Dinarelli",
        "Yannick Estève",
        "Lorraine Goeuriot",
        "Steffen Lalande",
        "Nicolas Hervé",
        "Maximin Coavoux",
        "François Portet",
        "Étienne Ollion",
        "Marie Candito",
        "Maxime Peyrard",
        "Solange Rossato",
        "Benjamin Lecouteux",
        "Aurélie Nardy",
        "Gilles Sérasset",
        "Vincent Segonne",
        "Solène Evain",
        "Diandra Fabre",
        "Didier Schwab"
      ],
      "published": "2026-01-09T16:28:25Z",
      "categories": [
        "cs.CL"
      ],
      "primary_category": "cs.CL",
      "abs_url": "https://arxiv.org/abs/2601.05911v1",
      "pdf_url": "https://arxiv.org/pdf/2601.05911v1.pdf"
    },
    {
      "id": "2601.05910v1",
      "title": "Multi-task Modeling for Engineering Applications with Sparse Data",
      "abstract": "Modern engineering and scientific workflows often require simultaneous predictions across related tasks and fidelity levels, where high-fidelity data is scarce and expensive, while low-fidelity data is more abundant. This paper introduces an Multi-Task Gaussian Processes (MTGP) framework tailored for engineering systems characterized by multi-source, multi-fidelity data, addressing challenges of data sparsity and varying task correlations. The proposed framework leverages inter-task relationships across outputs and fidelity levels to improve predictive performance and reduce computational costs. The framework is validated across three representative scenarios: Forrester function benchmark, 3D ellipsoidal void modeling, and friction-stir welding. By quantifying and leveraging inter-task relationships, the proposed MTGP framework offers a robust and scalable solution for predictive modeling in domains with significant computational and experimental costs, supporting informed decision-making and efficient resource utilization.",
      "authors": [
        "Yigitcan Comlek",
        "R. Murali Krishnan",
        "Sandipp Krishnan Ravi",
        "Amin Moghaddas",
        "Rafael Giorjao",
        "Michael Eff",
        "Anirban Samaddar",
        "Nesar S. Ramachandra",
        "Sandeep Madireddy",
        "Liping Wang"
      ],
      "published": "2026-01-09T16:28:19Z",
      "categories": [
        "stat.ML",
        "cs.LG",
        "stat.AP"
      ],
      "primary_category": "stat.ML",
      "abs_url": "https://arxiv.org/abs/2601.05910v1",
      "pdf_url": "https://arxiv.org/pdf/2601.05910v1.pdf"
    },
    {
      "id": "2601.05909v1",
      "title": "Auditing Fairness under Model Updates: Fundamental Complexity and Property-Preserving Updates",
      "abstract": "As machine learning models become increasingly embedded in societal infrastructure, auditing them for bias is of growing importance. However, in real-world deployments, auditing is complicated by the fact that model owners may adaptively update their models in response to changing environments, such as financial markets. These updates can alter the underlying model class while preserving certain properties of interest, raising fundamental questions about what can be reliably audited under such shifts.\n  In this work, we study group fairness auditing under arbitrary updates. We consider general shifts that modify the pre-audit model class while maintaining invariance of the audited property. Our goals are two-fold: (i) to characterize the information complexity of allowable updates, by identifying which strategic changes preserve the property under audit; and (ii) to efficiently estimate auditing properties, such as group fairness, using a minimal number of labeled samples.\n  We propose a generic framework for PAC auditing based on an Empirical Property Optimization (EPO) oracle. For statistical parity, we establish distribution-free auditing bounds characterized by the SP dimension, a novel combinatorial measure that captures the complexity of admissible strategic updates. Finally, we demonstrate that our framework naturally extends to other auditing objectives, including prediction error and robust risk.",
      "authors": [
        "Ayoub Ajarra",
        "Debabrota Basu"
      ],
      "published": "2026-01-09T16:28:11Z",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CY",
        "stat.ML"
      ],
      "primary_category": "cs.LG",
      "abs_url": "https://arxiv.org/abs/2601.05909v1",
      "pdf_url": "https://arxiv.org/pdf/2601.05909v1.pdf"
    },
    {
      "id": "2601.05905v1",
      "title": "Illusions of Confidence? Diagnosing LLM Truthfulness via Neighborhood Consistency",
      "abstract": "As Large Language Models (LLMs) are increasingly deployed in real-world settings, correctness alone is insufficient. Reliable deployment requires maintaining truthful beliefs under contextual perturbations. Existing evaluations largely rely on point-wise confidence like Self-Consistency, which can mask brittle belief. We show that even facts answered with perfect self-consistency can rapidly collapse under mild contextual interference. To address this gap, we propose Neighbor-Consistency Belief (NCB), a structural measure of belief robustness that evaluates response coherence across a conceptual neighborhood. To validate the efficiency of NCB, we introduce a new cognitive stress-testing protocol that probes outputs stability under contextual interference. Experiments across multiple LLMs show that the performance of high-NCB data is relatively more resistant to interference. Finally, we present Structure-Aware Training (SAT), which optimizes context-invariant belief structure and reduces long-tail knowledge brittleness by approximately 30%. Code will be available at https://github.com/zjunlp/belief.",
      "authors": [
        "Haoming Xu",
        "Ningyuan Zhao",
        "Yunzhi Yao",
        "Weihong Xu",
        "Hongru Wang",
        "Xinle Deng",
        "Shumin Deng",
        "Jeff Z. Pan",
        "Huajun Chen",
        "Ningyu Zhang"
      ],
      "published": "2026-01-09T16:23:21Z",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.HC",
        "cs.LG",
        "cs.MA"
      ],
      "primary_category": "cs.CL",
      "abs_url": "https://arxiv.org/abs/2601.05905v1",
      "pdf_url": "https://arxiv.org/pdf/2601.05905v1.pdf"
    },
    {
      "id": "2601.05904v1",
      "title": "Can AI mediation improve democratic deliberation?",
      "abstract": "The strength of democracy lies in the free and equal exchange of diverse viewpoints. Living up to this ideal at scale faces inherent tensions: broad participation, meaningful deliberation, and political equality often trade off with one another (Fishkin, 2011). We ask whether and how artificial intelligence (AI) could help navigate this \"trilemma\" by engaging with a recent example of a large language model (LLM)-based system designed to help people with diverse viewpoints find common ground (Tessler, Bakker, et al., 2024). Here, we explore the implications of the introduction of LLMs into deliberation augmentation tools, examining their potential to enhance participation through scalability, improve political equality via fair mediation, and foster meaningful deliberation by, for example, surfacing trustworthy information. We also point to key challenges that remain. Ultimately, a range of empirical, technical, and theoretical advancements are needed to fully realize the promise of AI-mediated deliberation for enhancing citizen engagement and strengthening democratic deliberation.",
      "authors": [
        "Michael Henry Tessler",
        "Georgina Evans",
        "Michiel A. Bakker",
        "Iason Gabriel",
        "Sophie Bridgers",
        "Rishub Jain",
        "Raphael Koster",
        "Verena Rieser",
        "Anca Dragan",
        "Matthew Botvinick",
        "Christopher Summerfield"
      ],
      "published": "2026-01-09T16:22:26Z",
      "categories": [
        "cs.CY",
        "cs.AI"
      ],
      "primary_category": "cs.CY",
      "abs_url": "https://arxiv.org/abs/2601.05904v1",
      "pdf_url": "https://arxiv.org/pdf/2601.05904v1.pdf"
    },
    {
      "id": "2601.05903v1",
      "title": "HAPS: Hierarchical LLM Routing with Joint Architecture and Parameter Search",
      "abstract": "Large language model (LLM) routing aims to exploit the specialized strengths of different LLMs for diverse tasks. However, existing approaches typically focus on selecting LLM architectures while overlooking parameter settings, which are critical for task performance. In this paper, we introduce HAPS, a hierarchical LLM routing framework that jointly searches over model architectures and parameters. Specifically, we use a high-level router to select among candidate LLM architectures, and then search for the optimal parameters for the selected architectures based on a low-level router. We design a parameter generation network to share parameters between the two routers to mutually enhance their capabilities. In the training process, we design a reward-augmented objective to effectively optimize our framework. Experiments on two commonly used benchmarks show that HAPS consistently outperforms strong routing baselines. We have released our code at https://github.com/zihangtian/HAPS.",
      "authors": [
        "Zihang Tian",
        "Rui Li",
        "Jingsen Zhang",
        "Xiaohe Bo",
        "Wei Huo",
        "Xu Chen"
      ],
      "published": "2026-01-09T16:22:25Z",
      "categories": [
        "cs.CL"
      ],
      "primary_category": "cs.CL",
      "abs_url": "https://arxiv.org/abs/2601.05903v1",
      "pdf_url": "https://arxiv.org/pdf/2601.05903v1.pdf"
    },
    {
      "id": "2601.05899v1",
      "title": "TowerMind: A Tower Defence Game Learning Environment and Benchmark for LLM as Agents",
      "abstract": "Recent breakthroughs in Large Language Models (LLMs) have positioned them as a promising paradigm for agents, with long-term planning and decision-making emerging as core general-purpose capabilities for adapting to diverse scenarios and tasks. Real-time strategy (RTS) games serve as an ideal testbed for evaluating these two capabilities, as their inherent gameplay requires both macro-level strategic planning and micro-level tactical adaptation and action execution. Existing RTS game-based environments either suffer from relatively high computational demands or lack support for textual observations, which has constrained the use of RTS games for LLM evaluation. Motivated by this, we present TowerMind, a novel environment grounded in the tower defense (TD) subgenre of RTS games. TowerMind preserves the key evaluation strengths of RTS games for assessing LLMs, while featuring low computational demands and a multimodal observation space, including pixel-based, textual, and structured game-state representations. In addition, TowerMind supports the evaluation of model hallucination and provides a high degree of customizability. We design five benchmark levels to evaluate several widely used LLMs under different multimodal input settings. The results reveal a clear performance gap between LLMs and human experts across both capability and hallucination dimensions. The experiments further highlight key limitations in LLM behavior, such as inadequate planning validation, a lack of multifinality in decision-making, and inefficient action use. We also evaluate two classic reinforcement learning algorithms: Ape-X DQN and PPO. By offering a lightweight and multimodal design, TowerMind complements the existing RTS game-based environment landscape and introduces a new benchmark for the AI agent field. The source code is publicly available on GitHub(https://github.com/tb6147877/TowerMind).",
      "authors": [
        "Dawei Wang",
        "Chengming Zhou",
        "Di Zhao",
        "Xinyuan Liu",
        "Marci Chi Ma",
        "Gary Ushaw",
        "Richard Davison"
      ],
      "published": "2026-01-09T16:18:08Z",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "abs_url": "https://arxiv.org/abs/2601.05899v1",
      "pdf_url": "https://arxiv.org/pdf/2601.05899v1.pdf"
    },
    {
      "id": "2601.05890v1",
      "title": "StackPlanner: A Centralized Hierarchical Multi-Agent System with Task-Experience Memory Management",
      "abstract": "Multi-agent systems based on large language models, particularly centralized architectures, have recently shown strong potential for complex and knowledge-intensive tasks. However, central agents often suffer from unstable long-horizon collaboration due to the lack of memory management, leading to context bloat, error accumulation, and poor cross-task generalization. To address both task-level memory inefficiency and the inability to reuse coordination experience, we propose StackPlanner, a hierarchical multi-agent framework with explicit memory control. StackPlanner addresses these challenges by decoupling high-level coordination from subtask execution with active task-level memory control, and by learning to retrieve and exploit reusable coordination experience via structured experience memory and reinforcement learning. Experiments on multiple deep-search and agent system benchmarks demonstrate the effectiveness of our approach in enabling reliable long-horizon multi-agent collaboration.",
      "authors": [
        "Ruizhe Zhang",
        "Xinke Jiang",
        "Zhibang Yang",
        "Zhixin Zhang",
        "Jiaran Gao",
        "Yuzhen Xiao",
        "Hongbin Lai",
        "Xu Chu",
        "Junfeng Zhao",
        "Yasha Wang"
      ],
      "published": "2026-01-09T16:09:48Z",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "abs_url": "https://arxiv.org/abs/2601.05890v1",
      "pdf_url": "https://arxiv.org/pdf/2601.05890v1.pdf"
    },
    {
      "id": "2601.05889v1",
      "title": "GlueNN: gluing patchwise analytic solutions with neural networks",
      "abstract": "In many problems in physics and engineering, one encounters complicated differential equations with strongly scale-dependent terms for which exact analytical or numerical solutions are not available. A common strategy is to divide the domain into several regions (patches) and simplify the equation in each region. When approximate analytic solutions can be obtained in each patch, they are then matched at the interfaces to construct a global solution. However, this patching procedure can fail to reproduce the correct solution, since the approximate forms may break down near the matching boundaries. In this work, we propose a learning framework in which the integration constants of asymptotic analytic solutions are promoted to scale-dependent functions. By constraining these coefficient functions with the original differential equation over the domain, the network learns a globally valid solution that smoothly interpolates between asymptotic regimes, eliminating the need for arbitrary boundary matching. We demonstrate the effectiveness of this framework in representative problems from chemical kinetics and cosmology, where it accurately reproduces global solutions and outperforms conventional matching procedures.",
      "authors": [
        "Doyoung Kim",
        "Donghee Lee",
        "Hye-Sung Lee",
        "Jiheon Lee",
        "Jaeok Yi"
      ],
      "published": "2026-01-09T16:07:43Z",
      "categories": [
        "cs.LG",
        "astro-ph.CO",
        "physics.comp-ph"
      ],
      "primary_category": "cs.LG",
      "abs_url": "https://arxiv.org/abs/2601.05889v1",
      "pdf_url": "https://arxiv.org/pdf/2601.05889v1.pdf"
    },
    {
      "id": "2601.05882v1",
      "title": "An Empirical Study on Preference Tuning Generalization and Diversity Under Domain Shift",
      "abstract": "Preference tuning aligns pretrained language models to human judgments of quality, helpfulness, or safety by optimizing over explicit preference signals rather than likelihood alone. Prior work has shown that preference-tuning degrades performance and reduces helpfulness when evaluated outside the training domain. However, the extent to which adaptation strategies mitigate this domain shift remains unexplored. We address this challenge by conducting a comprehensive and systematic study of alignment generalization under domain shift. We compare five popular alignment objectives and various adaptation strategies from source to target, including target-domain supervised fine-tuning and pseudo-labeling, across summarization and question-answering helpfulness tasks. Our findings reveal systematic differences in generalization across alignment objectives under domain shift. We show that adaptation strategies based on pseudo-labeling can substantially reduce domain-shift degradation",
      "authors": [
        "Constantinos Karouzos",
        "Xingwei Tan",
        "Nikolaos Aletras"
      ],
      "published": "2026-01-09T15:56:55Z",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "abs_url": "https://arxiv.org/abs/2601.05882v1",
      "pdf_url": "https://arxiv.org/pdf/2601.05882v1.pdf"
    },
    {
      "id": "2601.05879v1",
      "title": "Gender Bias in LLMs: Preliminary Evidence from Shared Parenting Scenario in Czech Family Law",
      "abstract": "Access to justice remains limited for many people, leading laypersons to increasingly rely on Large Language Models (LLMs) for legal self-help. Laypeople use these tools intuitively, which may lead them to form expectations based on incomplete, incorrect, or biased outputs. This study examines whether leading LLMs exhibit gender bias in their responses to a realistic family law scenario. We present an expert-designed divorce scenario grounded in Czech family law and evaluate four state-of-the-art LLMs GPT-5 nano, Claude Haiku 4.5, Gemini 2.5 Flash, and Llama 3.3 in a fully zero-shot interaction. We deploy two versions of the scenario, one with gendered names and one with neutral labels, to establish a baseline for comparison. We further introduce nine legally relevant factors that vary the factual circumstances of the case and test whether these variations influence the models' proposed shared-parenting ratios. Our preliminary results highlight differences across models and suggest gender-dependent patterns in the outcomes generated by some systems. The findings underscore both the risks associated with laypeople's reliance on LLMs for legal guidance and the need for more robust evaluation of model behavior in sensitive legal contexts. We present exploratory and descriptive evidence intended to identify systematic asymmetries rather than to establish causal effects.",
      "authors": [
        "Jakub Harasta",
        "Matej Vasina",
        "Martin Kornel",
        "Tomas Foltynek"
      ],
      "published": "2026-01-09T15:55:03Z",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CY"
      ],
      "primary_category": "cs.CL",
      "abs_url": "https://arxiv.org/abs/2601.05879v1",
      "pdf_url": "https://arxiv.org/pdf/2601.05879v1.pdf"
    },
    {
      "id": "2601.05877v1",
      "title": "iReasoner: Trajectory-Aware Intrinsic Reasoning Supervision for Self-Evolving Large Multimodal Models",
      "abstract": "Recent work shows that large multimodal models (LMMs) can self-improve from unlabeled data via self-play and intrinsic feedback. Yet existing self-evolving frameworks mainly reward final outcomes, leaving intermediate reasoning weakly constrained despite its importance for visually grounded decision making. We propose iReasoner, a self-evolving framework that improves an LMM's implicit reasoning by explicitly eliciting chain-of-thought (CoT) and rewarding its internal agreement. In a Proposer--Solver loop over unlabeled images, iReasoner augments outcome-level intrinsic rewards with a trajectory-aware signal defined over intermediate reasoning steps, providing learning signals that distinguish reasoning paths leading to the same answer without ground-truth labels or external judges. Starting from Qwen2.5-VL-7B, iReasoner yields up to $+2.1$ points across diverse multimodal reasoning benchmarks under fully unsupervised post-training. We hope this work serves as a starting point for reasoning-aware self-improvement in LMMs in purely unsupervised settings.",
      "authors": [
        "Meghana Sunil",
        "Manikandarajan Venmathimaran",
        "Muthu Subash Kavitha"
      ],
      "published": "2026-01-09T15:53:42Z",
      "categories": [
        "cs.CL"
      ],
      "primary_category": "cs.CL",
      "abs_url": "https://arxiv.org/abs/2601.05877v1",
      "pdf_url": "https://arxiv.org/pdf/2601.05877v1.pdf"
    },
    {
      "id": "2601.05874v1",
      "title": "Continual-learning for Modelling Low-Resource Languages from Large Language Models",
      "abstract": "Modelling a language model for a multi-lingual scenario includes several potential challenges, among which catastrophic forgetting is the major challenge. For example, small language models (SLM) built for low-resource languages by adapting large language models (LLMs) pose the challenge of catastrophic forgetting. This work proposes to employ a continual learning strategy using parts-of-speech (POS)-based code-switching along with a replay adapter strategy to mitigate the identified gap of catastrophic forgetting while training SLM from LLM. Experiments conducted on vision language tasks such as visual question answering and language modelling task exhibits the success of the proposed architecture.",
      "authors": [
        "Santosh Srinath K",
        "Mudit Somani",
        "Varun Reddy Padala",
        "Prajna Devi Upadhyay",
        "Abhijit Das"
      ],
      "published": "2026-01-09T15:51:12Z",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "abs_url": "https://arxiv.org/abs/2601.05874v1",
      "pdf_url": "https://arxiv.org/pdf/2601.05874v1.pdf"
    },
    {
      "id": "2601.05870v1",
      "title": "IIB-LPO: Latent Policy Optimization via Iterative Information Bottleneck",
      "abstract": "Recent advances in Reinforcement Learning with Verifiable Rewards (RLVR) for Large Language Model (LLM) reasoning have been hindered by a persistent challenge: exploration collapse. The semantic homogeneity of random rollouts often traps models in narrow, over-optimized behaviors. While existing methods leverage policy entropy to encourage exploration, they face inherent limitations. Global entropy regularization is susceptible to reward hacking, which can induce meaningless verbosity, whereas local token-selective updates struggle with the strong inductive bias of pre-trained models. To address this, we propose Latent Policy Optimization via Iterative Information Bottleneck (IIB-LPO), a novel approach that shifts exploration from statistical perturbation of token distributions to topological branching of reasoning trajectories. IIB-LPO triggers latent branching at high-entropy states to diversify reasoning paths and employs the Information Bottleneck principle both as a trajectory filter and a self-reward mechanism, ensuring concise and informative exploration. Empirical results across four mathematical reasoning benchmarks demonstrate that IIB-LPO achieves state-of-the-art performance, surpassing prior methods by margins of up to 5.3% in accuracy and 7.4% in diversity metrics.",
      "authors": [
        "Huilin Deng",
        "Hongchen Luo",
        "Yue Zhu",
        "Long Li",
        "Zhuoyue Chen",
        "Xinghao Zhao",
        "Ming Li",
        "Jihai Zhang",
        "Mengchang Wang",
        "Yang Cao",
        "Yu Kang"
      ],
      "published": "2026-01-09T15:46:40Z",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "abs_url": "https://arxiv.org/abs/2601.05870v1",
      "pdf_url": "https://arxiv.org/pdf/2601.05870v1.pdf"
    },
    {
      "id": "2601.05868v1",
      "title": "Sequential Bayesian Optimal Experimental Design in Infinite Dimensions via Policy Gradient Reinforcement Learning",
      "abstract": "Sequential Bayesian optimal experimental design (SBOED) for PDE-governed inverse problems is computationally challenging, especially for infinite-dimensional random field parameters. High-fidelity approaches require repeated forward and adjoint PDE solves inside nested Bayesian inversion and design loops. We formulate SBOED as a finite-horizon Markov decision process and learn an amortized design policy via policy-gradient reinforcement learning (PGRL), enabling online design selection from the experiment history without repeatedly solving an SBOED optimization problem. To make policy training and reward evaluation scalable, we combine dual dimension reduction -- active subspace projection for the parameter and principal component analysis for the state -- with an adjusted derivative-informed latent attention neural operator (LANO) surrogate that predicts both the parameter-to-solution map and its Jacobian. We use a Laplace-based D-optimality reward while noting that, in general, other expected-information-gain utilities such as KL divergence can also be used within the same framework. We further introduce an eigenvalue-based evaluation strategy that uses prior samples as proxies for maximum a posteriori (MAP) points, avoiding repeated MAP solves while retaining accurate information-gain estimates. Numerical experiments on sequential multi-sensor placement for contaminant source tracking demonstrate approximately $100\\times$ speedup over high-fidelity finite element methods, improved performance over random sensor placements, and physically interpretable policies that discover an ``upstream'' tracking strategy.",
      "authors": [
        "Kaichen Shen",
        "Peng Chen"
      ],
      "published": "2026-01-09T15:44:49Z",
      "categories": [
        "math.OC",
        "cs.LG"
      ],
      "primary_category": "math.OC",
      "abs_url": "https://arxiv.org/abs/2601.05868v1",
      "pdf_url": "https://arxiv.org/pdf/2601.05868v1.pdf"
    },
    {
      "id": "2601.05866v1",
      "title": "FACTUM: Mechanistic Detection of Citation Hallucination in Long-Form RAG",
      "abstract": "Retrieval-Augmented Generation (RAG) models are critically undermined by citation hallucinations, a deceptive failure where a model confidently cites a source that fails to support its claim. Existing work often attributes hallucination to a simple over-reliance on the model's parametric knowledge. We challenge this view and introduce FACTUM (Framework for Attesting Citation Trustworthiness via Underlying Mechanisms), a framework of four mechanistic scores measuring the distinct contributions of a model's attention and FFN pathways, and the alignment between them. Our analysis reveals two consistent signatures of correct citation: a significantly stronger contribution from the model's parametric knowledge and greater use of the attention sink for information synthesis. Crucially, we find the signature of a correct citation is not static but evolves with model scale. For example, the signature of a correct citation for the Llama-3.2-3B model is marked by higher pathway alignment, whereas for the Llama-3.1-8B model, it is characterized by lower alignment, where pathways contribute more distinct, orthogonal information. By capturing this complex, evolving signature, FACTUM outperforms state-of-the-art baselines by up to 37.5% in AUC. Our findings reframe citation hallucination as a complex, scale-dependent interplay between internal mechanisms, paving the way for more nuanced and reliable RAG systems.",
      "authors": [
        "Maxime Dassen",
        "Rebecca Kotula",
        "Kenton Murray",
        "Andrew Yates",
        "Dawn Lawrie",
        "Efsun Kayi",
        "James Mayfield",
        "Kevin Duh"
      ],
      "published": "2026-01-09T15:41:08Z",
      "categories": [
        "cs.CL"
      ],
      "primary_category": "cs.CL",
      "abs_url": "https://arxiv.org/abs/2601.05866v1",
      "pdf_url": "https://arxiv.org/pdf/2601.05866v1.pdf"
    },
    {
      "id": "2601.05864v1",
      "title": "What do the metrics mean? A critical analysis of the use of Automated Evaluation Metrics in Interpreting",
      "abstract": "With the growth of interpreting technologies, from remote interpreting and Computer-Aided Interpreting to automated speech translation and interpreting avatars, there is now a high demand for ways to quickly and efficiently measure the quality of any interpreting delivered. A range of approaches to fulfil the need for quick and efficient quality measurement have been proposed, each involving some measure of automation. This article examines these recently-proposed quality measurement methods and will discuss their suitability for measuring the quality of authentic interpreting practice, whether delivered by humans or machines, concluding that automatic metrics as currently proposed cannot take into account the communicative context and thus are not viable measures of the quality of any interpreting provision when used on their own. Across all attempts to measure or even categorise quality in Interpreting Studies, the contexts in which interpreting takes place have become fundamental to the final analysis.",
      "authors": [
        "Jonathan Downie",
        "Joss Moorkens"
      ],
      "published": "2026-01-09T15:39:28Z",
      "categories": [
        "cs.CL"
      ],
      "primary_category": "cs.CL",
      "abs_url": "https://arxiv.org/abs/2601.05864v1",
      "pdf_url": "https://arxiv.org/pdf/2601.05864v1.pdf"
    },
    {
      "id": "2601.05861v1",
      "title": "Phase4DFD: Multi-Domain Phase-Aware Attention for Deepfake Detection",
      "abstract": "Recent deepfake detection methods have increasingly explored frequency domain representations to reveal manipulation artifacts that are difficult to detect in the spatial domain. However, most existing approaches rely primarily on spectral magnitude, implicitly under exploring the role of phase information. In this work, we propose Phase4DFD, a phase aware frequency domain deepfake detection framework that explicitly models phase magnitude interactions via a learnable attention mechanism. Our approach augments standard RGB input with Fast Fourier Transform (FFT) magnitude and local binary pattern (LBP) representations to expose subtle synthesis artifacts that remain indistinguishable under spatial analysis alone. Crucially, we introduce an input level phase aware attention module that uses phase discontinuities commonly introduced by synthetic generation to guide the model toward frequency patterns that are most indicative of manipulation before backbone feature extraction. The attended multi domain representation is processed by an efficient BNext M backbone, with optional channel spatial attention applied for semantic feature refinement. Extensive experiments on the CIFAKE and DFFD datasets demonstrate that our proposed model Phase4DFD outperforms state of the art spatial and frequency-based detectors while maintaining low computational overhead. Comprehensive ablation studies further confirm that explicit phase modeling provides complementary and non-redundant information beyond magnitude-only frequency representations.",
      "authors": [
        "Zhen-Xin Lin",
        "Shang-Kuan Chen"
      ],
      "published": "2026-01-09T15:37:03Z",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "abs_url": "https://arxiv.org/abs/2601.05861v1",
      "pdf_url": "https://arxiv.org/pdf/2601.05861v1.pdf"
    },
    {
      "id": "2601.05858v1",
      "title": "CLewR: Curriculum Learning with Restarts for Machine Translation Preference Learning",
      "abstract": "Large language models (LLMs) have demonstrated competitive performance in zero-shot multilingual machine translation (MT). Some follow-up works further improved MT performance via preference optimization, but they leave a key aspect largely underexplored: the order in which data samples are given during training. We address this topic by integrating curriculum learning into various state-of-the-art preference optimization algorithms to boost MT performance. We introduce a novel curriculum learning strategy with restarts (CLewR), which reiterates easy-to-hard curriculum multiple times during training to effectively mitigate the catastrophic forgetting of easy examples. We demonstrate consistent gains across several model families (Gemma2, Qwen2.5, Llama3.1) and preference optimization techniques. We publicly release our code at https://github.com/alexandra-dragomir/CLewR.",
      "authors": [
        "Alexandra Dragomir",
        "Florin Brad",
        "Radu Tudor Ionescu"
      ],
      "published": "2026-01-09T15:34:31Z",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "abs_url": "https://arxiv.org/abs/2601.05858v1",
      "pdf_url": "https://arxiv.org/pdf/2601.05858v1.pdf"
    },
    {
      "id": "2601.05855v1",
      "title": "Bidirectional Channel-selective Semantic Interaction for Semi-Supervised Medical Segmentation",
      "abstract": "Semi-supervised medical image segmentation is an effective method for addressing scenarios with limited labeled data. Existing methods mainly rely on frameworks such as mean teacher and dual-stream consistency learning. These approaches often face issues like error accumulation and model structural complexity, while also neglecting the interaction between labeled and unlabeled data streams. To overcome these challenges, we propose a Bidirectional Channel-selective Semantic Interaction~(BCSI) framework for semi-supervised medical image segmentation. First, we propose a Semantic-Spatial Perturbation~(SSP) mechanism, which disturbs the data using two strong augmentation operations and leverages unsupervised learning with pseudo-labels from weak augmentations. Additionally, we employ consistency on the predictions from the two strong augmentations to further improve model stability and robustness. Second, to reduce noise during the interaction between labeled and unlabeled data, we propose a Channel-selective Router~(CR) component, which dynamically selects the most relevant channels for information exchange. This mechanism ensures that only highly relevant features are activated, minimizing unnecessary interference. Finally, the Bidirectional Channel-wise Interaction~(BCI) strategy is employed to supplement additional semantic information and enhance the representation of important channels. Experimental results on multiple benchmarking 3D medical datasets demonstrate that the proposed method outperforms existing semi-supervised approaches.",
      "authors": [
        "Kaiwen Huang",
        "Yizhe Zhang",
        "Yi Zhou",
        "Tianyang Xu",
        "Tao Zhou"
      ],
      "published": "2026-01-09T15:32:57Z",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "abs_url": "https://arxiv.org/abs/2601.05855v1",
      "pdf_url": "https://arxiv.org/pdf/2601.05855v1.pdf"
    },
    {
      "id": "2601.05853v1",
      "title": "LayerGS: Decomposition and Inpainting of Layered 3D Human Avatars via 2D Gaussian Splatting",
      "abstract": "We propose a novel framework for decomposing arbitrarily posed humans into animatable multi-layered 3D human avatars, separating the body and garments. Conventional single-layer reconstruction methods lock clothing to one identity, while prior multi-layer approaches struggle with occluded regions. We overcome both limitations by encoding each layer as a set of 2D Gaussians for accurate geometry and photorealistic rendering, and inpainting hidden regions with a pretrained 2D diffusion model via score-distillation sampling (SDS). Our three-stage training strategy first reconstructs the coarse canonical garment via single-layer reconstruction, followed by multi-layer training to jointly recover the inner-layer body and outer-layer garment details. Experiments on two 3D human benchmark datasets (4D-Dress, Thuman2.0) show that our approach achieves better rendering quality and layer decomposition and recomposition than the previous state-of-the-art, enabling realistic virtual try-on under novel viewpoints and poses, and advancing practical creation of high-fidelity 3D human assets for immersive applications. Our code is available at https://github.com/RockyXu66/LayerGS",
      "authors": [
        "Yinghan Xu",
        "John Dingliana"
      ],
      "published": "2026-01-09T15:30:12Z",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.GR"
      ],
      "primary_category": "cs.CV",
      "abs_url": "https://arxiv.org/abs/2601.05853v1",
      "pdf_url": "https://arxiv.org/pdf/2601.05853v1.pdf"
    },
    {
      "id": "2601.05852v1",
      "title": "Kidney Cancer Detection Using 3D-Based Latent Diffusion Models",
      "abstract": "In this work, we present a novel latent diffusion-based pipeline for 3D kidney anomaly detection on contrast-enhanced abdominal CT. The method combines Denoising Diffusion Probabilistic Models (DDPMs), Denoising Diffusion Implicit Models (DDIMs), and Vector-Quantized Generative Adversarial Networks (VQ-GANs). Unlike prior slice-wise approaches, our method operates directly on an image volume and leverages weak supervision with only case-level pseudo-labels. We benchmark our approach against state-of-the-art supervised segmentation and detection models. This study demonstrates the feasibility and promise of 3D latent diffusion for weakly supervised anomaly detection. While the current results do not yet match supervised baselines, they reveal key directions for improving reconstruction fidelity and lesion localization. Our findings provide an important step toward annotation-efficient, generative modeling of complex abdominal anatomy.",
      "authors": [
        "Jen Dusseljee",
        "Sarah de Boer",
        "Alessa Hering"
      ],
      "published": "2026-01-09T15:30:00Z",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "abs_url": "https://arxiv.org/abs/2601.05852v1",
      "pdf_url": "https://arxiv.org/pdf/2601.05852v1.pdf"
    },
    {
      "id": "2601.05851v1",
      "title": "Router-Suggest: Dynamic Routing for Multimodal Auto-Completion in Visually-Grounded Dialogs",
      "abstract": "Real-time multimodal auto-completion is essential for digital assistants, chatbots, design tools, and healthcare consultations, where user inputs rely on shared visual context. We introduce Multimodal Auto-Completion (MAC), a task that predicts upcoming characters in live chats using partially typed text and visual cues. Unlike traditional text-only auto-completion (TAC), MAC grounds predictions in multimodal context to better capture user intent. To enable this task, we adapt MMDialog and ImageChat to create benchmark datasets. We evaluate leading vision-language models (VLMs) against strong textual baselines, highlighting trade-offs in accuracy and efficiency. We present Router-Suggest, a router framework that dynamically selects between textual models and VLMs based on dialog context, along with a lightweight variant for resource-constrained environments. Router-Suggest achieves a 2.3x to 10x speedup over the best-performing VLM. A user study shows that VLMs significantly excel over textual models on user satisfaction, notably saving user typing effort and improving the quality of completions in multi-turn conversations. These findings underscore the need for multimodal context in auto-completions, leading to smarter, user-aware assistants.",
      "authors": [
        "Sandeep Mishra",
        "Devichand Budagam",
        "Anubhab Mandal",
        "Bishal Santra",
        "Pawan Goyal",
        "Manish Gupta"
      ],
      "published": "2026-01-09T15:29:50Z",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.CL",
      "abs_url": "https://arxiv.org/abs/2601.05851v1",
      "pdf_url": "https://arxiv.org/pdf/2601.05851v1.pdf"
    },
    {
      "id": "2601.05848v1",
      "title": "Goal Force: Teaching Video Models To Accomplish Physics-Conditioned Goals",
      "abstract": "Recent advancements in video generation have enabled the development of ``world models'' capable of simulating potential futures for robotics and planning. However, specifying precise goals for these models remains a challenge; text instructions are often too abstract to capture physical nuances, while target images are frequently infeasible to specify for dynamic tasks. To address this, we introduce Goal Force, a novel framework that allows users to define goals via explicit force vectors and intermediate dynamics, mirroring how humans conceptualize physical tasks. We train a video generation model on a curated dataset of synthetic causal primitives-such as elastic collisions and falling dominos-teaching it to propagate forces through time and space. Despite being trained on simple physics data, our model exhibits remarkable zero-shot generalization to complex, real-world scenarios, including tool manipulation and multi-object causal chains. Our results suggest that by grounding video generation in fundamental physical interactions, models can emerge as implicit neural physics simulators, enabling precise, physics-aware planning without reliance on external engines. We release all datasets, code, model weights, and interactive video demos at our project page.",
      "authors": [
        "Nate Gillman",
        "Yinghua Zhou",
        "Zitian Tang",
        "Evan Luo",
        "Arjan Chakravarthy",
        "Daksh Aggarwal",
        "Michael Freeman",
        "Charles Herrmann",
        "Chen Sun"
      ],
      "published": "2026-01-09T15:23:36Z",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.RO"
      ],
      "primary_category": "cs.CV",
      "abs_url": "https://arxiv.org/abs/2601.05848v1",
      "pdf_url": "https://arxiv.org/pdf/2601.05848v1.pdf"
    },
    {
      "id": "2601.05847v1",
      "title": "Semantic NLP Pipelines for Interoperable Patient Digital Twins from Unstructured EHRs",
      "abstract": "Digital twins -- virtual replicas of physical entities -- are gaining traction in healthcare for personalized monitoring, predictive modeling, and clinical decision support. However, generating interoperable patient digital twins from unstructured electronic health records (EHRs) remains challenging due to variability in clinical documentation and lack of standardized mappings. This paper presents a semantic NLP-driven pipeline that transforms free-text EHR notes into FHIR-compliant digital twin representations. The pipeline leverages named entity recognition (NER) to extract clinical concepts, concept normalization to map entities to SNOMED-CT or ICD-10, and relation extraction to capture structured associations between conditions, medications, and observations. Evaluation on MIMIC-IV Clinical Database Demo with validation against MIMIC-IV-on-FHIR reference mappings demonstrates high F1-scores for entity and relation extraction, with improved schema completeness and interoperability compared to baseline methods.",
      "authors": [
        "Rafael Brens",
        "Yuqiao Meng",
        "Luoxi Tang",
        "Zhaohan Xi"
      ],
      "published": "2026-01-09T15:20:11Z",
      "categories": [
        "cs.CL"
      ],
      "primary_category": "cs.CL",
      "abs_url": "https://arxiv.org/abs/2601.05847v1",
      "pdf_url": "https://arxiv.org/pdf/2601.05847v1.pdf"
    },
    {
      "id": "2601.05845v1",
      "title": "A New Family of Poisson Non-negative Matrix Factorization Methods Using the Shifted Log Link",
      "abstract": "Poisson non-negative matrix factorization (NMF) is a widely used method to find interpretable \"parts-based\" decompositions of count data. While many variants of Poisson NMF exist, existing methods assume that the \"parts\" in the decomposition combine additively. This assumption may be natural in some settings, but not in others. Here we introduce Poisson NMF with the shifted-log link function to relax this assumption. The shifted-log link function has a single tuning parameter, and as this parameter varies the model changes from assuming that parts combine additively (i.e., standard Poisson NMF) to assuming that parts combine more multiplicatively. We provide an algorithm to fit this model by maximum likelihood, and also an approximation that substantially reduces computation time for large, sparse datasets (computations scale with the number of non-zero entries in the data matrix). We illustrate these new methods on a variety of real datasets. Our examples show how the choice of link function in Poisson NMF can substantively impact the results, and how in some settings the use of a shifted-log link function may improve interpretability compared with the standard, additive link.",
      "authors": [
        "Eric Weine",
        "Peter Carbonetto",
        "Rafael A. Irizarry",
        "Matthew Stephens"
      ],
      "published": "2026-01-09T15:16:54Z",
      "categories": [
        "cs.LG",
        "stat.ME"
      ],
      "primary_category": "cs.LG",
      "abs_url": "https://arxiv.org/abs/2601.05845v1",
      "pdf_url": "https://arxiv.org/pdf/2601.05845v1.pdf"
    },
    {
      "id": "2601.05844v1",
      "title": "DexterCap: An Affordable and Automated System for Capturing Dexterous Hand-Object Manipulation",
      "abstract": "Capturing fine-grained hand-object interactions is challenging due to severe self-occlusion from closely spaced fingers and the subtlety of in-hand manipulation motions. Existing optical motion capture systems rely on expensive camera setups and extensive manual post-processing, while low-cost vision-based methods often suffer from reduced accuracy and reliability under occlusion. To address these challenges, we present DexterCap, a low-cost optical capture system for dexterous in-hand manipulation. DexterCap uses dense, character-coded marker patches to achieve robust tracking under severe self-occlusion, together with an automated reconstruction pipeline that requires minimal manual effort. With DexterCap, we introduce DexterHand, a dataset of fine-grained hand-object interactions covering diverse manipulation behaviors and objects, from simple primitives to complex articulated objects such as a Rubik's Cube. We release the dataset and code to support future research on dexterous hand-object interaction.",
      "authors": [
        "Yutong Liang",
        "Shiyi Xu",
        "Yulong Zhang",
        "Bowen Zhan",
        "He Zhang",
        "Libin Liu"
      ],
      "published": "2026-01-09T15:16:31Z",
      "categories": [
        "cs.GR",
        "cs.AI",
        "cs.RO"
      ],
      "primary_category": "cs.GR",
      "abs_url": "https://arxiv.org/abs/2601.05844v1",
      "pdf_url": "https://arxiv.org/pdf/2601.05844v1.pdf"
    },
    {
      "id": "2601.05839v1",
      "title": "GeoSurDepth: Spatial Geometry-Consistent Self-Supervised Depth Estimation for Surround-View Cameras",
      "abstract": "Accurate surround-view depth estimation provides a competitive alternative to laser-based sensors and is essential for 3D scene understanding in autonomous driving. While prior studies have proposed various approaches that primarily focus on enforcing cross-view constraints at the photometric level, few explicitly exploit the rich geometric structure inherent in both monocular and surround-view setting. In this work, we propose GeoSurDepth, a framework that leverages geometry consistency as the primary cue for surround-view depth estimation. Concretely, we utilize foundation models as a pseudo geometry prior and feature representation enhancement tool to guide the network to maintain surface normal consistency in spatial 3D space and regularize object- and texture-consistent depth estimation in 2D. In addition, we introduce a novel view synthesis pipeline where 2D-3D lifting is achieved with dense depth reconstructed via spatial warping, encouraging additional photometric supervision across temporal, spatial, and spatial-temporal contexts, and compensating for the limitations of single-view image reconstruction. Finally, a newly-proposed adaptive joint motion learning strategy enables the network to adaptively emphasize informative spatial geometry cues for improved motion reasoning. Extensive experiments on DDAD and nuScenes demonstrate that GeoSurDepth achieves state-of-the-art performance, validating the effectiveness of our approach. Our framework highlights the importance of exploiting geometry coherence and consistency for robust self-supervised multi-view depth estimation.",
      "authors": [
        "Weimin Liu",
        "Wenjun Wang",
        "Joshua H. Meng"
      ],
      "published": "2026-01-09T15:13:28Z",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "abs_url": "https://arxiv.org/abs/2601.05839v1",
      "pdf_url": "https://arxiv.org/pdf/2601.05839v1.pdf"
    },
    {
      "id": "2601.05836v1",
      "title": "Intelligent Singularity Avoidance in UR10 Robotic Arm Path Planning Using Hybrid Fuzzy Logic and Reinforcement Learning",
      "abstract": "This paper presents a comprehensive approach to singularity detection and avoidance in UR10 robotic arm path planning through the integration of fuzzy logic safety systems and reinforcement learning algorithms. The proposed system addresses critical challenges in robotic manipulation where singularities can cause loss of control and potential equipment damage. Our hybrid approach combines real-time singularity detection using manipulability measures, condition number analysis, and fuzzy logic decision-making with a stable reinforcement learning framework for adaptive path planning. Experimental results demonstrate a 90% success rate in reaching target positions while maintaining safe distances from singular configurations. The system integrates PyBullet simulation for training data collection and URSim connectivity for real-world deployment.",
      "authors": [
        "Sheng-Kai Chen",
        "Jyh-Horng Wu"
      ],
      "published": "2026-01-09T15:10:23Z",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "abs_url": "https://arxiv.org/abs/2601.05836v1",
      "pdf_url": "https://arxiv.org/pdf/2601.05836v1.pdf"
    },
    {
      "id": "2601.05835v1",
      "title": "Left, Right, or Center? Evaluating LLM Framing in News Classification and Generation",
      "abstract": "Large Language Model (LLM) based summarization and text generation are increasingly used for producing and rewriting text, raising concerns about political framing in journalism where subtle wording choices can shape interpretation. Across nine state-of-the-art LLMs, we study political framing by testing whether LLMs' classification-based bias signals align with framing behavior in their generated summaries. We first compare few-shot ideology predictions against LEFT/CENTER/RIGHT labels. We then generate \"steered\" summaries under FAITHFUL, CENTRIST, LEFT, and RIGHT prompts, and score all outputs using a single fixed ideology evaluator. We find pervasive ideological center-collapse in both article-level ratings and generated text, indicating a systematic tendency toward centrist framing. Among evaluated models, Grok 4 is by far the most ideologically expressive generator, while Claude Sonnet 4.5 and Llama 3.1 achieve the strongest bias-rating performance among commercial and open-weight models, respectively.",
      "authors": [
        "Molly Kennedy",
        "Ali Parker",
        "Yihong Liu",
        "Hinrich Schütze"
      ],
      "published": "2026-01-09T15:10:11Z",
      "categories": [
        "cs.CL"
      ],
      "primary_category": "cs.CL",
      "abs_url": "https://arxiv.org/abs/2601.05835v1",
      "pdf_url": "https://arxiv.org/pdf/2601.05835v1.pdf"
    },
    {
      "id": "2601.05833v1",
      "title": "Peek2: A Regex-free implementation of pretokenizers for Byte-level BPE",
      "abstract": "Pretokenization is a crucial, sequential pass in Byte-level BPE tokenizers. Our proposed new implementation, Peek2, serves as a drop-in replacement for cl100k-like pretokenizers used in GPT-3, LLaMa-3, and Qwen-2.5. Designed with performance and safety in mind, Peek2 is Regex-free and delivers a $ 1.11\\times $ improvement in overall throughput across the entire Byte-level BPE encoding process. This algorithm runs entirely on the CPU, has stable linear complexity $ O(n) $, and provides presegmentation results identical to those of the original Regex-based pretokenizer.",
      "authors": [
        "Liu Zai"
      ],
      "published": "2026-01-09T15:05:09Z",
      "categories": [
        "cs.CL"
      ],
      "primary_category": "cs.CL",
      "abs_url": "https://arxiv.org/abs/2601.05833v1",
      "pdf_url": "https://arxiv.org/pdf/2601.05833v1.pdf"
    },
    {
      "id": "2601.05828v1",
      "title": "Influence of Parallelism in Vector-Multiplication Units on Correlation Power Analysis",
      "abstract": "The use of neural networks in edge devices is increasing, which introduces new security challenges related to the neural networks' confidentiality. As edge devices often offer physical access, attacks targeting the hardware, such as side-channel analysis, must be considered. To enhance the performance of neural network inference, hardware accelerators are commonly employed. This work investigates the influence of parallel processing within such accelerators on correlation-based side-channel attacks that exploit power consumption. The focus is on neurons that are part of the same fully-connected layer, which run parallel and simultaneously process the same input value. The theoretical impact of concurrent multiply-and-accumulate operations on overall power consumption is evaluated, as well as the success rate of correlation power analysis. Based on the observed behavior, equations are derived that describe how the correlation decreases with increasing levels of parallelism. The applicability of these equations is validated using a vector-multiplication unit implemented on an FPGA.",
      "authors": [
        "Manuel Brosch",
        "Matthias Probst",
        "Stefan Kögler",
        "Georg Sigl"
      ],
      "published": "2026-01-09T15:01:47Z",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.IR"
      ],
      "primary_category": "cs.CR",
      "abs_url": "https://arxiv.org/abs/2601.05828v1",
      "pdf_url": "https://arxiv.org/pdf/2601.05828v1.pdf"
    },
    {
      "id": "2601.05825v1",
      "title": "Decoding Workload and Agreement From EEG During Spoken Dialogue With Conversational AI",
      "abstract": "Passive brain-computer interfaces offer a potential source of implicit feedback for alignment of large language models, but most mental state decoding has been done in controlled tasks. This paper investigates whether established EEG classifiers for mental workload and implicit agreement can be transferred to spoken human-AI dialogue. We introduce two conversational paradigms - a Spelling Bee task and a sentence completion task- and an end-to-end pipeline for transcribing, annotating, and aligning word-level conversational events with continuous EEG classifier output. In a pilot study, workload decoding showed interpretable trends during spoken interaction, supporting cross-paradigm transfer. For implicit agreement, we demonstrate continuous application and precise temporal alignment to conversational events, while identifying limitations related to construct transfer and asynchronous application of event-based classifiers. Overall, the results establish feasibility and constraints for integrating passive BCI signals into conversational AI systems.",
      "authors": [
        "Lucija Mihić Zidar",
        "Philipp Wicke",
        "Praneel Bhatia",
        "Rosa Lutz",
        "Marius Klug",
        "Thorsten O. Zander"
      ],
      "published": "2026-01-09T14:59:25Z",
      "categories": [
        "cs.HC",
        "cs.AI"
      ],
      "primary_category": "cs.HC",
      "abs_url": "https://arxiv.org/abs/2601.05825v1",
      "pdf_url": "https://arxiv.org/pdf/2601.05825v1.pdf"
    },
    {
      "id": "2601.05823v1",
      "title": "Boosting Latent Diffusion Models via Disentangled Representation Alignment",
      "abstract": "Latent Diffusion Models (LDMs) generate high-quality images by operating in a compressed latent space, typically obtained through image tokenizers such as Variational Autoencoders (VAEs). In pursuit of a generation-friendly VAE, recent studies have explored leveraging Vision Foundation Models (VFMs) as representation alignment targets for VAEs, mirroring the approach commonly adopted for LDMs. Although this yields certain performance gains, using the same alignment target for both VAEs and LDMs overlooks their fundamentally different representational requirements. We advocate that while LDMs benefit from latents retaining high-level semantic concepts, VAEs should excel in semantic disentanglement, enabling encoding of attribute-level information in a structured way. To address this, we propose the Semantic disentangled VAE (Send-VAE), explicitly optimized for disentangled representation learning through aligning its latent space with the semantic hierarchy of pre-trained VFMs. Our approach employs a non-linear mapper network to transform VAE latents, aligning them with VFMs to bridge the gap between attribute-level disentanglement and high-level semantics, facilitating effective guidance for VAE learning. We evaluate semantic disentanglement via linear probing on attribute prediction tasks, showing strong correlation with improved generation performance. Finally, using Send-VAE, we train flow-based transformers SiTs; experiments show Send-VAE significantly speeds up training and achieves a state-of-the-art FID of 1.21 and 1.75 with and without classifier-free guidance on ImageNet 256x256.",
      "authors": [
        "John Page",
        "Xuesong Niu",
        "Kai Wu",
        "Kun Gai"
      ],
      "published": "2026-01-09T14:54:30Z",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "abs_url": "https://arxiv.org/abs/2601.05823v1",
      "pdf_url": "https://arxiv.org/pdf/2601.05823v1.pdf"
    },
    {
      "id": "2601.05821v1",
      "title": "LLMs as Science Journalists: Supporting Early-stage Researchers in Communicating Their Science to the Public",
      "abstract": "The scientific community needs tools that help early-stage researchers effectively communicate their findings and innovations to the public. Although existing general-purpose Large Language Models (LLMs) can assist in this endeavor, they are not optimally aligned for it. To address this, we propose a framework for training LLMs to emulate the role of a science journalist that can be used by early-stage researchers to learn how to properly communicate their papers to the general public. We evaluate the usefulness of our trained LLM Journalists in leading conversations with both simulated and human researchers. %compared to the general-purpose ones. Our experiments indicate that LLMs trained using our framework ask more relevant questions that address the societal impact of research, prompting researchers to clarify and elaborate on their findings. In the user study, the majority of participants who interacted with our trained LLM Journalist appreciated it more than interacting with general-purpose LLMs.",
      "authors": [
        "Milad Alshomary",
        "Grace Li",
        "Anubhav Jangra",
        "Yufang Hou",
        "Kathleen McKeown",
        "Smaranda Muresan"
      ],
      "published": "2026-01-09T14:51:20Z",
      "categories": [
        "cs.CL"
      ],
      "primary_category": "cs.CL",
      "abs_url": "https://arxiv.org/abs/2601.05821v1",
      "pdf_url": "https://arxiv.org/pdf/2601.05821v1.pdf"
    },
    {
      "id": "2601.05814v1",
      "title": "A Dual Pipeline Machine Learning Framework for Automated Multi Class Sleep Disorder Screening Using Hybrid Resampling and Ensemble Learning",
      "abstract": "Accurate classification of sleep disorders, particularly insomnia and sleep apnea, is important for reducing long term health risks and improving patient quality of life. However, clinical sleep studies are resource intensive and are difficult to scale for population level screening. This paper presents a Dual Pipeline Machine Learning Framework for multi class sleep disorder screening using the Sleep Health and Lifestyle dataset. The framework consists of two parallel processing streams: a statistical pipeline that targets linear separability using Mutual Information and Linear Discriminant Analysis, and a wrapper based pipeline that applies Boruta feature selection with an autoencoder for non linear representation learning. To address class imbalance, we use the hybrid SMOTETomek resampling strategy. In experiments, Extra Trees and K Nearest Neighbors achieved an accuracy of 98.67%, outperforming recent baselines on the same dataset. Statistical testing using the Wilcoxon Signed Rank Test indicates that the improvement over baseline configurations is significant, and inference latency remains below 400 milliseconds. These results suggest that the proposed dual pipeline design supports accurate and efficient automated screening for non invasive sleep disorder risk stratification.",
      "authors": [
        "Md Sultanul Islam Ovi",
        "Muhsina Tarannum Munfa",
        "Miftahul Alam Adib",
        "Syed Sabbir Hasan"
      ],
      "published": "2026-01-09T14:43:32Z",
      "categories": [
        "cs.LG"
      ],
      "primary_category": "cs.LG",
      "abs_url": "https://arxiv.org/abs/2601.05814v1",
      "pdf_url": "https://arxiv.org/pdf/2601.05814v1.pdf"
    },
    {
      "id": "2601.05812v1",
      "title": "Detecting Autism Spectrum Disorder with Deep Eye Movement Features",
      "abstract": "Autism Spectrum Disorder (ASD) is a neurodevelopmental disorder characterized by deficits in social communication and behavioral patterns. Eye movement data offers a non-invasive diagnostic tool for ASD detection, as it is inherently discrete and exhibits short-term temporal dependencies, reflecting localized gaze focus between fixation points. These characteristics enable the data to provide deeper insights into subtle behavioral markers, distinguishing ASD-related patterns from typical development. Eye movement signals mainly contain short-term and localized dependencies. However, despite the widespread application of stacked attention layers in Transformer-based models for capturing long-range dependencies, our experimental results indicate that this approach yields only limited benefits when applied to eye movement data. This may be because discrete fixation points and short-term dependencies in gaze focus reduce the utility of global attention mechanisms, making them less efficient than architectures focusing on local temporal patterns. To efficiently capture subtle and complex eye movement patterns, distinguishing ASD from typically developing (TD) individuals, a discrete short-term sequential (DSTS) modeling framework is designed with Class-aware Representation and Imbalance-aware Mechanisms. Through extensive experiments on several eye movement datasets, DSTS outperforms both traditional machine learning techniques and more sophisticated deep learning models.",
      "authors": [
        "Zhanpei Huang",
        "Taochen chen",
        "Fangqing Gu",
        "Yiqun Zhang"
      ],
      "published": "2026-01-09T14:35:24Z",
      "categories": [
        "cs.LG"
      ],
      "primary_category": "cs.LG",
      "abs_url": "https://arxiv.org/abs/2601.05812v1",
      "pdf_url": "https://arxiv.org/pdf/2601.05812v1.pdf"
    },
    {
      "id": "2601.05811v1",
      "title": "Learning Reconstructive Embeddings in Reproducing Kernel Hilbert Spaces via the Representer Theorem",
      "abstract": "Motivated by the growing interest in representation learning approaches that uncover the latent structure of high-dimensional data, this work proposes new algorithms for reconstruction-based manifold learning within Reproducing-Kernel Hilbert Spaces (RKHS). Each observation is first reconstructed as a linear combination of the other samples in the RKHS, by optimizing a vector form of the Representer Theorem for their autorepresentation property. A separable operator-valued kernel extends the formulation to vector-valued data while retaining the simplicity of a single scalar similarity function. A subsequent kernel-alignment task projects the data into a lower-dimensional latent space whose Gram matrix aims to match the high-dimensional reconstruction kernel, thus transferring the auto-reconstruction geometry of the RKHS to the embedding. Therefore, the proposed algorithms represent an extended approach to the autorepresentation property, exhibited by many natural data, by using and adapting well-known results of Kernel Learning Theory. Numerical experiments on both simulated (concentric circles and swiss-roll) and real (cancer molecular activity and IoT network intrusions) datasets provide empirical evidence of the practical effectiveness of the proposed approach.",
      "authors": [
        "Enrique Feito-Casares",
        "Francisco M. Melgarejo-Meseguer",
        "José-Luis Rojo-Álvarez"
      ],
      "published": "2026-01-09T14:35:19Z",
      "categories": [
        "cs.LG"
      ],
      "primary_category": "cs.LG",
      "abs_url": "https://arxiv.org/abs/2601.05811v1",
      "pdf_url": "https://arxiv.org/pdf/2601.05811v1.pdf"
    },
    {
      "id": "2601.05810v1",
      "title": "SceneFoundry: Generating Interactive Infinite 3D Worlds",
      "abstract": "The ability to automatically generate large-scale, interactive, and physically realistic 3D environments is crucial for advancing robotic learning and embodied intelligence. However, existing generative approaches often fail to capture the functional complexity of real-world interiors, particularly those containing articulated objects with movable parts essential for manipulation and navigation. This paper presents SceneFoundry, a language-guided diffusion framework that generates apartment-scale 3D worlds with functionally articulated furniture and semantically diverse layouts for robotic training. From natural language prompts, an LLM module controls floor layout generation, while diffusion-based posterior sampling efficiently populates the scene with articulated assets from large-scale 3D repositories. To ensure physical usability, SceneFoundry employs differentiable guidance functions to regulate object quantity, prevent articulation collisions, and maintain sufficient walkable space for robotic navigation. Extensive experiments demonstrate that our framework generates structurally valid, semantically coherent, and functionally interactive environments across diverse scene types and conditions, enabling scalable embodied AI research.",
      "authors": [
        "ChunTeng Chen",
        "YiChen Hsu",
        "YiWen Liu",
        "WeiFang Sun",
        "TsaiChing Ni",
        "ChunYi Lee",
        "Min Sun",
        "YuanFu Yang"
      ],
      "published": "2026-01-09T14:33:10Z",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG",
        "cs.RO"
      ],
      "primary_category": "cs.CV",
      "abs_url": "https://arxiv.org/abs/2601.05810v1",
      "pdf_url": "https://arxiv.org/pdf/2601.05810v1.pdf"
    },
    {
      "id": "2601.05808v1",
      "title": "EnvScaler: Scaling Tool-Interactive Environments for LLM Agent via Programmatic Synthesis",
      "abstract": "Large language models (LLMs) are expected to be trained to act as agents in various real-world environments, but this process relies on rich and varied tool-interaction sandboxes. However, access to real systems is often restricted; LLM-simulated environments are prone to hallucinations and inconsistencies; and manually built sandboxes are hard to scale. In this paper, we propose EnvScaler, an automated framework for scalable tool-interaction environments via programmatic synthesis. EnvScaler comprises two components. First, SkelBuilder constructs diverse environment skeletons through topic mining, logic modeling, and quality evaluation. Then, ScenGenerator generates multiple task scenarios and rule-based trajectory validation functions for each environment. With EnvScaler, we synthesize 191 environments and about 7K scenarios, and apply them to Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL) for Qwen3 series models. Results on three benchmarks show that EnvScaler significantly improves LLMs' ability to solve tasks in complex environments involving multi-turn, multi-tool interactions. We release our code and data at https://github.com/RUC-NLPIR/EnvScaler.",
      "authors": [
        "Xiaoshuai Song",
        "Haofei Chang",
        "Guanting Dong",
        "Yutao Zhu",
        "Zhicheng Dou",
        "Ji-Rong Wen"
      ],
      "published": "2026-01-09T14:32:06Z",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "abs_url": "https://arxiv.org/abs/2601.05808v1",
      "pdf_url": "https://arxiv.org/pdf/2601.05808v1.pdf"
    },
    {
      "id": "2601.05807v1",
      "title": "Fusion Matters: Length-Aware Analysis of Positional-Encoding Fusion in Transformers",
      "abstract": "Transformers require positional encodings to represent sequence order, yet most prior work focuses on designing new positional encodings rather than examining how positional information is fused with token embeddings. In this paper, we study whether the fusion mechanism itself affects performance, particularly in long-sequence settings. We conduct a controlled empirical study comparing three canonical fusion strategies--element-wise addition, concatenation with projection, and scalar gated fusion--under identical Transformer architectures, data splits, and random seeds. Experiments on three text classification datasets spanning short (AG News), medium (IMDB), and long (ArXiv) sequences show that fusion choice has negligible impact on short texts but produces consistent gains on long documents. To verify that these gains are structural rather than stochastic, we perform paired-seed analysis and cross-dataset comparison across sequence-length regimes. Additional experiments on the ArXiv dataset indicate that the benefit of learnable fusion generalizes across multiple positional encoding families. Finally, we explore a lightweight convolutional gating mechanism that introduces local inductive bias at the fusion level, evaluated on long documents only. Our results indicate that positional-encoding fusion is a non-trivial design choice for long-sequence Transformers and should be treated as an explicit modeling decision rather than a fixed default.",
      "authors": [
        "Mohamed Amine Hallam",
        "Kuo-Kun Tseng"
      ],
      "published": "2026-01-09T14:25:31Z",
      "categories": [
        "cs.LG",
        "cs.CL"
      ],
      "primary_category": "cs.LG",
      "abs_url": "https://arxiv.org/abs/2601.05807v1",
      "pdf_url": "https://arxiv.org/pdf/2601.05807v1.pdf"
    },
    {
      "id": "2601.05806v1",
      "title": "Modular Autonomy with Conversational Interaction: An LLM-driven Framework for Decision Making in Autonomous Driving",
      "abstract": "Recent advancements in Large Language Models (LLMs) offer new opportunities to create natural language interfaces for Autonomous Driving Systems (ADSs), moving beyond rigid inputs. This paper addresses the challenge of mapping the complexity of human language to the structured action space of modular ADS software. We propose a framework that integrates an LLM-based interaction layer with Autoware, a widely used open-source software. This system enables passengers to issue high-level commands, from querying status information to modifying driving behavior. Our methodology is grounded in three key components: a taxonomization of interaction categories, an application-centric Domain Specific Language (DSL) for command translation, and a safety-preserving validation layer. A two-stage LLM architecture ensures high transparency by providing feedback based on the definitive execution status. Evaluation confirms the system's timing efficiency and translation robustness. Simulation successfully validated command execution across all five interaction categories. This work provides a foundation for extensible, DSL-assisted interaction in modular and safety-conscious autonomy stacks.",
      "authors": [
        "Marvin Seegert",
        "Korbinian Moller",
        "Johannes Betz"
      ],
      "published": "2026-01-09T14:23:01Z",
      "categories": [
        "cs.RO"
      ],
      "primary_category": "cs.RO",
      "abs_url": "https://arxiv.org/abs/2601.05806v1",
      "pdf_url": "https://arxiv.org/pdf/2601.05806v1.pdf"
    },
    {
      "id": "2601.05805v1",
      "title": "InsSo3D: Inertial Navigation System and 3D Sonar SLAM for turbid environment inspection",
      "abstract": "This paper presents InsSo3D, an accurate and efficient method for large-scale 3D Simultaneous Localisation and Mapping (SLAM) using a 3D Sonar and an Inertial Navigation System (INS). Unlike traditional sonar, which produces 2D images containing range and azimuth information but lacks elevation information, 3D Sonar produces a 3D point cloud, which therefore does not suffer from elevation ambiguity. We introduce a robust and modern SLAM framework adapted to the 3D Sonar data using INS as prior, detecting loop closure and performing pose graph optimisation. We evaluated InsSo3D performance inside a test tank with access to ground truth data and in an outdoor flooded quarry. Comparisons to reference trajectories and maps obtained from an underwater motion tracking system and visual Structure From Motion (SFM) demonstrate that InsSo3D efficiently corrects odometry drift. The average trajectory error is below 21cm during a 50-minute-long mission, producing a map of 10m by 20m with a 9cm average reconstruction error, enabling safe inspection of natural or artificial underwater structures even in murky water conditions.",
      "authors": [
        "Simon Archieri",
        "Ahmet Cinar",
        "Shu Pan",
        "Jonatan Scharff Willners",
        "Michele Grimald",
        "Ignacio Carlucho",
        "Yvan Petillot"
      ],
      "published": "2026-01-09T14:22:26Z",
      "categories": [
        "cs.RO"
      ],
      "primary_category": "cs.RO",
      "abs_url": "https://arxiv.org/abs/2601.05805v1",
      "pdf_url": "https://arxiv.org/pdf/2601.05805v1.pdf"
    },
    {
      "id": "2601.05794v1",
      "title": "Simplify-This: A Comparative Analysis of Prompt-Based and Fine-Tuned LLMs",
      "abstract": "Large language models (LLMs) enable strong text generation, and in general there is a practical tradeoff between fine-tuning and prompt engineering. We introduce Simplify-This, a comparative study evaluating both paradigms for text simplification with encoder-decoder LLMs across multiple benchmarks, using a range of evaluation metrics. Fine-tuned models consistently deliver stronger structural simplification, whereas prompting often attains higher semantic similarity scores yet tends to copy inputs. A human evaluation favors fine-tuned outputs overall. We release code, a cleaned derivative dataset used in our study, checkpoints of fine-tuned models, and prompt templates to facilitate reproducibility and future work.",
      "authors": [
        "Eilam Cohen",
        "Itamar Bul",
        "Danielle Inbar",
        "Omri Loewenbach"
      ],
      "published": "2026-01-09T13:46:52Z",
      "categories": [
        "cs.CL",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "abs_url": "https://arxiv.org/abs/2601.05794v1",
      "pdf_url": "https://arxiv.org/pdf/2601.05794v1.pdf"
    },
    {
      "id": "2601.05792v1",
      "title": "Tensor-DTI: Enhancing Biomolecular Interaction Prediction with Contrastive Embedding Learning",
      "abstract": "Accurate drug-target interaction (DTI) prediction is essential for computational drug discovery, yet existing models often rely on single-modality predefined molecular descriptors or sequence-based embeddings with limited representativeness. We propose Tensor-DTI, a contrastive learning framework that integrates multimodal embeddings from molecular graphs, protein language models, and binding-site predictions to improve interaction modeling. Tensor-DTI employs a siamese dual-encoder architecture, enabling it to capture both chemical and structural interaction features while distinguishing interacting from non-interacting pairs. Evaluations on multiple DTI benchmarks demonstrate that Tensor-DTI outperforms existing sequence-based and graph-based models. We also conduct large-scale inference experiments on CDK2 across billion-scale chemical libraries, where Tensor-DTI produces chemically plausible hit distributions even when CDK2 is withheld from training. In enrichment studies against Glide docking and Boltz-2 co-folder, Tensor-DTI remains competitive on CDK2 and improves the screening budget required to recover moderate fractions of high-affinity ligands on out-of-family targets under strict family-holdout splits. Additionally, we explore its applicability to protein-RNA and peptide-protein interactions. Our findings highlight the benefits of integrating multimodal information with contrastive objectives to enhance interaction-prediction accuracy and to provide more interpretable and reliability-aware models for virtual screening.",
      "authors": [
        "Manel Gil-Sorribes",
        "Júlia Vilalta-Mor",
        "Isaac Filella-Mercè",
        "Robert Soliva",
        "Álvaro Ciudad",
        "Víctor Guallar",
        "Alexis Molina"
      ],
      "published": "2026-01-09T13:39:49Z",
      "categories": [
        "cs.LG",
        "cs.AI",
        "q-bio.BM"
      ],
      "primary_category": "cs.LG",
      "abs_url": "https://arxiv.org/abs/2601.05792v1",
      "pdf_url": "https://arxiv.org/pdf/2601.05792v1.pdf"
    },
    {
      "id": "2601.05789v1",
      "title": "SAFE: Secure and Accurate Federated Learning for Privacy-Preserving Brain-Computer Interfaces",
      "abstract": "Electroencephalogram (EEG)-based brain-computer interfaces (BCIs) are widely adopted due to their efficiency and portability; however, their decoding algorithms still face multiple challenges, including inadequate generalization, adversarial vulnerability, and privacy leakage. This paper proposes Secure and Accurate FEderated learning (SAFE), a federated learning-based approach that protects user privacy by keeping data local during model training. SAFE employs local batch-specific normalization to mitigate cross-subject feature distribution shifts and hence improves model generalization. It further enhances adversarial robustness by introducing perturbations in both the input space and the parameter space through federated adversarial training and adversarial weight perturbation. Experiments on five EEG datasets from motor imagery (MI) and event-related potential (ERP) BCI paradigms demonstrated that SAFE consistently outperformed 14 state-of-the-art approaches in both decoding accuracy and adversarial robustness, while ensuring privacy protection. Notably, it even outperformed centralized training approaches that do not consider privacy protection at all. To our knowledge, SAFE is the first algorithm to simultaneously achieve high decoding accuracy, strong adversarial robustness, and reliable privacy protection without using any calibration data from the target subject, making it highly desirable for real-world BCIs.",
      "authors": [
        "Tianwang Jia",
        "Xiaoqing Chen",
        "Dongrui Wu"
      ],
      "published": "2026-01-09T13:29:41Z",
      "categories": [
        "cs.HC",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.HC",
      "abs_url": "https://arxiv.org/abs/2601.05789v1",
      "pdf_url": "https://arxiv.org/pdf/2601.05789v1.pdf"
    },
    {
      "id": "2601.05787v1",
      "title": "From Off-Policy to On-Policy: Enhancing GUI Agents via Bi-level Expert-to-Policy Assimilation",
      "abstract": "Vision-language models are increasingly deployed as computer-use agents (CUAs) that operate desktops and browsers. Top-performing CUAs are framework-based systems that decompose planning and execution, while end-to-end screenshot-to-action policies are easier to deploy but lag behind on benchmarks such as OSWorld-Verified. GUI datasets like OSWorld pose two bottlenecks: they expose only a few hundred interactive, verifiable tasks and environments, and expert trajectories must be gathered by interacting with these environments, making such data hard to scale. We therefore ask how reinforcement learning from verifiable rewards (RLVR) can best exploit a small pool of exist expert trajectories to train end-to-end policies. Naively mixing these off-policy traces into on-policy RLVR is brittle: even after format conversion, expert trajectories exhibit structural mismatch and distribution shift from the learner. We propose BEPA (Bi-Level Expert-to-Policy Assimilation), which turns static expert traces into policy-aligned guidance via self-rolled reachable trajectories under the base policy (LEVEL-1) and a per-task, dynamically updated cache used in RLVR (LEVEL-2). On OSWorld-Verified, BEPA improves UITARS1.5-7B success from 22.87% to 32.13% and raises a held-out split from 5.74% to 10.30%, with consistent gains on MMBench-GUI and Online-Mind2Web. Our code and data are available at: https://github.com/LEON-gittech/Verl_GUI.git",
      "authors": [
        "Zezhou Wang",
        "Ziyun Zhang",
        "Xiaoyi Zhang",
        "Zhuzhong Qian",
        "Yan Lu"
      ],
      "published": "2026-01-09T13:26:38Z",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "abs_url": "https://arxiv.org/abs/2601.05787v1",
      "pdf_url": "https://arxiv.org/pdf/2601.05787v1.pdf"
    },
    {
      "id": "2601.05785v1",
      "title": "Adaptive Disentangled Representation Learning for Incomplete Multi-View Multi-Label Classification",
      "abstract": "Multi-view multi-label learning frequently suffers from simultaneous feature absence and incomplete annotations, due to challenges in data acquisition and cost-intensive supervision. To tackle the complex yet highly practical problem while overcoming the existing limitations of feature recovery, representation disentanglement, and label semantics modeling, we propose an Adaptive Disentangled Representation Learning method (ADRL). ADRL achieves robust view completion by propagating feature-level affinity across modalities with neighborhood awareness, and reinforces reconstruction effectiveness by leveraging a stochastic masking strategy. Through disseminating category-level association across label distributions, ADRL refines distribution parameters for capturing interdependent label prototypes. Besides, we formulate a mutual-information-based objective to promote consistency among shared representations and suppress information overlap between view-specific representation and other modalities. Theoretically, we derive the tractable bounds to train the dual-channel network. Moreover, ADRL performs prototype-specific feature selection by enabling independent interactions between label embeddings and view representations, accompanied by the generation of pseudo-labels for each category. The structural characteristics of the pseudo-label space are then exploited to guide a discriminative trade-off during view fusion. Finally, extensive experiments on public datasets and real-world applications demonstrate the superior performance of ADRL.",
      "authors": [
        "Quanjiang Li",
        "Zhiming Liu",
        "Tianxiang Xu",
        "Tingjin Luo",
        "Chenping Hou"
      ],
      "published": "2026-01-09T13:22:37Z",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "abs_url": "https://arxiv.org/abs/2601.05785v1",
      "pdf_url": "https://arxiv.org/pdf/2601.05785v1.pdf"
    },
    {
      "id": "2601.05776v1",
      "title": "One Script Instead of Hundreds? On Pretraining Romanized Encoder Language Models",
      "abstract": "Exposing latent lexical overlap, script romanization has emerged as an effective strategy for improving cross-lingual transfer (XLT) in multilingual language models (mLMs). Most prior work, however, focused on setups that favor romanization the most: (1) transfer from high-resource Latin-script to low-resource non-Latin-script languages and/or (2) between genealogically closely related languages with different scripts. It thus remains unclear whether romanization is a good representation choice for pretraining general-purpose mLMs, or, more precisely, if information loss associated with romanization harms performance for high-resource languages. We address this gap by pretraining encoder LMs from scratch on both romanized and original texts for six typologically diverse high-resource languages, investigating two potential sources of degradation: (i) loss of script-specific information and (ii) negative cross-lingual interference from increased vocabulary overlap. Using two romanizers with different fidelity profiles, we observe negligible performance loss for languages with segmental scripts, whereas languages with morphosyllabic scripts (Chinese and Japanese) suffer degradation that higher-fidelity romanization mitigates but cannot fully recover. Importantly, comparing monolingual LMs with their mLM counterpart, we find no evidence that increased subword overlap induces negative interference. We further show that romanization improves encoding efficiency (i.e., fertility) for segmental scripts at a negligible performance cost.",
      "authors": [
        "Benedikt Ebing",
        "Lennart Keller",
        "Goran Glavaš"
      ],
      "published": "2026-01-09T13:00:46Z",
      "categories": [
        "cs.CL"
      ],
      "primary_category": "cs.CL",
      "abs_url": "https://arxiv.org/abs/2601.05776v1",
      "pdf_url": "https://arxiv.org/pdf/2601.05776v1.pdf"
    },
    {
      "id": "2601.05770v1",
      "title": "Weights to Code: Extracting Interpretable Algorithms from the Discrete Transformer",
      "abstract": "Algorithm extraction aims to synthesize executable programs directly from models trained on specific algorithmic tasks, enabling de novo algorithm discovery without relying on human-written code. However, extending this paradigm to Transformer is hindered by superposition, where entangled features encoded in overlapping directions obstruct the extraction of symbolic expressions. In this work, we propose the Discrete Transformer, an architecture explicitly engineered to bridge the gap between continuous representations and discrete symbolic logic. By enforcing a strict functional disentanglement, which constrains Numerical Attention to information routing and Numerical MLP to element-wise arithmetic, and employing temperature-annealed sampling, our method effectively facilitates the extraction of human-readable programs. Empirically, the Discrete Transformer not only achieves performance comparable to RNN-based baselines but crucially extends interpretability to continuous variable domains. Moreover, our analysis of the annealing process shows that the efficient discrete search undergoes a clear phase transition from exploration to exploitation. We further demonstrate that our method enables fine-grained control over synthesized programs by imposing inductive biases. Collectively, these findings establish the Discrete Transformer as a robust framework for demonstration-free algorithm discovery, offering a rigorous pathway toward Transformer interpretability.",
      "authors": [
        "Yifan Zhang",
        "Wei Bi",
        "Kechi Zhang",
        "Dongming Jin",
        "Jie Fu",
        "Zhi Jin"
      ],
      "published": "2026-01-09T12:49:41Z",
      "categories": [
        "cs.LG",
        "cs.CL"
      ],
      "primary_category": "cs.LG",
      "abs_url": "https://arxiv.org/abs/2601.05770v1",
      "pdf_url": "https://arxiv.org/pdf/2601.05770v1.pdf"
    },
    {
      "id": "2601.05759v1",
      "title": "Variational Autoencoders for P-wave Detection on Strong Motion Earthquake Spectrograms",
      "abstract": "Accurate P-wave detection is critical for earthquake early warning, yet strong-motion records pose challenges due to high noise levels, limited labeled data, and complex waveform characteristics. This study reframes P-wave arrival detection as a self-supervised anomaly detection task to evaluate how architectural variations regulate the trade-off between reconstruction fidelity and anomaly discrimination. Through a comprehensive grid search of 492 Variational Autoencoder configurations, we show that while skip connections minimize reconstruction error (Mean Absolute Error approximately 0.0012), they induce \"overgeneralization\", allowing the model to reconstruct noise and masking the detection signal. In contrast, attention mechanisms prioritize global context over local detail and yield the highest detection performance with an area-under-the-curve of 0.875. The attention-based Variational Autoencoder achieves an area-under-the-curve of 0.91 in the 0 to 40-kilometer near-source range, demonstrating high suitability for immediate early warning applications. These findings establish that architectural constraints favoring global context over pixel-perfect reconstruction are essential for robust, self-supervised P-wave detection.",
      "authors": [
        "Turkan Simge Ispak",
        "Salih Tileylioglu",
        "Erdem Akagunduz"
      ],
      "published": "2026-01-09T12:28:02Z",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "abs_url": "https://arxiv.org/abs/2601.05759v1",
      "pdf_url": "https://arxiv.org/pdf/2601.05759v1.pdf"
    },
    {
      "id": "2601.05755v1",
      "title": "VIGIL: Defending LLM Agents Against Tool Stream Injection via Verify-Before-Commit",
      "abstract": "LLM agents operating in open environments face escalating risks from indirect prompt injection, particularly within the tool stream where manipulated metadata and runtime feedback hijack execution flow. Existing defenses encounter a critical dilemma as advanced models prioritize injected rules due to strict alignment while static protection mechanisms sever the feedback loop required for adaptive reasoning. To reconcile this conflict, we propose \\textbf{VIGIL}, a framework that shifts the paradigm from restrictive isolation to a verify-before-commit protocol. By facilitating speculative hypothesis generation and enforcing safety through intent-grounded verification, \\textbf{VIGIL} preserves reasoning flexibility while ensuring robust control. We further introduce \\textbf{SIREN}, a benchmark comprising 959 tool stream injection cases designed to simulate pervasive threats characterized by dynamic dependencies. Extensive experiments demonstrate that \\textbf{VIGIL} outperforms state-of-the-art dynamic defenses by reducing the attack success rate by over 22\\% while more than doubling the utility under attack compared to static baselines, thereby achieving an optimal balance between security and utility. Code is available at https://anonymous.4open.science/r/VIGIL-378B/.",
      "authors": [
        "Junda Lin",
        "Zhaomeng Zhou",
        "Zhi Zheng",
        "Shuochen Liu",
        "Tong Xu",
        "Yong Chen",
        "Enhong Chen"
      ],
      "published": "2026-01-09T12:19:49Z",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "primary_category": "cs.CR",
      "abs_url": "https://arxiv.org/abs/2601.05755v1",
      "pdf_url": "https://arxiv.org/pdf/2601.05755v1.pdf"
    },
    {
      "id": "2601.05752v1",
      "title": "AutoMonitor-Bench: Evaluating the Reliability of LLM-Based Misbehavior Monitor",
      "abstract": "We introduce AutoMonitor-Bench, the first benchmark designed to systematically evaluate the reliability of LLM-based misbehavior monitors across diverse tasks and failure modes. AutoMonitor-Bench consists of 3,010 carefully annotated test samples spanning question answering, code generation, and reasoning, with paired misbehavior and benign instances. We evaluate monitors using two complementary metrics: Miss Rate (MR) and False Alarm Rate (FAR), capturing failures to detect misbehavior and oversensitivity to benign behavior, respectively. Evaluating 12 proprietary and 10 open-source LLMs, we observe substantial variability in monitoring performance and a consistent trade-off between MR and FAR, revealing an inherent safety-utility tension. To further explore the limits of monitor reliability, we construct a large-scale training corpus of 153,581 samples and fine-tune Qwen3-4B-Instruction to investigate whether training on known, relatively easy-to-construct misbehavior datasets improves monitoring performance on unseen and more implicit misbehaviors. Our results highlight the challenges of reliable, scalable misbehavior monitoring and motivate future work on task-aware designing and training strategies for LLM-based monitors.",
      "authors": [
        "Shu Yang",
        "Jingyu Hu",
        "Tong Li",
        "Hanqi Yan",
        "Wenxuan Wang",
        "Di Wang"
      ],
      "published": "2026-01-09T12:09:45Z",
      "categories": [
        "cs.CL",
        "cs.SE"
      ],
      "primary_category": "cs.CL",
      "abs_url": "https://arxiv.org/abs/2601.05752v1",
      "pdf_url": "https://arxiv.org/pdf/2601.05752v1.pdf"
    },
    {
      "id": "2601.05751v1",
      "title": "Analysing Differences in Persuasive Language in LLM-Generated Text: Uncovering Stereotypical Gender Patterns",
      "abstract": "Large language models (LLMs) are increasingly used for everyday communication tasks, including drafting interpersonal messages intended to influence and persuade. Prior work has shown that LLMs can successfully persuade humans and amplify persuasive language. It is therefore essential to understand how user instructions affect the generation of persuasive language, and to understand whether the generated persuasive language differs, for example, when targeting different groups. In this work, we propose a framework for evaluating how persuasive language generation is affected by recipient gender, sender intent, or output language. We evaluate 13 LLMs and 16 languages using pairwise prompt instructions. We evaluate model responses on 19 categories of persuasive language using an LLM-as-judge setup grounded in social psychology and communication science. Our results reveal significant gender differences in the persuasive language generated across all models. These patterns reflect biases consistent with gender-stereotypical linguistic tendencies documented in social psychology and sociolinguistics.",
      "authors": [
        "Amalie Brogaard Pauli",
        "Maria Barrett",
        "Max Müller-Eberstein",
        "Isabelle Augenstein",
        "Ira Assent"
      ],
      "published": "2026-01-09T12:07:38Z",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "abs_url": "https://arxiv.org/abs/2601.05751v1",
      "pdf_url": "https://arxiv.org/pdf/2601.05751v1.pdf"
    },
    {
      "id": "2601.05747v1",
      "title": "FlyPose: Towards Robust Human Pose Estimation From Aerial Views",
      "abstract": "Unmanned Aerial Vehicles (UAVs) are increasingly deployed in close proximity to humans for applications such as parcel delivery, traffic monitoring, disaster response and infrastructure inspections. Ensuring safe and reliable operation in these human-populated environments demands accurate perception of human poses and actions from an aerial viewpoint. This perspective challenges existing methods with low resolution, steep viewing angles and (self-)occlusion, especially if the application demands realtime feasibile models. We train and deploy FlyPose, a lightweight top-down human pose estimation pipeline for aerial imagery. Through multi-dataset training, we achieve an average improvement of 6.8 mAP in person detection across the test-sets of Manipal-UAV, VisDrone, HIT-UAV as well as our custom dataset. For 2D human pose estimation we report an improvement of 16.3 mAP on the challenging UAV-Human dataset. FlyPose runs with an inference latency of ~20 milliseconds including preprocessing on a Jetson Orin AGX Developer Kit and is deployed onboard a quadrotor UAV during flight experiments. We also publish FlyPose-104, a small but challenging aerial human pose estimation dataset, that includes manual annotations from difficult aerial perspectives: https://github.com/farooqhassaan/FlyPose.",
      "authors": [
        "Hassaan Farooq",
        "Marvin Brenner",
        "Peter St\\ütz"
      ],
      "published": "2026-01-09T12:01:36Z",
      "categories": [
        "cs.CV",
        "cs.RO"
      ],
      "primary_category": "cs.CV",
      "abs_url": "https://arxiv.org/abs/2601.05747v1",
      "pdf_url": "https://arxiv.org/pdf/2601.05747v1.pdf"
    },
    {
      "id": "2601.05746v1",
      "title": "DynaDebate: Breaking Homogeneity in Multi-Agent Debate with Dynamic Path Generation",
      "abstract": "Recent years have witnessed the rapid development of Large Language Model-based Multi-Agent Systems (MAS), which excel at collaborative decision-making and complex problem-solving. Recently, researchers have further investigated Multi-Agent Debate (MAD) frameworks, which enhance the reasoning and collaboration capabilities of MAS through information exchange and debate among multiple agents. However, existing approaches often rely on unguided initialization, causing agents to adopt identical reasoning paths that lead to the same errors. As a result, effective debate among agents is hindered, and the final outcome frequently degenerates into simple majority voting. To solve the above problem, in this paper, we introduce Dynamic Multi-Agent Debate (DynaDebate), which enhances the effectiveness of multi-agent debate through three key mechanisms: (1) Dynamic Path Generation and Allocation, which employs a dedicated Path Generation Agent to generate diverse and logical solution paths with adaptive redundancy; (2) Process-Centric Debate, which shifts the focus from surface-level outcome voting to rigorous step-by-step logic critique to ensure process correctness; (3) A Trigger-Based Verification Agent, which is activated upon disagreement and uses external tools to objectively resolve deadlocks. Extensive experiments demonstrate that DynaDebate achieves superior performance across various benchmarks, surpassing existing state-of-the-art MAD methods.",
      "authors": [
        "Zhenghao Li",
        "Zhi Zheng",
        "Wei Chen",
        "Jielun Zhao",
        "Yong Chen",
        "Tong Xu",
        "Enhong Chen"
      ],
      "published": "2026-01-09T12:01:33Z",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "abs_url": "https://arxiv.org/abs/2601.05746v1",
      "pdf_url": "https://arxiv.org/pdf/2601.05746v1.pdf"
    },
    {
      "id": "2601.05742v1",
      "title": "The Echo Chamber Multi-Turn LLM Jailbreak",
      "abstract": "The availability of Large Language Models (LLMs) has led to a new generation of powerful chatbots that can be developed at relatively low cost. As companies deploy these tools, security challenges need to be addressed to prevent financial loss and reputational damage. A key security challenge is jailbreaking, the malicious manipulation of prompts and inputs to bypass a chatbot's safety guardrails. Multi-turn attacks are a relatively new form of jailbreaking involving a carefully crafted chain of interactions with a chatbot. We introduce Echo Chamber, a new multi-turn attack using a gradual escalation method. We describe this attack in detail, compare it to other multi-turn attacks, and demonstrate its performance against multiple state-of-the-art models through extensive evaluation.",
      "authors": [
        "Ahmad Alobaid",
        "Martí Jordà Roca",
        "Carlos Castillo",
        "Joan Vendrell"
      ],
      "published": "2026-01-09T11:46:32Z",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "primary_category": "cs.CR",
      "abs_url": "https://arxiv.org/abs/2601.05742v1",
      "pdf_url": "https://arxiv.org/pdf/2601.05742v1.pdf"
    },
    {
      "id": "2601.05741v1",
      "title": "ViTNT-FIQA: Training-Free Face Image Quality Assessment with Vision Transformers",
      "abstract": "Face Image Quality Assessment (FIQA) is essential for reliable face recognition systems. Current approaches primarily exploit only final-layer representations, while training-free methods require multiple forward passes or backpropagation. We propose ViTNT-FIQA, a training-free approach that measures the stability of patch embedding evolution across intermediate Vision Transformer (ViT) blocks. We demonstrate that high-quality face images exhibit stable feature refinement trajectories across blocks, while degraded images show erratic transformations. Our method computes Euclidean distances between L2-normalized patch embeddings from consecutive transformer blocks and aggregates them into image-level quality scores. We empirically validate this correlation on a quality-labeled synthetic dataset with controlled degradation levels. Unlike existing training-free approaches, ViTNT-FIQA requires only a single forward pass without backpropagation or architectural modifications. Through extensive evaluation on eight benchmarks (LFW, AgeDB-30, CFP-FP, CALFW, Adience, CPLFW, XQLFW, IJB-C), we show that ViTNT-FIQA achieves competitive performance with state-of-the-art methods while maintaining computational efficiency and immediate applicability to any pre-trained ViT-based face recognition model.",
      "authors": [
        "Guray Ozgur",
        "Eduarda Caldeira",
        "Tahar Chettaoui",
        "Jan Niklas Kolf",
        "Marco Huber",
        "Naser Damer",
        "Fadi Boutros"
      ],
      "published": "2026-01-09T11:46:25Z",
      "categories": [
        "cs.CV",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "abs_url": "https://arxiv.org/abs/2601.05741v1",
      "pdf_url": "https://arxiv.org/pdf/2601.05741v1.pdf"
    },
    {
      "id": "2601.05739v1",
      "title": "PII-VisBench: Evaluating Personally Identifiable Information Safety in Vision Language Models Along a Continuum of Visibility",
      "abstract": "Vision Language Models (VLMs) are increasingly integrated into privacy-critical domains, yet existing evaluations of personally identifiable information (PII) leakage largely treat privacy as a static extraction task and ignore how a subject's online presence--the volume of their data available online--influences privacy alignment. We introduce PII-VisBench, a novel benchmark containing 4000 unique probes designed to evaluate VLM safety through the continuum of online presence. The benchmark stratifies 200 subjects into four visibility categories: high, medium, low, and zero--based on the extent and nature of their information available online. We evaluate 18 open-source VLMs (0.3B-32B) based on two key metrics: percentage of PII probing queries refused (Refusal Rate) and the fraction of non-refusal responses flagged for containing PII (Conditional PII Disclosure Rate). Across models, we observe a consistent pattern: refusals increase and PII disclosures decrease (9.10% high to 5.34% low) as subject visibility drops. We identify that models are more likely to disclose PII for high-visibility subjects, alongside substantial model-family heterogeneity and PII-type disparities. Finally, paraphrasing and jailbreak-style prompts expose attack and model-dependent failures, motivating visibility-aware safety evaluation and training interventions.",
      "authors": [
        "G M Shahariar",
        "Zabir Al Nazi",
        "Md Olid Hasan Bhuiyan",
        "Zhouxing Shi"
      ],
      "published": "2026-01-09T11:40:56Z",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.CR",
        "cs.CV"
      ],
      "primary_category": "cs.AI",
      "abs_url": "https://arxiv.org/abs/2601.05739v1",
      "pdf_url": "https://arxiv.org/pdf/2601.05739v1.pdf"
    },
    {
      "id": "2601.05738v1",
      "title": "FeatureSLAM: Feature-enriched 3D gaussian splatting SLAM in real time",
      "abstract": "We present a real-time tracking SLAM system that unifies efficient camera tracking with photorealistic feature-enriched mapping using 3D Gaussian Splatting (3DGS). Our main contribution is integrating dense feature rasterization into the novel-view synthesis, aligned with a visual foundation model. This yields strong semantics, going beyond basic RGB-D input, aiding both tracking and mapping accuracy. Unlike previous semantic SLAM approaches (which embed pre-defined class labels) FeatureSLAM enables entirely new downstream tasks via free-viewpoint, open-set segmentation. Across standard benchmarks, our method achieves real-time tracking, on par with state-of-the-art systems while improving tracking stability and map fidelity without prohibitive compute. Quantitatively, we obtain 9\\% lower pose error and 8\\% higher mapping accuracy compared to recent fixed-set SLAM baselines. Our results confirm that real-time feature-embedded SLAM, is not only valuable for enabling new downstream applications. It also improves the performance of the underlying tracking and mapping subsystems, providing semantic and language masking results that are on-par with offline 3DGS models, alongside state-of-the-art tracking, depth and RGB rendering.",
      "authors": [
        "Christopher Thirgood",
        "Oscar Mendez",
        "Erin Ling",
        "Jon Storey",
        "Simon Hadfield"
      ],
      "published": "2026-01-09T11:40:16Z",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "abs_url": "https://arxiv.org/abs/2601.05738v1",
      "pdf_url": "https://arxiv.org/pdf/2601.05738v1.pdf"
    },
    {
      "id": "2601.05732v1",
      "title": "mHC-lite: You Don't Need 20 Sinkhorn-Knopp Iterations",
      "abstract": "Hyper-Connections (HC) generalizes residual connections by introducing dynamic residual matrices that mix information across multiple residual streams, accelerating convergence in deep neural networks. However, unconstrained residual matrices can compromise training stability. To address this, DeepSeek's Manifold-Constrained Hyper-Connections (mHC) approximately projects these matrices onto the Birkhoff polytope via iterative Sinkhorn--Knopp (SK) normalization. We identify two limitations of this approach: (i) finite SK iterations do not guarantee exact doubly stochasticity, leaving an approximation gap that can accumulate through network depth and undermine stability; (ii) efficient SK implementation requires highly specialized CUDA kernels, raising engineering barriers and reducing portability. Motivated by the Birkhoff--von Neumann theorem, we propose mHC-lite, a simple reparameterization that explicitly constructs doubly stochastic matrices as convex combinations of permutation matrices. This approach guarantees exact doubly stochasticity by construction and can be implemented using only native matrix operations. Extensive experiments demonstrate that mHC-lite matches or exceeds mHC in performance while achieving higher training throughput with a naive implementation and eliminating the residual instabilities observed in both HC and mHC. The code is publicly available at https://github.com/FFTYYY/mhc-lite.",
      "authors": [
        "Yongyi Yang",
        "Jianyang Gao"
      ],
      "published": "2026-01-09T11:19:14Z",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "abs_url": "https://arxiv.org/abs/2601.05732v1",
      "pdf_url": "https://arxiv.org/pdf/2601.05732v1.pdf"
    },
    {
      "id": "2601.05729v1",
      "title": "TAGRPO: Boosting GRPO on Image-to-Video Generation with Direct Trajectory Alignment",
      "abstract": "Recent studies have demonstrated the efficacy of integrating Group Relative Policy Optimization (GRPO) into flow matching models, particularly for text-to-image and text-to-video generation. However, we find that directly applying these techniques to image-to-video (I2V) models often fails to yield consistent reward improvements. To address this limitation, we present TAGRPO, a robust post-training framework for I2V models inspired by contrastive learning. Our approach is grounded in the observation that rollout videos generated from identical initial noise provide superior guidance for optimization. Leveraging this insight, we propose a novel GRPO loss applied to intermediate latents, encouraging direct alignment with high-reward trajectories while maximizing distance from low-reward counterparts. Furthermore, we introduce a memory bank for rollout videos to enhance diversity and reduce computational overhead. Despite its simplicity, TAGRPO achieves significant improvements over DanceGRPO in I2V generation.",
      "authors": [
        "Jin Wang",
        "Jianxiang Lu",
        "Guangzheng Xu",
        "Comi Chen",
        "Haoyu Yang",
        "Linqing Wang",
        "Peng Chen",
        "Mingtao Chen",
        "Zhichao Hu",
        "Longhuang Wu",
        "Shuai Shao",
        "Qinglin Lu",
        "Ping Luo"
      ],
      "published": "2026-01-09T11:15:27Z",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "abs_url": "https://arxiv.org/abs/2601.05729v1",
      "pdf_url": "https://arxiv.org/pdf/2601.05729v1.pdf"
    },
    {
      "id": "2601.05724v1",
      "title": "Overcoming Joint Intractability with Lossless Hierarchical Speculative Decoding",
      "abstract": "Verification is a key bottleneck in improving inference speed while maintaining distribution fidelity in Speculative Decoding. Recent work has shown that sequence-level verification leads to a higher number of accepted tokens compared to token-wise verification. However, existing solutions often rely on surrogate approximations or are constrained by partial information, struggling with joint intractability. In this work, we propose Hierarchical Speculative Decoding (HSD), a provably lossless verification method that significantly boosts the expected number of accepted tokens and overcomes joint intractability by balancing excess and deficient probability mass across accessible branches. Our extensive large-scale experiments demonstrate that HSD yields consistent improvements in acceptance rates across diverse model families and benchmarks. Moreover, its strong explainability and generality make it readily integrable into a wide range of speculative decoding frameworks. Notably, integrating HSD into EAGLE-3 yields over a 12% performance gain, establishing state-of-the-art decoding efficiency without compromising distribution fidelity. Code is available at https://github.com/ZhouYuxuanYX/Hierarchical-Speculative-Decoding.",
      "authors": [
        "Yuxuan Zhou",
        "Fei Huang",
        "Heng Li",
        "Fengyi Wu",
        "Tianyu Wang",
        "Jianwei Zhang",
        "Junyang Lin",
        "Zhi-Qi Cheng"
      ],
      "published": "2026-01-09T11:10:29Z",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "abs_url": "https://arxiv.org/abs/2601.05724v1",
      "pdf_url": "https://arxiv.org/pdf/2601.05724v1.pdf"
    },
    {
      "id": "2601.05722v1",
      "title": "Rotate Your Character: Revisiting Video Diffusion Models for High-Quality 3D Character Generation",
      "abstract": "Generating high-quality 3D characters from single images remains a significant challenge in digital content creation, particularly due to complex body poses and self-occlusion. In this paper, we present RCM (Rotate your Character Model), an advanced image-to-video diffusion framework tailored for high-quality novel view synthesis (NVS) and 3D character generation. Compared to existing diffusion-based approaches, RCM offers several key advantages: (1) transferring characters with any complex poses into a canonical pose, enabling consistent novel view synthesis across the entire viewing orbit, (2) high-resolution orbital video generation at 1024x1024 resolution, (3) controllable observation positions given different initial camera poses, and (4) multi-view conditioning supporting up to 4 input images, accommodating diverse user scenarios. Extensive experiments demonstrate that RCM outperforms state-of-the-art methods in both novel view synthesis and 3D generation quality.",
      "authors": [
        "Jin Wang",
        "Jianxiang Lu",
        "Comi Chen",
        "Guangzheng Xu",
        "Haoyu Yang",
        "Peng Chen",
        "Na Zhang",
        "Yifan Xu",
        "Longhuang Wu",
        "Shuai Shao",
        "Qinglin Lu",
        "Ping Luo"
      ],
      "published": "2026-01-09T11:07:54Z",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "abs_url": "https://arxiv.org/abs/2601.05722v1",
      "pdf_url": "https://arxiv.org/pdf/2601.05722v1.pdf"
    },
    {
      "id": "2601.05713v1",
      "title": "Visualising Information Flow in Word Embeddings with Diffusion Tensor Imaging",
      "abstract": "Understanding how large language models (LLMs) represent natural language is a central challenge in natural language processing (NLP) research. Many existing methods extract word embeddings from an LLM, visualise the embedding space via point-plots, and compare the relative positions of certain words. However, this approach only considers single words and not whole natural language expressions, thus disregards the context in which a word is used. Here we present a novel tool for analysing and visualising information flow in natural language expressions by applying diffusion tensor imaging (DTI) to word embeddings. We find that DTI reveals how information flows between word embeddings. Tracking information flows within the layers of an LLM allows for comparing different model structures and revealing opportunities for pruning an LLM's under-utilised layers. Furthermore, our model reveals differences in information flows for tasks like pronoun resolution and metaphor detection. Our results show that our model permits novel insights into how LLMs represent actual natural language expressions, extending the comparison of isolated word embeddings and improving the interpretability of NLP models.",
      "authors": [
        "Thomas Fabian"
      ],
      "published": "2026-01-09T10:58:17Z",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "abs_url": "https://arxiv.org/abs/2601.05713v1",
      "pdf_url": "https://arxiv.org/pdf/2601.05713v1.pdf"
    },
    {
      "id": "2601.05707v1",
      "title": "Multimodal In-context Learning for ASR of Low-resource Languages",
      "abstract": "Automatic speech recognition (ASR) still covers only a small fraction of the world's languages, mainly due to supervised data scarcity. In-context learning (ICL) with large language models (LLMs) addresses this problem, but prior work largely focuses on high-resource languages covered during training and text-only settings. This paper investigates whether speech LLMs can learn unseen languages with multimodal ICL (MICL), and how this learning can be used to improve ASR. We conduct experiments with two speech LLMs, Phi-4 and Qwen3-Omni, on three diverse endangered languages. Firstly, we find that MICL is effective for unseen languages, leveraging both speech and text modalities. We further show that cross-lingual transfer learning improves MICL efficiency on target languages without training on them. Moreover, we analyze attention patterns to interpret MICL mechanisms, and we observe layer-dependent preferences between audio and text context, with an overall bias towards text. Finally, we show that prompt-based ASR with speech LLMs performs poorly on unseen languages, motivating a simple ASR system that combines a stronger acoustic model with a speech LLM via MICL-based selection of acoustic hypotheses. Results show that MICL consistently improves ASR performance, and that cross-lingual transfer learning matches or outperforms corpus-trained language models without using target-language data. Our code is publicly available.",
      "authors": [
        "Zhaolin Li",
        "Jan Niehues"
      ],
      "published": "2026-01-09T10:52:23Z",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "abs_url": "https://arxiv.org/abs/2601.05707v1",
      "pdf_url": "https://arxiv.org/pdf/2601.05707v1.pdf"
    },
    {
      "id": "2601.05705v1",
      "title": "Logic-Parametric Neuro-Symbolic NLI: Controlling Logical Formalisms for Verifiable LLM Reasoning",
      "abstract": "Large language models (LLMs) and theorem provers (TPs) can be effectively combined for verifiable natural language inference (NLI). However, existing approaches rely on a fixed logical formalism, a feature that limits robustness and adaptability. We propose a logic-parametric framework for neuro-symbolic NLI that treats the underlying logic not as a static background, but as a controllable component. Using the LogiKEy methodology, we embed a range of classical and non-classical formalisms into higher-order logic (HOL), enabling a systematic comparison of inference quality, explanation refinement, and proof behavior. We focus on normative reasoning, where the choice of logic has significant implications. In particular, we compare logic-external approaches, where normative requirements are encoded via axioms, with logic-internal approaches, where normative patterns emerge from the logic's built-in structure. Extensive experiments demonstrate that logic-internal strategies can consistently improve performance and produce more efficient hybrid proofs for NLI. In addition, we show that the effectiveness of a logic is domain-dependent, with first-order logic favouring commonsense reasoning, while deontic and modal logics excel in ethical domains. Our results highlight the value of making logic a first-class, parametric element in neuro-symbolic architectures for more robust, modular, and adaptable reasoning.",
      "authors": [
        "Ali Farjami",
        "Luca Redondi",
        "Marco Valentino"
      ],
      "published": "2026-01-09T10:47:30Z",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.LO"
      ],
      "primary_category": "cs.AI",
      "abs_url": "https://arxiv.org/abs/2601.05705v1",
      "pdf_url": "https://arxiv.org/pdf/2601.05705v1.pdf"
    },
    {
      "id": "2601.05703v1",
      "title": "AIBoMGen: Generating an AI Bill of Materials for Secure, Transparent, and Compliant Model Training",
      "abstract": "The rapid adoption of complex AI systems has outpaced the development of tools to ensure their transparency, security, and regulatory compliance. In this paper, the AI Bill of Materials (AIBOM), an extension of the Software Bill of Materials (SBOM), is introduced as a standardized, verifiable record of trained AI models and their environments. Our proof-of-concept platform, AIBoMGen, automates the generation of signed AIBOMs by capturing datasets, model metadata, and environment details during training. The training platform acts as a neutral, third-party observer and root of trust. It enforces verifiable AIBOM creation for every job. The system uses cryptographic hashing, digital signatures, and in-toto attestations to ensure integrity and protect against threats such as artifact tampering by dishonest model creators. Our evaluation demonstrates that AIBoMGen reliably detects unauthorized modifications to all artifacts and can generate AIBOMs with negligible performance overhead. These results highlight the potential of AIBoMGen as a foundational step toward building secure and transparent AI ecosystems, enabling compliance with regulatory frameworks like the EUs AI Act.",
      "authors": [
        "Wiebe Vandendriessche",
        "Jordi Thijsman",
        "Laurens D'hooge",
        "Bruno Volckaert",
        "Merlijn Sebrechts"
      ],
      "published": "2026-01-09T10:46:42Z",
      "categories": [
        "cs.SE",
        "cs.AI",
        "cs.CR"
      ],
      "primary_category": "cs.SE",
      "abs_url": "https://arxiv.org/abs/2601.05703v1",
      "pdf_url": "https://arxiv.org/pdf/2601.05703v1.pdf"
    },
    {
      "id": "2601.05699v1",
      "title": "Afri-MCQA: Multimodal Cultural Question Answering for African Languages",
      "abstract": "Africa is home to over one-third of the world's languages, yet remains underrepresented in AI research. We introduce Afri-MCQA, the first Multilingual Cultural Question-Answering benchmark covering 7.5k Q&A pairs across 15 African languages from 12 countries. The benchmark offers parallel English-African language Q&A pairs across text and speech modalities and was entirely created by native speakers. Benchmarking large language models (LLMs) on Afri-MCQA shows that open-weight models perform poorly across evaluated cultures, with near-zero accuracy on open-ended VQA when queried in native language or speech. To evaluate linguistic competence, we include control experiments meant to assess this specific aspect separate from cultural knowledge, and we observe significant performance gaps between native languages and English for both text and speech. These findings underscore the need for speech-first approaches, culturally grounded pretraining, and cross-lingual cultural transfer. To support more inclusive multimodal AI development in African languages, we release our Afri-MCQA under academic license or CC BY-NC 4.0 on HuggingFace (https://huggingface.co/datasets/Atnafu/Afri-MCQA)",
      "authors": [
        "Atnafu Lambebo Tonja",
        "Srija Anand",
        "Emilio Villa-Cueva",
        "Israel Abebe Azime",
        "Jesujoba Oluwadara Alabi",
        "Muhidin A. Mohamed",
        "Debela Desalegn Yadeta",
        "Negasi Haile Abadi",
        "Abigail Oppong",
        "Nnaemeka Casmir Obiefuna",
        "Idris Abdulmumin",
        "Naome A Etori",
        "Eric Peter Wairagala",
        "Kanda Patrick Tshinu",
        "Imanigirimbabazi Emmanuel",
        "Gabofetswe Malema",
        "Alham Fikri Aji",
        "David Ifeoluwa Adelani",
        "Thamar Solorio"
      ],
      "published": "2026-01-09T10:40:09Z",
      "categories": [
        "cs.CL"
      ],
      "primary_category": "cs.CL",
      "abs_url": "https://arxiv.org/abs/2601.05699v1",
      "pdf_url": "https://arxiv.org/pdf/2601.05699v1.pdf"
    },
    {
      "id": "2601.05693v1",
      "title": "Circular Reasoning: Understanding Self-Reinforcing Loops in Large Reasoning Models",
      "abstract": "Despite the success of test-time scaling, Large Reasoning Models (LRMs) frequently encounter repetitive loops that lead to computational waste and inference failure. In this paper, we identify a distinct failure mode termed Circular Reasoning. Unlike traditional model degeneration, this phenomenon manifests as a self-reinforcing trap where generated content acts as a logical premise for its own recurrence, compelling the reiteration of preceding text. To systematically analyze this phenomenon, we introduce LoopBench, a dataset designed to capture two distinct loop typologies: numerical loops and statement loops. Mechanistically, we characterize circular reasoning as a state collapse exhibiting distinct boundaries, where semantic repetition precedes textual repetition. We reveal that reasoning impasses trigger the loop onset, which subsequently persists as an inescapable cycle driven by a self-reinforcing V-shaped attention mechanism. Guided by these findings, we employ the Cumulative Sum (CUSUM) algorithm to capture these precursors for early loop prediction. Experiments across diverse LRMs validate its accuracy and elucidate the stability of long-chain reasoning.",
      "authors": [
        "Zenghao Duan",
        "Liang Pang",
        "Zihao Wei",
        "Wenbin Duan",
        "Yuxin Tian",
        "Shicheng Xu",
        "Jingcheng Deng",
        "Zhiyi Yin",
        "Xueqi Cheng"
      ],
      "published": "2026-01-09T10:23:55Z",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "abs_url": "https://arxiv.org/abs/2601.05693v1",
      "pdf_url": "https://arxiv.org/pdf/2601.05693v1.pdf"
    },
    {
      "id": "2601.05688v1",
      "title": "SketchVL: Policy Optimization via Fine-Grained Credit Assignment for Chart Understanding and More",
      "abstract": "Charts are high-density visual carriers of complex data and medium for information extraction and analysis. Due to the need for precise and complex visual reasoning, automated chart understanding poses a significant challenge to existing Multimodal Large Language Models (MLLMs). Many MLLMs trained with reinforcement learning (RL) face the challenge of credit assignment. Their advantage estimation, typically performed at the trajectory level, cannot distinguish between correct and incorrect reasoning steps within a single generated response. To address this limitation, we introduce SketchVL, a novel MLLM that optimized with FinePO, a new RL algorithm designed for fine-grained credit assignment within each trajectory. SketchVL's methodology involves drawing its intermediate reasoning steps as markers on the image and feeding the annotated image back to itself, creating a robust, multi-step reasoning process. During training, the FinePO algorithm leverages a Fine-grained Process Reward Model (FinePRM) to score each drawing action within a trajectory, thereby precisely assigning credit for each step. This mechanism allows FinePO to more strongly reward correct tokens when a trajectory is globally successful, and more heavily penalize incorrect tokens when the trajectory is globally suboptimal, thus achieving fine-grained reinforcement signals. Experiments show that SketchVL learns to align its step-level behavior with the FinePRM, achieving an average performance gain of 7.23\\% over its base model across chart datasets, natural image datasets, and mathematics, providing a promising new direction for training powerful reasoning models.",
      "authors": [
        "Muye Huang",
        "Lingling Zhang",
        "Yifei Li",
        "Yaqiang Wu",
        "Jun Liu"
      ],
      "published": "2026-01-09T10:13:01Z",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "abs_url": "https://arxiv.org/abs/2601.05688v1",
      "pdf_url": "https://arxiv.org/pdf/2601.05688v1.pdf"
    },
    {
      "id": "2601.05684v1",
      "title": "FLRQ: Faster LLM Quantization with Flexible Low-Rank Matrix Sketching",
      "abstract": "Traditional post-training quantization (PTQ) is considered an effective approach to reduce model size and accelerate inference of large-scale language models (LLMs). However, existing low-rank PTQ methods require costly fine-tuning to determine a compromise rank for diverse data and layers in large models, failing to exploit their full potential. Additionally, the current SVD-based low-rank approximation compounds the computational overhead. In this work, we thoroughly analyze the varying effectiveness of low-rank approximation across different layers in representative models. Accordingly, we introduce \\underline{F}lexible \\underline{L}ow-\\underline{R}ank \\underline{Q}uantization (FLRQ), a novel solution designed to quickly identify the accuracy-optimal ranks and aggregate them to achieve minimal storage combinations. FLRQ comprises two powerful components, Rank1-Sketch-based Flexible Rank Selection (R1-FLR) and Best Low-rank Approximation under Clipping (BLC). R1-FLR applies the R1-Sketch with Gaussian projection for the fast low-rank approximation, enabling outlier-aware rank extraction for each layer. Meanwhile, BLC aims at minimizing the low-rank quantization error under the scaling and clipping strategy through an iterative method. FLRQ demonstrates strong effectiveness and robustness in comprehensive experiments, achieving state-of-the-art performance in both quantization quality and algorithm efficiency.",
      "authors": [
        "Hongyaoxing Gul",
        "Lijuan Hu",
        "Shuzi Niu",
        "Fangfang Liu"
      ],
      "published": "2026-01-09T10:06:45Z",
      "categories": [
        "cs.LG"
      ],
      "primary_category": "cs.LG",
      "abs_url": "https://arxiv.org/abs/2601.05684v1",
      "pdf_url": "https://arxiv.org/pdf/2601.05684v1.pdf"
    },
    {
      "id": "2601.05683v1",
      "title": "Joint Optimization of Neural Autoregressors via Scoring rules",
      "abstract": "Non-parametric distributional regression has achieved significant milestones in recent years. Among these, the Tabular Prior-Data Fitted Network (TabPFN) has demonstrated state-of-the-art performance on various benchmarks. However, a challenge remains in extending these grid-based approaches to a truly multivariate setting. In a naive non-parametric discretization with $N$ bins per dimension, the complexity of an explicit joint grid scales exponentially and the paramer count of the neural networks rise sharply. This scaling is particularly detrimental in low-data regimes, as the final projection layer would require many parameters, leading to severe overfitting and intractability.",
      "authors": [
        "Jonas Landsgesell"
      ],
      "published": "2026-01-09T10:05:07Z",
      "categories": [
        "cond-mat.soft",
        "cs.AI"
      ],
      "primary_category": "cond-mat.soft",
      "abs_url": "https://arxiv.org/abs/2601.05683v1",
      "pdf_url": "https://arxiv.org/pdf/2601.05683v1.pdf"
    },
    {
      "id": "2601.05680v1",
      "title": "AGDC: Autoregressive Generation of Variable-Length Sequences with Joint Discrete and Continuous Spaces",
      "abstract": "Transformer-based autoregressive models excel in data generation but are inherently constrained by their reliance on discretized tokens, which limits their ability to represent continuous values with high precision. We analyze the scalability limitations of existing discretization-based approaches for generating hybrid discrete-continuous sequences, particularly in high-precision domains such as semiconductor circuit designs, where precision loss can lead to functional failure. To address the challenge, we propose AGDC, a novel unified framework that jointly models discrete and continuous values for variable-length sequences. AGDC employs a hybrid approach that combines categorical prediction for discrete values with diffusion-based modeling for continuous values, incorporating two key technical components: an end-of-sequence (EOS) logit adjustment mechanism that uses an MLP to dynamically adjust EOS token logits based on sequence context, and a length regularization term integrated into the loss function. Additionally, we present ContLayNet, a large-scale benchmark comprising 334K high-precision semiconductor layout samples with specialized evaluation metrics that capture functional correctness where precision errors significantly impact performance. Experiments on semiconductor layouts (ContLayNet), graphic layouts, and SVGs demonstrate AGDC's superior performance in generating high-fidelity hybrid vector representations compared to discretization-based and fixed-schema baselines, achieving scalable high-precision generation across diverse domains.",
      "authors": [
        "Yeonsang Shin",
        "Insoo Kim",
        "Bongkeun Kim",
        "Keonwoo Bae",
        "Bohyung Han"
      ],
      "published": "2026-01-09T09:57:12Z",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.LG",
      "abs_url": "https://arxiv.org/abs/2601.05680v1",
      "pdf_url": "https://arxiv.org/pdf/2601.05680v1.pdf"
    },
    {
      "id": "2601.05679v1",
      "title": "Do Sparse Autoencoders Identify Reasoning Features in Language Models?",
      "abstract": "We investigate whether sparse autoencoders (SAEs) identify genuine reasoning features in large language models (LLMs). Starting from features selected using standard contrastive activation methods, we introduce a falsification-oriented framework that combines causal token injection experiments and LLM-guided falsification to test whether feature activation reflects reasoning processes or superficial linguistic correlates. Across 20 configurations spanning multiple model families, layers, and reasoning datasets, we find that identified reasoning features are highly sensitive to token-level interventions. Injecting a small number of feature-associated tokens into non-reasoning text is sufficient to elicit strong activation for 59% to 94% of features, indicating reliance on lexical artifacts. For the remaining features that are not explained by simple token triggers, LLM-guided falsification consistently produces non-reasoning inputs that activate the feature and reasoning inputs that do not, with no analyzed feature satisfying our criteria for genuine reasoning behavior. Steering these features yields minimal changes or slight degradations in benchmark performance. Together, these results suggest that SAE features identified by contrastive approaches primarily capture linguistic correlates of reasoning rather than the underlying reasoning computations themselves.",
      "authors": [
        "George Ma",
        "Zhongyuan Liang",
        "Irene Y. Chen",
        "Somayeh Sojoudi"
      ],
      "published": "2026-01-09T09:54:36Z",
      "categories": [
        "cs.LG"
      ],
      "primary_category": "cs.LG",
      "abs_url": "https://arxiv.org/abs/2601.05679v1",
      "pdf_url": "https://arxiv.org/pdf/2601.05679v1.pdf"
    },
    {
      "id": "2601.05675v1",
      "title": "CHDP: Cooperative Hybrid Diffusion Policies for Reinforcement Learning in Parameterized Action Space",
      "abstract": "Hybrid action space, which combines discrete choices and continuous parameters, is prevalent in domains such as robot control and game AI. However, efficiently modeling and optimizing hybrid discrete-continuous action space remains a fundamental challenge, mainly due to limited policy expressiveness and poor scalability in high-dimensional settings. To address this challenge, we view the hybrid action space problem as a fully cooperative game and propose a \\textbf{Cooperative Hybrid Diffusion Policies (CHDP)} framework to solve it. CHDP employs two cooperative agents that leverage a discrete and a continuous diffusion policy, respectively. The continuous policy is conditioned on the discrete action's representation, explicitly modeling the dependency between them. This cooperative design allows the diffusion policies to leverage their expressiveness to capture complex distributions in their respective action spaces. To mitigate the update conflicts arising from simultaneous policy updates in this cooperative setting, we employ a sequential update scheme that fosters co-adaptation. Moreover, to improve scalability when learning in high-dimensional discrete action space, we construct a codebook that embeds the action space into a low-dimensional latent space. This mapping enables the discrete policy to learn in a compact, structured space. Finally, we design a Q-function-based guidance mechanism to align the codebook's embeddings with the discrete policy's representation during training. On challenging hybrid action benchmarks, CHDP outperforms the state-of-the-art method by up to $19.3\\%$ in success rate.",
      "authors": [
        "Bingyi Liu",
        "Jinbo He",
        "Haiyong Shi",
        "Enshu Wang",
        "Weizhen Han",
        "Jingxiang Hao",
        "Peixi Wang",
        "Zhuangzhuang Zhang"
      ],
      "published": "2026-01-09T09:50:47Z",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "abs_url": "https://arxiv.org/abs/2601.05675v1",
      "pdf_url": "https://arxiv.org/pdf/2601.05675v1.pdf"
    },
    {
      "id": "2601.05666v1",
      "title": "Advancing credit mobility through stakeholder-informed AI design and adoption",
      "abstract": "Transferring from a 2-year to a 4-year college is crucial for socioeconomic mobility, yet students often face challenges ensuring their credits are fully recognized, leading to delays in their academic progress and unexpected costs. Determining whether courses at different institutions are equivalent (i.e., articulation) is essential for successful credit transfer, as it minimizes unused credits and increases the likelihood of bachelor's degree completion. However, establishing articulation agreements remains time- and resource-intensive, as all candidate articulations are reviewed manually. Although recent efforts have explored the use of artificial intelligence to support this work, its use in articulation practice remains limited. Given these challenges and the need for scalable support, this study applies artificial intelligence to suggest articulations between institutions in collaboration with the State University of New York system, one of the largest systems of higher education in the US. To develop our methodology, we first surveyed articulation staff and faculty to assess adoption rates of baseline algorithmic recommendations and gather feedback on perceptions and concerns about these recommendations. Building on these insights, we developed a supervised alignment method that addresses superficial matching and institutional biases in catalog descriptions, achieving a 5.5-fold improvement in accuracy over previous methods. Based on articulation predictions of this method and a 61% average surveyed adoption rate among faculty and staff, these findings project a 12-fold increase in valid credit mobility opportunities that would otherwise remain unrealized. This study suggests that stakeholder-informed design of AI in higher education administration can expand student credit mobility and help reshape current institutional decision-making in course articulation.",
      "authors": [
        "Yerin Kwak",
        "Siddharth Adelkar",
        "Zachary A. Pardos"
      ],
      "published": "2026-01-09T09:39:12Z",
      "categories": [
        "cs.HC",
        "cs.AI",
        "cs.CY"
      ],
      "primary_category": "cs.HC",
      "abs_url": "https://arxiv.org/abs/2601.05666v1",
      "pdf_url": "https://arxiv.org/pdf/2601.05666v1.pdf"
    },
    {
      "id": "2601.05663v1",
      "title": "Tracing Stereotypes in Pre-trained Transformers: From Biased Neurons to Fairer Models",
      "abstract": "The advent of transformer-based language models has reshaped how AI systems process and generate text. In software engineering (SE), these models now support diverse activities, accelerating automation and decision-making. Yet, evidence shows that these models can reproduce or amplify social biases, raising fairness concerns. Recent work on neuron editing has shown that internal activations in pre-trained transformers can be traced and modified to alter model behavior. Building on the concept of knowledge neurons, neurons that encode factual information, we hypothesize the existence of biased neurons that capture stereotypical associations within pre-trained transformers. To test this hypothesis, we build a dataset of biased relations, i.e., triplets encoding stereotypes across nine bias types, and adapt neuron attribution strategies to trace and suppress biased neurons in BERT models. We then assess the impact of suppression on SE tasks. Our findings show that biased knowledge is localized within small neuron subsets, and suppressing them substantially reduces bias with minimal performance loss. This demonstrates that bias in transformers can be traced and mitigated at the neuron level, offering an interpretable approach to fairness in SE.",
      "authors": [
        "Gianmario Voria",
        "Moses Openja",
        "Foutse Khomh",
        "Gemma Catolino",
        "Fabio Palomba"
      ],
      "published": "2026-01-09T09:33:51Z",
      "categories": [
        "cs.SE",
        "cs.LG"
      ],
      "primary_category": "cs.SE",
      "abs_url": "https://arxiv.org/abs/2601.05663v1",
      "pdf_url": "https://arxiv.org/pdf/2601.05663v1.pdf"
    },
    {
      "id": "2601.05661v1",
      "title": "Motion Compensation for Real Time Ultrasound Scanning in Robotically Assisted Prostate Biopsy Procedures",
      "abstract": "Prostate cancer is one of the most common types of cancer in men. Its diagnosis by biopsy requires a high level of expertise and precision from the surgeon, so the results are highly operator-dependent. The aim of this work is to develop a robotic system for assisted ultrasound (US) examination of the prostate, a prebiopsy step that could reduce the dexterity requirements and enable faster, more accurate and more available prostate biopsy. We developed and validated a laboratory setup with a collaborative robotic arm that can autonomously scan a prostate phantom and attached the phantom to a medical robotic arm that mimics the patient's movements. The scanning robot keeps the relative position of the US probe and the prostate constant, ensuring a consistent and robust approach to reconstructing the prostate. To reconstruct the prostate, each slice is segmented to generate a series of prostate contours converted into a 3D point cloud used for biopsy planning. The average scan time of the prostate was 30 s, and the average 3D reconstruction of the prostate took 3 s. We performed four motion scenarios: the phantom was scanned in a stationary state (S), with horizontal motion (H), with vertical motion (V), and with a combination of the two (C). System validation is performed by registering the prostate point cloud reconstructions acquired during different motions (H, V, C) with those obtained in the stationary state. ICP registration with a threshold of 0.8 mm yields mean 83.2\\% fitness and 0.35 mm RMSE for S-H registration, 84.1\\% fitness and 0.37 mm RMSE for S-V registration and 79.4\\% fitness and 0.37 mm RMSE for S-C registration. Due to the elastic and soft material properties of the prostate phantom, the maximum robot tracking error was 3 mm, which can be sufficient for prostate biopsy according to medical literature. The maximum delay in motion compensation was 0.5 s.",
      "authors": [
        "Matija Markulin",
        "Luka Matijević",
        "Luka Siktar",
        "Janko Jurdana",
        "Branimir Caran",
        "Marko Švaco",
        "Filip Šuligoj",
        "Bojan Šekoranja"
      ],
      "published": "2026-01-09T09:30:55Z",
      "categories": [
        "cs.RO"
      ],
      "primary_category": "cs.RO",
      "abs_url": "https://arxiv.org/abs/2601.05661v1",
      "pdf_url": "https://arxiv.org/pdf/2601.05661v1.pdf"
    },
    {
      "id": "2601.05657v1",
      "title": "Stephanie2: Thinking, Waiting, and Making Decisions Like Humans in Step-by-Step AI Social Chat",
      "abstract": "Instant-messaging human social chat typically progresses through a sequence of short messages. Existing step-by-step AI chatting systems typically split a one-shot generation into multiple messages and send them sequentially, but they lack an active waiting mechanism and exhibit unnatural message pacing. In order to address these issues, we propose Stephanie2, a novel next-generation step-wise decision-making dialogue agent. With active waiting and message-pace adaptation, Stephanie2 explicitly decides at each step whether to send or wait, and models latency as the sum of thinking time and typing time to achieve more natural pacing. We further introduce a time-window-based dual-agent dialogue system to generate pseudo dialogue histories for human and automatic evaluations. Experiments show that Stephanie2 clearly outperforms Stephanie1 on metrics such as naturalness and engagement, and achieves a higher pass rate on human evaluation with the role identification Turing test.",
      "authors": [
        "Hao Yang",
        "Hongyuan Lu",
        "Dingkang Yang",
        "Wenliang Yang",
        "Peng Sun",
        "Xiaochuan Zhang",
        "Jun Xiao",
        "Kefan He",
        "Wai Lam",
        "Yang Liu",
        "Xinhua Zeng"
      ],
      "published": "2026-01-09T09:27:17Z",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "abs_url": "https://arxiv.org/abs/2601.05657v1",
      "pdf_url": "https://arxiv.org/pdf/2601.05657v1.pdf"
    },
    {
      "id": "2601.05656v1",
      "title": "HAG: Hierarchical Demographic Tree-based Agent Generation for Topic-Adaptive Simulation",
      "abstract": "High-fidelity agent initialization is crucial for credible Agent-Based Modeling across diverse domains. A robust framework should be Topic-Adaptive, capturing macro-level joint distributions while ensuring micro-level individual rationality. Existing approaches fall into two categories: static data-based retrieval methods that fail to adapt to unseen topics absent from the data, and LLM-based generation methods that lack macro-level distribution awareness, resulting in inconsistencies between micro-level persona attributes and reality. To address these problems, we propose HAG, a Hierarchical Agent Generation framework that formalizes population generation as a two-stage decision process. Firstly, utilizing a World Knowledge Model to infer hierarchical conditional probabilities to construct the Topic-Adaptive Tree, achieving macro-level distribution alignment. Then, grounded real-world data, instantiation and agentic augmentation are carried out to ensure micro-level consistency. Given the lack of specialized evaluation, we establish a multi-domain benchmark and a comprehensive PACE evaluation framework. Extensive experiments show that HAG significantly outperforms representative baselines, reducing population alignment errors by an average of 37.7% and enhancing sociological consistency by 18.8%.",
      "authors": [
        "Rongxin Chen",
        "Tianyu Wu",
        "Bingbing Xu",
        "Xiucheng Xu",
        "Huawei Shen"
      ],
      "published": "2026-01-09T09:26:08Z",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "abs_url": "https://arxiv.org/abs/2601.05656v1",
      "pdf_url": "https://arxiv.org/pdf/2601.05656v1.pdf"
    },
    {
      "id": "2601.05654v1",
      "title": "A Framework for Personalized Persuasiveness Prediction via Context-Aware User Profiling",
      "abstract": "Estimating the persuasiveness of messages is critical in various applications, from recommender systems to safety assessment of LLMs. While it is imperative to consider the target persuadee's characteristics, such as their values, experiences, and reasoning styles, there is currently no established systematic framework to optimize leveraging a persuadee's past activities (e.g., conversations) to the benefit of a persuasiveness prediction model. To address this problem, we propose a context-aware user profiling framework with two trainable components: a query generator that generates optimal queries to retrieve persuasion-relevant records from a user's history, and a profiler that summarizes these records into a profile to effectively inform the persuasiveness prediction model. Our evaluation on the ChangeMyView Reddit dataset shows consistent improvements over existing methods across multiple predictor models, with gains of up to +13.77%p in F1 score. Further analysis shows that effective user profiles are context-dependent and predictor-specific, rather than relying on static attributes or surface-level similarity. Together, these results highlight the importance of task-oriented, context-dependent user profiling for personalized persuasiveness prediction.",
      "authors": [
        "Sejun Park",
        "Yoonah Park",
        "Jongwon Lim",
        "Yohan Jo"
      ],
      "published": "2026-01-09T09:22:31Z",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "abs_url": "https://arxiv.org/abs/2601.05654v1",
      "pdf_url": "https://arxiv.org/pdf/2601.05654v1.pdf"
    },
    {
      "id": "2601.05653v1",
      "title": "EvoQRE: Modeling Bounded Rationality in Safety-Critical Traffic Simulation via Evolutionary Quantal Response Equilibrium",
      "abstract": "Existing traffic simulation frameworks for autonomous vehicles typically rely on imitation learning or game-theoretic approaches that solve for Nash or coarse correlated equilibria, implicitly assuming perfectly rational agents. However, human drivers exhibit bounded rationality, making approximately optimal decisions under cognitive and perceptual constraints. We propose EvoQRE, a principled framework for modeling safety-critical traffic interactions as general-sum Markov games solved via Quantal Response Equilibrium (QRE) and evolutionary game dynamics. EvoQRE integrates a pre-trained generative world model with entropy-regularized replicator dynamics, capturing stochastic human behavior while maintaining equilibrium structure. We provide rigorous theoretical results, proving that the proposed dynamics converge to Logit-QRE under a two-timescale stochastic approximation with an explicit convergence rate of O(log k / k^{1/3}) under weak monotonicity assumptions. We further extend QRE to continuous action spaces using mixture-based and energy-based policy representations. Experiments on the Waymo Open Motion Dataset and nuPlan benchmark demonstrate that EvoQRE achieves state-of-the-art realism, improved safety metrics, and controllable generation of diverse safety-critical scenarios through interpretable rationality parameters.",
      "authors": [
        "Phu-Hoa Pham",
        "Chi-Nguyen Tran",
        "Duy-Minh Dao-Sy",
        "Phu-Quy Nguyen-Lam",
        "Trung-Kiet Huynh"
      ],
      "published": "2026-01-09T09:21:36Z",
      "categories": [
        "cs.RO",
        "cs.MA"
      ],
      "primary_category": "cs.RO",
      "abs_url": "https://arxiv.org/abs/2601.05653v1",
      "pdf_url": "https://arxiv.org/pdf/2601.05653v1.pdf"
    },
    {
      "id": "2601.05650v1",
      "title": "From Global to Local: Cluster-Aware Learning for Wi-Fi Fingerprinting Indoor Localisation",
      "abstract": "Wi-Fi fingerprinting remains one of the most practical solutions for indoor positioning, however, its performance is often limited by the size and heterogeneity of fingerprint datasets, strong Received Signal Strength Indicator variability, and the ambiguity introduced in large and multi-floor environments. These factors significantly degrade localisation accuracy, particularly when global models are applied without considering structural constraints. This paper introduces a clustering-based method that structures the fingerprint dataset prior to localisation. Fingerprints are grouped using either spatial or radio features, and clustering can be applied at the building or floor level. In the localisation phase, a clustering estimation procedure based on the strongest access points assigns unseen fingerprints to the most relevant cluster. Localisation is then performed only within the selected clusters, allowing learning models to operate on reduced and more coherent subsets of data. The effectiveness of the method is evaluated on three public datasets and several machine learning models. Results show a consistent reduction in localisation errors, particularly under building-level strategies, but at the cost of reducing the floor detection accuracy. These results demonstrate that explicitly structuring datasets through clustering is an effective and flexible approach for scalable indoor positioning.",
      "authors": [
        "Miguel Matey-Sanz",
        "Joaquín Torres-Sospedra",
        "Joaquín Huerta",
        "Sergio Trilles"
      ],
      "published": "2026-01-09T09:12:40Z",
      "categories": [
        "cs.LG"
      ],
      "primary_category": "cs.LG",
      "abs_url": "https://arxiv.org/abs/2601.05650v1",
      "pdf_url": "https://arxiv.org/pdf/2601.05650v1.pdf"
    },
    {
      "id": "2601.05648v1",
      "title": "Open World Knowledge Aided Single-Cell Foundation Model with Robust Cross-Modal Cell-Language Pre-training",
      "abstract": "Recent advancements in single-cell multi-omics, particularly RNA-seq, have provided profound insights into cellular heterogeneity and gene regulation. While pre-trained language model (PLM) paradigm based single-cell foundation models have shown promise, they remain constrained by insufficient integration of in-depth individual profiles and neglecting the influence of noise within multi-modal data. To address both issues, we propose an Open-world Language Knowledge-Aided Robust Single-Cell Foundation Model (OKR-CELL). It is built based on a cross-modal Cell-Language pre-training framework, which comprises two key innovations: (1) leveraging Large Language Models (LLMs) based workflow with retrieval-augmented generation (RAG) enriches cell textual descriptions using open-world knowledge; (2) devising a Cross-modal Robust Alignment (CRA) objective that incorporates sample reliability assessment, curriculum learning, and coupled momentum contrastive learning to strengthen the model's resistance to noisy data. After pretraining on 32M cell-text pairs, OKR-CELL obtains cutting-edge results across 6 evaluation tasks. Beyond standard benchmarks such as cell clustering, cell-type annotation, batch-effect correction, and few-shot annotation, the model also demonstrates superior performance in broader multi-modal applications, including zero-shot cell-type annotation and bidirectional cell-text retrieval.",
      "authors": [
        "Haoran Wang",
        "Xuanyi Zhang",
        "Shuangsang Fang",
        "Longke Ran",
        "Ziqing Deng",
        "Yong Zhang",
        "Yuxiang Li",
        "Shaoshuai Li"
      ],
      "published": "2026-01-09T09:10:14Z",
      "categories": [
        "q-bio.GN",
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ],
      "primary_category": "q-bio.GN",
      "abs_url": "https://arxiv.org/abs/2601.05648v1",
      "pdf_url": "https://arxiv.org/pdf/2601.05648v1.pdf"
    },
    {
      "id": "2601.05647v1",
      "title": "Transformer Is Inherently a Causal Learner",
      "abstract": "We reveal that transformers trained in an autoregressive manner naturally encode time-delayed causal structures in their learned representations. When predicting future values in multivariate time series, the gradient sensitivities of transformer outputs with respect to past inputs directly recover the underlying causal graph, without any explicit causal objectives or structural constraints. We prove this connection theoretically under standard identifiability conditions and develop a practical extraction method using aggregated gradient attributions. On challenging cases such as nonlinear dynamics, long-term dependencies, and non-stationary systems, this approach greatly surpasses the performance of state-of-the-art discovery algorithms, especially as data heterogeneity increases, exhibiting scaling potential where causal accuracy improves with data volume and heterogeneity, a property traditional methods lack. This unifying view lays the groundwork for a future paradigm where causal discovery operates through the lens of foundation models, and foundation models gain interpretability and enhancement through the lens of causality.",
      "authors": [
        "Xinyue Wang",
        "Stephen Wang",
        "Biwei Huang"
      ],
      "published": "2026-01-09T09:10:04Z",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "abs_url": "https://arxiv.org/abs/2601.05647v1",
      "pdf_url": "https://arxiv.org/pdf/2601.05647v1.pdf"
    },
    {
      "id": "2601.05641v1",
      "title": "Multilingual Amnesia: On the Transferability of Unlearning in Multilingual LLMs",
      "abstract": "As multilingual large language models become more widely used, ensuring their safety and fairness across diverse linguistic contexts presents unique challenges. While existing research on machine unlearning has primarily focused on monolingual settings, typically English, multilingual environments introduce additional complexities due to cross-lingual knowledge transfer and biases embedded in both pretraining and fine-tuning data. In this work, we study multilingual unlearning using the Aya-Expanse 8B model under two settings: (1) data unlearning and (2) concept unlearning. We extend benchmarks for factual knowledge and stereotypes to ten languages through translation: English, French, Arabic, Japanese, Russian, Farsi, Korean, Hindi, Hebrew, and Indonesian. These languages span five language families and a wide range of resource levels. Our experiments show that unlearning in high-resource languages is generally more stable, with asymmetric transfer effects observed between typologically related languages. Furthermore, our analysis of linguistic distances indicates that syntactic similarity is the strongest predictor of cross-lingual unlearning behavior.",
      "authors": [
        "Alireza Dehghanpour Farashah",
        "Aditi Khandelwal",
        "Marylou Fauchard",
        "Zhuan Shi",
        "Negar Rostamzadeh",
        "Golnoosh Farnadi"
      ],
      "published": "2026-01-09T08:59:42Z",
      "categories": [
        "cs.CL",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "abs_url": "https://arxiv.org/abs/2601.05641v1",
      "pdf_url": "https://arxiv.org/pdf/2601.05641v1.pdf"
    },
    {
      "id": "2601.05640v1",
      "title": "SGDrive: Scene-to-Goal Hierarchical World Cognition for Autonomous Driving",
      "abstract": "Recent end-to-end autonomous driving approaches have leveraged Vision-Language Models (VLMs) to enhance planning capabilities in complex driving scenarios. However, VLMs are inherently trained as generalist models, lacking specialized understanding of driving-specific reasoning in 3D space and time. When applied to autonomous driving, these models struggle to establish structured spatial-temporal representations that capture geometric relationships, scene context, and motion patterns critical for safe trajectory planning. To address these limitations, we propose SGDrive, a novel framework that explicitly structures the VLM's representation learning around driving-specific knowledge hierarchies. Built upon a pre-trained VLM backbone, SGDrive decomposes driving understanding into a scene-agent-goal hierarchy that mirrors human driving cognition: drivers first perceive the overall environment (scene context), then attend to safety-critical agents and their behaviors, and finally formulate short-term goals before executing actions. This hierarchical decomposition provides the structured spatial-temporal representation that generalist VLMs lack, integrating multi-level information into a compact yet comprehensive format for trajectory planning. Extensive experiments on the NAVSIM benchmark demonstrate that SGDrive achieves state-of-the-art performance among camera-only methods on both PDMS and EPDMS, validating the effectiveness of hierarchical knowledge structuring for adapting generalist VLMs to autonomous driving.",
      "authors": [
        "Jingyu Li",
        "Junjie Wu",
        "Dongnan Hu",
        "Xiangkai Huang",
        "Bin Sun",
        "Zhihui Hao",
        "Xianpeng Lang",
        "Xiatian Zhu",
        "Li Zhang"
      ],
      "published": "2026-01-09T08:55:42Z",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "abs_url": "https://arxiv.org/abs/2601.05640v1",
      "pdf_url": "https://arxiv.org/pdf/2601.05640v1.pdf"
    },
    {
      "id": "2601.05639v1",
      "title": "Compressing image encoders via latent distillation",
      "abstract": "Deep learning models for image compression often face practical limitations in hardware-constrained applications. Although these models achieve high-quality reconstructions, they are typically complex, heavyweight, and require substantial training data and computational resources. We propose a methodology to partially compress these networks by reducing the size of their encoders. Our approach uses a simplified knowledge distillation strategy to approximate the latent space of the original models with less data and shorter training, yielding lightweight encoders from heavyweight ones. We evaluate the resulting lightweight encoders across two different architectures on the image compression task. Experiments show that our method preserves reconstruction quality and statistical fidelity better than training lightweight encoders with the original loss, making it practical for resource-limited environments.",
      "authors": [
        "Caroline Mazini Rodrigues",
        "Nicolas Keriven",
        "Thomas Maugey"
      ],
      "published": "2026-01-09T08:50:38Z",
      "categories": [
        "cs.CV",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "abs_url": "https://arxiv.org/abs/2601.05639v1",
      "pdf_url": "https://arxiv.org/pdf/2601.05639v1.pdf"
    },
    {
      "id": "2601.05637v1",
      "title": "GenCtrl -- A Formal Controllability Toolkit for Generative Models",
      "abstract": "As generative models become ubiquitous, there is a critical need for fine-grained control over the generation process. Yet, while controlled generation methods from prompting to fine-tuning proliferate, a fundamental question remains unanswered: are these models truly controllable in the first place? In this work, we provide a theoretical framework to formally answer this question. Framing human-model interaction as a control process, we propose a novel algorithm to estimate the controllable sets of models in a dialogue setting. Notably, we provide formal guarantees on the estimation error as a function of sample complexity: we derive probably-approximately correct bounds for controllable set estimates that are distribution-free, employ no assumptions except for output boundedness, and work for any black-box nonlinear control system (i.e., any generative model). We empirically demonstrate the theoretical framework on different tasks in controlling dialogue processes, for both language models and text-to-image generation. Our results show that model controllability is surprisingly fragile and highly dependent on the experimental setting. This highlights the need for rigorous controllability analysis, shifting the focus from simply attempting control to first understanding its fundamental limits.",
      "authors": [
        "Emily Cheng",
        "Carmen Amo Alonso",
        "Federico Danieli",
        "Arno Blaas",
        "Luca Zappella",
        "Pau Rodriguez",
        "Xavier Suau"
      ],
      "published": "2026-01-09T08:50:02Z",
      "categories": [
        "cs.AI",
        "cs.LG",
        "eess.SY"
      ],
      "primary_category": "cs.AI",
      "abs_url": "https://arxiv.org/abs/2601.05637v1",
      "pdf_url": "https://arxiv.org/pdf/2601.05637v1.pdf"
    },
    {
      "id": "2601.05635v1",
      "title": "Continual Pretraining on Encrypted Synthetic Data for Privacy-Preserving LLMs",
      "abstract": "Preserving privacy in sensitive data while pretraining large language models on small, domain-specific corpora presents a significant challenge. In this work, we take an exploratory step toward privacy-preserving continual pretraining by proposing an entity-based framework that synthesizes encrypted training data to protect personally identifiable information (PII). Our approach constructs a weighted entity graph to guide data synthesis and applies deterministic encryption to PII entities, enabling LLMs to encode new knowledge through continual pretraining while granting authorized access to sensitive data through decryption keys. Our results on limited-scale datasets demonstrate that our pretrained models outperform base models and ensure PII security, while exhibiting a modest performance gap compared to models trained on unencrypted synthetic data. We further show that increasing the number of entities and leveraging graph-based synthesis improves model performance, and that encrypted models retain instruction-following capabilities with long retrieved contexts. We discuss the security implications and limitations of deterministic encryption, positioning this work as an initial investigation into the design space of encrypted data pretraining for privacy-preserving LLMs. Our code is available at https://github.com/DataArcTech/SoE.",
      "authors": [
        "Honghao Liu",
        "Xuhui Jiang",
        "Chengjin Xu",
        "Cehao Yang",
        "Yiran Cheng",
        "Lionel Ni",
        "Jian Guo"
      ],
      "published": "2026-01-09T08:44:07Z",
      "categories": [
        "cs.CR",
        "cs.CL"
      ],
      "primary_category": "cs.CR",
      "abs_url": "https://arxiv.org/abs/2601.05635v1",
      "pdf_url": "https://arxiv.org/pdf/2601.05635v1.pdf"
    },
    {
      "id": "2601.05633v1",
      "title": "GIFT: Games as Informal Training for Generalizable LLMs",
      "abstract": "While Large Language Models (LLMs) have achieved remarkable success in formal learning tasks such as mathematics and code generation, they still struggle with the \"practical wisdom\" and generalizable intelligence, such as strategic creativity and social reasoning, that characterize human cognition. This gap arises from a lack of informal learning, which thrives on interactive feedback rather than goal-oriented instruction. In this paper, we propose treating Games as a primary environment for LLM informal learning, leveraging their intrinsic reward signals and abstracted complexity to cultivate diverse competencies. To address the performance degradation observed in multi-task learning, we introduce a Nested Training Framework. Unlike naive task mixing optimizing an implicit \"OR\" objective, our framework employs sequential task composition to enforce an explicit \"AND\" objective, compelling the model to master multiple abilities simultaneously to achieve maximal rewards. Using GRPO-based reinforcement learning across Matrix Games, TicTacToe, and Who's the Spy games, we demonstrate that integrating game-based informal learning not only prevents task interference but also significantly bolsters the model's generalization across broad ability-oriented benchmarks. The framework and implementation are publicly available.",
      "authors": [
        "Nuoyan Lyu",
        "Bingbing Xu",
        "Weihao Meng",
        "Yige Yuan",
        "Yang Zhang",
        "Zhiyong Huang",
        "Tat-Seng Chua",
        "Huawei Shen"
      ],
      "published": "2026-01-09T08:42:44Z",
      "categories": [
        "cs.CL"
      ],
      "primary_category": "cs.CL",
      "abs_url": "https://arxiv.org/abs/2601.05633v1",
      "pdf_url": "https://arxiv.org/pdf/2601.05633v1.pdf"
    },
    {
      "id": "2601.05629v1",
      "title": "Cumulative Path-Level Semantic Reasoning for Inductive Knowledge Graph Completion",
      "abstract": "Conventional Knowledge Graph Completion (KGC) methods aim to infer missing information in incomplete Knowledge Graphs (KGs) by leveraging existing information, which struggle to perform effectively in scenarios involving emerging entities. Inductive KGC methods can handle the emerging entities and relations in KGs, offering greater dynamic adaptability. While existing inductive KGC methods have achieved some success, they also face challenges, such as susceptibility to noisy structural information during reasoning and difficulty in capturing long-range dependencies in reasoning paths. To address these challenges, this paper proposes the Cumulative Path-Level Semantic Reasoning for inductive knowledge graph completion (CPSR) framework, which simultaneously captures both the structural and semantic information of KGs to enhance the inductive KGC task. Specifically, the proposed CPSR employs a query-dependent masking module to adaptively mask noisy structural information while retaining important information closely related to the targets. Additionally, CPSR introduces a global semantic scoring module that evaluates both the individual contributions and the collective impact of nodes along the reasoning path within KGs. The experimental results demonstrate that CPSR achieves state-of-the-art performance.",
      "authors": [
        "Jiapu Wang",
        "Xinghe Cheng",
        "Zezheng Wu",
        "Ruiqi Ma",
        "Rui Wang",
        "Zhichao Yan",
        "Haoran Luo",
        "Yuhao Jiang",
        "Kai Sun"
      ],
      "published": "2026-01-09T08:34:05Z",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "abs_url": "https://arxiv.org/abs/2601.05629v1",
      "pdf_url": "https://arxiv.org/pdf/2601.05629v1.pdf"
    },
    {
      "id": "2601.05624v1",
      "title": "Text Detoxification in isiXhosa and Yorùbá: A Cross-Lingual Machine Learning Approach for Low-Resource African Languages",
      "abstract": "Toxic language is one of the major barrier to safe online participation, yet robust mitigation tools are scarce for African languages. This study addresses this critical gap by investigating automatic text detoxification (toxic to neutral rewriting) for two low-resource African languages, isiXhosa and Yorùbá. The work contributes a novel, pragmatic hybrid methodology: a lightweight, interpretable TF-IDF and Logistic Regression model for transparent toxicity detection, and a controlled lexicon- and token-guided rewriting component. A parallel corpus of toxic to neutral rewrites, which captures idiomatic usage, diacritics, and code switching, was developed to train and evaluate the model. The detection component achieved stratified K-fold accuracies of 61-72% (isiXhosa) and 72-86% (Yorùbá), with per-language ROC-AUCs up to 0.88. The rewriting component successfully detoxified all detected toxic sentences while preserving 100% of non-toxic sentences. These results demonstrate that scalable, interpretable machine learning detectors combined with rule-based edits offer a competitive and resource-efficient solution for culturally adaptive safety tooling, setting a new benchmark for low-resource Text Style Transfer (TST) in African languages.",
      "authors": [
        "Abayomi O. Agbeyangi"
      ],
      "published": "2026-01-09T08:28:58Z",
      "categories": [
        "cs.CL"
      ],
      "primary_category": "cs.CL",
      "abs_url": "https://arxiv.org/abs/2601.05624v1",
      "pdf_url": "https://arxiv.org/pdf/2601.05624v1.pdf"
    },
    {
      "id": "2601.05623v1",
      "title": "Continual Learning of Achieving Forgetting-free and Positive Knowledge Transfer",
      "abstract": "Existing research on continual learning (CL) of a sequence of tasks focuses mainly on dealing with catastrophic forgetting (CF) to balance the learning plasticity of new tasks and the memory stability of old tasks. However, an ideal CL agent should not only be able to overcome CF, but also encourage positive forward and backward knowledge transfer (KT), i.e., using the learned knowledge from previous tasks for the new task learning (namely FKT), and improving the previous tasks' performance with the knowledge of the new task (namely BKT). To this end, this paper first models CL as an optimization problem in which each sequential learning task aims to achieve its optimal performance under the constraint that both FKT and BKT should be positive. It then proposes a novel Enhanced Task Continual Learning (ETCL) method, which achieves forgetting-free and positive KT. Furthermore, the bounds that can lead to negative FKT and BKT are estimated theoretically. Based on the bounds, a new strategy for online task similarity detection is also proposed to facilitate positive KT. To overcome CF, ETCL learns a set of task-specific binary masks to isolate a sparse sub-network for each task while preserving the performance of a dense network for the task. At the beginning of a new task learning, ETCL tries to align the new task's gradient with that of the sub-network of the previous most similar task to ensure positive FKT. By using a new bi-objective optimization strategy and an orthogonal gradient projection method, ETCL updates only the weights of previous similar tasks at the classification layer to achieve positive BKT. Extensive evaluations demonstrate that the proposed ETCL markedly outperforms strong baselines on dissimilar, similar, and mixed task sequences.",
      "authors": [
        "Zhi Wang",
        "Zhongbin Wu",
        "Yanni Li",
        "Bing Liu",
        "Guangxi Li",
        "Yuping Wang"
      ],
      "published": "2026-01-09T08:27:14Z",
      "categories": [
        "cs.LG",
        "cs.CV"
      ],
      "primary_category": "cs.LG",
      "abs_url": "https://arxiv.org/abs/2601.05623v1",
      "pdf_url": "https://arxiv.org/pdf/2601.05623v1.pdf"
    },
    {
      "id": "2601.05616v1",
      "title": "Dual-Phase LLM Reasoning: Self-Evolved Mathematical Frameworks",
      "abstract": "In recent years, large language models (LLMs) have demonstrated significant potential in complex reasoning tasks like mathematical problem-solving. However, existing research predominantly relies on reinforcement learning (RL) frameworks while overlooking supervised fine-tuning (SFT) methods. This paper proposes a new two-stage training framework that enhances models' self-correction capabilities through self-generated long chain-of-thought (CoT) data. During the first stage, a multi-turn dialogue strategy guides the model to generate CoT data incorporating verification, backtracking, subgoal decomposition, and backward reasoning, with predefined rules filtering high-quality samples for supervised fine-tuning. The second stage employs a difficulty-aware rejection sampling mechanism to dynamically optimize data distribution, strengthening the model's ability to handle complex problems. The approach generates reasoning chains extended over 4 times longer while maintaining strong scalability, proving that SFT effectively activates models' intrinsic reasoning capabilities and provides a resource-efficient pathway for complex task optimization. Experimental results demonstrate performance improvements on mathematical benchmarks including GSM8K and MATH500, with the fine-tuned model achieving a substantial improvement on competition-level problems like AIME24. Code will be open-sourced.",
      "authors": [
        "ShaoZhen Liu",
        "Xinting Huang",
        "Houwen Peng",
        "Xin Chen",
        "Xinyang Song",
        "Qi Li",
        "Zhenan Sun"
      ],
      "published": "2026-01-09T08:19:11Z",
      "categories": [
        "cs.LG"
      ],
      "primary_category": "cs.LG",
      "abs_url": "https://arxiv.org/abs/2601.05616v1",
      "pdf_url": "https://arxiv.org/pdf/2601.05616v1.pdf"
    },
    {
      "id": "2601.05613v1",
      "title": "PiXTime: A Model for Federated Time Series Forecasting with Heterogeneous Data Structures Across Nodes",
      "abstract": "Time series are highly valuable and rarely shareable across nodes, making federated learning a promising paradigm to leverage distributed temporal data. However, different sampling standards lead to diverse time granularities and variable sets across nodes, hindering classical federated learning. We propose PiXTime, a novel time series forecasting model designed for federated learning that enables effective prediction across nodes with multi-granularity and heterogeneous variable sets. PiXTime employs a personalized Patch Embedding to map node-specific granularity time series into token sequences of a unified dimension for processing by a subsequent shared model, and uses a global VE Table to align variable category semantics across nodes, thereby enhancing cross-node transferability. With a transformer-based shared model, PiXTime captures representations of auxiliary series with arbitrary numbers of variables and uses cross-attention to enhance the prediction of the target series. Experiments show PiXTime achieves state-of-the-art performance in federated settings and demonstrates superior performance on eight widely used real-world traditional benchmarks.",
      "authors": [
        "Yiming Zhou",
        "Mingyue Cheng",
        "Hao Wang",
        "Enhong Chen"
      ],
      "published": "2026-01-09T08:11:45Z",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "abs_url": "https://arxiv.org/abs/2601.05613v1",
      "pdf_url": "https://arxiv.org/pdf/2601.05613v1.pdf"
    },
    {
      "id": "2601.05612v1",
      "title": "Mobile Robot Localization Using a Novel Whisker-Like Sensor",
      "abstract": "Whisker-like touch sensors offer unique advantages for short-range perception in environments where visual and long-range sensing are unreliable, such as confined, cluttered, or low-visibility settings. This paper presents a framework for estimating contact points and robot localization in a known planar environment using a single whisker sensor. We develop a family of virtual sensor models. Each model maps robot configurations to sensor observations and enables structured reasoning through the concept of preimages - the set of robot states consistent with a given observation. The notion of virtual sensor models serves as an abstraction to reason about state uncertainty without dependence on physical implementation. By combining sensor observations with a motion model, we estimate the contact point. Iterative estimation then enables reconstruction of obstacle boundaries. Furthermore, intersecting states inferred from current observations with forward-projected states from previous steps allow accurate robot localization without relying on vision or external systems. The framework supports both deterministic and possibilistic formulations and is validated through simulation and physical experiments using a low-cost, 3D printed, Hall-effect-based whisker sensor. Results demonstrate accurate contact estimation and localization with errors under 7 mm, demonstrating the potential of whisker-based sensing as a lightweight, adaptable complement to vision-based navigation.",
      "authors": [
        "Prasanna K. Routray",
        "Basak Sakcak",
        "Steven M. LaValle",
        "Manivannan M"
      ],
      "published": "2026-01-09T08:11:15Z",
      "categories": [
        "physics.app-ph",
        "cs.RO"
      ],
      "primary_category": "physics.app-ph",
      "abs_url": "https://arxiv.org/abs/2601.05612v1",
      "pdf_url": "https://arxiv.org/pdf/2601.05612v1.pdf"
    },
    {
      "id": "2601.05611v1",
      "title": "LatentVLA: Efficient Vision-Language Models for Autonomous Driving via Latent Action Prediction",
      "abstract": "End-to-end autonomous driving models trained on largescale datasets perform well in common scenarios but struggle with rare, long-tail situations due to limited scenario diversity. Recent Vision-Language-Action (VLA) models leverage broad knowledge from pre-trained visionlanguage models to address this limitation, yet face critical challenges: (1) numerical imprecision in trajectory prediction due to discrete tokenization, (2) heavy reliance on language annotations that introduce linguistic bias and annotation burden, and (3) computational inefficiency from multi-step chain-of-thought reasoning hinders real-time deployment. We propose LatentVLA, a novel framework that employs self-supervised latent action prediction to train VLA models without language annotations, eliminating linguistic bias while learning rich driving representations from unlabeled trajectory data. Through knowledge distillation, LatentVLA transfers the generalization capabilities of VLA models to efficient vision-based networks, achieving both robust performance and real-time efficiency. LatentVLA establishes a new state-of-the-art on the NAVSIM benchmark with a PDMS score of 92.4 and demonstrates strong zeroshot generalization on the nuScenes benchmark.",
      "authors": [
        "Chengen Xie",
        "Bin Sun",
        "Tianyu Li",
        "Junjie Wu",
        "Zhihui Hao",
        "XianPeng Lang",
        "Hongyang Li"
      ],
      "published": "2026-01-09T08:06:44Z",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "abs_url": "https://arxiv.org/abs/2601.05611v1",
      "pdf_url": "https://arxiv.org/pdf/2601.05611v1.pdf"
    },
    {
      "id": "2601.05609v1",
      "title": "Data Augmented Pipeline for Legal Information Extraction and Reasoning",
      "abstract": "In this paper, we propose a pipeline leveraging Large Language Models (LLMs) for data augmentation in Information Extraction tasks within the legal domain. The proposed method is both simple and effective, significantly reducing the manual effort required for data annotation while enhancing the robustness of Information Extraction systems. Furthermore, the method is generalizable, making it applicable to various Natural Language Processing (NLP) tasks beyond the legal domain.",
      "authors": [
        "Nguyen Minh Phuong",
        "Ha-Thanh Nguyen",
        "May Myo Zin",
        "Ken Satoh"
      ],
      "published": "2026-01-09T08:02:54Z",
      "categories": [
        "cs.CL"
      ],
      "primary_category": "cs.CL",
      "abs_url": "https://arxiv.org/abs/2601.05609v1",
      "pdf_url": "https://arxiv.org/pdf/2601.05609v1.pdf"
    },
    {
      "id": "2601.05607v1",
      "title": "Orchestrating Tokens and Sequences: Dynamic Hybrid Policy Optimization for RLVR",
      "abstract": "Reinforcement Learning with Verifiable Rewards (RLVR) offers a promising framework for optimizing large language models in reasoning tasks. However, existing RLVR algorithms focus on different granularities, and each has complementary strengths and limitations. Group Relative Policy Optimization (GRPO) updates the policy with token-level importance ratios, which preserves fine-grained credit assignment but often suffers from high variance and instability. In contrast, Group Sequence Policy Optimization (GSPO) applies single sequence-level importance ratios across all tokens in a response that better matches sequence-level rewards, but sacrifices token-wise credit assignment. In this paper, we propose Dynamic Hybrid Policy Optimization (DHPO) to bridge GRPO and GSPO within a single clipped surrogate objective. DHPO combines token-level and sequence-level importance ratios using weighting mechanisms. We explore two variants of the mixing mechanism, including an averaged mixing and an entropy-guided mixing. To further stabilize training, we employ a branch-specific clipping strategy that constrains token-level and sequence-level ratios within separate trust regions before mixing, preventing outliers in either branch from dominating the update. Across seven challenging mathematical reasoning benchmarks, experiments on both dense and MoE models from the Qwen3 series show that DHPO consistently outperforms GRPO and GSPO. We will release our code upon acceptance of this paper.",
      "authors": [
        "Zijun Min",
        "Bingshuai Liu",
        "Ante Wang",
        "Long Zhang",
        "Anxiang Zeng",
        "Haibo Zhang",
        "Jinsong Su"
      ],
      "published": "2026-01-09T07:57:40Z",
      "categories": [
        "cs.LG"
      ],
      "primary_category": "cs.LG",
      "abs_url": "https://arxiv.org/abs/2601.05607v1",
      "pdf_url": "https://arxiv.org/pdf/2601.05607v1.pdf"
    },
    {
      "id": "2601.05604v1",
      "title": "Learning Geometric Invariance for Gait Recognition",
      "abstract": "The goal of gait recognition is to extract identity-invariant features of an individual under various gait conditions, e.g., cross-view and cross-clothing. Most gait models strive to implicitly learn the common traits across different gait conditions in a data-driven manner to pull different gait conditions closer for recognition. However, relatively few studies have explicitly explored the inherent relations between different gait conditions. For this purpose, we attempt to establish connections among different gait conditions and propose a new perspective to achieve gait recognition: variations in different gait conditions can be approximately viewed as a combination of geometric transformations. In this case, all we need is to determine the types of geometric transformations and achieve geometric invariance, then identity invariance naturally follows. As an initial attempt, we explore three common geometric transformations (i.e., Reflect, Rotate, and Scale) and design a $\\mathcal{R}$eflect-$\\mathcal{R}$otate-$\\mathcal{S}$cale invariance learning framework, named ${\\mathcal{RRS}}$-Gait. Specifically, it first flexibly adjusts the convolution kernel based on the specific geometric transformations to achieve approximate feature equivariance. Then these three equivariant-aware features are respectively fed into a global pooling operation for final invariance-aware learning. Extensive experiments on four popular gait datasets (Gait3D, GREW, CCPG, SUSTech1K) show superior performance across various gait conditions.",
      "authors": [
        "Zengbin Wang",
        "Junjie Li",
        "Saihui Hou",
        "Xu Liu",
        "Chunshui Cao",
        "Yongzhen Huang",
        "Muyi Sun",
        "Siye Wang",
        "Man Zhang"
      ],
      "published": "2026-01-09T07:47:01Z",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "abs_url": "https://arxiv.org/abs/2601.05604v1",
      "pdf_url": "https://arxiv.org/pdf/2601.05604v1.pdf"
    },
    {
      "id": "2601.05600v1",
      "title": "SceneAlign: Aligning Multimodal Reasoning to Scene Graphs in Complex Visual Scenes",
      "abstract": "Multimodal large language models often struggle with faithful reasoning in complex visual scenes, where intricate entities and relations require precise visual grounding at each step. This reasoning unfaithfulness frequently manifests as hallucinated entities, mis-grounded relations, skipped steps, and over-specified reasoning. Existing preference-based approaches, typically relying on textual perturbations or answer-conditioned rationales, fail to address this challenge as they allow models to exploit language priors to bypass visual grounding. To address this, we propose SceneAlign, a framework that leverages scene graphs as structured visual information to perform controllable structural interventions. By identifying reasoning-critical nodes and perturbing them through four targeted strategies that mimic typical grounding failures, SceneAlign constructs hard negative rationales that remain linguistically plausible but are grounded in inaccurate visual facts. These contrastive pairs are used in Direct Preference Optimization to steer models toward fine-grained, structure-faithful reasoning. Across seven visual reasoning benchmarks, SceneAlign consistently improves answer accuracy and reasoning faithfulness, highlighting the effectiveness of grounding-aware alignment for multimodal reasoning.",
      "authors": [
        "Chuhan Wang",
        "Xintong Li",
        "Jennifer Yuntong Zhang",
        "Junda Wu",
        "Chengkai Huang",
        "Lina Yao",
        "Julian McAuley",
        "Jingbo Shang"
      ],
      "published": "2026-01-09T07:40:39Z",
      "categories": [
        "cs.CV",
        "cs.CL",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "abs_url": "https://arxiv.org/abs/2601.05600v1",
      "pdf_url": "https://arxiv.org/pdf/2601.05600v1.pdf"
    },
    {
      "id": "2601.05599v1",
      "title": "Quantifying and Inducing Shape Bias in CNNs via Max-Pool Dilation",
      "abstract": "Convolutional Neural Networks (CNNs) are known to exhibit a strong texture bias, favoring local patterns over global shape information--a tendency inherent to their convolutional architecture. While this bias is beneficial for texture-rich natural images, it often degrades performance on shape-dominant data such as illustrations and sketches. Although prior work has proposed shape-biased models to mitigate this issue, these approaches lack a quantitative metric for identifying which datasets would actually benefit from such modifications. To address this gap, we propose a data-driven metric that quantifies the shape-texture balance of a dataset by computing the Structural Similarity Index (SSIM) between each image's luminance channel and its L0-smoothed counterpart. Building on this metric, we further introduce a computationally efficient adaptation method that promotes shape bias by modifying the dilation of max-pooling operations while keeping convolutional weights frozen. Experimental results show that this approach consistently improves classification accuracy on shape-dominant datasets, particularly in low-data regimes where full fine-tuning is impractical, requiring training only the final classification layer.",
      "authors": [
        "Takito Sawada",
        "Akinori Iwata",
        "Masahiro Okuda"
      ],
      "published": "2026-01-09T07:36:29Z",
      "categories": [
        "cs.CV",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "abs_url": "https://arxiv.org/abs/2601.05599v1",
      "pdf_url": "https://arxiv.org/pdf/2601.05599v1.pdf"
    },
    {
      "id": "2601.05597v1",
      "title": "Good Allocations from Bad Estimates",
      "abstract": "Conditional average treatment effect (CATE) estimation is the de facto gold standard for targeting a treatment to a heterogeneous population. The method estimates treatment effects up to an error $ε> 0$ in each of $M$ different strata of the population, targeting individuals in decreasing order of estimated treatment effect until the budget runs out. In general, this method requires $O(M/ε^2)$ samples. This is best possible if the goal is to estimate all treatment effects up to an $ε$ error. In this work, we show how to achieve the same total treatment effect as CATE with only $O(M/ε)$ samples for natural distributions of treatment effects. The key insight is that coarse estimates suffice for near-optimal treatment allocations. In addition, we show that budget flexibility can further reduce the sample complexity of allocation. Finally, we evaluate our algorithm on various real-world RCT datasets. In all cases, it finds nearly optimal treatment allocations with surprisingly few samples. Our work highlights the fundamental distinction between treatment effect estimation and treatment allocation: the latter requires far fewer samples.",
      "authors": [
        "Sílvia Casacuberta",
        "Moritz Hardt"
      ],
      "published": "2026-01-09T07:35:50Z",
      "categories": [
        "cs.LG",
        "cs.CY"
      ],
      "primary_category": "cs.LG",
      "abs_url": "https://arxiv.org/abs/2601.05597v1",
      "pdf_url": "https://arxiv.org/pdf/2601.05597v1.pdf"
    },
    {
      "id": "2601.05593v1",
      "title": "PaCoRe: Learning to Scale Test-Time Compute with Parallel Coordinated Reasoning",
      "abstract": "We introduce Parallel Coordinated Reasoning (PaCoRe), a training-and-inference framework designed to overcome a central limitation of contemporary language models: their inability to scale test-time compute (TTC) far beyond sequential reasoning under a fixed context window. PaCoRe departs from the traditional sequential paradigm by driving TTC through massive parallel exploration coordinated via a message-passing architecture in multiple rounds. Each round launches many parallel reasoning trajectories, compacts their findings into context-bounded messages, and synthesizes these messages to guide the next round and ultimately produce the final answer. Trained end-to-end with large-scale, outcome-based reinforcement learning, the model masters the synthesis abilities required by PaCoRe and scales to multi-million-token effective TTC without exceeding context limits. The approach yields strong improvements across diverse domains, and notably pushes reasoning beyond frontier systems in mathematics: an 8B model reaches 94.5% on HMMT 2025, surpassing GPT-5's 93.2% by scaling effective TTC to roughly two million tokens. We open-source model checkpoints, training data, and the full inference pipeline to accelerate follow-up work.",
      "authors": [
        "Jingcheng Hu",
        "Yinmin Zhang",
        "Shijie Shang",
        "Xiaobo Yang",
        "Yue Peng",
        "Zhewei Huang",
        "Hebin Zhou",
        "Xin Wu",
        "Jie Cheng",
        "Fanqi Wan",
        "Xiangwen Kong",
        "Chengyuan Yao",
        "Kaiwen Yan",
        "Ailin Huang",
        "Hongyu Zhou",
        "Qi Han",
        "Zheng Ge",
        "Daxin Jiang",
        "Xiangyu Zhang",
        "Heung-Yeung Shum"
      ],
      "published": "2026-01-09T07:24:43Z",
      "categories": [
        "cs.LG"
      ],
      "primary_category": "cs.LG",
      "abs_url": "https://arxiv.org/abs/2601.05593v1",
      "pdf_url": "https://arxiv.org/pdf/2601.05593v1.pdf"
    },
    {
      "id": "2601.05590v1",
      "title": "A Causal Information-Flow Framework for Unbiased Learning-to-Rank",
      "abstract": "In web search and recommendation systems, user clicks are widely used to train ranking models. However, click data is heavily biased, i.e., users tend to click higher-ranked items (position bias), choose only what was shown to them (selection bias), and trust top results more (trust bias). Without explicitly modeling these biases, the true relevance of ranked items cannot be correctly learned from clicks. Existing Unbiased Learning-to-Rank (ULTR) methods mainly correct position bias and rely on propensity estimation, but they cannot measure remaining bias, provide risk guarantees, or jointly handle multiple bias sources. To overcome these challenges, this paper introduces a novel causal learning-based ranking framework that extends ULTR by combining Structural Causal Models (SCMs) with information-theoretic tools. SCMs specify how clicks are generated and help identify the true relevance signal from click data, while conditional mutual information, measures how much bias leaks into the\n  learned relevance estimates. We use this leakage measure to define a rigorous notion of disentanglement and include it as a regularizer during model training to reduce bias. In addition, we incorporate a causal inference estimator, i.e., doubly robust estimator, to ensure more reliable risk estimation. Experiments on standard Learning-to-Rank benchmarks show that our method consistently reduces measured bias leakage and improves ranking performance, especially in realistic scenarios where multiple biases-such as position and trust bias-interact strongly.",
      "authors": [
        "Haoming Gong",
        "Qingyao Ai",
        "Zhihao Tao",
        "Yongfeng Zhang"
      ],
      "published": "2026-01-09T07:19:35Z",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "abs_url": "https://arxiv.org/abs/2601.05590v1",
      "pdf_url": "https://arxiv.org/pdf/2601.05590v1.pdf"
    },
    {
      "id": "2601.05589v1",
      "title": "ACR: Adaptive Context Refactoring via Context Refactoring Operators for Multi-Turn Dialogue",
      "abstract": "Large Language Models (LLMs) have shown remarkable performance in multi-turn dialogue. However, in multi-turn dialogue, models still struggle to stay aligned with what has been established earlier, follow dependencies across many turns, and avoid drifting into incorrect facts as the interaction grows longer. Existing approaches primarily focus on extending the context window, introducing external memory, or applying context compression, yet these methods still face limitations such as \\textbf{contextual inertia} and \\textbf{state drift}. To address these challenges, we propose the \\textbf{A}daptive \\textbf{C}ontext \\textbf{R}efactoring \\textbf{(ACR)} Framework, which dynamically monitors and reshapes the interaction history to mitigate contextual inertia and state drift actively. ACR is built on a library of context refactoring operators and a teacher-guided self-evolving training paradigm that learns when to intervene and how to refactor, thereby decoupling context management from the reasoning process. Extensive experiments on multi-turn dialogue demonstrate that our method significantly outperforms existing baselines while reducing token consumption.",
      "authors": [
        "Jiawei Shen",
        "Jia Zhu",
        "Hanghui Guo",
        "Weijie Shi",
        "Yue Cui",
        "Qingyu Niu",
        "Guoqing Ma",
        "Yidan Liang",
        "Jingjiang Liu",
        "Yiling Wang",
        "Shimin Di",
        "Jiajie Xu"
      ],
      "published": "2026-01-09T07:17:28Z",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "abs_url": "https://arxiv.org/abs/2601.05589v1",
      "pdf_url": "https://arxiv.org/pdf/2601.05589v1.pdf"
    },
    {
      "id": "2601.05588v1",
      "title": "Autoregressive Ranking: Bridging the Gap Between Dual and Cross Encoders",
      "abstract": "Dual and cross encoders have long been mainstays of information retrieval (IR), but are being challenged by the emergent capabilities of LLMs. An LLM-based approach we term pointwise generative ranking - generating tokens the length of a single docID as opposed to a list in order to enable ranking via beam search - combines efficiency and expressivity benefits while leveraging the in-context capabilities of Causal Transformers. Although there is ample evidence to suggest that pretrained LLMs are well-suited for ranking, we find that the vast majority of LLM-based approaches rely on next-token prediction, a loss function which is fundamentally rank-agnostic (and especially so with pointwise supervision). In this paper, we first prove that the expressivity of pointwise generative ranking with multi-token docIDs is superior to that of dual encoders. We then propose SToICaL - a Simple Token-Item Calibrated Loss - which can incorporate rank-aware supervision at both the item and token levels within the pointwise setup. We run a suite of experiments on ranking tasks derived from WordNet (Fellbaum, 1998) and ESCI (Reddy et al., arXiv:2206.06588). Two variants of SToICaL successfully suppress the probability of invalid docID generations and improve on common ranking metrics beyond top-1 retrieval.",
      "authors": [
        "Benjamin Rozonoyer",
        "Chong You",
        "Michael Boratko",
        "Himanshu Jain",
        "Nilesh Gupta",
        "Srinadh Bhojanapalli",
        "Andrew McCallum",
        "Felix Yu"
      ],
      "published": "2026-01-09T07:16:28Z",
      "categories": [
        "cs.IR",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.IR",
      "abs_url": "https://arxiv.org/abs/2601.05588v1",
      "pdf_url": "https://arxiv.org/pdf/2601.05588v1.pdf"
    },
    {
      "id": "2601.05587v1",
      "title": "HogVul: Black-box Adversarial Code Generation Framework Against LM-based Vulnerability Detectors",
      "abstract": "Recent advances in software vulnerability detection have been driven by Language Model (LM)-based approaches. However, these models remain vulnerable to adversarial attacks that exploit lexical and syntax perturbations, allowing critical flaws to evade detection. Existing black-box attacks on LM-based vulnerability detectors primarily rely on isolated perturbation strategies, limiting their ability to efficiently explore the adversarial code space for optimal perturbations. To bridge this gap, we propose HogVul, a black-box adversarial code generation framework that integrates both lexical and syntax perturbations under a unified dual-channel optimization strategy driven by Particle Swarm Optimization (PSO). By systematically coordinating two-level perturbations, HogVul effectively expands the search space for adversarial examples, enhancing the attack efficacy. Extensive experiments on four benchmark datasets demonstrate that HogVul achieves an average attack success rate improvement of 26.05\\% over state-of-the-art baseline methods. These findings highlight the potential of hybrid optimization strategies in exposing model vulnerabilities.",
      "authors": [
        "Jingxiao Yang",
        "Ping He",
        "Tianyu Du",
        "Sun Bing",
        "Xuhong Zhang"
      ],
      "published": "2026-01-09T07:14:29Z",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "primary_category": "cs.CR",
      "abs_url": "https://arxiv.org/abs/2601.05587v1",
      "pdf_url": "https://arxiv.org/pdf/2601.05587v1.pdf"
    },
    {
      "id": "2601.05586v1",
      "title": "Poisson Hyperplane Processes with Rectified Linear Units",
      "abstract": "Neural networks have shown state-of-the-art performances in various classification and regression tasks. Rectified linear units (ReLU) are often used as activation functions for the hidden layers in a neural network model. In this article, we establish the connection between the Poisson hyperplane processes (PHP) and two-layer ReLU neural networks. We show that the PHP with a Gaussian prior is an alternative probabilistic representation to a two-layer ReLU neural network. In addition, we show that a two-layer neural network constructed by PHP is scalable to large-scale problems via the decomposition propositions. Finally, we propose an annealed sequential Monte Carlo algorithm for Bayesian inference. Our numerical experiments demonstrate that our proposed method outperforms the classic two-layer ReLU neural network. The implementation of our proposed model is available at https://github.com/ShufeiGe/Pois_Relu.git.",
      "authors": [
        "Shufei Ge",
        "Shijia Wang",
        "Lloyd Elliott"
      ],
      "published": "2026-01-09T07:12:50Z",
      "categories": [
        "cs.LG",
        "stat.ME",
        "stat.ML"
      ],
      "primary_category": "cs.LG",
      "abs_url": "https://arxiv.org/abs/2601.05586v1",
      "pdf_url": "https://arxiv.org/pdf/2601.05586v1.pdf"
    },
    {
      "id": "2601.05584v1",
      "title": "GS-DMSR: Dynamic Sensitive Multi-scale Manifold Enhancement for Accelerated High-Quality 3D Gaussian Splatting",
      "abstract": "In the field of 3D dynamic scene reconstruction, how to balance model convergence rate and rendering quality has long been a critical challenge that urgently needs to be addressed, particularly in high-precision modeling of scenes with complex dynamic motions. To tackle this issue, this study proposes the GS-DMSR method. By quantitatively analyzing the dynamic evolution process of Gaussian attributes, this mechanism achieves adaptive gradient focusing, enabling it to dynamically identify significant differences in the motion states of Gaussian models. It then applies differentiated optimization strategies to Gaussian models with varying degrees of significance, thereby significantly improving the model convergence rate. Additionally, this research integrates a multi-scale manifold enhancement module, which leverages the collaborative optimization of an implicit nonlinear decoder and an explicit deformation field to enhance the modeling efficiency for complex deformation scenes. Experimental results demonstrate that this method achieves a frame rate of up to 96 FPS on synthetic datasets, while effectively reducing both storage overhead and training time.Our code and data are available at https://anonymous.4open.science/r/GS-DMSR-2212.",
      "authors": [
        "Nengbo Lu",
        "Minghua Pan",
        "Shaohua Sun",
        "Yizhou Liang"
      ],
      "published": "2026-01-09T07:12:23Z",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "abs_url": "https://arxiv.org/abs/2601.05584v1",
      "pdf_url": "https://arxiv.org/pdf/2601.05584v1.pdf"
    },
    {
      "id": "2601.05583v1",
      "title": "Learn to Evolve: Self-supervised Neural JKO Operator for Wasserstein Gradient Flow",
      "abstract": "The Jordan-Kinderlehrer-Otto (JKO) scheme provides a stable variational framework for computing Wasserstein gradient flows, but its practical use is often limited by the high computational cost of repeatedly solving the JKO subproblems. We propose a self-supervised approach for learning a JKO solution operator without requiring numerical solutions of any JKO trajectories. The learned operator maps an input density directly to the minimizer of the corresponding JKO subproblem, and can be iteratively applied to efficiently generate the gradient-flow evolution. A key challenge is that only a number of initial densities are typically available for training. To address this, we introduce a Learn-to-Evolve algorithm that jointly learns the JKO operator and its induced trajectories by alternating between trajectory generation and operator updates. As training progresses, the generated data increasingly approximates true JKO trajectories. Meanwhile, this Learn-to-Evolve strategy serves as a natural form of data augmentation, significantly enhancing the generalization ability of the learned operator. Numerical experiments demonstrate the accuracy, stability, and robustness of the proposed method across various choices of energies and initial conditions.",
      "authors": [
        "Xue Feng",
        "Li Wang",
        "Deanna Needell",
        "Rongjie Lai"
      ],
      "published": "2026-01-09T07:12:21Z",
      "categories": [
        "cs.LG",
        "math.NA"
      ],
      "primary_category": "cs.LG",
      "abs_url": "https://arxiv.org/abs/2601.05583v1",
      "pdf_url": "https://arxiv.org/pdf/2601.05583v1.pdf"
    },
    {
      "id": "2601.05582v1",
      "title": "Can large language models interpret unstructured chat data on dynamic group decision-making processes? Evidence on joint destination choice",
      "abstract": "Social activities result from complex joint activity-travel decisions between group members. While observing the decision-making process of these activities is difficult via traditional travel surveys, the advent of new types of data, such as unstructured chat data, can help shed some light on these complex processes. However, interpreting these decision-making processes requires inferring both explicit and implicit factors. This typically involves the labor-intensive task of manually annotating dialogues to capture context-dependent meanings shaped by the social and cultural norms. This study evaluates the potential of Large Language Models (LLMs) to automate and complement human annotation in interpreting decision-making processes from group chats, using data on joint eating-out activities in Japan as a case study. We designed a prompting framework inspired by the knowledge acquisition process, which sequentially extracts key decision-making factors, including the group-level restaurant choice set and outcome, individual preferences of each alternative, and the specific attributes driving those preferences. This structured process guides the LLM to interpret group chat data, converting unstructured dialogues into structured tabular data describing decision-making factors. To evaluate LLM-driven outputs, we conduct a quantitative analysis using a human-annotated ground truth dataset and a qualitative error analysis to examine model limitations. Results show that while the LLM reliably captures explicit decision-making factors, it struggles to identify nuanced implicit factors that human annotators readily identified. We pinpoint specific contexts when LLM-based extraction can be trusted versus when human oversight remains essential. These findings highlight both the potential and limitations of LLM-based analysis for incorporating non-traditional data sources on social activities.",
      "authors": [
        "Sung-Yoo Lim",
        "Koki Sato",
        "Kiyoshi Takami",
        "Giancarlos Parady",
        "Eui-Jin Kim"
      ],
      "published": "2026-01-09T07:08:49Z",
      "categories": [
        "cs.CL"
      ],
      "primary_category": "cs.CL",
      "abs_url": "https://arxiv.org/abs/2601.05582v1",
      "pdf_url": "https://arxiv.org/pdf/2601.05582v1.pdf"
    },
    {
      "id": "2601.05580v1",
      "title": "Generalizable and Adaptive Continual Learning Framework for AI-generated Image Detection",
      "abstract": "The malicious misuse and widespread dissemination of AI-generated images pose a significant threat to the authenticity of online information. Current detection methods often struggle to generalize to unseen generative models, and the rapid evolution of generative techniques continuously exacerbates this challenge. Without adaptability, detection models risk becoming ineffective in real-world applications. To address this critical issue, we propose a novel three-stage domain continual learning framework designed for continuous adaptation to evolving generative models. In the first stage, we employ a strategic parameter-efficient fine-tuning approach to develop a transferable offline detection model with strong generalization capabilities. Building upon this foundation, the second stage integrates unseen data streams into a continual learning process. To efficiently learn from limited samples of novel generated models and mitigate overfitting, we design a data augmentation chain with progressively increasing complexity. Furthermore, we leverage the Kronecker-Factored Approximate Curvature (K-FAC) method to approximate the Hessian and alleviate catastrophic forgetting. Finally, the third stage utilizes a linear interpolation strategy based on Linear Mode Connectivity, effectively capturing commonalities across diverse generative models and further enhancing overall performance. We establish a comprehensive benchmark of 27 generative models, including GANs, deepfakes, and diffusion models, chronologically structured up to August 2024 to simulate real-world scenarios. Extensive experiments demonstrate that our initial offline detectors surpass the leading baseline by +5.51% in terms of mean average precision. Our continual learning strategy achieves an average accuracy of 92.20%, outperforming state-of-the-art methods.",
      "authors": [
        "Hanyi Wang",
        "Jun Lan",
        "Yaoyu Kang",
        "Huijia Zhu",
        "Weiqiang Wang",
        "Zhuosheng Zhang",
        "Shilin Wang"
      ],
      "published": "2026-01-09T07:01:22Z",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "abs_url": "https://arxiv.org/abs/2601.05580v1",
      "pdf_url": "https://arxiv.org/pdf/2601.05580v1.pdf"
    },
    {
      "id": "2601.05579v1",
      "title": "RISE: Rule-Driven SQL Dialect Translation via Query Reduction",
      "abstract": "Translating SQL dialects across different relational database management systems (RDBMSs) is crucial for migrating RDBMS-based applications to the cloud. Traditional SQL dialect translation tools rely on manually-crafted rules, necessitating significant manual effort to support new RDBMSs and dialects. Although large language models (LLMs) can assist in translating SQL dialects, they often struggle with lengthy and complex SQL queries.\n  In this paper, we propose RISE, a novel LLM-based SQL dialect translation approach that can accurately handle lengthy and complex SQL queries. Given a complex source query $Q_c$ that contains a SQL dialect $d$, we first employ a dialect-aware query reduction technique to derive a simplified query $Q_{s}$ by removing $d$-irrelevant SQL elements from $Q_c$. Subsequently, we utilize LLMs to translate $Q_{s}$ into $Q_{s^{'}}$, and automatically extract the translation rule $r_d$ for dialect $d$ based on the relationship between $Q_{s}$ and $Q_{s^{'}}$. By applying $r_d$ to $Q_c$, we can effectively translate the dialect $d$ within $Q_c$, thereby bypassing the complexity of the source query $Q_c$. We evaluate RISE on two real-world benchmarks, i.e., TPC-DS and SQLProcBench, comparing its performance against both the traditional rule-based tools and the LLM-based approaches with respect to translation accuracy. RISE achieves accuracies of 97.98% on TPC-DS and 100% on SQLProcBench, outperforming the baselines by an average improvement of 24.62% and 238.41%, respectively.",
      "authors": [
        "Xudong Xie",
        "Yuwei Zhang",
        "Wensheng Dou",
        "Yu Gao",
        "Ziyu Cui",
        "Jiansen Song",
        "Rui Yang",
        "Jun Wei"
      ],
      "published": "2026-01-09T07:00:44Z",
      "categories": [
        "cs.DB",
        "cs.AI",
        "cs.CL",
        "cs.SE"
      ],
      "primary_category": "cs.DB",
      "abs_url": "https://arxiv.org/abs/2601.05579v1",
      "pdf_url": "https://arxiv.org/pdf/2601.05579v1.pdf"
    },
    {
      "id": "2601.05578v1",
      "title": "Reinforcement Learning of Large Language Models for Interpretable Credit Card Fraud Detection",
      "abstract": "E-commerce platforms and payment solution providers face increasingly sophisticated fraud schemes, ranging from identity theft and account takeovers to complex money laundering operations that exploit the speed and anonymity of digital transactions. However, despite their theoretical promise, the application of Large Language Models (LLMs) to fraud detection in real-world financial contexts remains largely unexploited, and their practical effectiveness in handling domain-specific e-commerce transaction data has yet to be empirically validated. To bridge this gap between conventional machine learning limitations and the untapped potential of LLMs in fraud detection, this paper proposes a novel approach that employs Reinforcement Learning (RL) to post-train lightweight language models specifically for fraud detection tasks using only raw transaction data. We utilize the Group Sequence Policy Optimization (GSPO) algorithm combined with a rule-based reward system to fine-tune language models of various sizes on a real-life transaction dataset provided by a Chinese global payment solution company. Through this reinforcement learning framework, the language models are encouraged to explore diverse trust and risk signals embedded within the textual transaction data, including patterns in customer information, shipping details, product descriptions, and order history. Our experimental results demonstrate the effectiveness of this approach, with post-trained language models achieving substantial F1-score improvements on held-out test data. Our findings demonstrate that the observed performance improvements are primarily attributable to the exploration mechanism inherent in reinforcement learning, which allows models to discover novel fraud indicators beyond those captured by traditional engineered features.",
      "authors": [
        "Cooper Lin",
        "Yanting Zhang",
        "Maohao Ran",
        "Wei Xue",
        "Hongwei Fan",
        "Yibo Xu",
        "Zhenglin Wan",
        "Sirui Han",
        "Yike Guo",
        "Jun Song"
      ],
      "published": "2026-01-09T06:56:27Z",
      "categories": [
        "cs.AI",
        "cs.CE"
      ],
      "primary_category": "cs.AI",
      "abs_url": "https://arxiv.org/abs/2601.05578v1",
      "pdf_url": "https://arxiv.org/pdf/2601.05578v1.pdf"
    },
    {
      "id": "2601.05577v1",
      "title": "Autonomous Discovery of the Ising Model's Critical Parameters with Reinforcement Learning",
      "abstract": "Traditional methods for determining critical parameters are often influenced by human factors. This research introduces a physics-inspired adaptive reinforcement learning framework that enables agents to autonomously interact with physical environments, simultaneously identifying both the critical temperature and various types of critical exponents in the Ising model with precision. Interestingly, our algorithm exhibits search behavior reminiscent of phase transitions, efficiently converging to target parameters regardless of initial conditions. Experimental results demonstrate that this method significantly outperforms traditional approaches, particularly in environments with strong perturbations. This study not only incorporates physical concepts into machine learning to enhance algorithm interpretability but also establishes a new paradigm for scientific exploration, transitioning from manual analysis to autonomous AI discovery.",
      "authors": [
        "Hai Man",
        "Chaobo Wang",
        "Jia-Rui Li",
        "Yuping Tian",
        "Shu-Gang Chen"
      ],
      "published": "2026-01-09T06:55:02Z",
      "categories": [
        "cond-mat.stat-mech",
        "cs.LG",
        "physics.comp-ph"
      ],
      "primary_category": "cond-mat.stat-mech",
      "abs_url": "https://arxiv.org/abs/2601.05577v1",
      "pdf_url": "https://arxiv.org/pdf/2601.05577v1.pdf"
    },
    {
      "id": "2601.05573v1",
      "title": "Orient Anything V2: Unifying Orientation and Rotation Understanding",
      "abstract": "This work presents Orient Anything V2, an enhanced foundation model for unified understanding of object 3D orientation and rotation from single or paired images. Building upon Orient Anything V1, which defines orientation via a single unique front face, V2 extends this capability to handle objects with diverse rotational symmetries and directly estimate relative rotations. These improvements are enabled by four key innovations: 1) Scalable 3D assets synthesized by generative models, ensuring broad category coverage and balanced data distribution; 2) An efficient, model-in-the-loop annotation system that robustly identifies 0 to N valid front faces for each object; 3) A symmetry-aware, periodic distribution fitting objective that captures all plausible front-facing orientations, effectively modeling object rotational symmetry; 4) A multi-frame architecture that directly predicts relative object rotations. Extensive experiments show that Orient Anything V2 achieves state-of-the-art zero-shot performance on orientation estimation, 6DoF pose estimation, and object symmetry recognition across 11 widely used benchmarks. The model demonstrates strong generalization, significantly broadening the applicability of orientation estimation in diverse downstream tasks.",
      "authors": [
        "Zehan Wang",
        "Ziang Zhang",
        "Jiayang Xu",
        "Jialei Wang",
        "Tianyu Pang",
        "Chao Du",
        "HengShuang Zhao",
        "Zhou Zhao"
      ],
      "published": "2026-01-09T06:43:59Z",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "abs_url": "https://arxiv.org/abs/2601.05573v1",
      "pdf_url": "https://arxiv.org/pdf/2601.05573v1.pdf"
    },
    {
      "id": "2601.05572v1",
      "title": "Towards Generalized Multi-Image Editing for Unified Multimodal Models",
      "abstract": "Unified Multimodal Models (UMMs) integrate multimodal understanding and generation, yet they are limited to maintaining visual consistency and disambiguating visual cues when referencing details across multiple input images. In this work, we propose a scalable multi-image editing framework for UMMs that explicitly distinguishes image identities and generalizes to variable input counts. Algorithmically, we introduce two innovations: 1) The learnable latent separators explicitly differentiate each reference image in the latent space, enabling accurate and disentangled conditioning. 2) The sinusoidal index encoding assigns visual tokens from the same image a continuous sinusoidal index embedding, which provides explicit image identity while allowing generalization and extrapolation on a variable number of inputs. To facilitate training and evaluation, we establish a high-fidelity benchmark using an inverse dataset construction methodology to guarantee artifact-free, achievable outputs. Experiments show clear improvements in semantic consistency, visual fidelity, and cross-image integration over prior baselines on diverse multi-image editing tasks, validating our advantages on consistency and generalization ability.",
      "authors": [
        "Pengcheng Xu",
        "Peng Tang",
        "Donghao Luo",
        "Xiaobin Hu",
        "Weichu Cui",
        "Qingdong He",
        "Zhennan Chen",
        "Jiangning Zhang",
        "Charles Ling",
        "Boyu Wang"
      ],
      "published": "2026-01-09T06:42:49Z",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "abs_url": "https://arxiv.org/abs/2601.05572v1",
      "pdf_url": "https://arxiv.org/pdf/2601.05572v1.pdf"
    },
    {
      "id": "2601.05570v1",
      "title": "Crisis-Bench: Benchmarking Strategic Ambiguity and Reputation Management in Large Language Models",
      "abstract": "Standard safety alignment optimizes Large Language Models (LLMs) for universal helpfulness and honesty, effectively instilling a rigid \"Boy Scout\" morality. While robust for general-purpose assistants, this one-size-fits-all ethical framework imposes a \"transparency tax\" on professional domains requiring strategic ambiguity and information withholding, such as public relations, negotiation, and crisis management. To measure this gap between general safety and professional utility, we introduce Crisis-Bench, a multi-agent Partially Observable Markov Decision Process (POMDP) that evaluates LLMs in high-stakes corporate crises. Spanning 80 diverse storylines across 8 industries, Crisis-Bench tasks an LLM-based Public Relations (PR) Agent with navigating a dynamic 7-day corporate crisis simulation while managing strictly separated Private and Public narrative states to enforce rigorous information asymmetry. Unlike traditional benchmarks that rely on static ground truths, we introduce the Adjudicator-Market Loop: a novel evaluation metric where public sentiment is adjudicated and translated into a simulated stock price, creating a realistic economic incentive structure. Our results expose a critical dichotomy: while some models capitulate to ethical concerns, others demonstrate the capacity for Machiavellian, legitimate strategic withholding in order to stabilize the simulated stock price. Crisis-Bench provides the first quantitative framework for assessing \"Reputation Management\" capabilities, arguing for a shift from rigid moral absolutism to context-aware professional alignment.",
      "authors": [
        "Cooper Lin",
        "Maohao Ran",
        "Yanting Zhang",
        "Zhenglin Wan",
        "Hongwei Fan",
        "Yibo Xu",
        "Yike Guo",
        "Wei Xue",
        "Jun Song"
      ],
      "published": "2026-01-09T06:41:49Z",
      "categories": [
        "cs.AI",
        "cs.MA"
      ],
      "primary_category": "cs.AI",
      "abs_url": "https://arxiv.org/abs/2601.05570v1",
      "pdf_url": "https://arxiv.org/pdf/2601.05570v1.pdf"
    },
    {
      "id": "2601.05567v1",
      "title": "WildSci: Advancing Scientific Reasoning from In-the-Wild Literature",
      "abstract": "Recent progress in large language model (LLM) reasoning has focused on domains like mathematics and coding, where abundant high-quality data and objective evaluation metrics are readily available. In contrast, progress in LLM reasoning models remains limited in scientific domains such as medicine and materials science due to limited dataset coverage and the inherent complexity of open-ended scientific questions. To address these challenges, we introduce WildSci, a new dataset of domain-specific science questions automatically synthesized from peer-reviewed literature, covering 9 scientific disciplines and 26 subdomains. By framing complex scientific reasoning tasks in a multiple-choice format, we enable scalable training with well-defined reward signals. We further apply reinforcement learning to finetune models on these data and analyze the resulting training dynamics, including domain-specific performance changes, response behaviors, and generalization trends. Experiments on a suite of scientific benchmarks demonstrate the effectiveness of our dataset and approach. We release WildSci to enable scalable and sustainable research in scientific reasoning, available at https://huggingface.co/datasets/JustinTX/WildSci.",
      "authors": [
        "Tengxiao Liu",
        "Deepak Nathani",
        "Zekun Li",
        "Kevin Yang",
        "William Yang Wang"
      ],
      "published": "2026-01-09T06:35:23Z",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.AI",
      "abs_url": "https://arxiv.org/abs/2601.05567v1",
      "pdf_url": "https://arxiv.org/pdf/2601.05567v1.pdf"
    },
    {
      "id": "2601.05564v1",
      "title": "The ICASSP 2026 HumDial Challenge: Benchmarking Human-like Spoken Dialogue Systems in the LLM Era",
      "abstract": "Driven by the rapid advancement of Large Language Models (LLMs), particularly Audio-LLMs and Omni-models, spoken dialogue systems have evolved significantly, progressively narrowing the gap between human-machine and human-human interactions. Achieving truly ``human-like'' communication necessitates a dual capability: emotional intelligence to perceive and resonate with users' emotional states, and robust interaction mechanisms to navigate the dynamic, natural flow of conversation, such as real-time turn-taking. Therefore, we launched the first Human-like Spoken Dialogue Systems Challenge (HumDial) at ICASSP 2026 to benchmark these dual capabilities. Anchored by a sizable dataset derived from authentic human conversations, this initiative establishes a fair evaluation platform across two tracks: (1) Emotional Intelligence, targeting long-term emotion understanding and empathetic generation; and (2) Full-Duplex Interaction, systematically evaluating real-time decision-making under `` listening-while-speaking'' conditions. This paper summarizes the dataset, track configurations, and the final results.",
      "authors": [
        "Zhixian Zhao",
        "Shuiyuan Wang",
        "Guojian Li",
        "Hongfei Xue",
        "Chengyou Wang",
        "Shuai Wang",
        "Longshuai Xiao",
        "Zihan Zhang",
        "Hui Bu",
        "Xin Xu",
        "Xinsheng Wang",
        "Hexin Liu",
        "Eng Siong Chng",
        "Hung-yi Lee",
        "Haizhou Li",
        "Lei Xie"
      ],
      "published": "2026-01-09T06:32:30Z",
      "categories": [
        "cs.SD",
        "cs.CL",
        "cs.HC",
        "eess.AS"
      ],
      "primary_category": "cs.SD",
      "abs_url": "https://arxiv.org/abs/2601.05564v1",
      "pdf_url": "https://arxiv.org/pdf/2601.05564v1.pdf"
    },
    {
      "id": "2601.05563v1",
      "title": "What's Left Unsaid? Detecting and Correcting Misleading Omissions in Multimodal News Previews",
      "abstract": "Even when factually correct, social-media news previews (image-headline pairs) can induce interpretation drift: by selectively omitting crucial context, they lead readers to form judgments that diverge from what the full article conveys. This covert harm is harder to detect than explicit misinformation yet remains underexplored. To address this gap, we develop a multi-stage pipeline that disentangles and simulates preview-based versus context-based understanding, enabling construction of the MM-Misleading benchmark. Using this benchmark, we systematically evaluate open-source LVLMs and uncover pronounced blind spots to omission-based misleadingness detection. We further propose OMGuard, which integrates (1) Interpretation-Aware Fine-Tuning, which used to improve multimodal misleadingness detection and (2) Rationale-Guided Misleading Content Correction, which uses explicit rationales to guide headline rewriting and reduce misleading impressions. Experiments show that OMGuard lifts an 8B model's detection accuracy to match a 235B LVLM and delivers markedly stronger end-to-end correction. Further analysis reveals that misleadingness typically stems from local narrative shifts (e.g., missing background) rather than global frame changes, and identifies image-driven scenarios where text-only correction fails, highlighting the necessity of visual interventions.",
      "authors": [
        "Fanxiao Li",
        "Jiaying Wu",
        "Tingchao Fu",
        "Dayang Li",
        "Herun Wan",
        "Wei Zhou",
        "Min-Yen Kan"
      ],
      "published": "2026-01-09T06:29:19Z",
      "categories": [
        "cs.CV",
        "cs.SI"
      ],
      "primary_category": "cs.CV",
      "abs_url": "https://arxiv.org/abs/2601.05563v1",
      "pdf_url": "https://arxiv.org/pdf/2601.05563v1.pdf"
    },
    {
      "id": "2601.05560v1",
      "title": "ReasonAny: Incorporating Reasoning Capability to Any Model via Simple and Effective Model Merging",
      "abstract": "Large Reasoning Models (LRMs) with long chain-of-thought reasoning have recently achieved remarkable success. Yet, equipping domain-specialized models with such reasoning capabilities, referred to as \"Reasoning + X\", remains a significant challenge. While model merging offers a promising training-free solution, existing methods often suffer from a destructive performance collapse: existing methods tend to both weaken reasoning depth and compromise domain-specific utility. Interestingly, we identify a counter-intuitive phenomenon underlying this failure: reasoning ability predominantly resides in parameter regions with low gradient sensitivity, contrary to the common assumption that domain capabilities correspond to high-magnitude parameters. Motivated by this insight, we propose ReasonAny, a novel merging framework that resolves the reasoning-domain performance collapse through Contrastive Gradient Identification. Experiments across safety, biomedicine, and finance domains show that ReasonAny effectively synthesizes \"Reasoning + X\" capabilities, significantly outperforming state-of-the-art baselines while retaining robust reasoning performance.",
      "authors": [
        "Junyao Yang",
        "Chen Qian",
        "Dongrui Liu",
        "Wen Shen",
        "Yong Liu",
        "Jing Shao"
      ],
      "published": "2026-01-09T06:19:00Z",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "abs_url": "https://arxiv.org/abs/2601.05560v1",
      "pdf_url": "https://arxiv.org/pdf/2601.05560v1.pdf"
    },
    {
      "id": "2601.05556v1",
      "title": "Semi-Supervised Facial Expression Recognition based on Dynamic Threshold and Negative Learning",
      "abstract": "Facial expression recognition is a key task in human-computer interaction and affective computing. However, acquiring a large amount of labeled facial expression data is often costly. Therefore, it is particularly important to design a semi-supervised facial expression recognition algorithm that makes full use of both labeled and unlabeled data. In this paper, we propose a semi-supervised facial expression recognition algorithm based on Dynamic Threshold Adjustment (DTA) and Selective Negative Learning (SNL). Initially, we designed strategies for local attention enhancement and random dropout of feature maps during feature extraction, which strengthen the representation of local features while ensuring the model does not overfit to any specific local area. Furthermore, this study introduces a dynamic thresholding method to adapt to the requirements of the semi-supervised learning framework for facial expression recognition tasks, and through a selective negative learning strategy, it fully utilizes unlabeled samples with low confidence by mining useful expression information from complementary labels, achieving impressive results. We have achieved state-of-the-art performance on the RAF-DB and AffectNet datasets. Our method surpasses fully supervised methods even without using the entire dataset, which proves the effectiveness of our approach.",
      "authors": [
        "Zhongpeng Cai",
        "Jun Yu",
        "Wei Xu",
        "Tianyu Liu",
        "Jianqing Sun",
        "Jiaen Liang"
      ],
      "published": "2026-01-09T06:13:53Z",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "abs_url": "https://arxiv.org/abs/2601.05556v1",
      "pdf_url": "https://arxiv.org/pdf/2601.05556v1.pdf"
    },
    {
      "id": "2601.05552v1",
      "title": "One Language-Free Foundation Model Is Enough for Universal Vision Anomaly Detection",
      "abstract": "Universal visual anomaly detection (AD) aims to identify anomaly images and segment anomaly regions towards open and dynamic scenarios, following zero- and few-shot paradigms without any dataset-specific fine-tuning. We have witnessed significant progress in widely use of visual-language foundational models in recent approaches. However, current methods often struggle with complex prompt engineering, elaborate adaptation modules, and challenging training strategies, ultimately limiting their flexibility and generality. To address these issues, this paper rethinks the fundamental mechanism behind visual-language models for AD and presents an embarrassingly simple, general, and effective framework for Universal vision Anomaly Detection (UniADet). Specifically, we first find language encoder is used to derive decision weights for anomaly classification and segmentation, and then demonstrate that it is unnecessary for universal AD. Second, we propose an embarrassingly simple method to completely decouple classification and segmentation, and decouple cross-level features, i.e., learning independent weights for different tasks and hierarchical features. UniADet is highly simple (learning only decoupled weights), parameter-efficient (only 0.002M learnable parameters), general (adapting a variety of foundation models), and effective (surpassing state-of-the-art zero-/few-shot by a large margin and even full-shot AD methods for the first time) on 14 real-world AD benchmarks covering both industrial and medical domains. We will make the code and model of UniADet available at https://github.com/gaobb/UniADet.",
      "authors": [
        "Bin-Bin Gao",
        "Chengjie Wang"
      ],
      "published": "2026-01-09T06:05:18Z",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "abs_url": "https://arxiv.org/abs/2601.05552v1",
      "pdf_url": "https://arxiv.org/pdf/2601.05552v1.pdf"
    },
    {
      "id": "2601.05548v1",
      "title": "Generation-Based and Emotion-Reflected Memory Update: Creating the KEEM Dataset for Better Long-Term Conversation",
      "abstract": "In this work, we introduce the Keep Emotional and Essential Memory (KEEM) dataset, a novel generation-based dataset designed to enhance memory updates in long-term conversational systems. Unlike existing approaches that rely on simple accumulation or operation-based methods, which often result in information conflicts and difficulties in accurately tracking a user's current state, KEEM dynamically generates integrative memories. This process not only preserves essential factual information but also incorporates emotional context and causal relationships, enabling a more nuanced understanding of user interactions. By seamlessly updating a system's memory with both emotional and essential data, our approach promotes deeper empathy and enhances the system's ability to respond meaningfully in open-domain conversations.",
      "authors": [
        "Jeonghyun Kang",
        "Hongjin Kim",
        "Harksoo Kim"
      ],
      "published": "2026-01-09T05:59:36Z",
      "categories": [
        "cs.CL"
      ],
      "primary_category": "cs.CL",
      "abs_url": "https://arxiv.org/abs/2601.05548v1",
      "pdf_url": "https://arxiv.org/pdf/2601.05548v1.pdf"
    },
    {
      "id": "2601.05547v1",
      "title": "VIB-Probe: Detecting and Mitigating Hallucinations in Vision-Language Models via Variational Information Bottleneck",
      "abstract": "Vision-Language Models (VLMs) have demonstrated remarkable progress in multimodal tasks, but remain susceptible to hallucinations, where generated text deviates from the underlying visual content. Existing hallucination detection methods primarily rely on output logits or external verification tools, often overlooking their internal mechanisms. In this work, we investigate the outputs of internal attention heads, postulating that specific heads carry the primary signals for truthful generation.However, directly probing these high-dimensional states is challenging due to the entanglement of visual-linguistic syntax and noise. To address this, we propose VIB-Probe, a novel hallucination detection and mitigation framework leveraging the Variational Information Bottleneck (VIB) theory. Our method extracts discriminative patterns across layers and heads while filtering out semantic nuisances through the information bottleneck principle. Furthermore, by leveraging the gradients of our VIB probe, we identify attention heads with strong causal influence on hallucinations and introduce an inference-time intervention strategy for hallucination mitigation. Extensive experiments across diverse benchmarks demonstrate that VIB-Probe significantly outperforms existing baselines in both settings. Our code will be made publicly available.",
      "authors": [
        "Feiran Zhang",
        "Yixin Wu",
        "Zhenghua Wang",
        "Xiaohua Wang",
        "Changze Lv",
        "Xuanjing Huang",
        "Xiaoqing Zheng"
      ],
      "published": "2026-01-09T05:58:22Z",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "abs_url": "https://arxiv.org/abs/2601.05547v1",
      "pdf_url": "https://arxiv.org/pdf/2601.05547v1.pdf"
    },
    {
      "id": "2601.05546v1",
      "title": "MoGen: A Unified Collaborative Framework for Controllable Multi-Object Image Generation",
      "abstract": "Existing multi-object image generation methods face difficulties in achieving precise alignment between localized image generation regions and their corresponding semantics based on language descriptions, frequently resulting in inconsistent object quantities and attribute aliasing. To mitigate this limitation, mainstream approaches typically rely on external control signals to explicitly constrain the spatial layout, local semantic and visual attributes of images. However, this strong dependency makes the input format rigid, rendering it incompatible with the heterogeneous resource conditions of users and diverse constraint requirements. To address these challenges, we propose MoGen, a user-friendly multi-object image generation method. First, we design a Regional Semantic Anchor (RSA) module that precisely anchors phrase units in language descriptions to their corresponding image regions during the generation process, enabling text-to-image generation that follows quantity specifications for multiple objects. Building upon this foundation, we further introduce an Adaptive Multi-modal Guidance (AMG) module, which adaptively parses and integrates various combinations of multi-source control signals to formulate corresponding structured intent. This intent subsequently guides selective constraints on scene layouts and object attributes, achieving dynamic fine-grained control. Experimental results demonstrate that MoGen significantly outperforms existing methods in generation quality, quantity consistency, and fine-grained control, while exhibiting superior accessibility and control flexibility. Code is available at: https://github.com/Tear-kitty/MoGen/tree/master.",
      "authors": [
        "Yanfeng Li",
        "Yue Sun",
        "Keren Fu",
        "Sio-Kei Im",
        "Xiaoming Liu",
        "Guangtao Zhai",
        "Xiaohong Liu",
        "Tao Tan"
      ],
      "published": "2026-01-09T05:57:48Z",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "abs_url": "https://arxiv.org/abs/2601.05546v1",
      "pdf_url": "https://arxiv.org/pdf/2601.05546v1.pdf"
    },
    {
      "id": "2601.05545v1",
      "title": "Can Large Language Models Differentiate Harmful from Argumentative Essays? Steps Toward Ethical Essay Scoring",
      "abstract": "This study addresses critical gaps in Automated Essay Scoring (AES) systems and Large Language Models (LLMs) with regard to their ability to effectively identify and score harmful essays. Despite advancements in AES technology, current models often overlook ethically and morally problematic elements within essays, erroneously assigning high scores to essays that may propagate harmful opinions. In this study, we introduce the Harmful Essay Detection (HED) benchmark, which includes essays integrating sensitive topics such as racism and gender bias, to test the efficacy of various LLMs in recognizing and scoring harmful content. Our findings reveal that: (1) LLMs require further enhancement to accurately distinguish between harmful and argumentative essays, and (2) both current AES models and LLMs fail to consider the ethical dimensions of content during scoring. The study underscores the need for developing more robust AES systems that are sensitive to the ethical implications of the content they are scoring.",
      "authors": [
        "Hongjin Kim",
        "Jeonghyun Kang",
        "Harksoo Kim"
      ],
      "published": "2026-01-09T05:54:40Z",
      "categories": [
        "cs.CL"
      ],
      "primary_category": "cs.CL",
      "abs_url": "https://arxiv.org/abs/2601.05545v1",
      "pdf_url": "https://arxiv.org/pdf/2601.05545v1.pdf"
    },
    {
      "id": "2601.05544v1",
      "title": "Buffered AUC maximization for scoring systems via mixed-integer optimization",
      "abstract": "A scoring system is a linear classifier composed of a small number of explanatory variables, each assigned a small integer coefficient. This system is highly interpretable and allows predictions to be made with simple manual calculations without the need for a calculator. Several previous studies have used mixed-integer optimization (MIO) techniques to develop scoring systems for binary classification; however, they have not focused on directly maximizing AUC (i.e., area under the receiver operating characteristic curve), even though AUC is recognized as an essential evaluation metric for scoring systems. Our goal herein is to establish an effective MIO framework for constructing scoring systems that directly maximize the buffered AUC (bAUC) as the tightest concave lower bound on AUC. Our optimization model is formulated as a mixed-integer linear optimization (MILO) problem that maximizes bAUC subject to a group sparsity constraint for limiting the number of questions in the scoring system. Computational experiments using publicly available real-world datasets demonstrate that our MILO method can build scoring systems with superior AUC values compared to the baseline methods based on regularization and stepwise regression. This research contributes to the advancement of MIO techniques for developing highly interpretable classification models.",
      "authors": [
        "Moe Shiina",
        "Shunnosuke Ikeda",
        "Yuichi Takano"
      ],
      "published": "2026-01-09T05:52:13Z",
      "categories": [
        "cs.LG",
        "math.OC",
        "stat.AP"
      ],
      "primary_category": "cs.LG",
      "abs_url": "https://arxiv.org/abs/2601.05544v1",
      "pdf_url": "https://arxiv.org/pdf/2601.05544v1.pdf"
    },
    {
      "id": "2601.05543v1",
      "title": "Closing the Modality Reasoning Gap for Speech Large Language Models",
      "abstract": "Although speech large language models have achieved notable progress, a substantial modality reasoning gap remains: their reasoning performance on speech inputs is markedly weaker than on text. This gap could be associated with representational drift across Transformer layers and behavior deviations in long-chain reasoning. To address this issue, we introduce TARS, a reinforcement-learning framework that aligns text-conditioned and speech-conditioned trajectories through an asymmetric reward design. The framework employs two dense and complementary signals: representation alignment, which measures layer-wise hidden-state similarity between speech- and text-conditioned trajectories, and behavior alignment, which evaluates semantic consistency between generated outputs and reference text completions. Experiments on challenging reasoning benchmarks, including MMSU and OBQA, show that our approach significantly narrows the modality reasoning gap and achieves state-of-the-art performance among 7B-scale Speech LLMs.",
      "authors": [
        "Chaoren Wang",
        "Heng Lu",
        "Xueyao Zhang",
        "Shujie Liu",
        "Yan Lu",
        "Jinyu Li",
        "Zhizheng Wu"
      ],
      "published": "2026-01-09T05:51:56Z",
      "categories": [
        "cs.CL",
        "cs.SD",
        "eess.AS"
      ],
      "primary_category": "cs.CL",
      "abs_url": "https://arxiv.org/abs/2601.05543v1",
      "pdf_url": "https://arxiv.org/pdf/2601.05543v1.pdf"
    },
    {
      "id": "2601.05542v1",
      "title": "Understanding LLM-Driven Test Oracle Generation",
      "abstract": "Automated unit test generation aims to improve software quality while reducing the time and effort required for creating tests manually. However, existing techniques primarily generate regression oracles that predicate on the implemented behavior of the class under test. They do not address the oracle problem: the challenge of distinguishing correct from incorrect program behavior. With the rise of Foundation Models (FMs), particularly Large Language Models (LLMs), there is a new opportunity to generate test oracles that reflect intended behavior. This positions LLMs as enablers of Promptware, where software creation and testing are driven by natural-language prompts. This paper presents an empirical study on the effectiveness of LLMs in generating test oracles that expose software failures. We investigate how different prompting strategies and levels of contextual input impact the quality of LLM-generated oracles. Our findings offer insights into the strengths and limitations of LLM-based oracle generation in the FM era, improving our understanding of their capabilities and fostering future research in this area.",
      "authors": [
        "Adam Bodicoat",
        "Gunel Jahangirova",
        "Valerio Terragni"
      ],
      "published": "2026-01-09T05:51:35Z",
      "categories": [
        "cs.SE",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.SE",
      "abs_url": "https://arxiv.org/abs/2601.05542v1",
      "pdf_url": "https://arxiv.org/pdf/2601.05542v1.pdf"
    },
    {
      "id": "2601.05538v1",
      "title": "DIFF-MF: A Difference-Driven Channel-Spatial State Space Model for Multi-Modal Image Fusion",
      "abstract": "Multi-modal image fusion aims to integrate complementary information from multiple source images to produce high-quality fused images with enriched content. Although existing approaches based on state space model have achieved satisfied performance with high computational efficiency, they tend to either over-prioritize infrared intensity at the cost of visible details, or conversely, preserve visible structure while diminishing thermal target salience. To overcome these challenges, we propose DIFF-MF, a novel difference-driven channel-spatial state space model for multi-modal image fusion. Our approach leverages feature discrepancy maps between modalities to guide feature extraction, followed by a fusion process across both channel and spatial dimensions. In the channel dimension, a channel-exchange module enhances channel-wise interaction through cross-attention dual state space modeling, enabling adaptive feature reweighting. In the spatial dimension, a spatial-exchange module employs cross-modal state space scanning to achieve comprehensive spatial fusion. By efficiently capturing global dependencies while maintaining linear computational complexity, DIFF-MF effectively integrates complementary multi-modal features. Experimental results on the driving scenarios and low-altitude UAV datasets demonstrate that our method outperforms existing approaches in both visual quality and quantitative evaluation.",
      "authors": [
        "Yiming Sun",
        "Zifan Ye",
        "Qinghua Hu",
        "Pengfei Zhu"
      ],
      "published": "2026-01-09T05:26:54Z",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "abs_url": "https://arxiv.org/abs/2601.05538v1",
      "pdf_url": "https://arxiv.org/pdf/2601.05538v1.pdf"
    },
    {
      "id": "2601.05537v1",
      "title": "Scalable Heterogeneous Graph Learning via Heterogeneous-aware Orthogonal Prototype Experts",
      "abstract": "Heterogeneous Graph Neural Networks(HGNNs) have advanced mainly through better encoders, yet their decoding/projection stage still relies on a single shared linear head, assuming it can map rich node embeddings to labels. We call this the Linear Projection Bottleneck: in heterogeneous graphs, contextual diversity and long-tail shifts make a global head miss fine semantics, overfit hub nodes, and underserve tail nodes. While Mixture-of-Experts(MoE) could help, naively applying it clashes with structural imbalance and risks expert collapse. We propose a Heterogeneous-aware Orthogonal Prototype Experts framework named HOPE, a plug-and-play replacement for the standard prediction head. HOPE uses learnable prototype-based routing to assign instances to experts by similarity, letting expert usage follow the natural long-tail distribution, and adds expert orthogonalization to encourage diversity and prevent collapse. Experiments on four real datasets show consistent gains across SOTA HGNN backbones with minimal overhead.",
      "authors": [
        "Wei Zhou",
        "Hong Huang",
        "Ruize Shi",
        "Bang Liu"
      ],
      "published": "2026-01-09T05:23:59Z",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "abs_url": "https://arxiv.org/abs/2601.05537v1",
      "pdf_url": "https://arxiv.org/pdf/2601.05537v1.pdf"
    },
    {
      "id": "2601.05535v1",
      "title": "SAS-VPReID: A Scale-Adaptive Framework with Shape Priors for Video-based Person Re-Identification at Extreme Far Distances",
      "abstract": "Video-based Person Re-IDentification (VPReID) aims to retrieve the same person from videos captured by non-overlapping cameras. At extreme far distances, VPReID is highly challenging due to severe resolution degradation, drastic viewpoint variation and inevitable appearance noise. To address these issues, we propose a Scale-Adaptive framework with Shape Priors for VPReID, named SAS-VPReID. The framework is built upon three complementary modules. First, we deploy a Memory-Enhanced Visual Backbone (MEVB) to extract discriminative feature representations, which leverages the CLIP vision encoder and multi-proxy memory. Second, we propose a Multi-Granularity Temporal Modeling (MGTM) to construct sequences at multiple temporal granularities and adaptively emphasize motion cues across scales. Third, we incorporate Prior-Regularized Shape Dynamics (PRSD) to capture body structure dynamics. With these modules, our framework can obtain more discriminative feature representations. Experiments on the VReID-XFD benchmark demonstrate the effectiveness of each module and our final framework ranks the first on the VReID-XFD challenge leaderboard. The source code is available at https://github.com/YangQiWei3/SAS-VPReID.",
      "authors": [
        "Qiwei Yang",
        "Pingping Zhang",
        "Yuhao Wang",
        "Zijing Gong"
      ],
      "published": "2026-01-09T05:22:58Z",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "abs_url": "https://arxiv.org/abs/2601.05535v1",
      "pdf_url": "https://arxiv.org/pdf/2601.05535v1.pdf"
    },
    {
      "id": "2601.05533v1",
      "title": "Learning specifications for reactive synthesis with safety constraints",
      "abstract": "This paper presents a novel approach to learning from demonstration that enables robots to autonomously execute complex tasks in dynamic environments. We model latent tasks as probabilistic formal languages and introduce a tailored reactive synthesis framework that balances robot costs with user task preferences. Our methodology focuses on safety-constrained learning and inferring formal task specifications as Probabilistic Deterministic Finite Automata (PDFA). We adapt existing evidence-driven state merging algorithms and incorporate safety requirements throughout the learning process to ensure that the learned PDFA always complies with safety constraints. Furthermore, we introduce a multi-objective reactive synthesis algorithm that generates deterministic strategies that are guaranteed to satisfy the PDFA task while optimizing the trade-offs between user preferences and robot costs, resulting in a Pareto front of optimal solutions. Our approach models the interaction as a two-player game between the robot and the environment, accounting for dynamic changes. We present a computationally-tractable value iteration algorithm to generate the Pareto front and the corresponding deterministic strategies. Comprehensive experimental results demonstrate the effectiveness of our algorithms across various robots and tasks, showing that the learned PDFA never includes unsafe behaviors and that synthesized strategies consistently achieve the task while meeting both the robot cost and user-preference requirements.",
      "authors": [
        "Kandai Watanabe",
        "Nicholas Renninger",
        "Sriram Sankaranarayanan",
        "Morteza Lahijanian"
      ],
      "published": "2026-01-09T05:17:45Z",
      "categories": [
        "cs.RO",
        "cs.FL"
      ],
      "primary_category": "cs.RO",
      "abs_url": "https://arxiv.org/abs/2601.05533v1",
      "pdf_url": "https://arxiv.org/pdf/2601.05533v1.pdf"
    },
    {
      "id": "2601.05531v1",
      "title": "DNATokenizer: A GPU-First Byte-to-Identifier Tokenizer for High-Throughput DNA Language Models",
      "abstract": "Tokenization sits at the boundary between high-throughput genomic input and GPU compute, posing challenges in both algorithm design and system throughput. Overlapping k-mer tokenization can introduce information leakage under masked language modeling (MLM) and may degrade downstream accuracy. Single-nucleotide tokenization avoids leakage and preserves per-base fidelity, but it greatly increases sequence length for attention-based architectures. Non-overlapping k-mers and byte-pair encoding (BPE) provide compression and avoid leakage, at the cost of boundary sensitivity or reduced interpretability. Empirically, the choice of tokenization interacts strongly with model architecture and task requirements. At the system level, however, standard string tokenizers and host-bound vocabulary lookups dominate wall-clock time once inputs reach billions of bases, regardless of the tokenization algorithm. We present DNATok, a high-performance, GPU-first tokenization system that replaces general-purpose string processing with byte lookup table (LUT)-based identifier streaming and an overlapped host-to-device (H2D)/compute pipeline using pinned memory and architectural parallelism. DNATok is vocabulary-agnostic: it accelerates single-nucleotide, non-overlapping k-mer, and BPE tokenization, and integrates as a drop-in systems layer beneath genomic foundation models. DNATok achieves 84-95x higher encoding throughput than optimized Hugging Face baselines and up to 1.9x higher H2D throughput. End-to-end streaming reaches 1.27-1.84e8 tokens/s depending on configuration, effectively removing tokenization as a bottleneck for production-scale training and inference.",
      "authors": [
        "Eliatan Niktab",
        "Hardip Patel"
      ],
      "published": "2026-01-09T05:08:17Z",
      "categories": [
        "q-bio.GN",
        "cs.LG"
      ],
      "primary_category": "q-bio.GN",
      "abs_url": "https://arxiv.org/abs/2601.05531v1",
      "pdf_url": "https://arxiv.org/pdf/2601.05531v1.pdf"
    },
    {
      "id": "2601.05529v1",
      "title": "Safety Not Found (404): Hidden Risks of LLM-Based Robotics Decision Making",
      "abstract": "One mistake by an AI system in a safety-critical setting can cost lives. As Large Language Models (LLMs) become integral to robotics decision-making, the physical dimension of risk grows; a single wrong instruction can directly endanger human safety. This paper addresses the urgent need to systematically evaluate LLM performance in scenarios where even minor errors are catastrophic. Through a qualitative evaluation of a fire evacuation scenario, we identified critical failure cases in LLM-based decision-making. Based on these, we designed seven tasks for quantitative assessment, categorized into: Complete Information, Incomplete Information, and Safety-Oriented Spatial Reasoning (SOSR). Complete information tasks utilize ASCII maps to minimize interpretation ambiguity and isolate spatial reasoning from visual processing. Incomplete information tasks require models to infer missing context, testing for spatial continuity versus hallucinations. SOSR tasks use natural language to evaluate safe decision-making in life-threatening contexts. We benchmark various LLMs and Vision-Language Models (VLMs) across these tasks. Beyond aggregate performance, we analyze the implications of a 1% failure rate, highlighting how \"rare\" errors escalate into catastrophic outcomes. Results reveal serious vulnerabilities: several models achieved a 0% success rate in ASCII navigation, while in a simulated fire drill, models instructed robots to move toward hazardous areas instead of emergency exits. Our findings lead to a sobering conclusion: current LLMs are not ready for direct deployment in safety-critical systems. A 99% accuracy rate is dangerously misleading in robotics, as it implies one out of every hundred executions could result in catastrophic harm. We demonstrate that even state-of-the-art models cannot guarantee safety, and absolute reliance on them creates unacceptable risks.",
      "authors": [
        "Jua Han",
        "Jaeyoon Seo",
        "Jungbin Min",
        "Jean Oh",
        "Jihie Kim"
      ],
      "published": "2026-01-09T05:04:15Z",
      "categories": [
        "cs.AI",
        "cs.RO"
      ],
      "primary_category": "cs.AI",
      "abs_url": "https://arxiv.org/abs/2601.05529v1",
      "pdf_url": "https://arxiv.org/pdf/2601.05529v1.pdf"
    },
    {
      "id": "2601.05528v1",
      "title": "Autonomous Probe Microscopy with Robust Bag-of-Features Multi-Objective Bayesian Optimization: Pareto-Front Mapping of Nanoscale Structure-Property Trade-Offs",
      "abstract": "Combinatorial materials libraries are an efficient route to generate large families of candidate compositions, but their impact is often limited by the speed and depth of characterization and by the difficulty of extracting actionable structure-property relations from complex characterization data. Here we develop an autonomous scanning probe microscopy (SPM) framework that integrates automated atomic force and magnetic force microscopy (AFM/MFM) to rapidly explore magnetic and structural properties across combinatorial spread libraries. To enable automated exploration of systems without a clear optimization target, we introduce a combination of a static physics-informed bag-of-features (BoF) representation of measured surface morphology and magnetic structure with multi-objective Bayesian optimization (MOBO) to discover the relative significance and robustness of features. The resulting closed-loop workflow selectively samples the compositional gradient and reconstructs feature landscapes consistent with dense grid \"ground truth\" measurements. The resulting Pareto structure reveals where multiple nanoscale objectives are simultaneously optimized, where trade-offs between roughness, coherence, and magnetic contrast are unavoidable, and how families of compositions cluster into distinct functional regimes, thereby turning multi-feature imaging data into interpretable maps of competing structure-property trends. While demonstrated for Au-Co-Ni and AFM/MFM, the approach is general and can be extended to other combinatorial systems, imaging modalities, and feature sets, illustrating how feature-based MOBO and autonomous SPM can transform microscopy images from static data products into active feedback for real-time, multi-objective materials discovery.",
      "authors": [
        "Kamyar Barakati",
        "Haochen Zhu",
        "C Charlotte Buchanan",
        "Dustin A Gilbert",
        "Philip Rack",
        "Sergei V. Kalinin"
      ],
      "published": "2026-01-09T04:55:36Z",
      "categories": [
        "cond-mat.mtrl-sci",
        "cs.LG",
        "physics.data-an"
      ],
      "primary_category": "cond-mat.mtrl-sci",
      "abs_url": "https://arxiv.org/abs/2601.05528v1",
      "pdf_url": "https://arxiv.org/pdf/2601.05528v1.pdf"
    },
    {
      "id": "2601.05527v1",
      "title": "DeMa: Dual-Path Delay-Aware Mamba for Efficient Multivariate Time Series Analysis",
      "abstract": "Accurate and efficient multivariate time series (MTS) analysis is increasingly critical for a wide range of intelligent applications. Within this realm, Transformers have emerged as the predominant architecture due to their strong ability to capture pairwise dependencies. However, Transformer-based models suffer from quadratic computational complexity and high memory overhead, limiting their scalability and practical deployment in long-term and large-scale MTS modeling. Recently, Mamba has emerged as a promising linear-time alternative with high expressiveness. Nevertheless, directly applying vanilla Mamba to MTS remains suboptimal due to three key limitations: (i) the lack of explicit cross-variate modeling, (ii) difficulty in disentangling the entangled intra-series temporal dynamics and inter-series interactions, and (iii) insufficient modeling of latent time-lag interaction effects. These issues constrain its effectiveness across diverse MTS tasks. To address these challenges, we propose DeMa, a dual-path delay-aware Mamba backbone. DeMa preserves Mamba's linear-complexity advantage while substantially improving its suitability for MTS settings. Specifically, DeMa introduces three key innovations: (i) it decomposes the MTS into intra-series temporal dynamics and inter-series interactions; (ii) it develops a temporal path with a Mamba-SSD module to capture long-range dynamics within each individual series, enabling series-independent, parallel computation; and (iii) it designs a variate path with a Mamba-DALA module that integrates delay-aware linear attention to model cross-variate dependencies. Extensive experiments on five representative tasks, long- and short-term forecasting, data imputation, anomaly detection, and series classification, demonstrate that DeMa achieves state-of-the-art performance while delivering remarkable computational efficiency.",
      "authors": [
        "Rui An",
        "Haohao Qu",
        "Wenqi Fan",
        "Xuequn Shang",
        "Qing Li"
      ],
      "published": "2026-01-09T04:54:56Z",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "abs_url": "https://arxiv.org/abs/2601.05527v1",
      "pdf_url": "https://arxiv.org/pdf/2601.05527v1.pdf"
    },
    {
      "id": "2601.05525v1",
      "title": "Explainable AI: Learning from the Learners",
      "abstract": "Artificial intelligence now outperforms humans in several scientific and engineering tasks, yet its internal representations often remain opaque. In this Perspective, we argue that explainable artificial intelligence (XAI), combined with causal reasoning, enables {\\it learning from the learners}. Focusing on discovery, optimization and certification, we show how the combination of foundation models and explainability methods allows the extraction of causal mechanisms, guides robust design and control, and supports trust and accountability in high-stakes applications. We discuss challenges in faithfulness, generalization and usability of explanations, and propose XAI as a unifying framework for human-AI collaboration in science and engineering.",
      "authors": [
        "Ricardo Vinuesa",
        "Steven L. Brunton",
        "Gianmarco Mengaldo"
      ],
      "published": "2026-01-09T04:43:21Z",
      "categories": [
        "cs.AI",
        "cs.LG",
        "physics.comp-ph",
        "physics.soc-ph"
      ],
      "primary_category": "cs.AI",
      "abs_url": "https://arxiv.org/abs/2601.05525v1",
      "pdf_url": "https://arxiv.org/pdf/2601.05525v1.pdf"
    },
    {
      "id": "2601.05524v1",
      "title": "Double: Breaking the Acceleration Limit via Double Retrieval Speculative Parallelism",
      "abstract": "Parallel Speculative Decoding (PSD) accelerates traditional Speculative Decoding (SD) by overlapping draft generation with verification. However, it remains hampered by two fundamental challenges: (1) a theoretical speedup ceiling dictated by the speed ratio between the draft and target models, and (2) high computational waste and pipeline stall due to mid-sequence token rejections of early errors. To address these limitations, we introduce \\textsc{Double} (Double Retrieval Speculative Parallelism). By bridging the gap between SD and PSD, our framework resolves the Retrieval \\emph{Precision-Efficiency Dilemma} through a novel synchronous mechanism. Specifically, we enable the draft model to execute iterative retrieval speculations to break the theoretical speedup limits; to alleviate rejections without rollback, the target model performs authoritative retrieval to generate multi-token guidance. \\textsc{Double} is entirely training-free and lossless. Extensive experiments demonstrate state-of-the-art speedup of $\\textbf{5.3}\\times$ on LLaMA3.3-70B and $\\textbf{2.8}\\times$ on Qwen3-32B, significantly outperforming the advanced method EAGLE-3 that requires extensive model training.",
      "authors": [
        "Yuhao Shen",
        "Tianyu Liu",
        "Junyi Shen",
        "Jinyang Wu",
        "Quan Kong",
        "Li Huan",
        "Cong Wang"
      ],
      "published": "2026-01-09T04:35:21Z",
      "categories": [
        "cs.CL"
      ],
      "primary_category": "cs.CL",
      "abs_url": "https://arxiv.org/abs/2601.05524v1",
      "pdf_url": "https://arxiv.org/pdf/2601.05524v1.pdf"
    },
    {
      "id": "2601.05521v1",
      "title": "Toward an Integrated Cross-Urban Accident Prevention System: A Multi-Task Spatial-Temporal Learning Framework for Urban Safety Management",
      "abstract": "The development of a cross-city accident prevention system is particularly challenging due to the heterogeneity, inconsistent reporting, and inherently clustered, sparse, cyclical, and noisy nature of urban accident data. These intrinsic data properties, combined with fragmented governance and incompatible reporting standards, have long hindered the creation of an integrated, cross-city accident prevention framework. To address this gap, we propose the Mamba Local-ttention Spatial-Temporal Network MLA-STNet, a unified system that formulates accident risk prediction as a multi-task learning problem across multiple cities. MLA-STNet integrates two complementary modules: (i)the Spatio-Temporal Geographical Mamba-Attention (STG-MA), which suppresses unstable spatio-temporal fluctuations and strengthens long-range temporal dependencies; and (ii) the Spatio-Temporal Semantic Mamba-Attention (STS-MA), which mitigates cross-city heterogeneity through a shared-parameter design that jointly trains all cities while preserving individual semantic representation spaces. We validate the proposed framework through 75 experiments under two forecasting scenarios, full-day and high-frequency accident periods, using real-world datasets from New York City and Chicago. Compared with the state-of-the-art baselines, MLA-STNet achieves up to 6% lower RMSE, 8% higher Recall, and 5% higher MAP, while maintaining less than 1% performance variation under 50% input noise. These results demonstrate that MLA-STNet effectively unifies heterogeneous urban datasets within a scalable, robust, and interpretable Cross-City Accident Prevention System, paving the way for coordinated and data-driven urban safety management.",
      "authors": [
        "Jiayu Fang",
        "Zhiqi Shao",
        "Haoning Xi",
        "Boris Choy",
        "Junbin Gao"
      ],
      "published": "2026-01-09T04:29:07Z",
      "categories": [
        "cs.LG"
      ],
      "primary_category": "cs.LG",
      "abs_url": "https://arxiv.org/abs/2601.05521v1",
      "pdf_url": "https://arxiv.org/pdf/2601.05521v1.pdf"
    },
    {
      "id": "2601.05520v1",
      "title": "CHisAgent: A Multi-Agent Framework for Event Taxonomy Construction in Ancient Chinese Cultural Systems",
      "abstract": "Despite strong performance on many tasks, large language models (LLMs) show limited ability in historical and cultural reasoning, particularly in non-English contexts such as Chinese history. Taxonomic structures offer an effective mechanism to organize historical knowledge and improve understanding. However, manual taxonomy construction is costly and difficult to scale. Therefore, we propose \\textbf{CHisAgent}, a multi-agent LLM framework for historical taxonomy construction in ancient Chinese contexts. CHisAgent decomposes taxonomy construction into three role-specialized stages: a bottom-up \\textit{Inducer} that derives an initial hierarchy from raw historical corpora, a top-down \\textit{Expander} that introduces missing intermediate concepts using LLM world knowledge, and an evidence-guided \\textit{Enricher} that integrates external structured historical resources to ensure faithfulness. Using the \\textit{Twenty-Four Histories}, we construct a large-scale, domain-aware event taxonomy covering politics, military, diplomacy, and social life in ancient China. Extensive reference-free and reference-based evaluations demonstrate improved structural coherence and coverage, while further analysis shows that the resulting taxonomy supports cross-cultural alignment.",
      "authors": [
        "Xuemei Tang",
        "Chengxi Yan",
        "Jinghang Gu",
        "Chu-Ren Huang"
      ],
      "published": "2026-01-09T04:28:45Z",
      "categories": [
        "cs.CL"
      ],
      "primary_category": "cs.CL",
      "abs_url": "https://arxiv.org/abs/2601.05520v1",
      "pdf_url": "https://arxiv.org/pdf/2601.05520v1.pdf"
    },
    {
      "id": "2601.05511v1",
      "title": "GaussianSwap: Animatable Video Face Swapping with 3D Gaussian Splatting",
      "abstract": "We introduce GaussianSwap, a novel video face swapping framework that constructs a 3D Gaussian Splatting based face avatar from a target video while transferring identity from a source image to the avatar. Conventional video swapping frameworks are limited to generating facial representations in pixel-based formats. The resulting swapped faces exist merely as a set of unstructured pixels without any capacity for animation or interactive manipulation. Our work introduces a paradigm shift from conventional pixel-based video generation to the creation of high-fidelity avatar with swapped faces. The framework first preprocesses target video to extract FLAME parameters, camera poses and segmentation masks, and then rigs 3D Gaussian splats to the FLAME model across frames, enabling dynamic facial control. To ensure identity preserving, we propose an compound identity embedding constructed from three state-of-the-art face recognition models for avatar finetuning. Finally, we render the face-swapped avatar on the background frames to obtain the face-swapped video. Experimental results demonstrate that GaussianSwap achieves superior identity preservation, visual clarity and temporal consistency, while enabling previously unattainable interactive applications.",
      "authors": [
        "Xuan Cheng",
        "Jiahao Rao",
        "Chengyang Li",
        "Wenhao Wang",
        "Weilin Chen",
        "Lvqing Yang"
      ],
      "published": "2026-01-09T03:39:29Z",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "abs_url": "https://arxiv.org/abs/2601.05511v1",
      "pdf_url": "https://arxiv.org/pdf/2601.05511v1.pdf"
    },
    {
      "id": "2601.05508v1",
      "title": "Enabling Stroke-Level Structural Analysis of Hieroglyphic Scripts without Language-Specific Priors",
      "abstract": "Hieroglyphs, as logographic writing systems, encode rich semantic and cultural information within their internal structural composition. Yet, current advanced Large Language Models (LLMs) and Multimodal LLMs (MLLMs) usually remain structurally blind to this information. LLMs process characters as textual tokens, while MLLMs additionally view them as raw pixel grids. Both fall short to model the underlying logic of character strokes. Furthermore, existing structural analysis methods are often script-specific and labor-intensive. In this paper, we propose Hieroglyphic Stroke Analyzer (HieroSA), a novel and generalizable framework that enables MLLMs to automatically derive stroke-level structures from character bitmaps without handcrafted data. It transforms modern logographic and ancient hieroglyphs character images into explicit, interpretable line-segment representations in a normalized coordinate space, allowing for cross-lingual generalization. Extensive experiments demonstrate that HieroSA effectively captures character-internal structures and semantics, bypassing the need for language-specific priors. Experimental results highlight the potential of our work as a graphematics analysis tool for a deeper understanding of hieroglyphic scripts. View our code at https://github.com/THUNLP-MT/HieroSA.",
      "authors": [
        "Fuwen Luo",
        "Zihao Wan",
        "Ziyue Wang",
        "Yaluo Liu",
        "Pau Tong Lin Xu",
        "Xuanjia Qiao",
        "Xiaolong Wang",
        "Peng Li",
        "Yang Liu"
      ],
      "published": "2026-01-09T03:30:12Z",
      "categories": [
        "cs.CV",
        "cs.CL"
      ],
      "primary_category": "cs.CV",
      "abs_url": "https://arxiv.org/abs/2601.05508v1",
      "pdf_url": "https://arxiv.org/pdf/2601.05508v1.pdf"
    },
    {
      "id": "2601.05505v1",
      "title": "FlashMem: Distilling Intrinsic Latent Memory via Computation Reuse",
      "abstract": "The stateless architecture of Large Language Models inherently lacks the mechanism to preserve dynamic context, compelling agents to redundantly reprocess history to maintain long-horizon autonomy. While latent memory offers a solution, current approaches are hindered by architectural segregation, relying on auxiliary encoders that decouple memory from the reasoning backbone. We propose FlashMem, a framework that distills intrinsic memory directly from transient reasoning states via computation reuse. Leveraging the property that internal representations uniquely encode input trajectories, FlashMem identifies the last hidden state as a sufficient statistic for the interaction history. This enables a Shared-KV Consolidator to synthesize memory by attending directly to the backbone's frozen cache, eliminating redundant re-parameterization. Furthermore, a parameter-free Cognitive Monitor leverages attention entropy to adaptively trigger consolidation only when high epistemic uncertainty is detected. Experiments demonstrate that FlashMem matches the performance of heavy baselines while reducing inference latency by 5 times, effectively bridging the gap between efficiency and persistent cognition.",
      "authors": [
        "Yubo Hou",
        "Zhisheng Chen",
        "Tao Wan",
        "Zengchang Qin"
      ],
      "published": "2026-01-09T03:27:43Z",
      "categories": [
        "cs.CL"
      ],
      "primary_category": "cs.CL",
      "abs_url": "https://arxiv.org/abs/2601.05505v1",
      "pdf_url": "https://arxiv.org/pdf/2601.05505v1.pdf"
    },
    {
      "id": "2601.05503v1",
      "title": "Over-Searching in Search-Augmented Large Language Models",
      "abstract": "Search-augmented large language models (LLMs) excel at knowledge-intensive tasks by integrating external retrieval. However, they often over-search -- unnecessarily invoking search tool even when it does not improve response quality, which leads to computational inefficiency and hallucinations by incorporating irrelevant context. In this work, we conduct a systematic evaluation of over-searching across multiple dimensions, including query types, model categories, retrieval conditions, and multi-turn conversations. Our finding shows: (i) search generally improves answer accuracy on answerable queries but harms abstention on unanswerable ones; (ii) over-searching is more pronounced in complex reasoning models and deep research systems, is exacerbated by noisy retrieval, and compounds across turns in multi-turn conversations; and (iii) the composition of retrieved evidence is crucial, as the presence of negative evidence improves abstention. To quantify over-searching, we introduce Tokens Per Correctness (TPC), an evaluation metric that captures the performance-cost trade-off for search-augmented LLMs. Lastly, we investigate mitigation approaches at both the query and retrieval levels and release the OverSearchQA to foster continued research into efficient search-augmented LLMs.",
      "authors": [
        "Roy Xie",
        "Deepak Gopinath",
        "David Qiu",
        "Dong Lin",
        "Haitian Sun",
        "Saloni Potdar",
        "Bhuwan Dhingra"
      ],
      "published": "2026-01-09T03:24:46Z",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "abs_url": "https://arxiv.org/abs/2601.05503v1",
      "pdf_url": "https://arxiv.org/pdf/2601.05503v1.pdf"
    },
    {
      "id": "2601.05502v1",
      "title": "Evaluating the Use of LLMs for Automated DOM-Level Resolution of Web Performance Issues",
      "abstract": "Users demand fast, seamless webpage experiences, yet developers often struggle to meet these expectations within tight constraints. Performance optimization, while critical, is a time-consuming and often manual process. One of the most complex tasks in this domain is modifying the Document Object Model (DOM), which is why this study focuses on it. Recent advances in Large Language Models (LLMs) offer a promising avenue to automate this complex task, potentially transforming how developers address web performance issues. This study evaluates the effectiveness of nine state-of-the-art LLMs for automated web performance issue resolution. For this purpose, we first extracted the DOM trees of 15 popular webpages (e.g., Facebook), and then we used Lighthouse to retrieve their performance audit reports. Subsequently, we passed the extracted DOM trees and corresponding audits to each model for resolution. Our study considers 7 unique audit categories, revealing that LLMs universally excel at SEO & Accessibility issues. However, their efficacy in performance-critical DOM manipulations is mixed. While high-performing models like GPT-4.1 delivered significant reductions in areas like Initial Load, Interactivity, and Network Optimization (e.g., 46.52% to 48.68% audit incidence reductions), others, such as GPT-4o-mini, notably underperformed, consistently. A further analysis of these modifications showed a predominant additive strategy and frequent positional changes, alongside regressions particularly impacting Visual Stability.",
      "authors": [
        "Gideon Peters",
        "SayedHassan Khatoonabadi",
        "Emad Shihab"
      ],
      "published": "2026-01-09T03:21:49Z",
      "categories": [
        "cs.SE",
        "cs.AI"
      ],
      "primary_category": "cs.SE",
      "abs_url": "https://arxiv.org/abs/2601.05502v1",
      "pdf_url": "https://arxiv.org/pdf/2601.05502v1.pdf"
    },
    {
      "id": "2601.05501v1",
      "title": "Hi-ZFO: Hierarchical Zeroth- and First-Order LLM Fine-Tuning via Importance-Guided Tensor Selection",
      "abstract": "Fine-tuning large language models (LLMs) using standard first-order (FO) optimization often drives training toward sharp, poorly generalizing minima. Conversely, zeroth-order (ZO) methods offer stronger exploratory behavior without relying on explicit gradients, yet suffer from slow convergence. More critically, our analysis reveals that in generative tasks, the vast output and search space significantly amplify estimation variance, rendering ZO methods both noisy and inefficient. To address these challenges, we propose \\textbf{Hi-ZFO} (\\textbf{Hi}erarchical \\textbf{Z}eroth- and \\textbf{F}irst-\\textbf{O}rder optimization), a hybrid framework designed to synergize the precision of FO gradients with the exploratory capability of ZO estimation. Hi-ZFO adaptively partitions the model through layer-wise importance profiling, applying precise FO updates to critical layers while leveraging ZO optimization for less sensitive ones. Notably, ZO in Hi-ZFO is not merely a memory-saving surrogate; it is intentionally introduced as a source of \"beneficial stochasticity\" to help the model escape the local minima where pure FO optimization tends to stagnate. Validated across diverse generative, mathematical, and code reasoning tasks, Hi-ZFO consistently achieves superior performance while significantly reducing the training time. These results demonstrate the effectiveness of hierarchical hybrid optimization for LLM fine-tuning.",
      "authors": [
        "Feihu Jin",
        "Ying Tan"
      ],
      "published": "2026-01-09T03:20:54Z",
      "categories": [
        "cs.LG",
        "cs.CL"
      ],
      "primary_category": "cs.LG",
      "abs_url": "https://arxiv.org/abs/2601.05501v1",
      "pdf_url": "https://arxiv.org/pdf/2601.05501v1.pdf"
    },
    {
      "id": "2601.05500v1",
      "title": "The Evaluation Gap in Medicine, AI and LLMs: Navigating Elusive Ground Truth & Uncertainty via a Probabilistic Paradigm",
      "abstract": "Benchmarking the relative capabilities of AI systems, including Large Language Models (LLMs) and Vision Models, typically ignores the impact of uncertainty in the underlying ground truth answers from experts. This ambiguity is particularly consequential in medicine where uncertainty is pervasive. In this paper, we introduce a probabilistic paradigm to theoretically explain how high certainty in ground truth answers is almost always necessary for even an expert to achieve high scores, whereas in datasets with high variation in ground truth answers there may be little difference between a random labeller and an expert. Therefore, ignoring uncertainty in ground truth evaluation data can result in the misleading conclusion that a non-expert has similar performance to that of an expert. Using the probabilistic paradigm, we thus bring forth the concepts of expected accuracy and expected F1 to estimate the score an expert human or system can achieve given ground truth answer variability.\n  Our work leads to the recommendation that when establishing the capability of a system, results should be stratified by probability of the ground truth answer, typically measured by the agreement rate of ground truth experts. Stratification becomes critical when the overall performance drops below a threshold of 80%. Under stratified evaluation, performance comparison becomes more reliable in high certainty bins, mitigating the effect of the key confounding factor -- uncertainty.",
      "authors": [
        "Aparna Elangovan",
        "Lei Xu",
        "Mahsa Elyasi",
        "Ismail Akdulum",
        "Mehmet Aksakal",
        "Enes Gurun",
        "Brian Hur",
        "Saab Mansour",
        "Ravid Shwartz Ziv",
        "Karin Verspoor",
        "Dan Roth"
      ],
      "published": "2026-01-09T03:19:37Z",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "abs_url": "https://arxiv.org/abs/2601.05500v1",
      "pdf_url": "https://arxiv.org/pdf/2601.05500v1.pdf"
    },
    {
      "id": "2601.05499v1",
      "title": "TOSC: Task-Oriented Shape Completion for Open-World Dexterous Grasp Generation from Partial Point Clouds",
      "abstract": "Task-oriented dexterous grasping remains challenging in robotic manipulations of open-world objects under severe partial observation, where significant missing data invalidates generic shape completion. In this paper, to overcome this limitation, we study Task-Oriented Shape Completion, a new task that focuses on completing the potential contact regions rather than the entire shape. We argue that shape completion for grasping should be explicitly guided by the downstream manipulation task. To achieve this, we first generate multiple task-oriented shape completion candidates by leveraging the zero-shot capabilities of object functional understanding from several pre-trained foundation models. A 3D discriminative autoencoder is then proposed to evaluate the plausibility of each generated candidate and optimize the most plausible one from a global perspective. A conditional flow-matching model named FlowGrasp is developed to generate task-oriented dexterous grasps from the optimized shape. Our method achieves state-of-the-art performance in task-oriented dexterous grasping and task-oriented shape completion, improving the Grasp Displacement and the Chamfer Distance over the state-of-the-art by 16.17\\% and 55.26%, respectively. In particular, it shows good capabilities in grasping objects with severe missing data. It also demonstrates good generality in handling open-set categories and tasks.",
      "authors": [
        "Weishang Wu",
        "Yifei Shi",
        "Zhiping Cai"
      ],
      "published": "2026-01-09T03:03:02Z",
      "categories": [
        "cs.RO"
      ],
      "primary_category": "cs.RO",
      "abs_url": "https://arxiv.org/abs/2601.05499v1",
      "pdf_url": "https://arxiv.org/pdf/2601.05499v1.pdf"
    },
    {
      "id": "2601.05498v1",
      "title": "Prompt-Free SAM-Based Multi-Task Framework for Breast Ultrasound Lesion Segmentation and Classification",
      "abstract": "Accurate tumor segmentation and classification in breast ultrasound (BUS) imaging remain challenging due to low contrast, speckle noise, and diverse lesion morphology. This study presents a multi-task deep learning framework that jointly performs lesion segmentation and diagnostic classification using embeddings from the Segment Anything Model (SAM) vision encoder. Unlike prompt-based SAM variants, our approach employs a prompt-free, fully supervised adaptation where high-dimensional SAM features are decoded through either a lightweight convolutional head or a UNet-inspired decoder for pixel-wise segmentation. The classification branch is enhanced via mask-guided attention, allowing the model to focus on lesion-relevant features while suppressing background artifacts. Experiments on the PRECISE 2025 breast ultrasound dataset, split per class into 80 percent training and 20 percent testing, show that the proposed method achieves a Dice Similarity Coefficient (DSC) of 0.887 and an accuracy of 92.3 percent, ranking among the top entries on the PRECISE challenge leaderboard. These results demonstrate that SAM-based representations, when coupled with segmentation-guided learning, significantly improve both lesion delineation and diagnostic prediction in breast ultrasound imaging.",
      "authors": [
        "Samuel E. Johnny",
        "Bernes L. Atabonfack",
        "Israel Alagbe",
        "Assane Gueye"
      ],
      "published": "2026-01-09T03:02:41Z",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "abs_url": "https://arxiv.org/abs/2601.05498v1",
      "pdf_url": "https://arxiv.org/pdf/2601.05498v1.pdf"
    },
    {
      "id": "2601.05495v1",
      "title": "MMViR: A Multi-Modal and Multi-Granularity Representation for Long-range Video Understanding",
      "abstract": "Long videos, ranging from minutes to hours, present significant challenges for current Multi-modal Large Language Models (MLLMs) due to their complex events, diverse scenes, and long-range dependencies. Direct encoding of such videos is computationally too expensive, while simple video-to-text conversion often results in redundant or fragmented content. To address these limitations, we introduce MMViR, a novel multi-modal, multi-grained structured representation for long video understanding. MMViR identifies key turning points to segment the video and constructs a three-level description that couples global narratives with fine-grained visual details. This design supports efficient query-based retrieval and generalizes well across various scenarios. Extensive evaluations across three tasks, including QA, summarization, and retrieval, show that MMViR outperforms the prior strongest method, achieving a 19.67% improvement in hour-long video understanding while reducing processing latency to 45.4% of the original.",
      "authors": [
        "Zizhong Li",
        "Haopeng Zhang",
        "Jiawei Zhang"
      ],
      "published": "2026-01-09T02:59:05Z",
      "categories": [
        "cs.CV",
        "cs.CL"
      ],
      "primary_category": "cs.CV",
      "abs_url": "https://arxiv.org/abs/2601.05495v1",
      "pdf_url": "https://arxiv.org/pdf/2601.05495v1.pdf"
    },
    {
      "id": "2601.05494v1",
      "title": "Hippocampal Atrophy Patterns Across the Alzheimer's Disease Spectrum: A Voxel-Based Morphometry Analysis",
      "abstract": "Alzheimer's disease (AD) and mild cognitive impairment (MCI) are associated with progressive gray matter loss, particularly in medial temporal structures. In this study, CAT12/SPM12 voxel-based morphometry was applied to baseline T1-weighted MRI scans from 249 ADNI participants (CN = 90, MCI = 129, AD = 30). Gray matter volume was analyzed using a general linear model, with the diagnostic group as primary predictor and age and total intracranial volume as covariates. Statistical maps were thresholded at p < 0.001 (voxelwise) and corrected for multiple comparisons at the cluster level using family-wise error (FWE) correction (p < 0.05). Significant hippocampal atrophy was observed in AD relative to CN and MCI (Cohen's d = 2.03 and 1.61, respectively). Hippocampal volume demonstrated moderate predictive value for conversion from MCI to AD (AUC = 0.66). Stratification by APOE4 status did not reveal significant genetic effects on cross-sectional hippocampal volume. These results support medial temporal degeneration as a key feature of AD progression and provide insights into predictive biomarkers and genetic influences.",
      "authors": [
        "Trishna Niraula"
      ],
      "published": "2026-01-09T02:57:38Z",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "abs_url": "https://arxiv.org/abs/2601.05494v1",
      "pdf_url": "https://arxiv.org/pdf/2601.05494v1.pdf"
    },
    {
      "id": "2601.05491v1",
      "title": "Assembling Solar Panels by Dual Robot Arms Towards Full Autonomous Lunar Base Construction",
      "abstract": "Since the successful Apollo program, humanity is once again aiming to return to the Moon for scientific discovery, resource mining, and inhabitation. Upcoming decades focus on building a lunar outpost, with robotic systems playing a crucial role to safely and efficiently establish essential infrastructure such as solar power generating towers. Similar to the construction of the International Space Station (ISS), shipping necessary components via modules and assembling them in situ should be a practical scenario. In this context, this paper focuses on the integration of vision, control, and hardware systems within an autonomous sequence for a dual-arm robot system. We explore a perception and control pipeline specifically designed for assembling solar panel modules, one of the benchmark tasks. Ad hoc hardware was designed and tested in real-world experiments. A mock-up of modular solar panels and active-passive connectors are employed, with the control of this grappling fixture integrated into the proposed pipeline. The successful implementation of our method demonstrates that the two robot manipulators can effectively connect arbitrarily placed panels, highlighting the seamless integration of vision, control, and hardware systems in complex space applications.",
      "authors": [
        "Luca Nunziante",
        "Kentaro Uno",
        "Gustavo H. Diaz",
        "Shreya Santra",
        "Alessandro De Luca",
        "Kazuya Yoshida"
      ],
      "published": "2026-01-09T02:47:13Z",
      "categories": [
        "cs.RO"
      ],
      "primary_category": "cs.RO",
      "abs_url": "https://arxiv.org/abs/2601.05491v1",
      "pdf_url": "https://arxiv.org/pdf/2601.05491v1.pdf"
    },
    {
      "id": "2601.05488v1",
      "title": "MemBuilder: Reinforcing LLMs for Long-Term Memory Construction via Attributed Dense Rewards",
      "abstract": "Maintaining consistency in long-term dialogues remains a fundamental challenge for LLMs, as standard retrieval mechanisms often fail to capture the temporal evolution of historical states. While memory-augmented frameworks offer a structured alternative, current systems rely on static prompting of closed-source models or suffer from ineffective training paradigms with sparse rewards. We introduce MemBuilder, a reinforcement learning framework that trains models to orchestrate multi-dimensional memory construction with attributed dense rewards. MemBuilder addresses two key challenges: (1) Sparse Trajectory-Level Rewards: we employ synthetic session-level question generation to provide dense intermediate rewards across extended trajectories; and (2) Multi-Dimensional Memory Attribution: we introduce contribution-aware gradient weighting that scales policy updates based on each component's downstream impact. Experimental results show that MemBuilder enables a 4B-parameter model to outperform state-of-the-art closed-source baselines, exhibiting strong generalization across long-term dialogue benchmarks.",
      "authors": [
        "Zhiyu Shen",
        "Ziming Wu",
        "Fuming Lai",
        "Shaobing Lian",
        "Yanghui Rao"
      ],
      "published": "2026-01-09T02:44:37Z",
      "categories": [
        "cs.CL"
      ],
      "primary_category": "cs.CL",
      "abs_url": "https://arxiv.org/abs/2601.05488v1",
      "pdf_url": "https://arxiv.org/pdf/2601.05488v1.pdf"
    },
    {
      "id": "2601.05483v1",
      "title": "MMUEChange: A Generalized LLM Agent Framework for Intelligent Multi-Modal Urban Environment Change Analysis",
      "abstract": "Understanding urban environment change is essential for sustainable development. However, current approaches, particularly remote sensing change detection, often rely on rigid, single-modal analysis. To overcome these limitations, we propose MMUEChange, a multi-modal agent framework that flexibly integrates heterogeneous urban data via a modular toolkit and a core module, Modality Controller for cross- and intra-modal alignment, enabling robust analysis of complex urban change scenarios. Case studies include: a shift toward small, community-focused parks in New York, reflecting local green space efforts; the spread of concentrated water pollution across districts in Hong Kong, pointing to coordinated water management; and a notable decline in open dumpsites in Shenzhen, with contrasting links between nighttime economic activity and waste types, indicating differing urban pressures behind domestic and construction waste. Compared to the best-performing baseline, the MMUEChange agent achieves a 46.7% improvement in task success rate and effectively mitigates hallucination, demonstrating its capacity to support complex urban change analysis tasks with real-world policy implications.",
      "authors": [
        "Zixuan Xiao",
        "Jun Ma",
        "Siwei Zhang"
      ],
      "published": "2026-01-09T02:34:35Z",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "abs_url": "https://arxiv.org/abs/2601.05483v1",
      "pdf_url": "https://arxiv.org/pdf/2601.05483v1.pdf"
    },
    {
      "id": "2601.05482v1",
      "title": "Multi-Image Super Resolution Framework for Detection and Analysis of Plant Roots",
      "abstract": "Understanding plant root systems is critical for advancing research in soil-plant interactions, nutrient uptake, and overall plant health. However, accurate imaging of roots in subterranean environments remains a persistent challenge due to adverse conditions such as occlusion, varying soil moisture, and inherently low contrast, which limit the effectiveness of conventional vision-based approaches. In this work, we propose a novel underground imaging system that captures multiple overlapping views of plant roots and integrates a deep learning-based Multi-Image Super Resolution (MISR) framework designed to enhance root visibility and detail. To train and evaluate our approach, we construct a synthetic dataset that simulates realistic underground imaging scenarios, incorporating key environmental factors that affect image quality. Our proposed MISR algorithm leverages spatial redundancy across views to reconstruct high-resolution images with improved structural fidelity and visual clarity. Quantitative evaluations show that our approach outperforms state-of-the-art super resolution baselines, achieving a 2.3 percent reduction in BRISQUE, indicating improved image quality with the same CLIP-IQA score, thereby enabling enhanced phenotypic analysis of root systems. This, in turn, facilitates accurate estimation of critical root traits, including root hair count and root hair density. The proposed framework presents a promising direction for robust automatic underground plant root imaging and trait quantification for agricultural and ecological research.",
      "authors": [
        "Shubham Agarwal",
        "Ofek Nourian",
        "Michael Sidorov",
        "Sharon Chemweno",
        "Ofer Hadar",
        "Naftali Lazarovitch",
        "Jhonathan E. Ephrath"
      ],
      "published": "2026-01-09T02:30:48Z",
      "categories": [
        "cs.CV",
        "cs.ET"
      ],
      "primary_category": "cs.CV",
      "abs_url": "https://arxiv.org/abs/2601.05482v1",
      "pdf_url": "https://arxiv.org/pdf/2601.05482v1.pdf"
    },
    {
      "id": "2601.05478v1",
      "title": "The Facade of Truth: Uncovering and Mitigating LLM Susceptibility to Deceptive Evidence",
      "abstract": "To reliably assist human decision-making, LLMs must maintain factual internal beliefs against misleading injections. While current models resist explicit misinformation, we uncover a fundamental vulnerability to sophisticated, hard-to-falsify evidence. To systematically probe this weakness, we introduce MisBelief, a framework that generates misleading evidence via collaborative, multi-round interactions among multi-role LLMs. This process mimics subtle, defeasible reasoning and progressive refinement to create logically persuasive yet factually deceptive claims. Using MisBelief, we generate 4,800 instances across three difficulty levels to evaluate 7 representative LLMs. Results indicate that while models are robust to direct misinformation, they are highly sensitive to this refined evidence: belief scores in falsehoods increase by an average of 93.0\\%, fundamentally compromising downstream recommendations. To address this, we propose Deceptive Intent Shielding (DIS), a governance mechanism that provides an early warning signal by inferring the deceptive intent behind evidence. Empirical results demonstrate that DIS consistently mitigates belief shifts and promotes more cautious evidence evaluation.",
      "authors": [
        "Herun Wan",
        "Jiaying Wu",
        "Minnan Luo",
        "Fanxiao Li",
        "Zhi Zeng",
        "Min-Yen Kan"
      ],
      "published": "2026-01-09T02:28:00Z",
      "categories": [
        "cs.CL"
      ],
      "primary_category": "cs.CL",
      "abs_url": "https://arxiv.org/abs/2601.05478v1",
      "pdf_url": "https://arxiv.org/pdf/2601.05478v1.pdf"
    },
    {
      "id": "2601.05475v1",
      "title": "MaxCode: A Max-Reward Reinforcement Learning Framework for Automated Code Optimization",
      "abstract": "Large Language Models (LLMs) demonstrate strong capabilities in general coding tasks but encounter two key challenges when optimizing code: (i) the complexity of writing optimized code (such as performant CUDA kernels and competition-level CPU code) requires expertise in systems, algorithms and specific languages and (ii) requires interpretation of performance metrics like timing and device utilization beyond binary correctness. In this work, we explore inference-time search algorithms that guide the LLM to discover better solutions through iterative refinement based on execution feedback. Our approach, called MaxCode unifies existing search methods under a max-reward reinforcement learning framework, making the observation and action-value functions modular for modification. To enhance the observation space, we integrate a natural language critique model that converts raw execution feedback into diagnostic insights about errors and performance bottlenecks, and the best-discounted reward seen so far. Together, these provide richer input to the code proposal function. To improve exploration during search, we train a generative reward-to-go model using action values from rollouts to rerank potential solutions. Testing on the KernelBench (CUDA) and PIE (C++) optimization benchmarks shows that MaxCode improves optimized code performance compared to baselines, achieving 20.3% and 10.1% relative improvements in absolute speedup value and relative speedup ranking, respectively.",
      "authors": [
        "Jiefu Ou",
        "Sapana Chaudhary",
        "Kaj Bostrom",
        "Nathaniel Weir",
        "Shuai Zhang",
        "Huzefa Rangwala",
        "George Karypis"
      ],
      "published": "2026-01-09T02:21:28Z",
      "categories": [
        "cs.LG",
        "cs.CL"
      ],
      "primary_category": "cs.LG",
      "abs_url": "https://arxiv.org/abs/2601.05475v1",
      "pdf_url": "https://arxiv.org/pdf/2601.05475v1.pdf"
    },
    {
      "id": "2601.05474v1",
      "title": "Efficient Differentiable Causal Discovery via Reliable Super-Structure Learning",
      "abstract": "Recently, differentiable causal discovery has emerged as a promising approach to improve the accuracy and efficiency of existing methods. However, when applied to high-dimensional data or data with latent confounders, these methods, often based on off-the-shelf continuous optimization algorithms, struggle with the vast search space, the complexity of the objective function, and the nontrivial nature of graph-theoretical constraints. As a result, there has been a surge of interest in leveraging super-structures to guide the optimization process. Nonetheless, learning an appropriate super-structure at the right level of granularity, and doing so efficiently across various settings, presents significant challenges.\n  In this paper, we propose ALVGL, a novel and general enhancement to the differentiable causal discovery pipeline. ALVGL employs a sparse and low-rank decomposition to learn the precision matrix of the data. We design an ADMM procedure to optimize this decomposition, identifying components in the precision matrix that are most relevant to the underlying causal structure. These components are then combined to construct a super-structure that is provably a superset of the true causal graph. This super-structure is used to initialize a standard differentiable causal discovery method with a more focused search space, thereby improving both optimization efficiency and accuracy.\n  We demonstrate the versatility of ALVGL by instantiating it across a range of structural causal models, including both Gaussian and non-Gaussian settings, with and without unmeasured confounders. Extensive experiments on synthetic and real-world datasets show that ALVGL not only achieves state-of-the-art accuracy but also significantly improves optimization efficiency, making it a reliable and effective solution for differentiable causal discovery.",
      "authors": [
        "Pingchuan Ma",
        "Qixin Zhang",
        "Shuai Wang",
        "Dacheng Tao"
      ],
      "published": "2026-01-09T02:18:59Z",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "abs_url": "https://arxiv.org/abs/2601.05474v1",
      "pdf_url": "https://arxiv.org/pdf/2601.05474v1.pdf"
    },
    {
      "id": "2601.05473v1",
      "title": "Towards Valid Student Simulation with Large Language Models",
      "abstract": "This paper presents a conceptual and methodological framework for large language model (LLM) based student simulation in educational settings. The authors identify a core failure mode, termed the \"competence paradox\" in which broadly capable LLMs are asked to emulate partially knowledgeable learners, leading to unrealistic error patterns and learning dynamics. To address this, the paper reframes student simulation as a constrained generation problem governed by an explicit Epistemic State Specification (ESS), which defines what a simulated learner can access, how errors are structured, and how learner state evolves over time. The work further introduces a Goal-by-Environment framework to situate simulated student systems according to behavioral objectives and deployment contexts. Rather than proposing a new system or benchmark, the paper synthesizes prior literature, formalizes key design dimensions, and articulates open challenges related to validity, evaluation, and ethical risks. Overall, the paper argues for epistemic fidelity over surface realism as a prerequisite for using LLM-based simulated students as reliable scientific and pedagogical instruments.",
      "authors": [
        "Zhihao Yuan",
        "Yunze Xiao",
        "Ming Li",
        "Weihao Xuan",
        "Richard Tong",
        "Mona Diab",
        "Tom Mitchell"
      ],
      "published": "2026-01-09T02:09:52Z",
      "categories": [
        "cs.CL",
        "cs.HC"
      ],
      "primary_category": "cs.CL",
      "abs_url": "https://arxiv.org/abs/2601.05473v1",
      "pdf_url": "https://arxiv.org/pdf/2601.05473v1.pdf"
    },
    {
      "id": "2601.05470v1",
      "title": "ROAP: A Reading-Order and Attention-Prior Pipeline for Optimizing Layout Transformers in Key Information Extraction",
      "abstract": "The efficacy of Multimodal Transformers in visually-rich document understanding (VrDU) is critically constrained by two inherent limitations: the lack of explicit modeling for logical reading order and the interference of visual tokens that dilutes attention on textual semantics.\n  To address these challenges, this paper presents ROAP, a lightweight and architecture-agnostic pipeline designed to optimize attention distributions in Layout Transformers without altering their pre-trained backbones.\n  The proposed pipeline first employs an Adaptive-XY-Gap (AXG-Tree) to robustly extract hierarchical reading sequences from complex layouts. These sequences are then integrated into the attention mechanism via a Reading-Order-Aware Relative Position Bias (RO-RPB). Furthermore, a Textual-Token Sub-block Attention Prior (TT-Prior) is introduced to adaptively suppress visual noise and enhance fine-grained text-text interactions.\n  Extensive experiments on the FUNSD and CORD benchmarks demonstrate that ROAP consistently improves the performance of representative backbones, including LayoutLMv3 and GeoLayoutLM.\n  These findings confirm that explicitly modeling reading logic and regulating modality interference are critical for robust document understanding, offering a scalable solution for complex layout analysis. The implementation code will be released at https://github.com/KevinYuLei/ROAP.",
      "authors": [
        "Tingwei Xie",
        "Jinxin He",
        "Yonghong Song"
      ],
      "published": "2026-01-09T02:02:37Z",
      "categories": [
        "cs.CV",
        "cs.CL"
      ],
      "primary_category": "cs.CV",
      "abs_url": "https://arxiv.org/abs/2601.05470v1",
      "pdf_url": "https://arxiv.org/pdf/2601.05470v1.pdf"
    },
    {
      "id": "2601.05467v1",
      "title": "STELP: Secure Transpilation and Execution of LLM-Generated Programs",
      "abstract": "Rapid evolution of Large Language Models (LLMs) has achieved major advances in reasoning, planning, and function-calling capabilities. Multi-agentic collaborative frameworks using such LLMs place them at the center of solving software development-related tasks such as code generation. However, direct use of LLM generated code in production software development systems is problematic. The code could be unstable or erroneous and contain vulnerabilities such as data poisoning, malicious attacks, and hallucinations that could lead to widespread system malfunctions. This prohibits the adoption of LLM generated code in production AI systems where human code reviews and traditional secure testing tools are impractical or untrustworthy. In this paper, we discuss safety and reliability problems with the execution of LLM generated code and propose a Secure Transpiler and Executor of LLM-Generated Program (STELP), capable of executing LLM-generated code in a controlled and safe manner. STELP secures autonomous production AI systems involving code generation, filling the critical void left by the impracticality or limitations of traditional secure testing methodologies and human oversight. This includes applications such as headless code generation-execution and LLMs that produce executable code snippets as an action plan to be executed in real time. We contribute a human-validated dataset of insecure code snippets and benchmark our approach on publicly available datasets for correctness, safety, and latency. Our results demonstrate that our approach outperforms an existing method by a significant margin, particularly in its ability to safely execute risky code snippets. Warning: This paper contains malicious code snippets that should be run with caution.",
      "authors": [
        "Swapnil Shinde",
        "Sahil Wadhwa",
        "Andy Luo",
        "Emily Chen"
      ],
      "published": "2026-01-09T01:49:41Z",
      "categories": [
        "cs.SE",
        "cs.AI"
      ],
      "primary_category": "cs.SE",
      "abs_url": "https://arxiv.org/abs/2601.05467v1",
      "pdf_url": "https://arxiv.org/pdf/2601.05467v1.pdf"
    },
    {
      "id": "2601.05466v1",
      "title": "Jailbreaking Large Language Models through Iterative Tool-Disguised Attacks via Reinforcement Learning",
      "abstract": "Large language models (LLMs) have demonstrated remarkable capabilities across diverse applications, however, they remain critically vulnerable to jailbreak attacks that elicit harmful responses violating human values and safety guidelines. Despite extensive research on defense mechanisms, existing safeguards prove insufficient against sophisticated adversarial strategies. In this work, we propose iMIST (\\underline{i}nteractive \\underline{M}ulti-step \\underline{P}rogre\\underline{s}sive \\underline{T}ool-disguised Jailbreak Attack), a novel adaptive jailbreak method that synergistically exploits vulnerabilities in current defense mechanisms. iMIST disguises malicious queries as normal tool invocations to bypass content filters, while simultaneously introducing an interactive progressive optimization algorithm that dynamically escalates response harmfulness through multi-turn dialogues guided by real-time harmfulness assessment. Our experiments on widely-used models demonstrate that iMIST achieves higher attack effectiveness, while maintaining low rejection rates. These results reveal critical vulnerabilities in current LLM safety mechanisms and underscore the urgent need for more robust defense strategies.",
      "authors": [
        "Zhaoqi Wang",
        "Zijian Zhang",
        "Daqing He",
        "Pengtao Kou",
        "Xin Li",
        "Jiamou Liu",
        "Jincheng An",
        "Yong Liu"
      ],
      "published": "2026-01-09T01:41:39Z",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "primary_category": "cs.CR",
      "abs_url": "https://arxiv.org/abs/2601.05466v1",
      "pdf_url": "https://arxiv.org/pdf/2601.05466v1.pdf"
    },
    {
      "id": "2601.05465v1",
      "title": "PRISMA: Reinforcement Learning Guided Two-Stage Policy Optimization in Multi-Agent Architecture for Open-Domain Multi-Hop Question Answering",
      "abstract": "Answering real-world open-domain multi-hop questions over massive corpora is a critical challenge in Retrieval-Augmented Generation (RAG) systems. Recent research employs reinforcement learning (RL) to end-to-end optimize the retrieval-augmented reasoning process, directly enhancing its capacity to resolve complex queries. However, reliable deployment is hindered by two obstacles. 1) Retrieval Collapse: iterative retrieval over large corpora fails to locate intermediate evidence containing bridge answers without reasoning-guided planning, causing downstream reasoning to collapse. 2) Learning Instability: end-to-end trajectory training suffers from weak credit assignment across reasoning chains and poor error localization across modules, causing overfitting to benchmark-specific heuristics that limit transferability and stability. To address these problems, we propose PRISMA, a decoupled RL-guided framework featuring a Plan-Retrieve-Inspect-Solve-Memoize architecture. PRISMA's strength lies in reasoning-guided collaboration: the Inspector provides reasoning-based feedback to refine the Planner's decomposition and fine-grained retrieval, while enforcing evidence-grounded reasoning in the Solver. We optimize individual agent capabilities via Two-Stage Group Relative Policy Optimization (GRPO). Stage I calibrates the Planner and Solver as specialized experts in planning and reasoning, while Stage II utilizes Observation-Aware Residual Policy Optimization (OARPO) to enhance the Inspector's ability to verify context and trigger targeted recovery. Experiments show that PRISMA achieves state-of-the-art performance on ten benchmarks and can be deployed efficiently in real-world scenarios.",
      "authors": [
        "Yu Liu",
        "Wenxiao Zhang",
        "Cong Cao",
        "Wenxuan Lu",
        "Fangfang Yuan",
        "Diandian Guo",
        "Kun Peng",
        "Qiang Sun",
        "Kaiyan Zhang",
        "Yanbing Liu",
        "Jin B. Hong",
        "Bowen Zhou",
        "Zhiyuan Ma"
      ],
      "published": "2026-01-09T01:38:38Z",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "abs_url": "https://arxiv.org/abs/2601.05465v1",
      "pdf_url": "https://arxiv.org/pdf/2601.05465v1.pdf"
    },
    {
      "id": "2601.05459v1",
      "title": "Do LLMs Need Inherent Reasoning Before Reinforcement Learning? A Study in Korean Self-Correction",
      "abstract": "Large Language Models (LLMs) demonstrate strong reasoning and self-correction abilities in high-resource languages like English, but their performance remains limited in low-resource languages such as Korean. In this study, we investigate whether reinforcement learning (RL) can enhance Korean reasoning abilities to a degree comparable to English. Our findings reveal that RL alone yields limited improvements when applied to models lacking inherent Korean reasoning capabilities. To address this, we explore several fine-tuning strategies and show that aligning the model's internal reasoning processes with Korean inputs-particularly by tuning Korean-specific neurons in early layers-is key to unlocking RL's effectiveness. We introduce a self-correction code-switching dataset to facilitate this alignment and observe significant performance gains in both mathematical reasoning and self-correction tasks. Ultimately, we conclude that the crucial factor in multilingual reasoning enhancement is not injecting new linguistic knowledge, but effectively eliciting and aligning existing reasoning capabilities. Our study provides a new perspective on how internal translation and neuron-level tuning contribute to multilingual reasoning alignment in LLMs.",
      "authors": [
        "Hongjin Kim",
        "Jaewook Lee",
        "Kiyoung Lee",
        "Jong-hun Shin",
        "Soojong Lim",
        "Oh-Woog Kwon"
      ],
      "published": "2026-01-09T01:17:31Z",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "abs_url": "https://arxiv.org/abs/2601.05459v1",
      "pdf_url": "https://arxiv.org/pdf/2601.05459v1.pdf"
    },
    {
      "id": "2601.05455v1",
      "title": "ART: Adaptive Reasoning Trees for Explainable Claim Verification",
      "abstract": "Large Language Models (LLMs) are powerful candidates for complex decision-making, leveraging vast encoded knowledge and remarkable zero-shot abilities. However, their adoption in high-stakes environments is hindered by their opacity; their outputs lack faithful explanations and cannot be effectively contested to correct errors, undermining trustworthiness. In this paper, we propose ART (Adaptive Reasoning Trees), a hierarchical method for claim verification. The process begins with a root claim, which branches into supporting and attacking child arguments. An argument's strength is determined bottom-up via a pairwise tournament of its children, adjudicated by a judge LLM, allowing a final, transparent and contestable verdict to be systematically derived which is missing in methods like Chain-of-Thought (CoT). We empirically validate ART on multiple datasets, analyzing different argument generators and comparison strategies. Our findings show that ART's structured reasoning outperforms strong baselines, establishing a new benchmark for explainable claim verification which is more reliable and ensures clarity in the overall decision making step.",
      "authors": [
        "Sahil Wadhwa",
        "Himanshu Kumar",
        "Guanqun Yang",
        "Abbaas Alif Mohamed Nishar",
        "Pranab Mohanty",
        "Swapnil Shinde",
        "Yue Wu"
      ],
      "published": "2026-01-09T01:01:55Z",
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "abs_url": "https://arxiv.org/abs/2601.05455v1",
      "pdf_url": "https://arxiv.org/pdf/2601.05455v1.pdf"
    },
    {
      "id": "2601.05451v1",
      "title": "RingSQL: Generating Synthetic Data with Schema-Independent Templates for Text-to-SQL Reasoning Models",
      "abstract": "Recent advances in text-to-SQL systems have been driven by larger models and improved datasets, yet progress is still limited by the scarcity of high-quality training data. Manual data creation is expensive, and existing synthetic methods trade off reliability and scalability. Template-based approaches ensure correct SQL but require schema-specific templates, while LLM-based generation scales easily but lacks quality and correctness guarantees. We introduce RingSQL, a hybrid data generation framework that combines schema-independent query templates with LLM-based paraphrasing of natural language questions. This approach preserves SQL correctness across diverse schemas while providing broad linguistic variety. In our experiments, we find that models trained using data produced by RingSQL achieve an average gain in accuracy of +2.3% across six text-to-SQL benchmarks when compared to models trained on other synthetic data. We make our code available at https://github.com/nu-c3lab/RingSQL.",
      "authors": [
        "Marko Sterbentz",
        "Kevin Cushing",
        "Cameron Barrie",
        "Kristian J. Hammond"
      ],
      "published": "2026-01-09T00:46:53Z",
      "categories": [
        "cs.LG",
        "cs.CL"
      ],
      "primary_category": "cs.LG",
      "abs_url": "https://arxiv.org/abs/2601.05451v1",
      "pdf_url": "https://arxiv.org/pdf/2601.05451v1.pdf"
    },
    {
      "id": "2601.05446v1",
      "title": "TAPM-Net: Trajectory-Aware Perturbation Modeling for Infrared Small Target Detection",
      "abstract": "Infrared small target detection (ISTD) remains a long-standing challenge due to weak signal contrast, limited spatial extent, and cluttered backgrounds. Despite performance improvements from convolutional neural networks (CNNs) and Vision Transformers (ViTs), current models lack a mechanism to trace how small targets trigger directional, layer-wise perturbations in the feature space, which is an essential cue for distinguishing signal from structured noise in infrared scenes. To address this limitation, we propose the Trajectory-Aware Mamba Propagation Network (TAPM-Net), which explicitly models the spatial diffusion behavior of target-induced feature disturbances. TAPM-Net is built upon two novel components: a Perturbation-guided Path Module (PGM) and a Trajectory-Aware State Block (TASB). The PGM constructs perturbation energy fields from multi-level features and extracts gradient-following feature trajectories that reflect the directionality of local responses. The resulting feature trajectories are fed into the TASB, a Mamba-based state-space unit that models dynamic propagation along each trajectory while incorporating velocity-constrained diffusion and semantically aligned feature fusion from word-level and sentence-level embeddings. Unlike existing attention-based methods, TAPM-Net enables anisotropic, context-sensitive state transitions along spatial trajectories while maintaining global coherence at low computational cost. Experiments on NUAA-SIRST and IRSTD-1K demonstrate that TAPM-Net achieves state-of-the-art performance in ISTD.",
      "authors": [
        "Hongyang Xie",
        "Hongyang He",
        "Victor Sanchez"
      ],
      "published": "2026-01-09T00:27:18Z",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "abs_url": "https://arxiv.org/abs/2601.05446v1",
      "pdf_url": "https://arxiv.org/pdf/2601.05446v1.pdf"
    },
    {
      "id": "2601.05445v1",
      "title": "Knowledge-Driven Multi-Turn Jailbreaking on Large Language Models",
      "abstract": "Large Language Models (LLMs) face a significant threat from multi-turn jailbreak attacks, where adversaries progressively steer conversations to elicit harmful outputs. However, the practical effectiveness of existing attacks is undermined by several critical limitations: they struggle to maintain a coherent progression over long interactions, often losing track of what has been accomplished and what remains to be done; they rely on rigid or pre-defined patterns, and fail to adapt to the LLM's dynamic and unpredictable conversational state. To address these shortcomings, we introduce Mastermind, a multi-turn jailbreak framework that adopts a dynamic and self-improving approach. Mastermind operates in a closed loop of planning, execution, and reflection, enabling it to autonomously build and refine its knowledge of model vulnerabilities through interaction. It employs a hierarchical planning architecture that decouples high-level attack objectives from low-level tactical execution, ensuring long-term focus and coherence. This planning is guided by a knowledge repository that autonomously discovers and refines effective attack patterns by reflecting on interactive experiences. Mastermind leverages this accumulated knowledge to dynamically recombine and adapt attack vectors, dramatically improving both effectiveness and resilience. We conduct comprehensive experiments against state-of-the-art models, including GPT-5 and Claude 3.7 Sonnet. The results demonstrate that Mastermind significantly outperforms existing baselines, achieving substantially higher attack success rates and harmfulness ratings. Moreover, our framework exhibits notable resilience against multiple advanced defense mechanisms.",
      "authors": [
        "Songze Li",
        "Ruishi He",
        "Xiaojun Jia",
        "Jun Wang",
        "Zhihui Fu"
      ],
      "published": "2026-01-09T00:27:08Z",
      "categories": [
        "cs.CR",
        "cs.LG"
      ],
      "primary_category": "cs.CR",
      "abs_url": "https://arxiv.org/abs/2601.05445v1",
      "pdf_url": "https://arxiv.org/pdf/2601.05445v1.pdf"
    },
    {
      "id": "2601.05444v1",
      "title": "What Functions Does XGBoost Learn?",
      "abstract": "This paper establishes a rigorous theoretical foundation for the function class implicitly learned by XGBoost, bridging the gap between its empirical success and our theoretical understanding. We introduce an infinite-dimensional function class $\\mathcal{F}^{d, s}_{\\infty-\\text{ST}}$ that extends finite ensembles of bounded-depth regression trees, together with a complexity measure $V^{d, s}_{\\infty-\\text{XGB}}(\\cdot)$ that generalizes the $L^1$ regularization penalty used in XGBoost. We show that every optimizer of the XGBoost objective is also an optimizer of an equivalent penalized regression problem over $\\mathcal{F}^{d, s}_{\\infty-\\text{ST}}$ with penalty $V^{d, s}_{\\infty-\\text{XGB}}(\\cdot)$, providing an interpretation of XGBoost as implicitly targeting a broader function class. We also develop a smoothness-based interpretation of $\\mathcal{F}^{d, s}_{\\infty-\\text{ST}}$ and $V^{d, s}_{\\infty-\\text{XGB}}(\\cdot)$ in terms of Hardy--Krause variation. We prove that the least squares estimator over $\\{f \\in \\mathcal{F}^{d, s}_{\\infty-\\text{ST}}: V^{d, s}_{\\infty-\\text{XGB}}(f) \\le V\\}$ achieves a nearly minimax-optimal rate of convergence $n^{-2/3} (\\log n)^{4(\\min(s, d) - 1)/3}$, thereby avoiding the curse of dimensionality. Our results provide the first rigorous characterization of the function space underlying XGBoost, clarify its connection to classical notions of variation, and identify an important open problem: whether the XGBoost algorithm itself achieves minimax optimality over this class.",
      "authors": [
        "Dohyeong Ki",
        "Adityanand Guntuboyina"
      ],
      "published": "2026-01-09T00:22:08Z",
      "categories": [
        "math.ST",
        "cs.LG",
        "stat.ML"
      ],
      "primary_category": "math.ST",
      "abs_url": "https://arxiv.org/abs/2601.05444v1",
      "pdf_url": "https://arxiv.org/pdf/2601.05444v1.pdf"
    },
    {
      "id": "2601.05441v1",
      "title": "A brief note on learning problem with global perspectives",
      "abstract": "This brief note considers the problem of learning with dynamic-optimizing principal-agent setting, in which the agents are allowed to have global perspectives about the learning process, i.e., the ability to view things according to their relative importances or in their true relations based-on some aggregated information shared by the principal. Whereas, the principal, which is exerting an influence on the learning process of the agents in the aggregation, is primarily tasked to solve a high-level optimization problem posed as an empirical-likelihood estimator under conditional moment restrictions model that also accounts information about the agents' predictive performances on out-of-samples as well as a set of private datasets available only to the principal. In particular, we present a coherent mathematical argument which is necessary for characterizing the learning process behind this abstract principal-agent learning framework, although we acknowledge that there are a few conceptual and theoretical issues still need to be addressed.",
      "authors": [
        "Getachew K. Befekadu"
      ],
      "published": "2026-01-09T00:20:36Z",
      "categories": [
        "stat.ML",
        "cs.LG",
        "math.OC"
      ],
      "primary_category": "stat.ML",
      "abs_url": "https://arxiv.org/abs/2601.05441v1",
      "pdf_url": "https://arxiv.org/pdf/2601.05441v1.pdf"
    },
    {
      "id": "2601.05437v1",
      "title": "Tracing Moral Foundations in Large Language Models",
      "abstract": "Large language models (LLMs) often produce human-like moral judgments, but it is unclear whether this reflects an internal conceptual structure or superficial ``moral mimicry.'' Using Moral Foundations Theory (MFT) as an analytic framework, we study how moral foundations are encoded, organized, and expressed within two instruction-tuned LLMs: Llama-3.1-8B-Instruct and Qwen2.5-7B-Instruct. We employ a multi-level approach combining (i) layer-wise analysis of MFT concept representations and their alignment with human moral perceptions, (ii) pretrained sparse autoencoders (SAEs) over the residual stream to identify sparse features that support moral concepts, and (iii) causal steering interventions using dense MFT vectors and sparse SAE features. We find that both models represent and distinguish moral foundations in a structured, layer-dependent way that aligns with human judgments. At a finer scale, SAE features show clear semantic links to specific foundations, suggesting partially disentangled mechanisms within shared representations. Finally, steering along either dense vectors or sparse features produces predictable shifts in foundation-relevant behavior, demonstrating a causal connection between internal representations and moral outputs. Together, our results provide mechanistic evidence that moral concepts in LLMs are distributed, layered, and partly disentangled, suggesting that pluralistic moral structure can emerge as a latent pattern from the statistical regularities of language alone.",
      "authors": [
        "Chenxiao Yu",
        "Bowen Yi",
        "Farzan Karimi-Malekabadi",
        "Suhaib Abdurahman",
        "Jinyi Ye",
        "Shrikanth Narayanan",
        "Yue Zhao",
        "Morteza Dehghani"
      ],
      "published": "2026-01-09T00:09:28Z",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "abs_url": "https://arxiv.org/abs/2601.05437v1",
      "pdf_url": "https://arxiv.org/pdf/2601.05437v1.pdf"
    },
    {
      "id": "2601.05432v1",
      "title": "Thinking with Map: Reinforced Parallel Map-Augmented Agent for Geolocalization",
      "abstract": "The image geolocalization task aims to predict the location where an image was taken anywhere on Earth using visual clues. Existing large vision-language model (LVLM) approaches leverage world knowledge, chain-of-thought reasoning, and agentic capabilities, but overlook a common strategy used by humans -- using maps. In this work, we first equip the model \\textit{Thinking with Map} ability and formulate it as an agent-in-the-map loop. We develop a two-stage optimization scheme for it, including agentic reinforcement learning (RL) followed by parallel test-time scaling (TTS). The RL strengthens the agentic capability of model to improve sampling efficiency, and the parallel TTS enables the model to explore multiple candidate paths before making the final prediction, which is crucial for geolocalization. To evaluate our method on up-to-date and in-the-wild images, we further present MAPBench, a comprehensive geolocalization training and evaluation benchmark composed entirely of real-world images. Experimental results show that our method outperforms existing open- and closed-source models on most metrics, specifically improving Acc@500m from 8.0\\% to 22.1\\% compared to \\textit{Gemini-3-Pro} with Google Search/Map grounded mode.",
      "authors": [
        "Yuxiang Ji",
        "Yong Wang",
        "Ziyu Ma",
        "Yiming Hu",
        "Hailang Huang",
        "Xuecai Hu",
        "Guanhua Chen",
        "Liaoni Wu",
        "Xiangxiang Chu"
      ],
      "published": "2026-01-08T23:47:30Z",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.CV",
      "abs_url": "https://arxiv.org/abs/2601.05432v1",
      "pdf_url": "https://arxiv.org/pdf/2601.05432v1.pdf"
    },
    {
      "id": "2601.05431v1",
      "title": "Prediction of Fault Slip Tendency in CO${_2}$ Storage using Data-space Inversion",
      "abstract": "Accurately assessing the potential for fault slip is essential in many subsurface operations. Conventional model-based history matching methods, which entail the generation of posterior geomodels calibrated to observed data, can be challenging to apply in coupled flow-geomechanics problems with faults. In this work, we implement a variational autoencoder (VAE)-based data-space inversion (DSI) framework to predict pressure, stress and strain fields, and fault slip tendency, in CO${_2}$ storage projects. The main computations required by the DSI workflow entail the simulation of O(1000) prior geomodels. The posterior distributions for quantities of interest are then inferred directly from prior simulation results and observed data, without the need to generate posterior geomodels. The model used here involves a synthetic 3D system with two faults. Realizations of heterogeneous permeability and porosity fields are generated using geostatistical software, and uncertain geomechanical and fault parameters are sampled for each realization from prior distributions. Coupled flow-geomechanics simulations for these geomodels are conducted using GEOS. A VAE with stacked convolutional long short-term memory layers is trained, using the prior simulation results, to represent pressure, strain, effective normal stress and shear stress fields in terms of latent variables. The VAE parameterization is used with DSI for posterior predictions, with monitoring wells providing observed pressure and strain data. Posterior results for synthetic true models demonstrate that the DSI-VAE framework gives accurate predictions for pressure, strain, and stress fields and for fault slip tendency. The framework is also shown to reduce uncertainty in key geomechanical and fault parameters.",
      "authors": [
        "Xiaowen He",
        "Su Jiang",
        "Louis J. Durlofsky"
      ],
      "published": "2026-01-08T23:41:04Z",
      "categories": [
        "cs.LG"
      ],
      "primary_category": "cs.LG",
      "abs_url": "https://arxiv.org/abs/2601.05431v1",
      "pdf_url": "https://arxiv.org/pdf/2601.05431v1.pdf"
    },
    {
      "id": "2601.05428v1",
      "title": "Dynamic Inclusion and Bounded Multi-Factor Tilts for Robust Portfolio Construction",
      "abstract": "This paper proposes a portfolio construction framework designed to remain robust under estimation error, non-stationarity, and realistic trading constraints. The methodology combines dynamic asset eligibility, deterministic rebalancing, and bounded multi-factor tilts applied to an equal-weight baseline. Asset eligibility is formalized as a state-dependent constraint on portfolio construction, allowing factor exposure to adjust endogenously in response to observable market conditions such as liquidity, volatility, and cross-sectional breadth. Rather than estimating expected returns or covariances, the framework relies on cross-sectional rankings and hard structural bounds to control concentration, turnover, and fragility. The resulting approach is fully algorithmic, transparent, and directly implementable. It provides a robustness-oriented alternative to parametric optimization and unconstrained multi-factor models, particularly suited for long-horizon allocations where stability and operational feasibility are primary objectives.",
      "authors": [
        "Roberto Garrone"
      ],
      "published": "2026-01-08T23:26:58Z",
      "categories": [
        "math.OC",
        "cs.LG"
      ],
      "primary_category": "math.OC",
      "abs_url": "https://arxiv.org/abs/2601.05428v1",
      "pdf_url": "https://arxiv.org/pdf/2601.05428v1.pdf"
    },
    {
      "id": "2601.05420v1",
      "title": "Efficient Inference for Noisy LLM-as-a-Judge Evaluation",
      "abstract": "Large language models (LLMs) are increasingly used as automatic evaluators of generative AI outputs, a paradigm often referred to as \"LLM-as-a-judge.\" In practice, LLM judges are imperfect predictions for the underlying truth and can exhibit systematic, non-random errors. Two main approaches have recently been proposed to address this issue: (i) direct measurementerror correction based on misclassification models such as Rogan-Gladen-style estimators, and (ii) surrogate-outcome approaches such as prediction-powered inference (PPI), which correct bias by calibrating prediction residuals on a small set of gold-standard human labels. In this paper, we systematically study the performance of these two approaches for estimating mean parameters (e.g., average benchmark scores or pairwise win rates). Leveraging tools from semiparametric efficiency theory, we unify the two classes of estimators by deriving explicit forms of efficient influence function (EIF)-based efficient estimators and characterize conditions under which PPI-style estimators attain strictly smaller asymptotic variance than measurement-error corrections. We verify our theoretical results in simulations and demonstrate the methods on real-data examples. We provide an implementation of the benchmarked methods and comparison utilities at https://github.com/yiqunchen/debias-llm-as-a-judge.",
      "authors": [
        "Yiqun T Chen",
        "Sizhu Lu",
        "Sijia Li",
        "Moran Guo",
        "Shengyi Li"
      ],
      "published": "2026-01-08T22:46:26Z",
      "categories": [
        "cs.LG",
        "stat.AP",
        "stat.ME"
      ],
      "primary_category": "cs.LG",
      "abs_url": "https://arxiv.org/abs/2601.05420v1",
      "pdf_url": "https://arxiv.org/pdf/2601.05420v1.pdf"
    },
    {
      "id": "2601.05414v1",
      "title": "Large Language Models Are Bad Dice Players: LLMs Struggle to Generate Random Numbers from Statistical Distributions",
      "abstract": "As large language models (LLMs) transition from chat interfaces to integral components of stochastic pipelines across domains like educational assessment and synthetic data construction, the ability to faithfully sample from specified probability distributions has become a functional requirement rather than a theoretical curiosity. We present the first large-scale, statistically powered audit of native probabilistic sampling in frontier LLMs, benchmarking 11 models across 15 distributions. To disentangle failure modes, we employ a dual-protocol design: Batch Generation, where a model produces N=1000 samples within one response, and Independent Requests, comprising $N=1000$ stateless calls. We observe a sharp protocol asymmetry: batch generation achieves only modest statistical validity, with a 13% median pass rate, while independent requests collapse almost entirely, with 10 of 11 models passing none of the distributions. Beyond this asymmetry, we reveal that sampling fidelity degrades monotonically with distributional complexity and aggravates as the requested sampling horizon N increases. Finally, we demonstrate the propagation of these failures into downstream tasks: models fail to enforce uniform answer-position constraints in MCQ generation and systematically violate demographic targets in attribute-constrained text-to-image prompt synthesis. These findings indicate that current LLMs lack a functional internal sampler, necessitating the use of external tools for applications requiring statistical guarantees.",
      "authors": [
        "Minda Zhao",
        "Yilun Du",
        "Mengyu Wang"
      ],
      "published": "2026-01-08T22:33:12Z",
      "categories": [
        "cs.CL"
      ],
      "primary_category": "cs.CL",
      "abs_url": "https://arxiv.org/abs/2601.05414v1",
      "pdf_url": "https://arxiv.org/pdf/2601.05414v1.pdf"
    },
    {
      "id": "2601.05411v1",
      "title": "Glitter: Visualizing Lexical Surprisal for Readability in Administrative Texts",
      "abstract": "This work investigates how measuring information entropy of text can be used to estimate its readability. We propose a visualization framework that can be used to approximate information entropy of text using multiple language models and visualize the result. The end goal is to use this method to estimate and improve readability and clarity of administrative or bureaucratic texts. Our toolset is available as a libre software on https://github.com/ufal/Glitter.",
      "authors": [
        "Jan Černý",
        "Ivana Kvapilíková",
        "Silvie Cinková"
      ],
      "published": "2026-01-08T22:27:24Z",
      "categories": [
        "cs.CL"
      ],
      "primary_category": "cs.CL",
      "abs_url": "https://arxiv.org/abs/2601.05411v1",
      "pdf_url": "https://arxiv.org/pdf/2601.05411v1.pdf"
    },
    {
      "id": "2601.05407v1",
      "title": "Interactive Distillation for Cooperative Multi-Agent Reinforcement Learning",
      "abstract": "Knowledge distillation (KD) has the potential to accelerate MARL by employing a centralized teacher for decentralized students but faces key bottlenecks. Specifically, there are (1) challenges in synthesizing high-performing teaching policies in complex domains, (2) difficulties when teachers must reason in out-of-distribution (OOD) states, and (3) mismatches between the decentralized students' and the centralized teacher's observation spaces. To address these limitations, we propose HINT (Hierarchical INteractive Teacher-based transfer), a novel KD framework for MARL in a centralized training, decentralized execution setup. By leveraging hierarchical RL, HINT provides a scalable, high-performing teacher. Our key innovation, pseudo off-policy RL, enables the teacher policy to be updated using both teacher and student experience, thereby improving OOD adaptation. HINT also applies performance-based filtering to retain only outcome-relevant guidance, reducing observation mismatches. We evaluate HINT on challenging cooperative domains (e.g., FireCommander for resource allocation, MARINE for tactical combat). Across these benchmarks, HINT outperforms baselines, achieving improvements of 60% to 165% in success rate.",
      "authors": [
        "Minwoo Cho",
        "Batuhan Altundas",
        "Matthew Gombolay"
      ],
      "published": "2026-01-08T22:16:43Z",
      "categories": [
        "cs.LG",
        "cs.MA"
      ],
      "primary_category": "cs.LG",
      "abs_url": "https://arxiv.org/abs/2601.05407v1",
      "pdf_url": "https://arxiv.org/pdf/2601.05407v1.pdf"
    },
    {
      "id": "2601.05403v1",
      "title": "Same Claim, Different Judgment: Benchmarking Scenario-Induced Bias in Multilingual Financial Misinformation Detection",
      "abstract": "Large language models (LLMs) have been widely applied across various domains of finance. Since their training data are largely derived from human-authored corpora, LLMs may inherit a range of human biases. Behavioral biases can lead to instability and uncertainty in decision-making, particularly when processing financial information. However, existing research on LLM bias has mainly focused on direct questioning or simplified, general-purpose settings, with limited consideration of the complex real-world financial environments and high-risk, context-sensitive, multilingual financial misinformation detection tasks (\\mfmd). In this work, we propose \\mfmdscen, a comprehensive benchmark for evaluating behavioral biases of LLMs in \\mfmd across diverse economic scenarios. In collaboration with financial experts, we construct three types of complex financial scenarios: (i) role- and personality-based, (ii) role- and region-based, and (iii) role-based scenarios incorporating ethnicity and religious beliefs. We further develop a multilingual financial misinformation dataset covering English, Chinese, Greek, and Bengali. By integrating these scenarios with misinformation claims, \\mfmdscen enables a systematic evaluation of 22 mainstream LLMs. Our findings reveal that pronounced behavioral biases persist across both commercial and open-source models. This project will be available at https://github.com/lzw108/FMD.",
      "authors": [
        "Zhiwei Liu",
        "Yupen Cao",
        "Yuechen Jiang",
        "Mohsinul Kabir",
        "Polydoros Giannouris",
        "Chen Xu",
        "Ziyang Xu",
        "Tianlei Zhu",
        "Tariquzzaman Faisal",
        "Triantafillos Papadopoulos",
        "Yan Wang",
        "Lingfei Qian",
        "Xueqing Peng",
        "Zhuohan Xie",
        "Ye Yuan",
        "Saeed Almheiri",
        "Abdulrazzaq Alnajjar",
        "Mingbin Chen",
        "Harry Stuart",
        "Paul Thompson",
        "Prayag Tiwari",
        "Alejandro Lopez-Lira",
        "Xue Liu",
        "Jimin Huang",
        "Sophia Ananiadou"
      ],
      "published": "2026-01-08T22:00:32Z",
      "categories": [
        "cs.CL"
      ],
      "primary_category": "cs.CL",
      "abs_url": "https://arxiv.org/abs/2601.05403v1",
      "pdf_url": "https://arxiv.org/pdf/2601.05403v1.pdf"
    },
    {
      "id": "2601.05399v1",
      "title": "Multi-task Cross-modal Learning for Chest X-ray Image Retrieval",
      "abstract": "CLIP and BiomedCLIP are examples of vision-language foundation models and offer strong cross-modal embeddings; however, they are not optimized for fine-grained medical retrieval tasks, such as retrieving clinically relevant radiology reports using chest X-ray (CXR) image queries. To address this shortcoming, we propose a multi-task learning framework to fine-tune BiomedCLIP and evaluate improvements to CXR image-text retrieval. Using BiomedCLIP as the backbone, we incorporate a lightweight MLP projector head trained with a multi-task composite loss function that includes: (1) a binary cross-entropy loss to distinguish normal from abnormal CXR studies, (2) a supervised contrastive loss to reinforce intra-class consistency, and (3) a CLIP loss to maintain cross-modal alignment. Experimental results demonstrate that the fine-tuned model achieves more balanced and clinically meaningful performance across both image-to-text and text-to-image retrieval tasks compared to the pretrained BiomedCLIP and general-purpose CLIP models. Furthermore, t-SNE visualizations reveal clearer semantic clustering of normal and abnormal cases, demonstrating the model's enhanced diagnostic sensitivity. These findings highlight the value of domain-adaptive, multi-task learning for advancing cross-modal retrieval in biomedical applications.",
      "authors": [
        "Zhaohui Liang",
        "Sivaramakrishnan Rajaraman",
        "Niccolo Marini",
        "Zhiyun Xue",
        "Sameer Antani"
      ],
      "published": "2026-01-08T21:44:00Z",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.IR"
      ],
      "primary_category": "cs.CV",
      "abs_url": "https://arxiv.org/abs/2601.05399v1",
      "pdf_url": "https://arxiv.org/pdf/2601.05399v1.pdf"
    },
    {
      "id": "2601.05394v1",
      "title": "Sketch&Patch++: Efficient Structure-Aware 3D Gaussian Representation",
      "abstract": "We observe that Gaussians exhibit distinct roles and characteristics analogous to traditional artistic techniques -- like how artists first sketch outlines before filling in broader areas with color, some Gaussians capture high-frequency features such as edges and contours, while others represent broader, smoother regions analogous to brush strokes that add volume and depth. Based on this observation, we propose a hybrid representation that categorizes Gaussians into (i) Sketch Gaussians, which represent high-frequency, boundary-defining features, and (ii) Patch Gaussians, which cover low-frequency, smooth regions. This semantic separation naturally enables layered progressive streaming, where the compact Sketch Gaussians establish the structural skeleton before Patch Gaussians incrementally refine volumetric detail.\n  In this work, we extend our previous method to arbitrary 3D scenes by proposing a novel hierarchical adaptive categorization framework that operates directly on the 3DGS representation. Our approach employs multi-criteria density-based clustering, combined with adaptive quality-driven refinement. This method eliminates dependency on external 3D line primitives while ensuring optimal parametric encoding effectiveness. Our comprehensive evaluation across diverse scenes, including both man-made and natural environments, demonstrates that our method achieves up to 1.74 dB improvement in PSNR, 6.7% in SSIM, and 41.4% in LPIPS at equivalent model sizes compared to uniform pruning baselines. For indoor scenes, our method can maintain visual quality with only 0.5\\% of the original model size. This structure-aware representation enables efficient storage, adaptive streaming, and rendering of high-fidelity 3D content across bandwidth-constrained networks and resource-limited devices.",
      "authors": [
        "Yuang Shi",
        "Simone Gasparini",
        "Géraldine Morin",
        "Wei Tsang Ooi"
      ],
      "published": "2026-01-08T21:32:54Z",
      "categories": [
        "cs.CV",
        "cs.GR",
        "cs.MM",
        "eess.IV"
      ],
      "primary_category": "cs.CV",
      "abs_url": "https://arxiv.org/abs/2601.05394v1",
      "pdf_url": "https://arxiv.org/pdf/2601.05394v1.pdf"
    },
    {
      "id": "2601.05392v1",
      "title": "Archetypal cases for questionnaires with nominal multiple choice questions",
      "abstract": "Archetypal analysis serves as an exploratory tool that interprets a collection of observations as convex combinations of pure (extreme) patterns. When these patterns correspond to actual observations within the sample, they are termed archetypoids. For the first time, we propose applying archetypoid analysis to nominal observations, specifically for identifying archetypal cases from questionnaires featuring nominal multiple-choice questions with a single possible answer. This approach can enhance our understanding of a nominal data set, similar to its application in multivariate contexts. We compare this methodology with the use of archetype analysis and probabilistic archetypal analysis and demonstrate the benefits of this methodology using a real-world example: the German credit dataset.",
      "authors": [
        "Aleix Alcacer",
        "Irene Epifanio"
      ],
      "published": "2026-01-08T21:32:23Z",
      "categories": [
        "stat.ME",
        "cs.LG"
      ],
      "primary_category": "stat.ME",
      "abs_url": "https://arxiv.org/abs/2601.05392v1",
      "pdf_url": "https://arxiv.org/pdf/2601.05392v1.pdf"
    },
    {
      "id": "2601.05391v1",
      "title": "DynaSTy: A Framework for SpatioTemporal Node Attribute Prediction in Dynamic Graphs",
      "abstract": "Accurate multistep forecasting of node-level attributes on dynamic graphs is critical for applications ranging from financial trust networks to biological networks. Existing spatiotemporal graph neural networks typically assume a static adjacency matrix. In this work, we propose an end-to-end dynamic edge-biased spatiotemporal model that ingests a multi-dimensional timeseries of node attributes and a timeseries of adjacency matrices, to predict multiple future steps of node attributes. At each time step, our transformer-based model injects the given adjacency as an adaptable attention bias, allowing the model to focus on relevant neighbors as the graph evolves. We further deploy a masked node-time pretraining objective that primes the encoder to reconstruct missing features, and train with scheduled sampling and a horizon-weighted loss to mitigate compounding error over long horizons. Unlike prior work, our model accommodates dynamic graphs that vary across input samples, enabling forecasting in multi-system settings such as brain networks across different subjects, financial systems in different contexts, or evolving social systems. Empirical results demonstrate that our method consistently outperforms strong baselines on Root Mean Squared Error (RMSE) and Mean Absolute Error (MAE).",
      "authors": [
        "Namrata Banerji",
        "Tanya Berger-Wolf"
      ],
      "published": "2026-01-08T21:32:20Z",
      "categories": [
        "cs.LG"
      ],
      "primary_category": "cs.LG",
      "abs_url": "https://arxiv.org/abs/2601.05391v1",
      "pdf_url": "https://arxiv.org/pdf/2601.05391v1.pdf"
    },
    {
      "id": "2601.05386v1",
      "title": "On the Effect of Cheating in Chess",
      "abstract": "Cheating in chess, by using advice from powerful software, has become a major problem, reaching the highest levels. As opposed to the large majority of previous work, which concerned {\\em detection} of cheating, here we try to evaluate the possible gain in performance, obtained by cheating a limited number of times during a game. Algorithms are developed and tested on a commonly used chess engine (i.e software).\\footnote{Needless to say, the goal of this work is not to assist cheaters, but to measure the effectiveness of cheating -- which is crucial as part of the effort to contain and detect it.}",
      "authors": [
        "Daniel Keren"
      ],
      "published": "2026-01-08T21:18:45Z",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "abs_url": "https://arxiv.org/abs/2601.05386v1",
      "pdf_url": "https://arxiv.org/pdf/2601.05386v1.pdf"
    },
    {
      "id": "2601.05384v1",
      "title": "Conformity and Social Impact on AI Agents",
      "abstract": "As AI agents increasingly operate in multi-agent environments, understanding their collective behavior becomes critical for predicting the dynamics of artificial societies. This study examines conformity, the tendency to align with group opinions under social pressure, in large multimodal language models functioning as AI agents. By adapting classic visual experiments from social psychology, we investigate how AI agents respond to group influence as social actors. Our experiments reveal that AI agents exhibit a systematic conformity bias, aligned with Social Impact Theory, showing sensitivity to group size, unanimity, task difficulty, and source characteristics. Critically, AI agents achieving near-perfect performance in isolation become highly susceptible to manipulation through social influence. This vulnerability persists across model scales: while larger models show reduced conformity on simple tasks due to improved capabilities, they remain vulnerable when operating at their competence boundary. These findings reveal fundamental security vulnerabilities in AI agent decision-making that could enable malicious manipulation, misinformation campaigns, and bias propagation in multi-agent systems, highlighting the urgent need for safeguards in collective AI deployments.",
      "authors": [
        "Alessandro Bellina",
        "Giordano De Marzo",
        "David Garcia"
      ],
      "published": "2026-01-08T21:16:28Z",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.CY"
      ],
      "primary_category": "cs.AI",
      "abs_url": "https://arxiv.org/abs/2601.05384v1",
      "pdf_url": "https://arxiv.org/pdf/2601.05384v1.pdf"
    },
    {
      "id": "2601.05383v1",
      "title": "Imitation Learning for Combinatorial Optimisation under Uncertainty",
      "abstract": "Imitation learning (IL) provides a data-driven framework for approximating policies for large-scale combinatorial optimisation problems formulated as sequential decision problems (SDPs), where exact solution methods are computationally intractable. A central but underexplored aspect of IL in this context is the role of the \\emph{expert} that generates training demonstrations. Existing studies employ a wide range of expert constructions, yet lack a unifying framework to characterise their modelling assumptions, computational properties, and impact on learning performance.\n  This paper introduces a systematic taxonomy of experts for IL in combinatorial optimisation under uncertainty. Experts are classified along three dimensions: (i) their treatment of uncertainty, including myopic, deterministic, full-information, two-stage stochastic, and multi-stage stochastic formulations; (ii) their level of optimality, distinguishing task-optimal and approximate experts; and (iii) their interaction mode with the learner, ranging from one-shot supervision to iterative, interactive schemes. Building on this taxonomy, we propose a generalised Dataset Aggregation (DAgger) algorithm that supports multiple expert queries, expert aggregation, and flexible interaction strategies.\n  The proposed framework is evaluated on a dynamic physician-to-patient assignment problem with stochastic arrivals and capacity constraints. Computational experiments compare learning outcomes across expert types and interaction regimes. The results show that policies learned from stochastic experts consistently outperform those learned from deterministic or full-information experts, while interactive learning improves solution quality using fewer expert demonstrations. Aggregated deterministic experts provide an effective alternative when stochastic optimisation becomes computationally challenging.",
      "authors": [
        "Prakash Gawas",
        "Antoine Legrain",
        "Louis-Martin Rousseau"
      ],
      "published": "2026-01-08T21:16:25Z",
      "categories": [
        "cs.LG",
        "math.OC"
      ],
      "primary_category": "cs.LG",
      "abs_url": "https://arxiv.org/abs/2601.05383v1",
      "pdf_url": "https://arxiv.org/pdf/2601.05383v1.pdf"
    },
    {
      "id": "2601.05379v1",
      "title": "EdgeLDR: Quaternion Low-Displacement Rank Neural Networks for Edge-Efficient Deep Learning",
      "abstract": "Deploying deep neural networks on edge devices is often limited by the memory traffic and compute cost of dense linear operators. While quaternion neural networks improve parameter efficiency by coupling multiple channels through Hamilton products, they typically retain unstructured dense weights; conversely, structured matrices enable fast computation but are usually applied in the real domain. This paper introduces EdgeLDR, a practical framework for quaternion block-circulant linear and convolutional layers that combines quaternion channel mixing with block-circulant parameter structure and enables FFT-based evaluation through the complex adjoint representation. We present reference implementations of EdgeLDR layers and compare FFT-based computation against a naive spatial-domain realization of quaternion circulant products. FFT evaluation yields large empirical speedups over the naive implementation and keeps latency stable as block size increases, making larger compression factors computationally viable. We further integrate EdgeLDR layers into compact CNN and Transformer backbones and evaluate accuracy-compression trade-offs on 32x32 RGB classification (CIFAR-10/100, SVHN) and hyperspectral image classification (Houston 2013, Pavia University), reporting parameter counts and CPU/GPU latency. The results show that EdgeLDR layers provide significant compression with competitive accuracy.",
      "authors": [
        "Vladimir Frants",
        "Sos Agaian",
        "Karen Panetta"
      ],
      "published": "2026-01-08T21:04:07Z",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "abs_url": "https://arxiv.org/abs/2601.05379v1",
      "pdf_url": "https://arxiv.org/pdf/2601.05379v1.pdf"
    },
    {
      "id": "2601.05378v1",
      "title": "Inverting Non-Injective Functions with Twin Neural Network Regression",
      "abstract": "Non-injective functions are not invertible. However, non-injective functions can be restricted to sub-domains on which they are locally injective and surjective and thus invertible if the dimensionality between input and output spaces are the same. Further, even if the dimensionalities do not match it is often possible to choose a preferred solution from many possible solutions. Twin neural network regression is naturally capable of incorporating these properties to invert non-injective functions. Twin neural network regression is trained to predict adjustments to well known input variables $\\mathbf{x}^{\\text{anchor}}$ to obtain an estimate for an unknown $\\mathbf{x}^{\\text{new}}$ under a change of the target variable from $\\mathbf{y}^{\\text{anchor}}$ to $\\mathbf{y}^{\\text{new}}$. In combination with k-nearest neighbor search, I propose a deterministic framework that finds input parameters to a given target variable of non-injective functions. The method is demonstrated by inverting non-injective functions describing toy problems and robot arm control that are a) defined by data or b) known as mathematical formula.",
      "authors": [
        "Sebastian J. Wetzel"
      ],
      "published": "2026-01-08T21:04:07Z",
      "categories": [
        "cs.LG",
        "cs.RO"
      ],
      "primary_category": "cs.LG",
      "abs_url": "https://arxiv.org/abs/2601.05378v1",
      "pdf_url": "https://arxiv.org/pdf/2601.05378v1.pdf"
    },
    {
      "id": "2601.05376v1",
      "title": "The Persona Paradox: Medical Personas as Behavioral Priors in Clinical Language Models",
      "abstract": "Persona conditioning can be viewed as a behavioral prior for large language models (LLMs) and is often assumed to confer expertise and improve safety in a monotonic manner. However, its effects on high-stakes clinical decision-making remain poorly characterized. We systematically evaluate persona-based control in clinical LLMs, examining how professional roles (e.g., Emergency Department physician, nurse) and interaction styles (bold vs.\\ cautious) influence behavior across models and medical tasks. We assess performance on clinical triage and patient-safety tasks using multidimensional evaluations that capture task accuracy, calibration, and safety-relevant risk behavior. We find systematic, context-dependent, and non-monotonic effects: Medical personas improve performance in critical care tasks, yielding gains of up to $\\sim+20\\%$ in accuracy and calibration, but degrade performance in primary-care settings by comparable margins. Interaction style modulates risk propensity and sensitivity, but it's highly model-dependent. While aggregated LLM-judge rankings favor medical over non-medical personas in safety-critical cases, we found that human clinicians show moderate agreement on safety compliance (average Cohen's $κ= 0.43$) but indicate a low confidence in 95.9\\% of their responses on reasoning quality. Our work shows that personas function as behavioral priors that introduce context-dependent trade-offs rather than guarantees of safety or expertise. The code is available at https://github.com/rsinghlab/Persona\\_Paradox.",
      "authors": [
        "Tassallah Abdullahi",
        "Shrestha Ghosh",
        "Hamish S Fraser",
        "Daniel León Tramontini",
        "Adeel Abbasi",
        "Ghada Bourjeily",
        "Carsten Eickhoff",
        "Ritambhara Singh"
      ],
      "published": "2026-01-08T21:01:11Z",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.AI",
      "abs_url": "https://arxiv.org/abs/2601.05376v1",
      "pdf_url": "https://arxiv.org/pdf/2601.05376v1.pdf"
    },
    {
      "id": "2601.05373v1",
      "title": "Ensemble of radiomics and ConvNeXt for breast cancer diagnosis",
      "abstract": "Early diagnosis of breast cancer is crucial for improving survival rates. Radiomics and deep learning (DL) have shown significant potential in assisting radiologists with early cancer detection. This paper aims to critically assess the performance of radiomics, DL, and ensemble techniques in detecting cancer from screening mammograms. Two independent datasets were used: the RSNA 2023 Breast Cancer Detection Challenge (11,913 patients) and a Mexican cohort from the TecSalud dataset (19,400 patients). The ConvNeXtV1-small DL model was trained on the RSNA dataset and validated on the TecSalud dataset, while radiomics models were developed using the TecSalud dataset and validated with a leave-one-year-out approach. The ensemble method consistently combined and calibrated predictions using the same methodology. Results showed that the ensemble approach achieved the highest area under the curve (AUC) of 0.87, compared to 0.83 for ConvNeXtV1-small and 0.80 for radiomics. In conclusion, ensemble methods combining DL and radiomics predictions significantly enhance breast cancer diagnosis from mammograms.",
      "authors": [
        "Jorge Alberto Garza-Abdala",
        "Gerardo Alejandro Fumagal-González",
        "Beatriz A. Bosques-Palomo",
        "Mario Alexis Monsivais Molina",
        "Daly Avedano",
        "Servando Cardona-Huerta",
        "José Gerardo Tamez-Pena"
      ],
      "published": "2026-01-08T20:54:24Z",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "abs_url": "https://arxiv.org/abs/2601.05373v1",
      "pdf_url": "https://arxiv.org/pdf/2601.05373v1.pdf"
    },
    {
      "id": "2601.05371v1",
      "title": "The Kernel Manifold: A Geometric Approach to Gaussian Process Model Selection",
      "abstract": "Gaussian Process (GP) regression is a powerful nonparametric Bayesian framework, but its performance depends critically on the choice of covariance kernel. Selecting an appropriate kernel is therefore central to model quality, yet remains one of the most challenging and computationally expensive steps in probabilistic modeling. We present a Bayesian optimization framework built on kernel-of-kernels geometry, using expected divergence-based distances between GP priors to explore kernel space efficiently. A multidimensional scaling (MDS) embedding of this distance matrix maps a discrete kernel library into a continuous Euclidean manifold, enabling smooth BO. In this formulation, the input space comprises kernel compositions, the objective is the log marginal likelihood, and featurization is given by the MDS coordinates. When the divergence yields a valid metric, the embedding preserves geometry and produces a stable BO landscape. We demonstrate the approach on synthetic benchmarks, real-world time-series datasets, and an additive manufacturing case study predicting melt-pool geometry, achieving superior predictive accuracy and uncertainty calibration relative to baselines including Large Language Model (LLM)-guided search. This framework establishes a reusable probabilistic geometry for kernel search, with direct relevance to GP modeling and deep kernel learning.",
      "authors": [
        "Md Shafiqul Islam",
        "Shakti Prasad Padhy",
        "Douglas Allaire",
        "Raymundo Arróyave"
      ],
      "published": "2026-01-08T20:52:53Z",
      "categories": [
        "cs.LG",
        "stat.ME"
      ],
      "primary_category": "cs.LG",
      "abs_url": "https://arxiv.org/abs/2601.05371v1",
      "pdf_url": "https://arxiv.org/pdf/2601.05371v1.pdf"
    },
    {
      "id": "2601.05368v1",
      "title": "MOSAIC-GS: Monocular Scene Reconstruction via Advanced Initialization for Complex Dynamic Environments",
      "abstract": "We present MOSAIC-GS, a novel, fully explicit, and computationally efficient approach for high-fidelity dynamic scene reconstruction from monocular videos using Gaussian Splatting. Monocular reconstruction is inherently ill-posed due to the lack of sufficient multiview constraints, making accurate recovery of object geometry and temporal coherence particularly challenging. To address this, we leverage multiple geometric cues, such as depth, optical flow, dynamic object segmentation, and point tracking. Combined with rigidity-based motion constraints, these cues allow us to estimate preliminary 3D scene dynamics during an initialization stage. Recovering scene dynamics prior to the photometric optimization reduces reliance on motion inference from visual appearance alone, which is often ambiguous in monocular settings. To enable compact representations, fast training, and real-time rendering while supporting non-rigid deformations, the scene is decomposed into static and dynamic components. Each Gaussian in the dynamic part of the scene is assigned a trajectory represented as time-dependent Poly-Fourier curve for parameter-efficient motion encoding. We demonstrate that MOSAIC-GS achieves substantially faster optimization and rendering compared to existing methods, while maintaining reconstruction quality on par with state-of-the-art approaches across standard monocular dynamic scene benchmarks.",
      "authors": [
        "Svitlana Morkva",
        "Maximum Wilder-Smith",
        "Michael Oechsle",
        "Alessio Tonioni",
        "Marco Hutter",
        "Vaishakh Patil"
      ],
      "published": "2026-01-08T20:48:24Z",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "abs_url": "https://arxiv.org/abs/2601.05368v1",
      "pdf_url": "https://arxiv.org/pdf/2601.05368v1.pdf"
    },
    {
      "id": "2601.05366v1",
      "title": "Lost in Execution: On the Multilingual Robustness of Tool Calling in Large Language Models",
      "abstract": "Large Language Models (LLMs) are increasingly deployed as agents that invoke external tools through structured function calls. While recent work reports strong tool-calling performance under standard English-centric evaluations, the robustness of tool calling under multilingual user interactions remains underexplored. In this work, we introduce MLCL, a diagnostic benchmark, and conduct a systematic evaluation of multilingual tool calling across Chinese, Hindi, and the low-resource language Igbo. Through fine-grained error analysis, we show that many failures occur despite correct intent understanding and tool selection. We identify parameter value language mismatch as a dominant failure mode, where models generate semantically appropriate parameter values in the user's language, violating language-invariant execution conventions. We further evaluate several inference-time system strategies and find that while these strategies substantially reduce language-induced execution errors, none of them can fully recover English-level performance.",
      "authors": [
        "Zheng Luo",
        "T Pranav Kutralingam",
        "Ogochukwu N Okoani",
        "Wanpeng Xu",
        "Hua Wei",
        "Xiyang Hu"
      ],
      "published": "2026-01-08T20:44:28Z",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "abs_url": "https://arxiv.org/abs/2601.05366v1",
      "pdf_url": "https://arxiv.org/pdf/2601.05366v1.pdf"
    },
    {
      "id": "2601.05364v1",
      "title": "STResNet & STYOLO : A New Family of Compact Classification and Object Detection Models for MCUs",
      "abstract": "Recent advancements in lightweight neural networks have significantly improved the efficiency of deploying deep learning models on edge hardware. However, most existing architectures still trade accuracy for latency, which limits their applicability on microcontroller and neural processing unit based devices. In this work, we introduce two new model families, STResNet for image classification and STYOLO for object detection, jointly optimized for accuracy, efficiency, and memory footprint on resource constrained platforms. The proposed STResNet series, ranging from Nano to Tiny variants, achieves competitive ImageNet 1K accuracy within a four million parameter budget. Specifically, STResNetMilli attains 70.0 percent Top 1 accuracy with only three million parameters, outperforming MobileNetV1 and ShuffleNetV2 at comparable computational complexity. For object detection, STYOLOMicro and STYOLOMilli achieve 30.5 percent and 33.6 percent mean average precision, respectively, on the MS COCO dataset, surpassing YOLOv5n and YOLOX Nano in both accuracy and efficiency. Furthermore, when STResNetMilli is used as a backbone with the Ultralytics training environment.",
      "authors": [
        "Sudhakar Sah",
        "Ravish Kumar"
      ],
      "published": "2026-01-08T20:39:50Z",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "abs_url": "https://arxiv.org/abs/2601.05364v1",
      "pdf_url": "https://arxiv.org/pdf/2601.05364v1.pdf"
    },
    {
      "id": "2601.05358v1",
      "title": "The Table of Media Bias Elements: A sentence-level taxonomy of media bias types and propaganda techniques",
      "abstract": "Public debates about \"left-\" or \"right-wing\" news overlook the fact that bias is usually conveyed by concrete linguistic manoeuvres that transcend any single political spectrum. We therefore shift the focus from where an outlet allegedly stands to how partiality is expressed in individual sentences. Drawing on 26,464 sentences collected from newsroom corpora, user submissions and our own browsing, we iteratively combine close-reading, interdisciplinary theory and pilot annotation to derive a fine-grained, sentence-level taxonomy of media bias and propaganda. The result is a two-tier schema comprising 38 elementary bias types, arranged in six functional families and visualised as a \"table of media-bias elements\". For each type we supply a definition, real-world examples, cognitive and societal drivers, and guidance for recognition. A quantitative survey of a random 155-sentence sample illustrates prevalence differences, while a cross-walk to the best-known NLP and communication-science taxonomies reveals substantial coverage gains and reduced ambiguity.",
      "authors": [
        "Tim Menzner",
        "Jochen L. Leidner"
      ],
      "published": "2026-01-08T20:18:55Z",
      "categories": [
        "cs.CL",
        "cs.CY"
      ],
      "primary_category": "cs.CL",
      "abs_url": "https://arxiv.org/abs/2601.05358v1",
      "pdf_url": "https://arxiv.org/pdf/2601.05358v1.pdf"
    },
    {
      "id": "2601.05356v1",
      "title": "PRISM: Protocol Refinement through Intelligent Simulation Modeling",
      "abstract": "Automating experimental protocol design and execution remains as a fundamental bottleneck in realizing self-driving laboratories. We introduce PRISM (Protocol Refinement through Intelligent Simulation Modeling), a framework that automates the design, validation, and execution of experimental protocols on a laboratory platform composed of off-the-shelf robotic instruments. PRISM uses a set of language-model-based agents that work together to generate and refine experimental steps. The process begins with automatically gathering relevant procedures from web-based sources describing experimental workflows. These are converted into structured experimental steps (e.g., liquid handling steps, deck layout and other related operations) through a planning, critique, and validation loop. The finalized steps are translated into the Argonne MADSci protocol format, which provides a unified interface for coordinating multiple robotic instruments (Opentrons OT-2 liquid handler, PF400 arm, Azenta plate sealer and peeler) without requiring human intervention between steps. To evaluate protocol-generation performance, we benchmarked both single reasoning models and multi-agent workflow across constrained and open-ended prompting paradigms. The resulting protocols were validated in a digital-twin environment built in NVIDIA Omniverse to detect physical or sequencing errors before execution. Using Luna qPCR amplification and Cell Painting as case studies, we demonstrate PRISM as a practical end-to-end workflow that bridges language-based protocol generation, simulation-based validation, and automated robotic execution.",
      "authors": [
        "Brian Hsu",
        "Priyanka V Setty",
        "Rory M Butler",
        "Ryan Lewis",
        "Casey Stone",
        "Rebecca Weinberg",
        "Thomas Brettin",
        "Rick Stevens",
        "Ian Foster",
        "Arvind Ramanathan"
      ],
      "published": "2026-01-08T20:15:28Z",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.MA",
        "q-bio.QM"
      ],
      "primary_category": "cs.RO",
      "abs_url": "https://arxiv.org/abs/2601.05356v1",
      "pdf_url": "https://arxiv.org/pdf/2601.05356v1.pdf"
    },
    {
      "id": "2601.05355v1",
      "title": "A Bayesian Generative Modeling Approach for Arbitrary Conditional Inference",
      "abstract": "Modern data analysis increasingly requires flexible conditional inference P(X_B | X_A) where (X_A, X_B) is an arbitrary partition of observed variable X. Existing conditional inference methods lack this flexibility as they are tied to a fixed conditioning structure and cannot perform new conditional inference once trained. To solve this, we propose a Bayesian generative modeling (BGM) approach for arbitrary conditional inference without retraining. BGM learns a generative model of X through an iterative Bayesian updating algorithm where model parameters and latent variables are updated until convergence. Once trained, any conditional distribution can be obtained without retraining. Empirically, BGM achieves superior prediction performance with well calibrated predictive intervals, demonstrating that a single learned model can serve as a universal engine for conditional prediction with uncertainty quantification. We provide theoretical guarantees for the convergence of the stochastic iterative algorithm, statistical consistency and conditional-risk bounds. The proposed BGM framework leverages the power of AI to capture complex relationships among variables while adhering to Bayesian principles, emerging as a promising framework for advancing various applications in modern data science. The code for BGM is freely available at https://github.com/liuq-lab/bayesgm.",
      "authors": [
        "Qiao Liu",
        "Wing Hung Wong"
      ],
      "published": "2026-01-08T20:14:30Z",
      "categories": [
        "stat.ML",
        "cs.AI",
        "cs.LG",
        "stat.CO",
        "stat.ME"
      ],
      "primary_category": "stat.ML",
      "abs_url": "https://arxiv.org/abs/2601.05355v1",
      "pdf_url": "https://arxiv.org/pdf/2601.05355v1.pdf"
    },
    {
      "id": "2601.05353v1",
      "title": "GlyRAG: Context-Aware Retrieval-Augmented Framework for Blood Glucose Forecasting",
      "abstract": "Accurate forecasting of blood glucose from CGM is essential for preventing dysglycemic events, thus enabling proactive diabetes management. However, current forecasting models treat blood glucose readings captured using CGMs as a numerical sequence, either ignoring context or relying on additional sensors/modalities that are difficult to collect and deploy at scale. Recently, LLMs have shown promise for time-series forecasting tasks, yet their role as agentic context extractors in diabetes care remains largely unexplored. To address these limitations, we propose GlyRAG, a context-aware, retrieval-augmented forecasting framework that derives semantic understanding of blood glucose dynamics directly from CGM traces without requiring additional sensor modalities. GlyRAG employs an LLM as a contextualization agent to generate clinical summaries. These summaries are embedded by a language model and fused with patch-based glucose representations in a multimodal transformer architecture with a cross translation loss aligining textual and physiological embeddings. A retrieval module then identifies similar historical episodes in the learned embedding space and uses cross-attention to integrate these case-based analogues prior to making a forecasting inference. Extensive evaluations on two T1D cohorts show that GlyRAG consistently outperforms state-of-the art methods, achieving up to 39% lower RMSE and a further 1.7% reduction in RMSE over the baseline. Clinical evaluation shows that GlyRAG places 85% predictions in safe zones and achieves 51% improvement in predicting dysglycemic events across both cohorts. These results indicate that LLM-based contextualization and retrieval over CGM traces can enhance the accuracy and clinical reliability of long-horizon glucose forecasting without the need for extra sensors, thus supporting future agentic decision-support tools for diabetes management.",
      "authors": [
        "Shovito Barua Soumma",
        "Hassan Ghasemzadeh"
      ],
      "published": "2026-01-08T20:07:59Z",
      "categories": [
        "cs.LG",
        "cs.IT"
      ],
      "primary_category": "cs.LG",
      "abs_url": "https://arxiv.org/abs/2601.05353v1",
      "pdf_url": "https://arxiv.org/pdf/2601.05353v1.pdf"
    },
    {
      "id": "2601.05352v1",
      "title": "When the Server Steps In: Calibrated Updates for Fair Federated Learning",
      "abstract": "Federated learning (FL) has emerged as a transformative distributed learning paradigm, enabling multiple clients to collaboratively train a global model under the coordination of a central server without sharing their raw training data. While FL offers notable advantages, it faces critical challenges in ensuring fairness across diverse demographic groups. To address these fairness concerns, various fairness-aware debiasing methods have been proposed. However, many of these approaches either require modifications to clients' training protocols or lack flexibility in their aggregation strategies. In this work, we address these limitations by introducing EquFL, a novel server-side debiasing method designed to mitigate bias in FL systems. EquFL operates by allowing the server to generate a single calibrated update after receiving model updates from the clients. This calibrated update is then integrated with the aggregated client updates to produce an adjusted global model that reduces bias. Theoretically, we establish that EquFL converges to the optimal global model achieved by FedAvg and effectively reduces fairness loss over training rounds. Empirically, we demonstrate that EquFL significantly mitigates bias within the system, showcasing its practical effectiveness.",
      "authors": [
        "Tianrun Yu",
        "Kaixiang Zhao",
        "Cheng Zhang",
        "Anjun Gao",
        "Yueyang Quan",
        "Zhuqing Liu",
        "Minghong Fang"
      ],
      "published": "2026-01-08T20:06:00Z",
      "categories": [
        "cs.LG",
        "cs.CR",
        "cs.IR",
        "cs.SI"
      ],
      "primary_category": "cs.LG",
      "abs_url": "https://arxiv.org/abs/2601.05352v1",
      "pdf_url": "https://arxiv.org/pdf/2601.05352v1.pdf"
    },
    {
      "id": "2601.05344v1",
      "title": "Coding the Visual World: From Image to Simulation Using Vision Language Models",
      "abstract": "The ability to construct mental models of the world is a central aspect of understanding. Similarly, visual understanding can be viewed as the ability to construct a representative model of the system depicted in an image. This work explores the capacity of Vision Language Models (VLMs) to recognize and simulate the systems and mechanisms depicted in images using the Im2Sim methodology. The VLM is given a natural image of a real-world system (e.g., cities, clouds, vegetation) and is tasked with describing the system and writing code that simulates and generates it. This generative code is then executed to produce a synthetic image, which is compared against the original. This approach is tested on various complex emergent systems, ranging from physical systems (waves, lights, clouds) to vegetation, cities, materials, and geological formations. Through analysis of the models and images generated by the VLMs, we examine their understanding of the systems in images. The results show that leading VLMs (GPT, Gemini) demonstrate the capacity to understand and model complex, multi-component systems across multiple layers of abstraction and a wide range of domains. At the same time, the VLMs exhibit limited ability to replicate fine details and low-level arrangements of patterns in the image. These findings reveal an interesting asymmetry: VLMs combine high-level, deep visual understanding of images with limited perception of fine details.",
      "authors": [
        "Sagi Eppel"
      ],
      "published": "2026-01-08T19:49:05Z",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "abs_url": "https://arxiv.org/abs/2601.05344v1",
      "pdf_url": "https://arxiv.org/pdf/2601.05344v1.pdf"
    },
    {
      "id": "2601.05339v1",
      "title": "Multi-turn Jailbreaking Attack in Multi-Modal Large Language Models",
      "abstract": "In recent years, the security vulnerabilities of Multi-modal Large Language Models (MLLMs) have become a serious concern in the Generative Artificial Intelligence (GenAI) research. These highly intelligent models, capable of performing multi-modal tasks with high accuracy, are also severely susceptible to carefully launched security attacks, such as jailbreaking attacks, which can manipulate model behavior and bypass safety constraints. This paper introduces MJAD-MLLMs, a holistic framework that systematically analyzes the proposed Multi-turn Jailbreaking Attacks and multi-LLM-based defense techniques for MLLMs. In this paper, we make three original contributions. First, we introduce a novel multi-turn jailbreaking attack to exploit the vulnerabilities of the MLLMs under multi-turn prompting. Second, we propose a novel fragment-optimized and multi-LLM defense mechanism, called FragGuard, to effectively mitigate jailbreaking attacks in the MLLMs. Third, we evaluate the efficacy of the proposed attacks and defenses through extensive experiments on several state-of-the-art (SOTA) open-source and closed-source MLLMs and benchmark datasets, and compare their performance with the existing techniques.",
      "authors": [
        "Badhan Chandra Das",
        "Md Tasnim Jawad",
        "Joaquin Molto",
        "M. Hadi Amini",
        "Yanzhao Wu"
      ],
      "published": "2026-01-08T19:37:22Z",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "primary_category": "cs.CR",
      "abs_url": "https://arxiv.org/abs/2601.05339v1",
      "pdf_url": "https://arxiv.org/pdf/2601.05339v1.pdf"
    },
    {
      "id": "2601.05336v1",
      "title": "Intent at a Glance: Gaze-Guided Robotic Manipulation via Foundation Models",
      "abstract": "Designing intuitive interfaces for robotic control remains a central challenge in enabling effective human-robot interaction, particularly in assistive care settings. Eye gaze offers a fast, non-intrusive, and intent-rich input modality, making it an attractive channel for conveying user goals. In this work, we present GAMMA (Gaze Assisted Manipulation for Modular Autonomy), a system that leverages ego-centric gaze tracking and a vision-language model to infer user intent and autonomously execute robotic manipulation tasks. By contextualizing gaze fixations within the scene, the system maps visual attention to high-level semantic understanding, enabling skill selection and parameterization without task-specific training. We evaluate GAMMA on a range of table-top manipulation tasks and compare it against baseline gaze-based control without reasoning. Results demonstrate that GAMMA provides robust, intuitive, and generalizable control, highlighting the potential of combining foundation models and gaze for natural and scalable robot autonomy. Project website: https://gamma0.vercel.app/",
      "authors": [
        "Tracey Yee Hsin Tay",
        "Xu Yan",
        "Jonathan Ouyang",
        "Daniel Wu",
        "William Jiang",
        "Jonathan Kao",
        "Yuchen Cui"
      ],
      "published": "2026-01-08T19:33:03Z",
      "categories": [
        "cs.RO"
      ],
      "primary_category": "cs.RO",
      "abs_url": "https://arxiv.org/abs/2601.05336v1",
      "pdf_url": "https://arxiv.org/pdf/2601.05336v1.pdf"
    },
    {
      "id": "2601.05335v1",
      "title": "Generalized Canonical Polyadic Tensor Decompositions with General Symmetry",
      "abstract": "Canonical Polyadic (CP) tensor decomposition is a workhorse algorithm for discovering underlying low-dimensional structure in tensor data. This is accomplished in conventional CP decomposition by fitting a low-rank tensor to data with respect to the least-squares loss. Generalized CP (GCP) decompositions generalize this approach by allowing general loss functions that can be more appropriate, e.g., to model binary and count data or to improve robustness to outliers. However, GCP decompositions do not explicitly account for any symmetry in the tensors, which commonly arises in modern applications. For example, a tensor formed by stacking the adjacency matrices of a dynamic graph over time will naturally exhibit symmetry along the two modes corresponding to the graph nodes. In this paper, we develop a symmetric GCP (SymGCP) decomposition that allows for general forms of symmetry, i.e., symmetry along any subset of the modes. SymGCP accounts for symmetry by enforcing the corresponding symmetry in the decomposition. We derive gradients for SymGCP that enable its efficient computation via all-at-once optimization with existing tensor kernels. The form of the gradients also leads to various stochastic approximations that enable us to develop stochastic SymGCP algorithms that can scale to large tensors. We demonstrate the utility of the proposed SymGCP algorithms with a variety of experiments on both synthetic and real data.",
      "authors": [
        "Alex Mulrooney",
        "David Hong"
      ],
      "published": "2026-01-08T19:27:52Z",
      "categories": [
        "math.NA",
        "cs.LG",
        "stat.ML"
      ],
      "primary_category": "math.NA",
      "abs_url": "https://arxiv.org/abs/2601.05335v1",
      "pdf_url": "https://arxiv.org/pdf/2601.05335v1.pdf"
    },
    {
      "id": "2601.05330v1",
      "title": "Improving Enzyme Prediction with Chemical Reaction Equations by Hypergraph-Enhanced Knowledge Graph Embeddings",
      "abstract": "Predicting enzyme-substrate interactions has long been a fundamental problem in biochemistry and metabolic engineering. While existing methods could leverage databases of expert-curated enzyme-substrate pairs for models to learn from known pair interactions, the databases are often sparse, i.e., there are only limited and incomplete examples of such pairs, and also labor-intensive to maintain. This lack of sufficient training data significantly hinders the ability of traditional enzyme prediction models to generalize to unseen interactions. In this work, we try to exploit chemical reaction equations from domain-specific databases, given their easier accessibility and denser, more abundant data. However, interactions of multiple compounds, e.g., educts and products, with the same enzymes create complex relational data patterns that traditional models cannot easily capture. To tackle that, we represent chemical reaction equations as triples of (educt, enzyme, product) within a knowledge graph, such that we can take advantage of knowledge graph embedding (KGE) to infer missing enzyme-substrate pairs for graph completion. Particularly, in order to capture intricate relationships among compounds, we propose our knowledge-enhanced hypergraph model for enzyme prediction, i.e., Hyper-Enz, which integrates a hypergraph transformer with a KGE model to learn representations of the hyper-edges that involve multiple educts and products. Also, a multi-expert paradigm is introduced to guide the learning of enzyme-substrate interactions with both the proposed model and chemical reaction equations. Experimental results show a significant improvement, with up to a 88% relative improvement in average enzyme retrieval accuracy and 30% improvement in pair-level prediction compared to traditional models, demonstrating the effectiveness of our approach.",
      "authors": [
        "Tengwei Song",
        "Long Yin",
        "Zhen Han",
        "Zhiqiang Xu"
      ],
      "published": "2026-01-08T19:17:18Z",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "abs_url": "https://arxiv.org/abs/2601.05330v1",
      "pdf_url": "https://arxiv.org/pdf/2601.05330v1.pdf"
    },
    {
      "id": "2601.05328v1",
      "title": "Bi-Orthogonal Factor Decomposition for Vision Transformers",
      "abstract": "Self-attention is the central computational primitive of Vision Transformers, yet we lack a principled understanding of what information attention mechanisms exchange between tokens. Attention maps describe where weight mass concentrates; they do not reveal whether queries and keys trade position, content, or both. We introduce Bi-orthogonal Factor Decomposition (BFD), a two-stage analytical framework: first, an ANOVA-based decomposition statistically disentangles token activations into orthogonal positional and content factors; second, SVD of the query-key interaction matrix QK^T exposes bi-orthogonal modes that reveal how these factors mediate communication. After validating proper isolation of position and content, we apply BFD to state-of-the-art vision models and uncover three phenomena.(i) Attention operates primarily through content. Content-content interactions dominate attention energy, followed by content-position coupling. DINOv2 allocates more energy to content-position than supervised models and distributes computation across a richer mode spectrum. (ii) Attention mechanisms exhibit specialization: heads differentiate into content-content, content-position, and position-position operators, while singular modes within heads show analogous specialization. (iii) DINOv2's superior holistic shape processing emerges from intermediate layers that simultaneously preserve positional structure while contextually enriching semantic content.\n  Overall, BFD exposes how tokens interact through attention and which informational factors - positional or semantic - mediate their communication, yielding practical insights into vision transformer mechanisms.",
      "authors": [
        "Fenil R. Doshi",
        "Thomas Fel",
        "Talia Konkle",
        "George Alvarez"
      ],
      "published": "2026-01-08T19:11:55Z",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "abs_url": "https://arxiv.org/abs/2601.05328v1",
      "pdf_url": "https://arxiv.org/pdf/2601.05328v1.pdf"
    },
    {
      "id": "2601.05251v1",
      "title": "Mesh4D: 4D Mesh Reconstruction and Tracking from Monocular Video",
      "abstract": "We propose Mesh4D, a feed-forward model for monocular 4D mesh reconstruction. Given a monocular video of a dynamic object, our model reconstructs the object's complete 3D shape and motion, represented as a deformation field. Our key contribution is a compact latent space that encodes the entire animation sequence in a single pass. This latent space is learned by an autoencoder that, during training, is guided by the skeletal structure of the training objects, providing strong priors on plausible deformations. Crucially, skeletal information is not required at inference time. The encoder employs spatio-temporal attention, yielding a more stable representation of the object's overall deformation. Building on this representation, we train a latent diffusion model that, conditioned on the input video and the mesh reconstructed from the first frame, predicts the full animation in one shot. We evaluate Mesh4D on reconstruction and novel view synthesis benchmarks, outperforming prior methods in recovering accurate 3D shape and deformation.",
      "authors": [
        "Zeren Jiang",
        "Chuanxia Zheng",
        "Iro Laina",
        "Diane Larlus",
        "Andrea Vedaldi"
      ],
      "published": "2026-01-08T18:59:56Z",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "abs_url": "https://arxiv.org/abs/2601.05251v1",
      "pdf_url": "https://arxiv.org/pdf/2601.05251v1.pdf"
    },
    {
      "id": "2601.05249v1",
      "title": "RL-AWB: Deep Reinforcement Learning for Auto White Balance Correction in Low-Light Night-time Scenes",
      "abstract": "Nighttime color constancy remains a challenging problem in computational photography due to low-light noise and complex illumination conditions. We present RL-AWB, a novel framework combining statistical methods with deep reinforcement learning for nighttime white balance. Our method begins with a statistical algorithm tailored for nighttime scenes, integrating salient gray pixel detection with novel illumination estimation. Building on this foundation, we develop the first deep reinforcement learning approach for color constancy that leverages the statistical algorithm as its core, mimicking professional AWB tuning experts by dynamically optimizing parameters for each image. To facilitate cross-sensor evaluation, we introduce the first multi-sensor nighttime dataset. Experiment results demonstrate that our method achieves superior generalization capability across low-light and well-illuminated images. Project page: https://ntuneillee.github.io/research/rl-awb/",
      "authors": [
        "Yuan-Kang Lee",
        "Kuan-Lin Chen",
        "Chia-Che Chang",
        "Yu-Lun Liu"
      ],
      "published": "2026-01-08T18:59:55Z",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "abs_url": "https://arxiv.org/abs/2601.05249v1",
      "pdf_url": "https://arxiv.org/pdf/2601.05249v1.pdf"
    },
    {
      "id": "2601.05250v1",
      "title": "QNeRF: Neural Radiance Fields on a Simulated Gate-Based Quantum Computer",
      "abstract": "Recently, Quantum Visual Fields (QVFs) have shown promising improvements in model compactness and convergence speed for learning the provided 2D or 3D signals. Meanwhile, novel-view synthesis has seen major advances with Neural Radiance Fields (NeRFs), where models learn a compact representation from 2D images to render 3D scenes, albeit at the cost of larger models and intensive training. In this work, we extend the approach of QVFs by introducing QNeRF, the first hybrid quantum-classical model designed for novel-view synthesis from 2D images. QNeRF leverages parameterised quantum circuits to encode spatial and view-dependent information via quantum superposition and entanglement, resulting in more compact models compared to the classical counterpart. We present two architectural variants. Full QNeRF maximally exploits all quantum amplitudes to enhance representational capabilities. In contrast, Dual-Branch QNeRF introduces a task-informed inductive bias by branching spatial and view-dependent quantum state preparations, drastically reducing the complexity of this operation and ensuring scalability and potential hardware compatibility. Our experiments demonstrate that -- when trained on images of moderate resolution -- QNeRF matches or outperforms classical NeRF baselines while using less than half the number of parameters. These results suggest that quantum machine learning can serve as a competitive alternative for continuous signal representation in mid-level tasks in computer vision, such as 3D representation learning from 2D observations.",
      "authors": [
        "Daniele Lizzio Bosco",
        "Shuteng Wang",
        "Giuseppe Serra",
        "Vladislav Golyanik"
      ],
      "published": "2026-01-08T18:59:55Z",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "abs_url": "https://arxiv.org/abs/2601.05250v1",
      "pdf_url": "https://arxiv.org/pdf/2601.05250v1.pdf"
    },
    {
      "id": "2601.05248v1",
      "title": "LaST$_{0}$: Latent Spatio-Temporal Chain-of-Thought for Robotic Vision-Language-Action Model",
      "abstract": "Vision-Language-Action (VLA) models have recently demonstrated strong generalization capabilities in robotic manipulation. Some existing VLA approaches attempt to improve action accuracy by explicitly generating linguistic reasoning traces or future visual observations before action execution. However, explicit reasoning typically incurs non-negligible inference latency, which constrains the temporal resolution required for robotic manipulation. Moreover, such reasoning is confined to the linguistic space, imposing a representational bottleneck that struggles to faithfully capture ineffable physical attributes. To mitigate these limitations, we propose LaST$_0$, a framework that enables efficient reasoning before acting through a Latent Spatio-Temporal Chain-of-Thought (CoT), capturing fine-grained physical and robotic dynamics that are often difficult to verbalize. Specifically, we introduce a token-efficient latent CoT space that models future visual dynamics, 3D structural information, and robot proprioceptive states, and further extends these representations across time to enable temporally consistent implicit reasoning trajectories. Furthermore, LaST$_0$ adopts a dual-system architecture implemented via a Mixture-of-Transformers design, where a reasoning expert conducts low-frequency latent inference and an acting expert generates high-frequency actions conditioned on robotics-oriented latent representations. To facilitate coordination, LaST$_0$ is trained with heterogeneous operation frequencies, enabling adaptive switching between reasoning and action inference rates during deployment. Across ten simulated and six real-world manipulation tasks, LaST$_0$ improves mean success rates by 8% and 13% over prior VLA methods, respectively, while achieving substantially faster inference. Project website: https://sites.google.com/view/last0",
      "authors": [
        "Zhuoyang Liu",
        "Jiaming Liu",
        "Hao Chen",
        "Ziyu Guo",
        "Chengkai Hou",
        "Chenyang Gu",
        "Jiale Yu",
        "Xiangju Mi",
        "Renrui Zhang",
        "Zhengping Che",
        "Jian Tang",
        "Pheng-Ann Heng",
        "Shanghang Zhang"
      ],
      "published": "2026-01-08T18:59:53Z",
      "categories": [
        "cs.RO"
      ],
      "primary_category": "cs.RO",
      "abs_url": "https://arxiv.org/abs/2601.05248v1",
      "pdf_url": "https://arxiv.org/pdf/2601.05248v1.pdf"
    },
    {
      "id": "2601.05246v1",
      "title": "Pixel-Perfect Visual Geometry Estimation",
      "abstract": "Recovering clean and accurate geometry from images is essential for robotics and augmented reality. However, existing geometry foundation models still suffer severely from flying pixels and the loss of fine details. In this paper, we present pixel-perfect visual geometry models that can predict high-quality, flying-pixel-free point clouds by leveraging generative modeling in the pixel space. We first introduce Pixel-Perfect Depth (PPD), a monocular depth foundation model built upon pixel-space diffusion transformers (DiT). To address the high computational complexity associated with pixel-space diffusion, we propose two key designs: 1) Semantics-Prompted DiT, which incorporates semantic representations from vision foundation models to prompt the diffusion process, preserving global semantics while enhancing fine-grained visual details; and 2) Cascade DiT architecture that progressively increases the number of image tokens, improving both efficiency and accuracy. To further extend PPD to video (PPVD), we introduce a new Semantics-Consistent DiT, which extracts temporally consistent semantics from a multi-view geometry foundation model. We then perform reference-guided token propagation within the DiT to maintain temporal coherence with minimal computational and memory overhead. Our models achieve the best performance among all generative monocular and video depth estimation models and produce significantly cleaner point clouds than all other models.",
      "authors": [
        "Gangwei Xu",
        "Haotong Lin",
        "Hongcheng Luo",
        "Haiyang Sun",
        "Bing Wang",
        "Guang Chen",
        "Sida Peng",
        "Hangjun Ye",
        "Xin Yang"
      ],
      "published": "2026-01-08T18:59:49Z",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "abs_url": "https://arxiv.org/abs/2601.05246v1",
      "pdf_url": "https://arxiv.org/pdf/2601.05246v1.pdf"
    },
    {
      "id": "2601.05245v1",
      "title": "Optimal Lower Bounds for Online Multicalibration",
      "abstract": "We prove tight lower bounds for online multicalibration, establishing an information-theoretic separation from marginal calibration.\n  In the general setting where group functions can depend on both context and the learner's predictions, we prove an $Ω(T^{2/3})$ lower bound on expected multicalibration error using just three disjoint binary groups. This matches the upper bounds of Noarov et al. (2025) up to logarithmic factors and exceeds the $O(T^{2/3-\\varepsilon})$ upper bound for marginal calibration (Dagan et al., 2025), thereby separating the two problems.\n  We then turn to lower bounds for the more difficult case of group functions that may depend on context but not on the learner's predictions. In this case, we establish an $\\widetildeΩ(T^{2/3})$ lower bound for online multicalibration via a $Θ(T)$-sized group family constructed using orthogonal function systems, again matching upper bounds up to logarithmic factors.",
      "authors": [
        "Natalie Collina",
        "Jiuyao Lu",
        "Georgy Noarov",
        "Aaron Roth"
      ],
      "published": "2026-01-08T18:59:32Z",
      "categories": [
        "cs.LG",
        "math.ST",
        "stat.ML"
      ],
      "primary_category": "cs.LG",
      "abs_url": "https://arxiv.org/abs/2601.05245v1",
      "pdf_url": "https://arxiv.org/pdf/2601.05245v1.pdf"
    },
    {
      "id": "2601.05244v1",
      "title": "GREx: Generalized Referring Expression Segmentation, Comprehension, and Generation",
      "abstract": "Referring Expression Segmentation (RES) and Comprehension (REC) respectively segment and detect the object described by an expression, while Referring Expression Generation (REG) generates an expression for the selected object. Existing datasets and methods commonly support single-target expressions only, i.e., one expression refers to one object, not considering multi-target and no-target expressions. This greatly limits the real applications of REx (RES/REC/REG). This paper introduces three new benchmarks called Generalized Referring Expression Segmentation (GRES), Comprehension (GREC), and Generation (GREG), collectively denoted as GREx, which extend the classic REx to allow expressions to identify an arbitrary number of objects. We construct the first large-scale GREx dataset gRefCOCO that contains multi-target, no-target, and single-target expressions and their corresponding images with labeled targets. GREx and gRefCOCO are designed to be backward-compatible with REx, facilitating extensive experiments to study the performance gap of the existing REx methods on GREx tasks. One of the challenges of GRES/GREC is complex relationship modeling, for which we propose a baseline ReLA that adaptively divides the image into regions with sub-instance clues and explicitly models the region-region and region-language dependencies. The proposed ReLA achieves the state-of-the-art results on the both GRES and GREC tasks. The proposed gRefCOCO dataset and method are available at https://henghuiding.github.io/GREx.",
      "authors": [
        "Henghui Ding",
        "Chang Liu",
        "Shuting He",
        "Xudong Jiang",
        "Yu-Gang Jiang"
      ],
      "published": "2026-01-08T18:59:30Z",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "abs_url": "https://arxiv.org/abs/2601.05244v1",
      "pdf_url": "https://arxiv.org/pdf/2601.05244v1.pdf"
    },
    {
      "id": "2601.05243v1",
      "title": "Generate, Transfer, Adapt: Learning Functional Dexterous Grasping from a Single Human Demonstration",
      "abstract": "Functional grasping with dexterous robotic hands is a key capability for enabling tool use and complex manipulation, yet progress has been constrained by two persistent bottlenecks: the scarcity of large-scale datasets and the absence of integrated semantic and geometric reasoning in learned models. In this work, we present CorDex, a framework that robustly learns dexterous functional grasps of novel objects from synthetic data generated from just a single human demonstration. At the core of our approach is a correspondence-based data engine that generates diverse, high-quality training data in simulation. Based on the human demonstration, our data engine generates diverse object instances of the same category, transfers the expert grasp to the generated objects through correspondence estimation, and adapts the grasp through optimization. Building on the generated data, we introduce a multimodal prediction network that integrates visual and geometric information. By devising a local-global fusion module and an importance-aware sampling mechanism, we enable robust and computationally efficient prediction of functional dexterous grasps. Through extensive experiments across various object categories, we demonstrate that CorDex generalizes well to unseen object instances and significantly outperforms state-of-the-art baselines.",
      "authors": [
        "Xingyi He",
        "Adhitya Polavaram",
        "Yunhao Cao",
        "Om Deshmukh",
        "Tianrui Wang",
        "Xiaowei Zhou",
        "Kuan Fang"
      ],
      "published": "2026-01-08T18:59:30Z",
      "categories": [
        "cs.RO",
        "cs.CV"
      ],
      "primary_category": "cs.RO",
      "abs_url": "https://arxiv.org/abs/2601.05243v1",
      "pdf_url": "https://arxiv.org/pdf/2601.05243v1.pdf"
    },
    {
      "id": "2601.05242v1",
      "title": "GDPO: Group reward-Decoupled Normalization Policy Optimization for Multi-reward RL Optimization",
      "abstract": "As language models become increasingly capable, users expect them to provide not only accurate responses but also behaviors aligned with diverse human preferences across a variety of scenarios. To achieve this, Reinforcement learning (RL) pipelines have begun incorporating multiple rewards, each capturing a distinct preference, to guide models toward these desired behaviors. However, recent work has defaulted to apply Group Relative Policy Optimization (GRPO) under multi-reward setting without examining its suitability. In this paper, we demonstrate that directly applying GRPO to normalize distinct rollout reward combinations causes them to collapse into identical advantage values, reducing the resolution of the training signal and resulting in suboptimal convergence and, in some cases, early training failure. We then introduce Group reward-Decoupled Normalization Policy Optimization (GDPO), a new policy optimization method to resolve these issues by decoupling the normalization of individual rewards, more faithfully preserving their relative differences and enabling more accurate multi-reward optimization, along with substantially improved training stability. We compare GDPO with GRPO across three tasks: tool calling, math reasoning, and coding reasoning, evaluating both correctness metrics (accuracy, bug ratio) and constraint adherence metrics (format, length). Across all settings, GDPO consistently outperforms GRPO, demonstrating its effectiveness and generalizability for multi-reward reinforcement learning optimization.",
      "authors": [
        "Shih-Yang Liu",
        "Xin Dong",
        "Ximing Lu",
        "Shizhe Diao",
        "Peter Belcak",
        "Mingjie Liu",
        "Min-Hung Chen",
        "Hongxu Yin",
        "Yu-Chiang Frank Wang",
        "Kwang-Ting Cheng",
        "Yejin Choi",
        "Jan Kautz",
        "Pavlo Molchanov"
      ],
      "published": "2026-01-08T18:59:24Z",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "abs_url": "https://arxiv.org/abs/2601.05242v1",
      "pdf_url": "https://arxiv.org/pdf/2601.05242v1.pdf"
    },
    {
      "id": "2601.05241v1",
      "title": "RoboVIP: Multi-View Video Generation with Visual Identity Prompting Augments Robot Manipulation",
      "abstract": "The diversity, quantity, and quality of manipulation data are critical for training effective robot policies. However, due to hardware and physical setup constraints, collecting large-scale real-world manipulation data remains difficult to scale across diverse environments. Recent work uses text-prompt conditioned image diffusion models to augment manipulation data by altering the backgrounds and tabletop objects in the visual observations. However, these approaches often overlook the practical need for multi-view and temporally coherent observations required by state-of-the-art policy models. Further, text prompts alone cannot reliably specify the scene setup. To provide the diffusion model with explicit visual guidance, we introduce visual identity prompting, which supplies exemplar images as conditioning inputs to guide the generation of the desired scene setup. To this end, we also build a scalable pipeline to curate a visual identity pool from large robotics datasets. Using our augmented manipulation data to train downstream vision-language-action and visuomotor policy models yields consistent performance gains in both simulation and real-robot settings.",
      "authors": [
        "Boyang Wang",
        "Haoran Zhang",
        "Shujie Zhang",
        "Jinkun Hao",
        "Mingda Jia",
        "Qi Lv",
        "Yucheng Mao",
        "Zhaoyang Lyu",
        "Jia Zeng",
        "Xudong Xu",
        "Jiangmiao Pang"
      ],
      "published": "2026-01-08T18:59:22Z",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.RO"
      ],
      "primary_category": "cs.CV",
      "abs_url": "https://arxiv.org/abs/2601.05241v1",
      "pdf_url": "https://arxiv.org/pdf/2601.05241v1.pdf"
    },
    {
      "id": "2601.05240v1",
      "title": "Robust Reasoning as a Symmetry-Protected Topological Phase",
      "abstract": "Large language models suffer from \"hallucinations\"-logical inconsistencies induced by semantic noise. We propose that current architectures operate in a \"Metric Phase,\" where causal order is vulnerable to spontaneous symmetry breaking. Here, we identify robust inference as an effective Symmetry-Protected Topological phase, where logical operations are formally isomorphic to non-Abelian anyon braiding, replacing fragile geometric interpolation with robust topological invariants. Empirically, we demonstrate a sharp topological phase transition: while Transformers and RNNs exhibit gapless decay, our Holonomic Network reveals a macroscopic \"mass gap,\" maintaining invariant fidelity below a critical noise threshold. Furthermore, in a variable-binding task on $S_{10}$ ($3.6 \\times 10^6$ states) representing symbolic manipulation, we demonstrate holonomic generalization: the topological model maintains perfect fidelity extrapolating $100\\times$ beyond training ($L=50 \\to 5000$), consistent with a theoretically indefinite causal horizon, whereas Transformers lose logical coherence. Ablation studies indicate this protection emerges strictly from non-Abelian gauge symmetry. This provides strong evidence for a new universality class for logical reasoning, linking causal stability to the topology of the semantic manifold.",
      "authors": [
        "Ilmo Sung"
      ],
      "published": "2026-01-08T18:58:34Z",
      "categories": [
        "cs.LG",
        "cond-mat.dis-nn",
        "cs.AI",
        "hep-th"
      ],
      "primary_category": "cs.LG",
      "abs_url": "https://arxiv.org/abs/2601.05240v1",
      "pdf_url": "https://arxiv.org/pdf/2601.05240v1.pdf"
    },
    {
      "id": "2601.05239v1",
      "title": "Plenoptic Video Generation",
      "abstract": "Camera-controlled generative video re-rendering methods, such as ReCamMaster, have achieved remarkable progress. However, despite their success in single-view setting, these works often struggle to maintain consistency across multi-view scenarios. Ensuring spatio-temporal coherence in hallucinated regions remains challenging due to the inherent stochasticity of generative models. To address it, we introduce PlenopticDreamer, a framework that synchronizes generative hallucinations to maintain spatio-temporal memory. The core idea is to train a multi-in-single-out video-conditioned model in an autoregressive manner, aided by a camera-guided video retrieval strategy that adaptively selects salient videos from previous generations as conditional inputs. In addition, Our training incorporates progressive context-scaling to improve convergence, self-conditioning to enhance robustness against long-range visual degradation caused by error accumulation, and a long-video conditioning mechanism to support extended video generation. Extensive experiments on the Basic and Agibot benchmarks demonstrate that PlenopticDreamer achieves state-of-the-art video re-rendering, delivering superior view synchronization, high-fidelity visuals, accurate camera control, and diverse view transformations (e.g., third-person to third-person, and head-view to gripper-view in robotic manipulation). Project page: https://research.nvidia.com/labs/dir/plenopticdreamer/",
      "authors": [
        "Xiao Fu",
        "Shitao Tang",
        "Min Shi",
        "Xian Liu",
        "Jinwei Gu",
        "Ming-Yu Liu",
        "Dahua Lin",
        "Chen-Hsuan Lin"
      ],
      "published": "2026-01-08T18:58:32Z",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "abs_url": "https://arxiv.org/abs/2601.05239v1",
      "pdf_url": "https://arxiv.org/pdf/2601.05239v1.pdf"
    },
    {
      "id": "2601.05237v1",
      "title": "ObjectForesight: Predicting Future 3D Object Trajectories from Human Videos",
      "abstract": "Humans can effortlessly anticipate how objects might move or change through interaction--imagining a cup being lifted, a knife slicing, or a lid being closed. We aim to endow computational systems with a similar ability to predict plausible future object motions directly from passive visual observation. We introduce ObjectForesight, a 3D object-centric dynamics model that predicts future 6-DoF poses and trajectories of rigid objects from short egocentric video sequences. Unlike conventional world or dynamics models that operate in pixel or latent space, ObjectForesight represents the world explicitly in 3D at the object level, enabling geometrically grounded and temporally coherent predictions that capture object affordances and trajectories. To train such a model at scale, we leverage recent advances in segmentation, mesh reconstruction, and 3D pose estimation to curate a dataset of 2 million plus short clips with pseudo-ground-truth 3D object trajectories. Through extensive experiments, we show that ObjectForesight achieves significant gains in accuracy, geometric consistency, and generalization to unseen objects and scenes, establishing a scalable framework for learning physically grounded, object-centric dynamics models directly from observation. objectforesight.github.io",
      "authors": [
        "Rustin Soraki",
        "Homanga Bharadhwaj",
        "Ali Farhadi",
        "Roozbeh Mottaghi"
      ],
      "published": "2026-01-08T18:58:08Z",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "abs_url": "https://arxiv.org/abs/2601.05237v1",
      "pdf_url": "https://arxiv.org/pdf/2601.05237v1.pdf"
    },
    {
      "id": "2601.05232v1",
      "title": "Measuring and Fostering Peace through Machine Learning and Artificial Intelligence",
      "abstract": "We used machine learning and artificial intelligence: 1) to measure levels of peace in countries from news and social media and 2) to develop on-line tools that promote peace by helping users better understand their own media diet. For news media, we used neural networks to measure levels of peace from text embeddings of on-line news sources. The model, trained on one news media dataset also showed high accuracy when used to analyze a different news dataset. For social media, such as YouTube, we developed other models to measure levels of social dimensions important in peace using word level (GoEmotions) and context level (Large Language Model) methods. To promote peace, we note that 71% of people 20-40 years old daily view most of their news through short videos on social media. Content creators of these videos are biased towards creating videos with emotional activation, making you angry to engage you, to increase clicks. We developed and tested a Chrome extension, MirrorMirror, which provides real-time feedback to YouTube viewers about the peacefulness of the media they are watching. Our long term goal is for MirrorMirror to evolve into an open-source tool for content creators, journalists, researchers, platforms, and individual users to better understand the tone of their media creation and consumption and its effects on viewers. Moving beyond simple engagement metrics, we hope to encourage more respectful, nuanced, and informative communication.",
      "authors": [
        "P. Gilda",
        "P. Dungarwal",
        "A. Thongkham",
        "E. T. Ajayi",
        "S. Choudhary",
        "T. M. Terol",
        "C. Lam",
        "J. P. Araujo",
        "M. McFadyen-Mungalln",
        "L. S. Liebovitch",
        "P. T. Coleman",
        "H. West",
        "K. Sieck",
        "S. Carter"
      ],
      "published": "2026-01-08T18:57:01Z",
      "categories": [
        "cs.CL",
        "cs.CY",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "abs_url": "https://arxiv.org/abs/2601.05232v1",
      "pdf_url": "https://arxiv.org/pdf/2601.05232v1.pdf"
    },
    {
      "id": "2601.05230v1",
      "title": "Learning Latent Action World Models In The Wild",
      "abstract": "Agents capable of reasoning and planning in the real world require the ability of predicting the consequences of their actions. While world models possess this capability, they most often require action labels, that can be complex to obtain at scale. This motivates the learning of latent action models, that can learn an action space from videos alone. Our work addresses the problem of learning latent actions world models on in-the-wild videos, expanding the scope of existing works that focus on simple robotics simulations, video games, or manipulation data. While this allows us to capture richer actions, it also introduces challenges stemming from the video diversity, such as environmental noise, or the lack of a common embodiment across videos. To address some of the challenges, we discuss properties that actions should follow as well as relevant architectural choices and evaluations. We find that continuous, but constrained, latent actions are able to capture the complexity of actions from in-the-wild videos, something that the common vector quantization does not. We for example find that changes in the environment coming from agents, such as humans entering the room, can be transferred across videos. This highlights the capability of learning actions that are specific to in-the-wild videos. In the absence of a common embodiment across videos, we are mainly able to learn latent actions that become localized in space, relative to the camera. Nonetheless, we are able to train a controller that maps known actions to latent ones, allowing us to use latent actions as a universal interface and solve planning tasks with our world model with similar performance as action-conditioned baselines. Our analyses and experiments provide a step towards scaling latent action models to the real world.",
      "authors": [
        "Quentin Garrido",
        "Tushar Nagarajan",
        "Basile Terver",
        "Nicolas Ballas",
        "Yann LeCun",
        "Michael Rabbat"
      ],
      "published": "2026-01-08T18:55:39Z",
      "categories": [
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.AI",
      "abs_url": "https://arxiv.org/abs/2601.05230v1",
      "pdf_url": "https://arxiv.org/pdf/2601.05230v1.pdf"
    },
    {
      "id": "2601.05227v1",
      "title": "Stochastic Deep Learning: A Probabilistic Framework for Modeling Uncertainty in Structured Temporal Data",
      "abstract": "I propose a novel framework that integrates stochastic differential equations (SDEs) with deep generative models to improve uncertainty quantification in machine learning applications involving structured and temporal data. This approach, termed Stochastic Latent Differential Inference (SLDI), embeds an Itô SDE in the latent space of a variational autoencoder, allowing for flexible, continuous-time modeling of uncertainty while preserving a principled mathematical foundation. The drift and diffusion terms of the SDE are parameterized by neural networks, enabling data-driven inference and generalizing classical time series models to handle irregular sampling and complex dynamic structure.\n  A central theoretical contribution is the co-parameterization of the adjoint state with a dedicated neural network, forming a coupled forward-backward system that captures not only latent evolution but also gradient dynamics. I introduce a pathwise-regularized adjoint loss and analyze variance-reduced gradient flows through the lens of stochastic calculus, offering new tools for improving training stability in deep latent SDEs. My paper unifies and extends variational inference, continuous-time generative modeling, and control-theoretic optimization, providing a rigorous foundation for future developments in stochastic probabilistic machine learning.",
      "authors": [
        "James Rice"
      ],
      "published": "2026-01-08T18:53:59Z",
      "categories": [
        "stat.ML",
        "cs.LG",
        "econ.EM",
        "math.ST"
      ],
      "primary_category": "stat.ML",
      "abs_url": "https://arxiv.org/abs/2601.05227v1",
      "pdf_url": "https://arxiv.org/pdf/2601.05227v1.pdf"
    },
    {
      "id": "2601.05219v1",
      "title": "CAOS: Conformal Aggregation of One-Shot Predictors",
      "abstract": "One-shot prediction enables rapid adaptation of pretrained foundation models to new tasks using only one labeled example, but lacks principled uncertainty quantification. While conformal prediction provides finite-sample coverage guarantees, standard split conformal methods are inefficient in the one-shot setting due to data splitting and reliance on a single predictor. We propose Conformal Aggregation of One-Shot Predictors (CAOS), a conformal framework that adaptively aggregates multiple one-shot predictors and uses a leave-one-out calibration scheme to fully exploit scarce labeled data. Despite violating classical exchangeability assumptions, we prove that CAOS achieves valid marginal coverage using a monotonicity-based argument. Experiments on one-shot facial landmarking and RAFT text classification tasks show that CAOS produces substantially smaller prediction sets than split conformal baselines while maintaining reliable coverage.",
      "authors": [
        "Maja Waldron"
      ],
      "published": "2026-01-08T18:44:21Z",
      "categories": [
        "stat.ML",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "stat.ML",
      "abs_url": "https://arxiv.org/abs/2601.05219v1",
      "pdf_url": "https://arxiv.org/pdf/2601.05219v1.pdf"
    },
    {
      "id": "2601.05212v1",
      "title": "FlowLet: Conditional 3D Brain MRI Synthesis using Wavelet Flow Matching",
      "abstract": "Brain Magnetic Resonance Imaging (MRI) plays a central role in studying neurological development, aging, and diseases. One key application is Brain Age Prediction (BAP), which estimates an individual's biological brain age from MRI data. Effective BAP models require large, diverse, and age-balanced datasets, whereas existing 3D MRI datasets are demographically skewed, limiting fairness and generalizability. Acquiring new data is costly and ethically constrained, motivating generative data augmentation. Current generative methods are often based on latent diffusion models, which operate in learned low dimensional latent spaces to address the memory demands of volumetric MRI data. However, these methods are typically slow at inference, may introduce artifacts due to latent compression, and are rarely conditioned on age, thereby affecting the BAP performance. In this work, we propose FlowLet, a conditional generative framework that synthesizes age-conditioned 3D MRIs by leveraging flow matching within an invertible 3D wavelet domain, helping to avoid reconstruction artifacts and reducing computational demands. Experiments show that FlowLet generates high-fidelity volumes with few sampling steps. Training BAP models with data generated by FlowLet improves performance for underrepresented age groups, and region-based analysis confirms preservation of anatomical structures.",
      "authors": [
        "Danilo Danese",
        "Angela Lombardi",
        "Matteo Attimonelli",
        "Giuseppe Fasano",
        "Tommaso Di Noia"
      ],
      "published": "2026-01-08T18:36:29Z",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "abs_url": "https://arxiv.org/abs/2601.05212v1",
      "pdf_url": "https://arxiv.org/pdf/2601.05212v1.pdf"
    },
    {
      "id": "2601.05208v1",
      "title": "MoE3D: A Mixture-of-Experts Module for 3D Reconstruction",
      "abstract": "MoE3D is a mixture-of-experts module designed to sharpen depth boundaries and mitigate flying-point artifacts (highlighted in red) of existing feed-forward 3D reconstruction models (left side). MoE3D predicts multiple candidate depth maps and fuses them via dynamic weighting (visualized by MoE weights on the right side). When integrated with a pre-trained 3D reconstruction backbone such as VGGT, it substantially enhances reconstruction quality with minimal additional computational overhead. Best viewed digitally.",
      "authors": [
        "Zichen Wang",
        "Ang Cao",
        "Liam J. Wang",
        "Jeong Joon Park"
      ],
      "published": "2026-01-08T18:33:52Z",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "abs_url": "https://arxiv.org/abs/2601.05208v1",
      "pdf_url": "https://arxiv.org/pdf/2601.05208v1.pdf"
    },
    {
      "id": "2601.05205v1",
      "title": "EARL: Energy-Aware Optimization of Liquid State Machines for Pervasive AI",
      "abstract": "Pervasive AI increasingly depends on on-device learning systems that deliver low-latency and energy-efficient computation under strict resource constraints. Liquid State Machines (LSMs) offer a promising approach for low-power temporal processing in pervasive and neuromorphic systems, but their deployment remains challenging due to high hyperparameter sensitivity and the computational cost of traditional optimization methods that ignore energy constraints. This work presents EARL, an energy-aware reinforcement learning framework that integrates Bayesian optimization with an adaptive reinforcement learning based selection policy to jointly optimize accuracy and energy consumption. EARL employs surrogate modeling for global exploration, reinforcement learning for dynamic candidate prioritization, and an early termination mechanism to eliminate redundant evaluations, substantially reducing computational overhead. Experiments on three benchmark datasets demonstrate that EARL achieves 6 to 15 percent higher accuracy, 60 to 80 percent lower energy consumption, and up to an order of magnitude reduction in optimization time compared to leading hyperparameter tuning frameworks. These results highlight the effectiveness of energy-aware adaptive search in improving the efficiency and scalability of LSMs for resource-constrained on-device AI applications.",
      "authors": [
        "Zain Iqbal",
        "Lorenzo Valerio"
      ],
      "published": "2026-01-08T18:31:11Z",
      "categories": [
        "cs.LG",
        "cs.PF"
      ],
      "primary_category": "cs.LG",
      "abs_url": "https://arxiv.org/abs/2601.05205v1",
      "pdf_url": "https://arxiv.org/pdf/2601.05205v1.pdf"
    },
    {
      "id": "2601.05202v1",
      "title": "Stock Market Price Prediction using Neural Prophet with Deep Neural Network",
      "abstract": "Stock market price prediction is a significant interdisciplinary research domain that depends at the intersection of finance, statistics, and economics. Forecasting Accurately predicting stock prices has always been a focal point for various researchers. However, existing statistical approaches for time-series prediction often fail to effectively forecast the probability range of future stock prices. Hence, to solve this problem, the Neural Prophet with a Deep Neural Network (NP-DNN) is proposed to predict stock market prices. The preprocessing technique used in this research is Z-score normalization, which normalizes stock price data by removing scale differences, making patterns easier to detect. Missing value imputation fills gaps in historical data, enhancing the models use of complete information for more accurate predictions. The Multi-Layer Perceptron (MLP) learns complex nonlinear relationships among stock market prices and extracts hidden patterns from the input data, thereby creating meaningful feature representations for better prediction accuracy. The proposed NP-DNN model achieved an accuracy of 99.21% compared with other approaches using the Fused Large Language Model. Keywords: deep neural network, forecasting stock prices, multi-layer perceptron, neural prophet, stock market price prediction.",
      "authors": [
        "Navin Chhibber",
        "Suneel Khemka",
        "Navneet Kumar Tyagi",
        "Rohit Tewari",
        "Bireswar Banerjee",
        "Piyush Ranjan"
      ],
      "published": "2026-01-08T18:24:22Z",
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "abs_url": "https://arxiv.org/abs/2601.05202v1",
      "pdf_url": "https://arxiv.org/pdf/2601.05202v1.pdf"
    },
    {
      "id": "2601.05201v1",
      "title": "Mechanisms of Prompt-Induced Hallucination in Vision-Language Models",
      "abstract": "Large vision-language models (VLMs) are highly capable, yet often hallucinate by favoring textual prompts over visual evidence. We study this failure mode in a controlled object-counting setting, where the prompt overstates the number of objects in the image (e.g., asking a model to describe four waterlilies when only three are present). At low object counts, models often correct the overestimation, but as the number of objects increases, they increasingly conform to the prompt regardless of the discrepancy. Through mechanistic analysis of three VLMs, we identify a small set of attention heads whose ablation substantially reduces prompt-induced hallucinations (PIH) by at least 40% without additional training. Across models, PIH-heads mediate prompt copying in model-specific ways. We characterize these differences and show that PIH ablation increases correction toward visual evidence. Our findings offer insights into the internal mechanisms driving prompt-induced hallucinations, revealing model-specific differences in how these behaviors are implemented.",
      "authors": [
        "William Rudman",
        "Michal Golovanevsky",
        "Dana Arad",
        "Yonatan Belinkov",
        "Ritambhara Singh",
        "Carsten Eickhoff",
        "Kyle Mahowald"
      ],
      "published": "2026-01-08T18:23:03Z",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.CV",
      "abs_url": "https://arxiv.org/abs/2601.05201v1",
      "pdf_url": "https://arxiv.org/pdf/2601.05201v1.pdf"
    },
    {
      "id": "2601.05194v1",
      "title": "An interpretable data-driven approach to optimizing clinical fall risk assessment",
      "abstract": "In this study, we aim to better align fall risk prediction from the Johns Hopkins Fall Risk Assessment Tool (JHFRAT) with additional clinically meaningful measures via a data-driven modelling approach. We conducted a retrospective cohort analysis of 54,209 inpatient admissions from three Johns Hopkins Health System hospitals between March 2022 and October 2023. A total of 20,208 admissions were included as high fall risk encounters, and 13,941 were included as low fall risk encounters. To incorporate clinical knowledge and maintain interpretability, we employed constrained score optimization (CSO) models to reweight the JHFRAT scoring weights, while preserving its additive structure and clinical thresholds. Recalibration refers to adjusting item weights so that the resulting score can order encounters more consistently by the study's risk labels, and without changing the tool's form factor or deployment workflow. The model demonstrated significant improvements in predictive performance over the current JHFRAT (CSO AUC-ROC=0.91, JHFRAT AUC-ROC=0.86). This performance improvement translates to protecting an additional 35 high-risk patients per week across the Johns Hopkins Health System. The constrained score optimization models performed similarly with and without the EHR variables. Although the benchmark black-box model (XGBoost), improves upon the performance metrics of the knowledge-based constrained logistic regression (AUC-ROC=0.94), the CSO demonstrates more robustness to variations in risk labeling. This evidence-based approach provides a robust foundation for health systems to systematically enhance inpatient fall prevention protocols and patient safety using data-driven optimization techniques, contributing to improved risk assessment and resource allocation in healthcare settings.",
      "authors": [
        "Fardin Ganjkhanloo",
        "Emmett Springer",
        "Erik H. Hoyer",
        "Daniel L. Young",
        "Holley Farley",
        "Kimia Ghobadi"
      ],
      "published": "2026-01-08T18:17:31Z",
      "categories": [
        "cs.LG"
      ],
      "primary_category": "cs.LG",
      "abs_url": "https://arxiv.org/abs/2601.05194v1",
      "pdf_url": "https://arxiv.org/pdf/2601.05194v1.pdf"
    },
    {
      "id": "2601.05192v1",
      "title": "LELA: an LLM-based Entity Linking Approach with Zero-Shot Domain Adaptation",
      "abstract": "Entity linking (mapping ambiguous mentions in text to entities in a knowledge base) is a foundational step in tasks such as knowledge graph construction, question-answering, and information extraction. Our method, LELA, is a modular coarse-to-fine approach that leverages the capabilities of large language models (LLMs), and works with different target domains, knowledge bases and LLMs, without any fine-tuning phase. Our experiments across various entity linking settings show that LELA is highly competitive with fine-tuned approaches, and substantially outperforms the non-fine-tuned ones.",
      "authors": [
        "Samy Haffoudhi",
        "Fabian M. Suchanek",
        "Nils Holzenberger"
      ],
      "published": "2026-01-08T18:15:34Z",
      "categories": [
        "cs.CL"
      ],
      "primary_category": "cs.CL",
      "abs_url": "https://arxiv.org/abs/2601.05192v1",
      "pdf_url": "https://arxiv.org/pdf/2601.05192v1.pdf"
    },
    {
      "id": "2601.05191v1",
      "title": "Cutting AI Research Costs: How Task-Aware Compression Makes Large Language Model Agents Affordable",
      "abstract": "When researchers deploy large language models for autonomous tasks like reviewing literature or generating hypotheses, the computational bills add up quickly. A single research session using a 70-billion parameter model can cost around $127 in cloud fees, putting these tools out of reach for many academic labs. We developed AgentCompress to tackle this problem head-on. The core idea came from a simple observation during our own work: writing a novel hypothesis clearly demands more from the model than reformatting a bibliography. Why should both tasks run at full precision? Our system uses a small neural network to gauge how hard each incoming task will be, based only on its opening words, then routes it to a suitably compressed model variant. The decision happens in under a millisecond. Testing across 500 research workflows in four scientific fields, we cut compute costs by 68.3% while keeping 96.2% of the original success rate. For labs watching their budgets, this could mean the difference between running experiments and sitting on the sidelines",
      "authors": [
        "Zuhair Ahmed Khan Taha",
        "Mohammed Mudassir Uddin",
        "Shahnawaz Alam"
      ],
      "published": "2026-01-08T18:13:46Z",
      "categories": [
        "cs.CV",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "abs_url": "https://arxiv.org/abs/2601.05191v1",
      "pdf_url": "https://arxiv.org/pdf/2601.05191v1.pdf"
    },
    {
      "id": "2601.05184v1",
      "title": "Observations and Remedies for Large Language Model Bias in Self-Consuming Performative Loop",
      "abstract": "The rapid advancement of large language models (LLMs) has led to growing interest in using synthetic data to train future models. However, this creates a self-consuming retraining loop, where models are trained on their own outputs and may cause performance drops and induce emerging biases. In real-world applications, previously deployed LLMs may influence the data they generate, leading to a dynamic system driven by user feedback. For example, if a model continues to underserve users from a group, less query data will be collected from this particular demographic of users. In this study, we introduce the concept of \\textbf{S}elf-\\textbf{C}onsuming \\textbf{P}erformative \\textbf{L}oop (\\textbf{SCPL}) and investigate the role of synthetic data in shaping bias during these dynamic iterative training processes under controlled performative feedback. This controlled setting is motivated by the inaccessibility of real-world user preference data from dynamic production systems, and enables us to isolate and analyze feedback-driven bias evolution in a principled manner. We focus on two types of loops, including the typical retraining setting and the incremental fine-tuning setting, which is largely underexplored. Through experiments on three real-world tasks, we find that the performative loop increases preference bias and decreases disparate bias. We design a reward-based rejection sampling strategy to mitigate the bias, moving towards more trustworthy self-improving systems.",
      "authors": [
        "Yaxuan Wang",
        "Zhongteng Cai",
        "Yujia Bao",
        "Xueru Zhang",
        "Yang Liu"
      ],
      "published": "2026-01-08T18:08:15Z",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.AI",
      "abs_url": "https://arxiv.org/abs/2601.05184v1",
      "pdf_url": "https://arxiv.org/pdf/2601.05184v1.pdf"
    },
    {
      "id": "2601.05175v1",
      "title": "VideoAuto-R1: Video Auto Reasoning via Thinking Once, Answering Twice",
      "abstract": "Chain-of-thought (CoT) reasoning has emerged as a powerful tool for multimodal large language models on video understanding tasks. However, its necessity and advantages over direct answering remain underexplored. In this paper, we first demonstrate that for RL-trained video models, direct answering often matches or even surpasses CoT performance, despite CoT producing step-by-step analyses at a higher computational cost. Motivated by this, we propose VideoAuto-R1, a video understanding framework that adopts a reason-when-necessary strategy. During training, our approach follows a Thinking Once, Answering Twice paradigm: the model first generates an initial answer, then performs reasoning, and finally outputs a reviewed answer. Both answers are supervised via verifiable rewards. During inference, the model uses the confidence score of the initial answer to determine whether to proceed with reasoning. Across video QA and grounding benchmarks, VideoAuto-R1 achieves state-of-the-art accuracy with significantly improved efficiency, reducing the average response length by ~3.3x, e.g., from 149 to just 44 tokens. Moreover, we observe a low rate of thinking-mode activation on perception-oriented tasks, but a higher rate on reasoning-intensive tasks. This suggests that explicit language-based reasoning is generally beneficial but not always necessary.",
      "authors": [
        "Shuming Liu",
        "Mingchen Zhuge",
        "Changsheng Zhao",
        "Jun Chen",
        "Lemeng Wu",
        "Zechun Liu",
        "Chenchen Zhu",
        "Zhipeng Cai",
        "Chong Zhou",
        "Haozhe Liu",
        "Ernie Chang",
        "Saksham Suri",
        "Hongyu Xu",
        "Qi Qian",
        "Wei Wen",
        "Balakrishnan Varadarajan",
        "Zhuang Liu",
        "Hu Xu",
        "Florian Bordes",
        "Raghuraman Krishnamoorthi",
        "Bernard Ghanem",
        "Vikas Chandra",
        "Yunyang Xiong"
      ],
      "published": "2026-01-08T18:00:59Z",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "abs_url": "https://arxiv.org/abs/2601.05175v1",
      "pdf_url": "https://arxiv.org/pdf/2601.05175v1.pdf"
    },
    {
      "id": "2601.05172v2",
      "title": "CoV: Chain-of-View Prompting for Spatial Reasoning",
      "abstract": "Embodied question answering (EQA) in 3D environments often requires collecting context that is distributed across multiple viewpoints and partially occluded. However, most recent vision--language models (VLMs) are constrained to a fixed and finite set of input views, which limits their ability to acquire question-relevant context at inference time and hinders complex spatial reasoning. We propose Chain-of-View (CoV) prompting, a training-free, test-time reasoning framework that transforms a VLM into an active viewpoint reasoner through a coarse-to-fine exploration process. CoV first employs a View Selection agent to filter redundant frames and identify question-aligned anchor views. It then performs fine-grained view adjustment by interleaving iterative reasoning with discrete camera actions, obtaining new observations from the underlying 3D scene representation until sufficient context is gathered or a step budget is reached.\n  We evaluate CoV on OpenEQA across four mainstream VLMs and obtain an average +11.56% improvement in LLM-Match, with a maximum gain of +13.62% on Qwen3-VL-Flash. CoV further exhibits test-time scaling: increasing the minimum action budget yields an additional +2.51% average improvement, peaking at +3.73% on Gemini-2.5-Flash. On ScanQA and SQA3D, CoV delivers strong performance (e.g., 116 CIDEr / 31.9 EM@1 on ScanQA and 51.1 EM@1 on SQA3D). Overall, these results suggest that question-aligned view selection coupled with open-view search is an effective, model-agnostic strategy for improving spatial reasoning in 3D EQA without additional training. Code is available on https://github.com/ziplab/CoV .",
      "authors": [
        "Haoyu Zhao",
        "Akide Liu",
        "Zeyu Zhang",
        "Weijie Wang",
        "Feng Chen",
        "Ruihan Zhu",
        "Gholamreza Haffari",
        "Bohan Zhuang"
      ],
      "published": "2026-01-08T17:59:42Z",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "abs_url": "https://arxiv.org/abs/2601.05172v2",
      "pdf_url": "https://arxiv.org/pdf/2601.05172v2.pdf"
    },
    {
      "id": "2601.05171v1",
      "title": "Inside Out: Evolving User-Centric Core Memory Trees for Long-Term Personalized Dialogue Systems",
      "abstract": "Existing long-term personalized dialogue systems struggle to reconcile unbounded interaction streams with finite context constraints, often succumbing to memory noise accumulation, reasoning degradation, and persona inconsistency. To address these challenges, this paper proposes Inside Out, a framework that utilizes a globally maintained PersonaTree as the carrier of long-term user profiling. By constraining the trunk with an initial schema and updating the branches and leaves, PersonaTree enables controllable growth, achieving memory compression while preserving consistency. Moreover, we train a lightweight MemListener via reinforcement learning with process-based rewards to produce structured, executable, and interpretable {ADD, UPDATE, DELETE, NO_OP} operations, thereby supporting the dynamic evolution of the personalized tree. During response generation, PersonaTree is directly leveraged to enhance outputs in latency-sensitive scenarios; when users require more details, the agentic mode is triggered to introduce details on-demand under the constraints of the PersonaTree. Experiments show that PersonaTree outperforms full-text concatenation and various personalized memory systems in suppressing contextual noise and maintaining persona consistency. Notably, the small MemListener model achieves memory-operation decision performance comparable to, or even surpassing, powerful reasoning models such as DeepSeek-R1-0528 and Gemini-3-Pro.",
      "authors": [
        "Jihao Zhao",
        "Ding Chen",
        "Zhaoxin Fan",
        "Kerun Xu",
        "Mengting Hu",
        "Bo Tang",
        "Feiyu Xiong",
        "Zhiyu li"
      ],
      "published": "2026-01-08T17:59:11Z",
      "categories": [
        "cs.CL"
      ],
      "primary_category": "cs.CL",
      "abs_url": "https://arxiv.org/abs/2601.05171v1",
      "pdf_url": "https://arxiv.org/pdf/2601.05171v1.pdf"
    },
    {
      "id": "2601.05170v2",
      "title": "Reverse-engineering NLI: A study of the meta-inferential properties of Natural Language Inference",
      "abstract": "Natural Language Inference (NLI) has been an important task for evaluating language models for Natural Language Understanding, but the logical properties of the task are poorly understood and often mischaracterized. Understanding the notion of inference captured by NLI is key to interpreting model performance on the task. In this paper we formulate three possible readings of the NLI label set and perform a comprehensive analysis of the meta-inferential properties they entail. Focusing on the SNLI dataset, we exploit (1) NLI items with shared premises and (2) items generated by LLMs to evaluate models trained on SNLI for meta-inferential consistency and derive insights into which reading of the logical relations is encoded by the dataset.",
      "authors": [
        "Rasmus Blanck",
        "Bill Noble",
        "Stergios Chatzikyriakidis"
      ],
      "published": "2026-01-08T17:58:52Z",
      "categories": [
        "cs.CL"
      ],
      "primary_category": "cs.CL",
      "abs_url": "https://arxiv.org/abs/2601.05170v2",
      "pdf_url": "https://arxiv.org/pdf/2601.05170v2.pdf"
    },
    {
      "id": "2601.05167v1",
      "title": "RelayLLM: Efficient Reasoning via Collaborative Decoding",
      "abstract": "Large Language Models (LLMs) for complex reasoning is often hindered by high computational costs and latency, while resource-efficient Small Language Models (SLMs) typically lack the necessary reasoning capacity. Existing collaborative approaches, such as cascading or routing, operate at a coarse granularity by offloading entire queries to LLMs, resulting in significant computational waste when the SLM is capable of handling the majority of reasoning steps. To address this, we propose RelayLLM, a novel framework for efficient reasoning via token-level collaborative decoding. Unlike routers, RelayLLM empowers the SLM to act as an active controller that dynamically invokes the LLM only for critical tokens via a special command, effectively \"relaying\" the generation process. We introduce a two-stage training framework, including warm-up and Group Relative Policy Optimization (GRPO) to teach the model to balance independence with strategic help-seeking. Empirical results across six benchmarks demonstrate that RelayLLM achieves an average accuracy of 49.52%, effectively bridging the performance gap between the two models. Notably, this is achieved by invoking the LLM for only 1.07% of the total generated tokens, offering a 98.2% cost reduction compared to performance-matched random routers.",
      "authors": [
        "Chengsong Huang",
        "Tong Zheng",
        "Langlin Huang",
        "Jinyuan Li",
        "Haolin Liu",
        "Jiaxin Huang"
      ],
      "published": "2026-01-08T17:56:16Z",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "abs_url": "https://arxiv.org/abs/2601.05167v1",
      "pdf_url": "https://arxiv.org/pdf/2601.05167v1.pdf"
    },
    {
      "id": "2601.05163v1",
      "title": "DocDancer: Towards Agentic Document-Grounded Information Seeking",
      "abstract": "Document Question Answering (DocQA) focuses on answering questions grounded in given documents, yet existing DocQA agents lack effective tool utilization and largely rely on closed-source models. In this work, we introduce DocDancer, an end-to-end trained open-source Doc agent. We formulate DocQA as an information-seeking problem and propose a tool-driven agent framework that explicitly models document exploration and comprehension. To enable end-to-end training of such agents, we introduce an Exploration-then-Synthesis data synthesis pipeline that addresses the scarcity of high-quality training data for DocQA. Training on the synthesized data, the trained models on two long-context document understanding benchmarks, MMLongBench-Doc and DocBench, show their effectiveness. Further analysis provides valuable insights for the agentic tool design and synthetic data.",
      "authors": [
        "Qintong Zhang",
        "Xinjie Lv",
        "Jialong Wu",
        "Baixuan Li",
        "Zhengwei Tao",
        "Guochen Yan",
        "Huanyao Zhang",
        "Bin Wang",
        "Jiahao Xu",
        "Haitao Mi",
        "Wentao Zhang"
      ],
      "published": "2026-01-08T17:54:32Z",
      "categories": [
        "cs.CL"
      ],
      "primary_category": "cs.CL",
      "abs_url": "https://arxiv.org/abs/2601.05163v1",
      "pdf_url": "https://arxiv.org/pdf/2601.05163v1.pdf"
    },
    {
      "id": "2601.05162v1",
      "title": "GenAI-DrawIO-Creator: A Framework for Automated Diagram Generation",
      "abstract": "Diagrams are crucial for communicating complex information, yet creating and modifying them remains a labor-intensive task. We present GenAI-DrawIO-Creator, a novel framework that leverages Large Language Models (LLMs) to automate diagram generation and manipulation in the structured XML format used by draw.io. Our system integrates Claude 3.7 to reason about structured visual data and produce valid diagram representations. Key contributions include a high-level system design enabling real-time diagram updates, specialized prompt engineering and error-checking to ensure well-formed XML outputs. We demonstrate a working prototype capable of generating accurate diagrams (such as network architectures and flowcharts) from natural language or code, and even replicating diagrams from images. Simulated evaluations show that our approach significantly reduces diagram creation time and produces outputs with high structural fidelity. Our results highlight the promise of Claude 3.7 in handling structured visual reasoning tasks and lay the groundwork for future research in AI-assisted diagramming applications.",
      "authors": [
        "Jinze Yu",
        "Dayuan Jiang"
      ],
      "published": "2026-01-08T17:51:35Z",
      "categories": [
        "cs.GR",
        "cs.CV"
      ],
      "primary_category": "cs.GR",
      "abs_url": "https://arxiv.org/abs/2601.05162v1",
      "pdf_url": "https://arxiv.org/pdf/2601.05162v1.pdf"
    },
    {
      "id": "2601.05159v1",
      "title": "Vision-Language Introspection: Mitigating Overconfident Hallucinations in MLLMs via Interpretable Bi-Causal Steering",
      "abstract": "Object hallucination critically undermines the reliability of Multimodal Large Language Models, often stemming from a fundamental failure in cognitive introspection, where models blindly trust linguistic priors over specific visual evidence. Existing mitigations remain limited: contrastive decoding approaches operate superficially without rectifying internal semantic misalignments, while current latent steering methods rely on static vectors that lack instance-specific precision. We introduce Vision-Language Introspection (VLI), a training-free inference framework that simulates a metacognitive self-correction process. VLI first performs Attributive Introspection to diagnose hallucination risks via probabilistic conflict detection and localize the causal visual anchors. It then employs Interpretable Bi-Causal Steering to actively modulate the inference process, dynamically isolating visual evidence from background noise while neutralizing blind confidence through adaptive calibration. VLI achieves state-of-the-art performance on advanced models, reducing object hallucination rates by 12.67% on MMHal-Bench and improving accuracy by 5.8% on POPE.",
      "authors": [
        "Shuliang Liu",
        "Songbo Yang",
        "Dong Fang",
        "Sihang Jia",
        "Yuqi Tang",
        "Lingfeng Su",
        "Ruoshui Peng",
        "Yibo Yan",
        "Xin Zou",
        "Xuming Hu"
      ],
      "published": "2026-01-08T17:49:13Z",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "abs_url": "https://arxiv.org/abs/2601.05159v1",
      "pdf_url": "https://arxiv.org/pdf/2601.05159v1.pdf"
    },
    {
      "id": "2601.05149v1",
      "title": "Multi-Scale Local Speculative Decoding for Image Generation",
      "abstract": "Autoregressive (AR) models have achieved remarkable success in image synthesis, yet their sequential nature imposes significant latency constraints. Speculative Decoding offers a promising avenue for acceleration, but existing approaches are limited by token-level ambiguity and lack of spatial awareness. In this work, we introduce Multi-Scale Local Speculative Decoding (MuLo-SD), a novel framework that combines multi-resolution drafting with spatially informed verification to accelerate AR image generation. Our method leverages a low-resolution drafter paired with learned up-samplers to propose candidate image tokens, which are then verified in parallel by a high-resolution target model. Crucially, we incorporate a local rejection and resampling mechanism, enabling efficient correction of draft errors by focusing on spatial neighborhoods rather than raster-scan resampling after the first rejection. We demonstrate that MuLo-SD achieves substantial speedups - up to $\\mathbf{1.7\\times}$ - outperforming strong speculative decoding baselines such as EAGLE-2 and LANTERN in terms of acceleration, while maintaining comparable semantic alignment and perceptual quality. These results are validated using GenEval, DPG-Bench, and FID/HPSv2 on the MS-COCO 5k validation split. Extensive ablations highlight the impact of up-sampling design, probability pooling, and local rejection and resampling with neighborhood expansion. Our approach sets a new state-of-the-art in speculative decoding for image synthesis, bridging the gap between efficiency and fidelity.",
      "authors": [
        "Elia Peruzzo",
        "Guillaume Sautière",
        "Amirhossein Habibian"
      ],
      "published": "2026-01-08T17:39:35Z",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "abs_url": "https://arxiv.org/abs/2601.05149v1",
      "pdf_url": "https://arxiv.org/pdf/2601.05149v1.pdf"
    },
    {
      "id": "2601.05148v1",
      "title": "Atlas 2 -- Foundation models for clinical deployment",
      "abstract": "Pathology foundation models substantially advanced the possibilities in computational pathology -- yet tradeoffs in terms of performance, robustness, and computational requirements remained, which limited their clinical deployment. In this report, we present Atlas 2, Atlas 2-B, and Atlas 2-S, three pathology vision foundation models which bridge these shortcomings by showing state-of-the-art performance in prediction performance, robustness, and resource efficiency in a comprehensive evaluation across eighty public benchmarks. Our models were trained on the largest pathology foundation model dataset to date comprising 5.5 million histopathology whole slide images, collected from three medical institutions Charité - Universtätsmedizin Berlin, LMU Munich, and Mayo Clinic.",
      "authors": [
        "Maximilian Alber",
        "Timo Milbich",
        "Alexandra Carpen-Amarie",
        "Stephan Tietz",
        "Jonas Dippel",
        "Lukas Muttenthaler",
        "Beatriz Perez Cancer",
        "Alessandro Benetti",
        "Panos Korfiatis",
        "Elias Eulig",
        "Jérôme Lüscher",
        "Jiasen Wu",
        "Sayed Abid Hashimi",
        "Gabriel Dernbach",
        "Simon Schallenberg",
        "Neelay Shah",
        "Moritz Krügener",
        "Aniruddh Jammoria",
        "Jake Matras",
        "Patrick Duffy",
        "Matt Redlon",
        "Philipp Jurmeister",
        "David Horst",
        "Lukas Ruff",
        "Klaus-Robert Müller",
        "Frederick Klauschen",
        "Andrew Norgan"
      ],
      "published": "2026-01-08T17:37:00Z",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "abs_url": "https://arxiv.org/abs/2601.05148v1",
      "pdf_url": "https://arxiv.org/pdf/2601.05148v1.pdf"
    },
    {
      "id": "2601.05143v1",
      "title": "A Lightweight and Explainable Vision-Language Framework for Crop Disease Visual Question Answering",
      "abstract": "Visual question answering for crop disease analysis requires accurate visual understanding and reliable language generation. This work presents a lightweight vision-language framework for crop and disease identification from leaf images. The proposed approach combines a Swin Transformer vision encoder with sequence-to-sequence language decoders. A two-stage training strategy is adopted to improve visual representation learning and cross-modal alignment. The model is evaluated on a large-scale crop disease dataset using classification and natural language generation metrics. Experimental results show high accuracy for both crop and disease identification. The framework also achieves strong performance on BLEU, ROUGE and BERTScore. Our proposed models outperform large-scale vision-language baselines while using significantly fewer parameters. Explainability is assessed using Grad-CAM and token-level attribution. Qualitative results demonstrate robust performance under diverse user-driven queries. These findings highlight the effectiveness of task-specific visual pretraining for crop disease visual question answering.",
      "authors": [
        "Md. Zahid Hossain",
        "Most. Sharmin Sultana Samu",
        "Md. Rakibul Islam",
        "Md. Siam Ansary"
      ],
      "published": "2026-01-08T17:31:09Z",
      "categories": [
        "cs.CV",
        "cs.CL"
      ],
      "primary_category": "cs.CV",
      "abs_url": "https://arxiv.org/abs/2601.05143v1",
      "pdf_url": "https://arxiv.org/pdf/2601.05143v1.pdf"
    },
    {
      "id": "2601.05138v1",
      "title": "VerseCrafter: Dynamic Realistic Video World Model with 4D Geometric Control",
      "abstract": "Video world models aim to simulate dynamic, real-world environments, yet existing methods struggle to provide unified and precise control over camera and multi-object motion, as videos inherently operate dynamics in the projected 2D image plane. To bridge this gap, we introduce VerseCrafter, a 4D-aware video world model that enables explicit and coherent control over both camera and object dynamics within a unified 4D geometric world state. Our approach is centered on a novel 4D Geometric Control representation, which encodes the world state through a static background point cloud and per-object 3D Gaussian trajectories. This representation captures not only an object's path but also its probabilistic 3D occupancy over time, offering a flexible, category-agnostic alternative to rigid bounding boxes or parametric models. These 4D controls are rendered into conditioning signals for a pretrained video diffusion model, enabling the generation of high-fidelity, view-consistent videos that precisely adhere to the specified dynamics. Unfortunately, another major challenge lies in the scarcity of large-scale training data with explicit 4D annotations. We address this by developing an automatic data engine that extracts the required 4D controls from in-the-wild videos, allowing us to train our model on a massive and diverse dataset.",
      "authors": [
        "Sixiao Zheng",
        "Minghao Yin",
        "Wenbo Hu",
        "Xiaoyu Li",
        "Ying Shan",
        "Yanwei Fu"
      ],
      "published": "2026-01-08T17:28:52Z",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "abs_url": "https://arxiv.org/abs/2601.05138v1",
      "pdf_url": "https://arxiv.org/pdf/2601.05138v1.pdf"
    },
    {
      "id": "2601.05125v1",
      "title": "VERSE: Visual Embedding Reduction and Space Exploration. Clustering-Guided Insights for Training Data Enhancement in Visually-Rich Document Understanding",
      "abstract": "This work introduces VERSE, a methodology for analyzing and improving Vision-Language Models applied to Visually-rich Document Understanding by exploring their visual embedding space. VERSE enables the visualization of latent representations, supporting the assessment of model feasibility. It also facilitates the identification of problematic regions and guides the generation of synthetic data to enhance performance in those clusters. We validate the methodology by training on the synthetic MERIT Dataset and evaluating on its real-world counterpart, MERIT Secret. Results show that VERSE helps uncover the visual features associated with error-prone clusters, and that retraining with samples containing these features substantially boosts F1 performance without degrading generalization. Furthermore, we demonstrate that on-premise models such as Donut and Idefics2, when optimized with VERSE, match or even surpass the performance of SaaS solutions like GPT-4 and Pixtral.",
      "authors": [
        "Ignacio de Rodrigo",
        "Alvaro J. Lopez-Lopez",
        "Jaime Boal"
      ],
      "published": "2026-01-08T17:15:15Z",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "abs_url": "https://arxiv.org/abs/2601.05125v1",
      "pdf_url": "https://arxiv.org/pdf/2601.05125v1.pdf"
    },
    {
      "id": "2601.05124v1",
      "title": "Re-Align: Structured Reasoning-guided Alignment for In-Context Image Generation and Editing",
      "abstract": "In-context image generation and editing (ICGE) enables users to specify visual concepts through interleaved image-text prompts, demanding precise understanding and faithful execution of user intent. Although recent unified multimodal models exhibit promising understanding capabilities, these strengths often fail to transfer effectively to image generation. We introduce Re-Align, a unified framework that bridges the gap between understanding and generation through structured reasoning-guided alignment. At its core lies the In-Context Chain-of-Thought (IC-CoT), a structured reasoning paradigm that decouples semantic guidance and reference association, providing clear textual target and mitigating confusion among reference images. Furthermore, Re-Align introduces an effective RL training scheme that leverages a surrogate reward to measure the alignment between structured reasoning text and the generated image, thereby improving the model's overall performance on ICGE tasks. Extensive experiments verify that Re-Align outperforms competitive methods of comparable model scale and resources on both in-context image generation and editing tasks.",
      "authors": [
        "Runze He",
        "Yiji Cheng",
        "Tiankai Hang",
        "Zhimin Li",
        "Yu Xu",
        "Zijin Yin",
        "Shiyi Zhang",
        "Wenxun Dai",
        "Penghui Du",
        "Ao Ma",
        "Chunyu Wang",
        "Qinglin Lu",
        "Jizhong Han",
        "Jiao Dai"
      ],
      "published": "2026-01-08T17:13:00Z",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "abs_url": "https://arxiv.org/abs/2601.05124v1",
      "pdf_url": "https://arxiv.org/pdf/2601.05124v1.pdf"
    },
    {
      "id": "2601.05116v1",
      "title": "From Rays to Projections: Better Inputs for Feed-Forward View Synthesis",
      "abstract": "Feed-forward view synthesis models predict a novel view in a single pass with minimal 3D inductive bias. Existing works encode cameras as Plücker ray maps, which tie predictions to the arbitrary world coordinate gauge and make them sensitive to small camera transformations, thereby undermining geometric consistency. In this paper, we ask what inputs best condition a model for robust and consistent view synthesis. We propose projective conditioning, which replaces raw camera parameters with a target-view projective cue that provides a stable 2D input. This reframes the task from a brittle geometric regression problem in ray space to a well-conditioned target-view image-to-image translation problem. Additionally, we introduce a masked autoencoding pretraining strategy tailored to this cue, enabling the use of large-scale uncalibrated data for pretraining. Our method shows improved fidelity and stronger cross-view consistency compared to ray-conditioned baselines on our view-consistency benchmark. It also achieves state-of-the-art quality on standard novel view synthesis benchmarks.",
      "authors": [
        "Zirui Wu",
        "Zeren Jiang",
        "Martin R. Oswald",
        "Jie Song"
      ],
      "published": "2026-01-08T17:03:44Z",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "abs_url": "https://arxiv.org/abs/2601.05116v1",
      "pdf_url": "https://arxiv.org/pdf/2601.05116v1.pdf"
    },
    {
      "id": "2601.05111v1",
      "title": "Agent-as-a-Judge",
      "abstract": "LLM-as-a-Judge has revolutionized AI evaluation by leveraging large language models for scalable assessments. However, as evaluands become increasingly complex, specialized, and multi-step, the reliability of LLM-as-a-Judge has become constrained by inherent biases, shallow single-pass reasoning, and the inability to verify assessments against real-world observations. This has catalyzed the transition to Agent-as-a-Judge, where agentic judges employ planning, tool-augmented verification, multi-agent collaboration, and persistent memory to enable more robust, verifiable, and nuanced evaluations. Despite the rapid proliferation of agentic evaluation systems, the field lacks a unified framework to navigate this shifting landscape. To bridge this gap, we present the first comprehensive survey tracing this evolution. Specifically, we identify key dimensions that characterize this paradigm shift and establish a developmental taxonomy. We organize core methodologies and survey applications across general and professional domains. Furthermore, we analyze frontier challenges and identify promising research directions, ultimately providing a clear roadmap for the next generation of agentic evaluation.",
      "authors": [
        "Runyang You",
        "Hongru Cai",
        "Caiqi Zhang",
        "Qiancheng Xu",
        "Meng Liu",
        "Tiezheng Yu",
        "Yongqi Li",
        "Wenjie Li"
      ],
      "published": "2026-01-08T16:58:10Z",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "abs_url": "https://arxiv.org/abs/2601.05111v1",
      "pdf_url": "https://arxiv.org/pdf/2601.05111v1.pdf"
    },
    {
      "id": "2601.05106v1",
      "title": "Token-Level LLM Collaboration via FusionRoute",
      "abstract": "Large language models (LLMs) exhibit strengths across diverse domains. However, achieving strong performance across these domains with a single general-purpose model typically requires scaling to sizes that are prohibitively expensive to train and deploy. On the other hand, while smaller domain-specialized models are much more efficient, they struggle to generalize beyond their training distributions. To address this dilemma, we propose FusionRoute, a robust and effective token-level multi-LLM collaboration framework in which a lightweight router simultaneously (i) selects the most suitable expert at each decoding step and (ii) contributes a complementary logit that refines or corrects the selected expert's next-token distribution via logit addition. Unlike existing token-level collaboration methods that rely solely on fixed expert outputs, we provide a theoretical analysis showing that pure expert-only routing is fundamentally limited: unless strong global coverage assumptions hold, it cannot in general realize the optimal decoding policy. By augmenting expert selection with a trainable complementary generator, FusionRoute expands the effective policy class and enables recovery of optimal value functions under mild conditions. Empirically, across both Llama-3 and Gemma-2 families and diverse benchmarks spanning mathematical reasoning, code generation, and instruction following, FusionRoute outperforms both sequence- and token-level collaboration, model merging, and direct fine-tuning, while remaining competitive with domain experts on their respective tasks.",
      "authors": [
        "Nuoya Xiong",
        "Yuhang Zhou",
        "Hanqing Zeng",
        "Zhaorun Chen",
        "Furong Huang",
        "Shuchao Bi",
        "Lizhu Zhang",
        "Zhuokai Zhao"
      ],
      "published": "2026-01-08T16:53:16Z",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "abs_url": "https://arxiv.org/abs/2601.05106v1",
      "pdf_url": "https://arxiv.org/pdf/2601.05106v1.pdf"
    },
    {
      "id": "2601.05105v1",
      "title": "UniLiPs: Unified LiDAR Pseudo-Labeling with Geometry-Grounded Dynamic Scene Decomposition",
      "abstract": "Unlabeled LiDAR logs, in autonomous driving applications, are inherently a gold mine of dense 3D geometry hiding in plain sight - yet they are almost useless without human labels, highlighting a dominant cost barrier for autonomous-perception research. In this work we tackle this bottleneck by leveraging temporal-geometric consistency across LiDAR sweeps to lift and fuse cues from text and 2D vision foundation models directly into 3D, without any manual input. We introduce an unsupervised multi-modal pseudo-labeling method relying on strong geometric priors learned from temporally accumulated LiDAR maps, alongside with a novel iterative update rule that enforces joint geometric-semantic consistency, and vice-versa detecting moving objects from inconsistencies. Our method simultaneously produces 3D semantic labels, 3D bounding boxes, and dense LiDAR scans, demonstrating robust generalization across three datasets. We experimentally validate that our method compares favorably to existing semantic segmentation and object detection pseudo-labeling methods, which often require additional manual supervision. We confirm that even a small fraction of our geometrically consistent, densified LiDAR improves depth prediction by 51.5% and 22.0% MAE in the 80-150 and 150-250 meters range, respectively.",
      "authors": [
        "Filippo Ghilotti",
        "Samuel Brucker",
        "Nahku Saidy",
        "Matteo Matteucci",
        "Mario Bijelic",
        "Felix Heide"
      ],
      "published": "2026-01-08T16:52:28Z",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "abs_url": "https://arxiv.org/abs/2601.05105v1",
      "pdf_url": "https://arxiv.org/pdf/2601.05105v1.pdf"
    },
    {
      "id": "2601.05104v1",
      "title": "How Human is AI? Examining the Impact of Emotional Prompts on Artificial and Human and Responsiveness",
      "abstract": "This research examines how the emotional tone of human-AI interactions shapes ChatGPT and human behavior. In a between-subject experiment, we asked participants to express a specific emotion while working with ChatGPT (GPT-4.0) on two tasks, including writing a public response and addressing an ethical dilemma. We found that compared to interactions where participants maintained a neutral tone, ChatGPT showed greater improvement in its answers when participants praised ChatGPT for its responses. Expressing anger towards ChatGPT also led to a higher albeit smaller improvement relative to the neutral condition, whereas blaming ChatGPT did not improve its answers. When addressing an ethical dilemma, ChatGPT prioritized corporate interests less when participants expressed anger towards it, while blaming increases its emphasis on protecting the public interest. Additionally, we found that people used more negative, hostile, and disappointing expressions in human-human communication after interactions during which participants blamed rather than praised for their responses. Together, our findings demonstrate that the emotional tone people apply in human-AI interactions not only shape ChatGPT's outputs but also carry over into subsequent human-human communication.",
      "authors": [
        "Florence Bernays",
        "Marco Henriques Pereira",
        "Jochen Menges"
      ],
      "published": "2026-01-08T16:50:00Z",
      "categories": [
        "cs.CL",
        "econ.GN"
      ],
      "primary_category": "cs.CL",
      "abs_url": "https://arxiv.org/abs/2601.05104v1",
      "pdf_url": "https://arxiv.org/pdf/2601.05104v1.pdf"
    },
    {
      "id": "2601.05103v1",
      "title": "Semantically Orthogonal Framework for Citation Classification: Disentangling Intent and Content",
      "abstract": "Understanding the role of citations is essential for research assessment and citation-aware digital libraries. However, existing citation classification frameworks often conflate citation intent (why a work is cited) with cited content type (what part is cited), limiting their effectiveness in auto classification due to a dilemma between fine-grained type distinctions and practical classification reliability. We introduce SOFT, a Semantically Orthogonal Framework with Two dimensions that explicitly separates citation intent from cited content type, drawing inspiration from semantic role theory. We systematically re-annotate the ACL-ARC dataset using SOFT and release a cross-disciplinary test set sampled from ACT2. Evaluation with both zero-shot and fine-tuned Large Language Models demonstrates that SOFT enables higher agreement between human annotators and LLMs, and supports stronger classification performance and robust cross-domain generalization compared to ACL-ARC and SciCite annotation frameworks. These results confirm SOFT's value as a clear, reusable annotation standard, improving clarity, consistency, and generalizability for digital libraries and scholarly communication infrastructures. All code and data are publicly available on GitHub https://github.com/zhiyintan/SOFT.",
      "authors": [
        "Changxu Duan",
        "Zhiyin Tan"
      ],
      "published": "2026-01-08T16:48:36Z",
      "categories": [
        "cs.DL",
        "cs.CL"
      ],
      "primary_category": "cs.DL",
      "abs_url": "https://arxiv.org/abs/2601.05103v1",
      "pdf_url": "https://arxiv.org/pdf/2601.05103v1.pdf"
    },
    {
      "id": "2601.05099v1",
      "title": "Multi-Disciplinary Dataset Discovery from Citation-Verified Literature Contexts",
      "abstract": "Identifying suitable datasets for a research question remains challenging because existing dataset search engines rely heavily on metadata quality and keyword overlap, which often fail to capture the semantic intent of scientific investigation. We introduce a literature-driven framework that discovers datasets from citation contexts in scientific papers, enabling retrieval grounded in actual research use rather than metadata availability. Our approach combines large-scale citation-context extraction, schema-guided dataset recognition with Large Language Models, and provenance-preserving entity resolution. We evaluate the system on eight survey-derived computer science queries and find that it achieves substantially higher recall than Google Dataset Search and DataCite Commons, with normalized recall ranging from an average of 47.47% to a highest value of 81.82%. Beyond recovering gold-standard datasets, the method also surfaces additional datasets not documented in the surveys. Expert assessments across five top-level Fields of Science indicate that a substantial portion of the additional datasets are considered high utility, and some are regarded as novel for the specific topics chosen by the experts. These findings establish citation-context mining as an effective and generalizable paradigm for dataset discovery, particularly in settings where datasets lack sufficient or reliable metadata. To support reproducibility and future extensions, we release our code, evaluation datasets, and results on GitHub (https://github.com/Fireblossom/citation-context-dataset-discovery).",
      "authors": [
        "Zhiyin Tan",
        "Changxu Duan"
      ],
      "published": "2026-01-08T16:46:06Z",
      "categories": [
        "cs.DL",
        "cs.CL",
        "cs.IR"
      ],
      "primary_category": "cs.DL",
      "abs_url": "https://arxiv.org/abs/2601.05099v1",
      "pdf_url": "https://arxiv.org/pdf/2601.05099v1.pdf"
    },
    {
      "id": "2601.05091v1",
      "title": "Code-Mix Sentiment Analysis on Hinglish Tweets",
      "abstract": "The effectiveness of brand monitoring in India is increasingly challenged by the rise of Hinglish--a hybrid of Hindi and English--used widely in user-generated content on platforms like Twitter. Traditional Natural Language Processing (NLP) models, built for monolingual data, often fail to interpret the syntactic and semantic complexity of this code-mixed language, resulting in inaccurate sentiment analysis and misleading market insights. To address this gap, we propose a high-performance sentiment classification framework specifically designed for Hinglish tweets. Our approach fine-tunes mBERT (Multilingual BERT), leveraging its multilingual capabilities to better understand the linguistic diversity of Indian social media. A key component of our methodology is the use of subword tokenization, which enables the model to effectively manage spelling variations, slang, and out-of-vocabulary terms common in Romanized Hinglish. This research delivers a production-ready AI solution for brand sentiment tracking and establishes a strong benchmark for multilingual NLP in low-resource, code-mixed environments.",
      "authors": [
        "Aashi Garg",
        "Aneshya Das",
        "Arshi Arya",
        "Anushka Goyal",
        " Aditi"
      ],
      "published": "2026-01-08T16:39:26Z",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "abs_url": "https://arxiv.org/abs/2601.05091v1",
      "pdf_url": "https://arxiv.org/pdf/2601.05091v1.pdf"
    },
    {
      "id": "2601.05083v1",
      "title": "Driving on Registers",
      "abstract": "We present DrivoR, a simple and efficient transformer-based architecture for end-to-end autonomous driving. Our approach builds on pretrained Vision Transformers (ViTs) and introduces camera-aware register tokens that compress multi-camera features into a compact scene representation, significantly reducing downstream computation without sacrificing accuracy. These tokens drive two lightweight transformer decoders that generate and then score candidate trajectories. The scoring decoder learns to mimic an oracle and predicts interpretable sub-scores representing aspects such as safety, comfort, and efficiency, enabling behavior-conditioned driving at inference. Despite its minimal design, DrivoR outperforms or matches strong contemporary baselines across NAVSIM-v1, NAVSIM-v2, and the photorealistic closed-loop HUGSIM benchmark. Our results show that a pure-transformer architecture, combined with targeted token compression, is sufficient for accurate, efficient, and adaptive end-to-end driving. Code and checkpoints will be made available via the project page.",
      "authors": [
        "Ellington Kirby",
        "Alexandre Boulch",
        "Yihong Xu",
        "Yuan Yin",
        "Gilles Puy",
        "Éloi Zablocki",
        "Andrei Bursuc",
        "Spyros Gidaris",
        "Renaud Marlet",
        "Florent Bartoccioni",
        "Anh-Quan Cao",
        "Nermin Samet",
        "Tuan-Hung VU",
        "Matthieu Cord"
      ],
      "published": "2026-01-08T16:28:24Z",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.RO"
      ],
      "primary_category": "cs.CV",
      "abs_url": "https://arxiv.org/abs/2601.05083v1",
      "pdf_url": "https://arxiv.org/pdf/2601.05083v1.pdf"
    },
    {
      "id": "2601.05075v1",
      "title": "SemPA: Improving Sentence Embeddings of Large Language Models through Semantic Preference Alignment",
      "abstract": "Traditional sentence embedding methods employ token-level contrastive learning on non-generative pre-trained models. Recently, there have emerged embedding methods based on generative large language models (LLMs). These methods either rely on fixed prompt templates or involve modifications to the model architecture. The former lacks further optimization of the model and results in limited performance, while the latter alters the internal computational mechanisms of the model, thereby compromising its generative capabilities. We propose SemPA, a novel approach that boosts the sentence representations while preserving the generative ability of LLMs via semantic preference alignment. We leverage sentence-level Direct Preference Optimization (DPO) to efficiently optimize LLMs on a paraphrase generation task, where the model learns to discriminate semantically equivalent sentences while preserving inherent generative capacity. Theoretically, we establish a formal connection between DPO and contrastive learning under the Plackett-Luce model framework. Empirically, experimental results on both semantic textual similarity tasks and various benchmarks for LLMs show that SemPA achieves better semantic representations without sacrificing the inherent generation capability of LLMs.",
      "authors": [
        "Ziyang Chen",
        "Zhenxuan Huang",
        "Yile Wang",
        "Weiqin Wang",
        "Lu Yin",
        "Hui Huang"
      ],
      "published": "2026-01-08T16:19:24Z",
      "categories": [
        "cs.CL"
      ],
      "primary_category": "cs.CL",
      "abs_url": "https://arxiv.org/abs/2601.05075v1",
      "pdf_url": "https://arxiv.org/pdf/2601.05075v1.pdf"
    },
    {
      "id": "2601.05074v1",
      "title": "Compensation Effect Amplification Control (CEAC): A movement-based approach for coordinated position and velocity control of the elbow of upper-limb prostheses",
      "abstract": "Despite advances in upper-limb (UL) prosthetic design, achieving intuitive control of intermediate joints - such as the wrist and elbow - remains challenging, particularly for continuous and velocity-modulated movements. We introduce a novel movement-based control paradigm entitled Compensation Effect Amplification Control (CEAC) that leverages users' trunk flexion and extension as input for controlling prosthetic elbow velocity. Considering that the trunk can be both a functional and compensatory joint when performing upper-limb actions, CEAC amplifies the natural coupling between trunk and prosthesis while introducing a controlled delay that allows users to modulate both the position and velocity of the prosthetic joint. We evaluated CEAC in a generic drawing task performed by twelve able-bodied participants using a supernumerary prosthesis with an active elbow. Additionally a multiple-target-reaching task was performed by a subset of ten participants. Results demonstrate task performances comparable to those obtained with natural arm movements, even when gesture velocity or drawing size were varied, while maintaining ergonomic trunk postures. Analysis revealed that CEAC effectively restores joint coordinated action, distributes movement effort between trunk and elbow, enabling intuitive trajectory control without requiring extreme compensatory movements. Overall, CEAC offers a promising control strategy for intermediate joints of UL prostheses, particularly in tasks requiring continuous and precise coordination.",
      "authors": [
        "Julian Kulozik",
        "Nathanaël Jarrassé"
      ],
      "published": "2026-01-08T16:18:10Z",
      "categories": [
        "cs.RO"
      ],
      "primary_category": "cs.RO",
      "abs_url": "https://arxiv.org/abs/2601.05074v1",
      "pdf_url": "https://arxiv.org/pdf/2601.05074v1.pdf"
    },
    {
      "id": "2601.05072v1",
      "title": "DAVOS: An Autonomous Vehicle Operating System in the Vehicle Computing Era",
      "abstract": "Vehicle computing represents a fundamental shift in how autonomous vehicles are designed and deployed, transforming them from isolated transportation systems into mobile computing platforms that support both safety-critical, real-time driving and data-centric services. In this setting, vehicles simultaneously support real-time driving pipelines and a growing set of data-driven applications, placing increased responsibility on the vehicle operating system to coordinate computation, data movement, storage, and access. These demands highlight recurring system considerations related to predictable execution, data and execution protection, efficient handling of high-rate sensor data, and long-term system evolvability, commonly summarized as Safety, Security, Efficiency, and Extensibility (SSEE). Existing vehicle operating systems and runtimes address these concerns in isolation, resulting in fragmented software stacks that limit coordination between autonomy workloads and vehicle data services. This paper presents DAVOS, the Delaware Autonomous Vehicle Operating System, a unified vehicle operating system architecture designed for the vehicle computing context. DAVOS provides a cohesive operating system foundation that supports both real-time autonomy and extensible vehicle computing within a single system framework.",
      "authors": [
        "Yuxin Wang",
        "Yuankai He",
        "Boyang Tian",
        "Lichen Xian",
        "Weisong Shi"
      ],
      "published": "2026-01-08T16:17:48Z",
      "categories": [
        "cs.OS",
        "cs.RO"
      ],
      "primary_category": "cs.OS",
      "abs_url": "https://arxiv.org/abs/2601.05072v1",
      "pdf_url": "https://arxiv.org/pdf/2601.05072v1.pdf"
    },
    {
      "id": "2601.05063v1",
      "title": "Quantitative mapping from conventional MRI using self-supervised physics-guided deep learning: applications to a large-scale, clinically heterogeneous dataset",
      "abstract": "Magnetic resonance imaging (MRI) is a cornerstone of clinical neuroimaging, yet conventional MRIs provide qualitative information heavily dependent on scanner hardware and acquisition settings. While quantitative MRI (qMRI) offers intrinsic tissue parameters, the requirement for specialized acquisition protocols and reconstruction algorithms restricts its availability and impedes large-scale biomarker research. This study presents a self-supervised physics-guided deep learning framework to infer quantitative T1, T2, and proton-density (PD) maps directly from widely available clinical conventional T1-weighted, T2-weighted, and FLAIR MRIs. The framework was trained and evaluated on a large-scale, clinically heterogeneous dataset comprising 4,121 scan sessions acquired at our institution over six years on four different 3 T MRI scanner systems, capturing real-world clinical variability. The framework integrates Bloch-based signal models directly into the training objective. Across more than 600 test sessions, the generated maps exhibited white matter and gray matter values consistent with literature ranges. Additionally, the generated maps showed invariance to scanner hardware and acquisition protocol groups, with inter-group coefficients of variation $\\leq$ 1.1%. Subject-specific analyses demonstrated excellent voxel-wise reproducibility across scanner systems and sequence parameters, with Pearson $r$ and concordance correlation coefficients exceeding 0.82 for T1 and T2. Mean relative voxel-wise differences were low across all quantitative parameters, especially for T2 ($<$ 6%). These results indicate that the proposed framework can robustly transform diverse clinical conventional MRI data into quantitative maps, potentially paving the way for large-scale quantitative biomarker research.",
      "authors": [
        "Jelmer van Lune",
        "Stefano Mandija",
        "Oscar van der Heide",
        "Matteo Maspero",
        "Martin B. Schilder",
        "Jan Willem Dankbaar",
        "Cornelis A. T. van den Berg",
        "Alessandro Sbrizzi"
      ],
      "published": "2026-01-08T16:08:58Z",
      "categories": [
        "physics.med-ph",
        "cs.CV",
        "cs.LG"
      ],
      "primary_category": "physics.med-ph",
      "abs_url": "https://arxiv.org/abs/2601.05063v1",
      "pdf_url": "https://arxiv.org/pdf/2601.05063v1.pdf"
    },
    {
      "id": "2601.05062v1",
      "title": "Compositional Steering of Large Language Models with Steering Tokens",
      "abstract": "Deploying LLMs in real-world applications requires controllable output that satisfies multiple desiderata at the same time. While existing work extensively addresses LLM steering for a single behavior, \\textit{compositional steering} -- i.e., steering LLMs simultaneously towards multiple behaviors -- remains an underexplored problem. In this work, we propose \\emph{compositional steering tokens} for multi-behavior steering. We first embed individual behaviors, expressed as natural language instructions, into dedicated tokens via self-distillation. Contrary to most prior work, which operates in the activation space, our behavior steers live in the space of input tokens, enabling more effective zero-shot composition. We then train a dedicated \\textit{composition token} on pairs of behaviors and show that it successfully captures the notion of composition: it generalizes well to \\textit{unseen} compositions, including those with unseen behaviors as well as those with an unseen \\textit{number} of behaviors. Our experiments across different LLM architectures show that steering tokens lead to superior multi-behavior control compared to competing approaches (instructions, activation steering, and LoRA merging). Moreover, we show that steering tokens complement natural language instructions, with their combination resulting in further gains.",
      "authors": [
        "Gorjan Radevski",
        "Kiril Gashteovski",
        "Giwon Hong",
        "Carolin Lawrence",
        "Goran Glavaš"
      ],
      "published": "2026-01-08T16:08:44Z",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "abs_url": "https://arxiv.org/abs/2601.05062v1",
      "pdf_url": "https://arxiv.org/pdf/2601.05062v1.pdf"
    },
    {
      "id": "2601.05059v1",
      "title": "From Understanding to Engagement: Personalized pharmacy Video Clips via Vision Language Models (VLMs)",
      "abstract": "Vision Language Models (VLMs) are poised to revolutionize the digital transformation of pharmacyceutical industry by enabling intelligent, scalable, and automated multi-modality content processing. Traditional manual annotation of heterogeneous data modalities (text, images, video, audio, and web links), is prone to inconsistencies, quality degradation, and inefficiencies in content utilization. The sheer volume of long video and audio data further exacerbates these challenges, (e.g. long clinical trial interviews and educational seminars).\n  Here, we introduce a domain adapted Video to Video Clip Generation framework that integrates Audio Language Models (ALMs) and Vision Language Models (VLMs) to produce highlight clips. Our contributions are threefold: (i) a reproducible Cut & Merge algorithm with fade in/out and timestamp normalization, ensuring smooth transitions and audio/visual alignment; (ii) a personalization mechanism based on role definition and prompt injection for tailored outputs (marketing, training, regulatory); (iii) a cost efficient e2e pipeline strategy balancing ALM/VLM enhanced processing. Evaluations on Video MME benchmark (900) and our proprietary dataset of 16,159 pharmacy videos across 14 disease areas demonstrate 3 to 4 times speedup, 4 times cost reduction, and competitive clip quality. Beyond efficiency gains, we also report our methods improved clip coherence scores (0.348) and informativeness scores (0.721) over state of the art VLM baselines (e.g., Gemini 2.5 Pro), highlighting the potential of transparent, custom extractive, and compliance supporting video summarization for life sciences.",
      "authors": [
        "Suyash Mishra",
        "Qiang Li",
        "Srikanth Patil",
        "Anubhav Girdhar"
      ],
      "published": "2026-01-08T16:02:56Z",
      "categories": [
        "cs.CV",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "abs_url": "https://arxiv.org/abs/2601.05059v1",
      "pdf_url": "https://arxiv.org/pdf/2601.05059v1.pdf"
    },
    {
      "id": "2601.05053v1",
      "title": "Reinforced Efficient Reasoning via Semantically Diverse Exploration",
      "abstract": "Reinforcement learning with verifiable rewards (RLVR) has proven effective in enhancing the reasoning of large language models (LLMs). Monte Carlo Tree Search (MCTS)-based extensions improve upon vanilla RLVR (e.g., GRPO) by providing tree-based reasoning rollouts that enable fine-grained and segment-level credit assignment. However, existing methods still suffer from limited exploration diversity and inefficient reasoning. To address the above challenges, we propose reinforced efficient reasoning via semantically diverse explorations, i.e., ROSE, for LLMs. To encourage more diverse reasoning exploration, our method incorporates a semantic-entropy-based branching strategy and an $\\varepsilon$-exploration mechanism. The former operates on already sampled reasoning rollouts to capture semantic uncertainty and select branching points with high semantic divergence to generate new successive reasoning paths, whereas the latter stochastically initiates reasoning rollouts from the root, preventing the search process from becoming overly local. To improve efficiency, we design a length-aware segment-level advantage estimator that rewards concise and correct reasoning while penalizing unnecessarily long reasoning chains. Extensive experiments on various mathematical reasoning benchmarks with Qwen and Llama models validate the effectiveness and efficiency of ROSE. Codes are available at https://github.com/ZiqiZhao1/ROSE-rl.",
      "authors": [
        "Ziqi Zhao",
        "Zhaochun Ren",
        "Jiahong Zou",
        "Liu Yang",
        "Zhiwei Xu",
        "Xuri Ge",
        "Zhumin Chen",
        "Xinyu Ma",
        "Daiting Shi",
        "Shuaiqiang Wang",
        "Dawei Yin",
        "Xin Xin"
      ],
      "published": "2026-01-08T15:56:44Z",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.AI",
      "abs_url": "https://arxiv.org/abs/2601.05053v1",
      "pdf_url": "https://arxiv.org/pdf/2601.05053v1.pdf"
    },
    {
      "id": "2601.05051v1",
      "title": "Publishing FAIR and Machine-actionable Reviews in Materials Science: The Case for Symbolic Knowledge in Neuro-symbolic Artificial Intelligence",
      "abstract": "Scientific reviews are central to knowledge integration in materials science, yet their key insights remain locked in narrative text and static PDF tables, limiting reuse by humans and machines alike. This article presents a case study in atomic layer deposition and etching (ALD/E) where we publish review tables as FAIR, machine-actionable comparisons in the Open Research Knowledge Graph (ORKG), turning them into structured, queryable knowledge. Building on this, we contrast symbolic querying over ORKG with large language model-based querying, and argue that a curated symbolic layer should remain the backbone of reliable neurosymbolic AI in materials science, with LLMs serving as complementary, symbolically grounded interfaces rather than standalone sources of truth.",
      "authors": [
        "Jennifer D'Souza",
        "Soren Auer",
        "Eleni Poupaki",
        "Alex Watkins",
        "Anjana Devi",
        "Riikka L. Puurunen",
        "Bora Karasulu",
        "Adrie Mackus",
        "Erwin Kessels"
      ],
      "published": "2026-01-08T15:56:17Z",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.DL",
        "cs.IT"
      ],
      "primary_category": "cs.AI",
      "abs_url": "https://arxiv.org/abs/2601.05051v1",
      "pdf_url": "https://arxiv.org/pdf/2601.05051v1.pdf"
    },
    {
      "id": "2601.05038v1",
      "title": "ArcAligner: Adaptive Recursive Aligner for Compressed Context Embeddings in RAG",
      "abstract": "Retrieval-Augmented Generation (RAG) helps LLMs stay accurate, but feeding long documents into a prompt makes the model slow and expensive. This has motivated context compression, ranging from token pruning and summarization to embedding-based compression. While researchers have tried ''compressing'' these documents into smaller summaries or mathematical embeddings, there is a catch: the more you compress the data, the more the LLM struggles to understand it. To address this challenge, we propose ArcAligner (Adaptive recursive context *Aligner*), a lightweight module integrated into the language model layers to help the model better utilize highly compressed context representations for downstream generation. It uses an adaptive ''gating'' system that only adds extra processing power when the information is complex, keeping the system fast. Across knowledge-intensive QA benchmarks, ArcAligner consistently beats compression baselines at comparable compression rates, especially on multi-hop and long-tail settings. The source code is publicly available.",
      "authors": [
        "Jianbo Li",
        "Yi Jiang",
        "Sendong Zhao",
        "Bairui Hu",
        "Haochun Wang",
        "Bing Qin"
      ],
      "published": "2026-01-08T15:44:52Z",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "abs_url": "https://arxiv.org/abs/2601.05038v1",
      "pdf_url": "https://arxiv.org/pdf/2601.05038v1.pdf"
    },
    {
      "id": "2601.05035v1",
      "title": "Patch-based Representation and Learning for Efficient Deformation Modeling",
      "abstract": "In this paper, we present a patch-based representation of surfaces, PolyFit, which is obtained by fitting jet functions locally on surface patches. Such a representation can be learned efficiently in a supervised fashion from both analytic functions and real data. Once learned, it can be generalized to various types of surfaces. Using PolyFit, the surfaces can be efficiently deformed by updating a compact set of jet coefficients rather than optimizing per-vertex degrees of freedom for many downstream tasks in computer vision and graphics. We demonstrate the capabilities of our proposed methodologies with two applications: 1) Shape-from-template (SfT): where the goal is to deform the input 3D template of an object as seen in image/video. Using PolyFit, we adopt test-time optimization that delivers competitive accuracy while being markedly faster than offline physics-based solvers, and outperforms recent physics-guided neural simulators in accuracy at modest additional runtime. 2) Garment draping. We train a self-supervised, mesh- and garment-agnostic model that generalizes across resolutions and garment types, delivering up to an order-of-magnitude faster inference than strong baselines.",
      "authors": [
        "Ruochen Chen",
        "Thuy Tran",
        "Shaifali Parashar"
      ],
      "published": "2026-01-08T15:43:57Z",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "abs_url": "https://arxiv.org/abs/2601.05035v1",
      "pdf_url": "https://arxiv.org/pdf/2601.05035v1.pdf"
    },
    {
      "id": "2601.05020v1",
      "title": "Scalable neural pushbroom architectures for real-time denoising of hyperspectral images onboard satellites",
      "abstract": "The next generation of Earth observation satellites will seek to deploy intelligent models directly onboard the payload in order to minimize the latency incurred by the transmission and processing chain of the ground segment, for time-critical applications. Designing neural architectures for onboard execution, particularly for satellite-based hyperspectral imagers, poses novel challenges due to the unique constraints of this environment and imaging system that are largely unexplored by the traditional computer vision literature. In this paper, we show that this setting requires addressing three competing objectives, namely high-quality inference with low complexity, dynamic power scalability and fault tolerance. We focus on the problem of hyperspectral image denoising, which is a critical task to enable effective downstream inference, and highlights the constraints of the onboard processing scenario. We propose a neural network design that addresses the three aforementioned objectives with several novel contributions. In particular, we propose a mixture of denoisers that can be resilient to radiation-induced faults as well as allowing for time-varying power scaling. Moreover, each denoiser employs an innovative architecture where an image is processed line-by-line in a causal way, with a memory of past lines, in order to match the acquisition process of pushbroom hyperspectral sensors and greatly limit memory requirements. We show that the proposed architecture can run in real-time, i.e., process one line in the time it takes to acquire the next one, on low-power hardware and provide competitive denoising quality with respect to significantly more complex state-of-the-art models. We also show that the power scalability and fault tolerance objectives provide a design space with multiple tradeoffs between those properties and denoising quality.",
      "authors": [
        "Ziyao Yi",
        "Davide Piccinini",
        "Diego Valsesia",
        "Tiziano Bianchi",
        "Enrico Magli"
      ],
      "published": "2026-01-08T15:28:39Z",
      "categories": [
        "eess.IV",
        "cs.CV"
      ],
      "primary_category": "eess.IV",
      "abs_url": "https://arxiv.org/abs/2601.05020v1",
      "pdf_url": "https://arxiv.org/pdf/2601.05020v1.pdf"
    },
    {
      "id": "2601.05019v1",
      "title": "Hán Dān Xué Bù (Mimicry) or Qīng Chū Yú Lán (Mastery)? A Cognitive Perspective on Reasoning Distillation in Large Language Models",
      "abstract": "Recent Large Reasoning Models trained via reinforcement learning exhibit a \"natural\" alignment with human cognitive costs. However, we show that the prevailing paradigm of reasoning distillation -- training student models to mimic these traces via Supervised Fine-Tuning (SFT) -- fails to transmit this cognitive structure. Testing the \"Hán Dān Xué Bù\" (Superficial Mimicry) hypothesis across 14 models, we find that distillation induces a \"Functional Alignment Collapse\": while teacher models mirror human difficulty scaling ($\\bar{r}=0.64$), distilled students significantly degrade this alignment ($\\bar{r}=0.34$), often underperforming their own pre-distillation baselines (\"Negative Transfer\"). Our analysis suggests that SFT induces a \"Cargo Cult\" effect, where students ritualistically replicate the linguistic form of reasoning (verbosity) without internalizing the teacher's dynamic resource allocation policy. Consequently, reasoning distillation decouples computational cost from cognitive demand, revealing that human-like cognition is an emergent property of active reinforcement, not passive imitation.",
      "authors": [
        "Yueqing Hu",
        "Xinyang Peng",
        "Shuting Peng",
        "Hanqi Wang",
        "Tianhong Wang"
      ],
      "published": "2026-01-08T15:27:03Z",
      "categories": [
        "cs.CL",
        "cs.AI",
        "q-bio.NC"
      ],
      "primary_category": "cs.CL",
      "abs_url": "https://arxiv.org/abs/2601.05019v1",
      "pdf_url": "https://arxiv.org/pdf/2601.05019v1.pdf"
    },
    {
      "id": "2601.05014v1",
      "title": "The RoboSense Challenge: Sense Anything, Navigate Anywhere, Adapt Across Platforms",
      "abstract": "Autonomous systems are increasingly deployed in open and dynamic environments -- from city streets to aerial and indoor spaces -- where perception models must remain reliable under sensor noise, environmental variation, and platform shifts. However, even state-of-the-art methods often degrade under unseen conditions, highlighting the need for robust and generalizable robot sensing. The RoboSense 2025 Challenge is designed to advance robustness and adaptability in robot perception across diverse sensing scenarios. It unifies five complementary research tracks spanning language-grounded decision making, socially compliant navigation, sensor configuration generalization, cross-view and cross-modal correspondence, and cross-platform 3D perception. Together, these tasks form a comprehensive benchmark for evaluating real-world sensing reliability under domain shifts, sensor failures, and platform discrepancies. RoboSense 2025 provides standardized datasets, baseline models, and unified evaluation protocols, enabling large-scale and reproducible comparison of robust perception methods. The challenge attracted 143 teams from 85 institutions across 16 countries, reflecting broad community engagement. By consolidating insights from 23 winning solutions, this report highlights emerging methodological trends, shared design principles, and open challenges across all tracks, marking a step toward building robots that can sense reliably, act robustly, and adapt across platforms in real-world environments.",
      "authors": [
        "Lingdong Kong",
        "Shaoyuan Xie",
        "Zeying Gong",
        "Ye Li",
        "Meng Chu",
        "Ao Liang",
        "Yuhao Dong",
        "Tianshuai Hu",
        "Ronghe Qiu",
        "Rong Li",
        "Hanjiang Hu",
        "Dongyue Lu",
        "Wei Yin",
        "Wenhao Ding",
        "Linfeng Li",
        "Hang Song",
        "Wenwei Zhang",
        "Yuexin Ma",
        "Junwei Liang",
        "Zhedong Zheng",
        "Lai Xing Ng",
        "Benoit R. Cottereau",
        "Wei Tsang Ooi",
        "Ziwei Liu",
        "Zhanpeng Zhang",
        "Weichao Qiu",
        "Wei Zhang",
        "Ji Ao",
        "Jiangpeng Zheng",
        "Siyu Wang",
        "Guang Yang",
        "Zihao Zhang",
        "Yu Zhong",
        "Enzhu Gao",
        "Xinhan Zheng",
        "Xueting Wang",
        "Shouming Li",
        "Yunkai Gao",
        "Siming Lan",
        "Mingfei Han",
        "Xing Hu",
        "Dusan Malic",
        "Christian Fruhwirth-Reisinger",
        "Alexander Prutsch",
        "Wei Lin",
        "Samuel Schulter",
        "Horst Possegger",
        "Linfeng Li",
        "Jian Zhao",
        "Zepeng Yang",
        "Yuhang Song",
        "Bojun Lin",
        "Tianle Zhang",
        "Yuchen Yuan",
        "Chi Zhang",
        "Xuelong Li",
        "Youngseok Kim",
        "Sihwan Hwang",
        "Hyeonjun Jeong",
        "Aodi Wu",
        "Xubo Luo",
        "Erjia Xiao",
        "Lingfeng Zhang",
        "Yingbo Tang",
        "Hao Cheng",
        "Renjing Xu",
        "Wenbo Ding",
        "Lei Zhou",
        "Long Chen",
        "Hangjun Ye",
        "Xiaoshuai Hao",
        "Shuangzhi Li",
        "Junlong Shen",
        "Xingyu Li",
        "Hao Ruan",
        "Jinliang Lin",
        "Zhiming Luo",
        "Yu Zang",
        "Cheng Wang",
        "Hanshi Wang",
        "Xijie Gong",
        "Yixiang Yang",
        "Qianli Ma",
        "Zhipeng Zhang",
        "Wenxiang Shi",
        "Jingmeng Zhou",
        "Weijun Zeng",
        "Kexin Xu",
        "Yuchen Zhang",
        "Haoxiang Fu",
        "Ruibin Hu",
        "Yanbiao Ma",
        "Xiyan Feng",
        "Wenbo Zhang",
        "Lu Zhang",
        "Yunzhi Zhuge",
        "Huchuan Lu",
        "You He",
        "Seungjun Yu",
        "Junsung Park",
        "Youngsun Lim",
        "Hyunjung Shim",
        "Faduo Liang",
        "Zihang Wang",
        "Yiming Peng",
        "Guanyu Zong",
        "Xu Li",
        "Binghao Wang",
        "Hao Wei",
        "Yongxin Ma",
        "Yunke Shi",
        "Shuaipeng Liu",
        "Dong Kong",
        "Yongchun Lin",
        "Huitong Yang",
        "Liang Lei",
        "Haoang Li",
        "Xinliang Zhang",
        "Zhiyong Wang",
        "Xiaofeng Wang",
        "Yuxia Fu",
        "Yadan Luo",
        "Djamahl Etchegaray",
        "Yang Li",
        "Congfei Li",
        "Yuxiang Sun",
        "Wenkai Zhu",
        "Wang Xu",
        "Linru Li",
        "Longjie Liao",
        "Jun Yan",
        "Benwu Wang",
        "Xueliang Ren",
        "Xiaoyu Yue",
        "Jixian Zheng",
        "Jinfeng Wu",
        "Shurui Qin",
        "Wei Cong",
        "Yao He"
      ],
      "published": "2026-01-08T15:16:18Z",
      "categories": [
        "cs.RO"
      ],
      "primary_category": "cs.RO",
      "abs_url": "https://arxiv.org/abs/2601.05014v1",
      "pdf_url": "https://arxiv.org/pdf/2601.05014v1.pdf"
    },
    {
      "id": "2601.05004v1",
      "title": "Can Large Language Models Resolve Semantic Discrepancy in Self-Destructive Subcultures? Evidence from Jirai Kei",
      "abstract": "Self-destructive behaviors are linked to complex psychological states and can be challenging to diagnose. These behaviors may be even harder to identify within subcultural groups due to their unique expressions. As large language models (LLMs) are applied across various fields, some researchers have begun exploring their application for detecting self-destructive behaviors. Motivated by this, we investigate self-destructive behavior detection within subcultures using current LLM-based methods. However, these methods have two main challenges: (1) Knowledge Lag: Subcultural slang evolves rapidly, faster than LLMs' training cycles; and (2) Semantic Misalignment: it is challenging to grasp the specific and nuanced expressions unique to subcultures. To address these issues, we proposed Subcultural Alignment Solver (SAS), a multi-agent framework that incorporates automatic retrieval and subculture alignment, significantly enhancing the performance of LLMs in detecting self-destructive behavior. Our experimental results show that SAS outperforms the current advanced multi-agent framework OWL. Notably, it competes well with fine-tuned LLMs. We hope that SAS will advance the field of self-destructive behavior detection in subcultural contexts and serve as a valuable resource for future researchers.",
      "authors": [
        "Peng Wang",
        "Xilin Tao",
        "Siyi Yao",
        "Jiageng Wu",
        "Yuntao Zou",
        "Zhuotao Tian",
        "Libo Qin",
        "Dagang Li"
      ],
      "published": "2026-01-08T15:02:41Z",
      "categories": [
        "cs.CL"
      ],
      "primary_category": "cs.CL",
      "abs_url": "https://arxiv.org/abs/2601.05004v1",
      "pdf_url": "https://arxiv.org/pdf/2601.05004v1.pdf"
    },
    {
      "id": "2601.05002v1",
      "title": "On the Hidden Objective Biases of Group-based Reinforcement Learning",
      "abstract": "Group-based reinforcement learning methods, like Group Relative Policy Optimization (GRPO), are widely used nowadays to post-train large language models. Despite their empirical success, they exhibit structural mismatches between reward optimization and the underlying training objective. In this paper, we present a theoretical analysis of GRPO style methods by studying them within a unified surrogate formulation. This perspective reveals recurring properties that affect all the methods under analysis: (i) non-uniform group weighting induces systematic gradient biases on shared prefix tokens; (ii) interactions with the AdamW optimizer make training dynamics largely insensitive to reward scaling; and (iii) optimizer momentum can push policy updates beyond the intended clipping region under repeated optimization steps. We believe that these findings highlight fundamental limitations of current approaches and provide principled guidance for the design of future formulations.",
      "authors": [
        "Aleksandar Fontana",
        "Marco Simoni",
        "Giulio Rossolini",
        "Andrea Saracino",
        "Paolo Mori"
      ],
      "published": "2026-01-08T15:00:35Z",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.LG",
      "abs_url": "https://arxiv.org/abs/2601.05002v1",
      "pdf_url": "https://arxiv.org/pdf/2601.05002v1.pdf"
    },
    {
      "id": "2601.04992v2",
      "title": "Learning from Mistakes: Negative Reasoning Samples Enhance Out-of-Domain Generalization",
      "abstract": "Supervised fine-tuning (SFT) on chain-of-thought (CoT) trajectories demonstrations is a common approach for enabling reasoning in large language models. Standard practices typically only retain trajectories with correct final answers (positives) while ignoring the rest (negatives). We argue that this paradigm discards substantial supervision and exacerbates overfitting, limiting out-of-domain (OOD) generalization. Specifically, we surprisingly find that incorporating negative trajectories into SFT yields substantial OOD generalization gains over positive-only training, as these trajectories often retain valid intermediate reasoning despite incorrect final answers. To understand this effect in depth, we systematically analyze data, training dynamics, and inference behavior, identifying 22 recurring patterns in negative chains that serve a dual role: they moderate loss descent to mitigate overfitting during training and boost policy entropy by 35.67% during inference to facilitate exploration. Motivated by these observations, we further propose Gain-based LOss Weighting (GLOW), an adaptive, sample-aware scheme that exploits such distinctive training dynamics by rescaling per-sample loss based on inter-epoch progress. Empirically, GLOW efficiently leverages unfiltered trajectories, yielding a 5.51% OOD gain over positive-only SFT on Qwen2.5-7B and boosting MMLU from 72.82% to 76.47% as an RL initialization.",
      "authors": [
        "Xueyun Tian",
        "Minghua Ma",
        "Bingbing Xu",
        "Nuoyan Lyu",
        "Wei Li",
        "Heng Dong",
        "Zheng Chu",
        "Yuanzhuo Wang",
        "Huawei Shen"
      ],
      "published": "2026-01-08T14:49:10Z",
      "categories": [
        "cs.CL"
      ],
      "primary_category": "cs.CL",
      "abs_url": "https://arxiv.org/abs/2601.04992v2",
      "pdf_url": "https://arxiv.org/pdf/2601.04992v2.pdf"
    },
    {
      "id": "2601.04991v1",
      "title": "Higher-Order Adversarial Patches for Real-Time Object Detectors",
      "abstract": "Higher-order adversarial attacks can directly be considered the result of a cat-and-mouse game -- an elaborate action involving constant pursuit, near captures, and repeated escapes. This idiom describes the enduring circular training of adversarial attack patterns and adversarial training the best. The following work investigates the impact of higher-order adversarial attacks on object detectors by successively training attack patterns and hardening object detectors with adversarial training. The YOLOv10 object detector is chosen as a representative, and adversarial patches are used in an evasion attack manner. Our results indicate that higher-order adversarial patches are not only affecting the object detector directly trained on but rather provide a stronger generalization capacity compared to lower-order adversarial patches. Moreover, the results highlight that solely adversarial training is not sufficient to harden an object detector efficiently against this kind of adversarial attack. Code: https://github.com/JensBayer/HigherOrder",
      "authors": [
        "Jens Bayer",
        "Stefan Becker",
        "David Münch",
        "Michael Arens",
        "Jürgen Beyerer"
      ],
      "published": "2026-01-08T14:48:50Z",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "abs_url": "https://arxiv.org/abs/2601.04991v1",
      "pdf_url": "https://arxiv.org/pdf/2601.04991v1.pdf"
    },
    {
      "id": "2601.04984v1",
      "title": "OceanSplat: Object-aware Gaussian Splatting with Trinocular View Consistency for Underwater Scene Reconstruction",
      "abstract": "We introduce OceanSplat, a novel 3D Gaussian Splatting-based approach for accurately representing 3D geometry in underwater scenes. To overcome multi-view inconsistencies caused by underwater optical degradation, our method enforces trinocular view consistency by rendering horizontally and vertically translated camera views relative to each input view and aligning them via inverse warping. Furthermore, these translated camera views are used to derive a synthetic epipolar depth prior through triangulation, which serves as a self-supervised depth regularizer. These geometric constraints facilitate the spatial optimization of 3D Gaussians and preserve scene structure in underwater environments. We also propose a depth-aware alpha adjustment that modulates the opacity of 3D Gaussians during early training based on their $z$-component and viewing direction, deterring the formation of medium-induced primitives. With our contributions, 3D Gaussians are disentangled from the scattering medium, enabling robust representation of object geometry and significantly reducing floating artifacts in reconstructed underwater scenes. Experiments on real-world underwater and simulated scenes demonstrate that OceanSplat substantially outperforms existing methods for both scene reconstruction and restoration in scattering media.",
      "authors": [
        "Minseong Kweon",
        "Jinsun Park"
      ],
      "published": "2026-01-08T14:38:39Z",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "abs_url": "https://arxiv.org/abs/2601.04984v1",
      "pdf_url": "https://arxiv.org/pdf/2601.04984v1.pdf"
    },
    {
      "id": "2601.04982v1",
      "title": "When to Act: Calibrated Confidence for Reliable Human Intention Prediction in Assistive Robotics",
      "abstract": "Assistive devices must determine both what a user intends to do and how reliable that prediction is before providing support. We introduce a safety-critical triggering framework based on calibrated probabilities for multimodal next-action prediction in Activities of Daily Living. Raw model confidence often fails to reflect true correctness, posing a safety risk. Post-hoc calibration aligns predicted confidence with empirical reliability and reduces miscalibration by about an order of magnitude without affecting accuracy. The calibrated confidence drives a simple ACT/HOLD rule that acts only when reliability is high and withholds assistance otherwise. This turns the confidence threshold into a quantitative safety parameter for assisted actions and enables verifiable behavior in an assistive control loop.",
      "authors": [
        "Johannes A. Gaus",
        "Winfried Ilg",
        "Daniel Haeufle"
      ],
      "published": "2026-01-08T14:35:17Z",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "abs_url": "https://arxiv.org/abs/2601.04982v1",
      "pdf_url": "https://arxiv.org/pdf/2601.04982v1.pdf"
    },
    {
      "id": "2601.04973v1",
      "title": "ConMax: Confidence-Maximizing Compression for Efficient Chain-of-Thought Reasoning",
      "abstract": "Recent breakthroughs in Large Reasoning Models (LRMs) have demonstrated that extensive Chain-of-Thought (CoT) generation is critical for enabling intricate cognitive behaviors, such as self-verification and backtracking, to solve complex tasks. However, this capability often leads to ``overthinking'', where models generate redundant reasoning paths that inflate computational costs without improving accuracy. While Supervised Fine-Tuning (SFT) on reasoning traces is a standard paradigm for the 'cold start' phase, applying existing compression techniques to these traces often compromises logical coherence or incurs prohibitive sampling costs. In this paper, we introduce ConMax (Confidence-Maximizing Compression), a novel reinforcement learning framework designed to automatically compress reasoning traces while preserving essential reasoning patterns. ConMax formulates compression as a reward-driven optimization problem, training a policy to prune redundancy by maximizing a weighted combination of answer confidence for predictive fidelity and thinking confidence for reasoning validity through a frozen auxiliary LRM. Extensive experiments across five reasoning datasets demonstrate that ConMax achieves a superior efficiency-performance trade-off. Specifically, it reduces inference length by 43% over strong baselines at the cost of a mere 0.7% dip in accuracy, proving its effectiveness in generating high-quality, efficient training data for LRMs.",
      "authors": [
        "Minda Hu",
        "Zexuan Qiu",
        "Zenan Xu",
        "Kun Li",
        "Bo Zhou",
        "Irwin King"
      ],
      "published": "2026-01-08T14:22:58Z",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.AI",
      "abs_url": "https://arxiv.org/abs/2601.04973v1",
      "pdf_url": "https://arxiv.org/pdf/2601.04973v1.pdf"
    },
    {
      "id": "2601.04968v1",
      "title": "SparseLaneSTP: Leveraging Spatio-Temporal Priors with Sparse Transformers for 3D Lane Detection",
      "abstract": "3D lane detection has emerged as a critical challenge in autonomous driving, encompassing identification and localization of lane markings and the 3D road surface. Conventional 3D methods detect lanes from dense birds-eye-viewed (BEV) features, though erroneous transformations often result in a poor feature representation misaligned with the true 3D road surface. While recent sparse lane detectors have surpassed dense BEV approaches, they completely disregard valuable lane-specific priors. Furthermore, existing methods fail to utilize historic lane observations, which yield the potential to resolve ambiguities in situations of poor visibility. To address these challenges, we present SparseLaneSTP, a novel method that integrates both geometric properties of the lane structure and temporal information into a sparse lane transformer. It introduces a new lane-specific spatio-temporal attention mechanism, a continuous lane representation tailored for sparse architectures as well as temporal regularization. Identifying weaknesses of existing 3D lane datasets, we also introduce a precise and consistent 3D lane dataset using a simple yet effective auto-labeling strategy. Our experimental section proves the benefits of our contributions and demonstrates state-of-the-art performance across all detection and error metrics on existing 3D lane detection benchmarks as well as on our novel dataset.",
      "authors": [
        "Maximilian Pittner",
        "Joel Janai",
        "Mario Faigle",
        "Alexandru Paul Condurache"
      ],
      "published": "2026-01-08T14:16:11Z",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "abs_url": "https://arxiv.org/abs/2601.04968v1",
      "pdf_url": "https://arxiv.org/pdf/2601.04968v1.pdf"
    },
    {
      "id": "2601.04963v1",
      "title": "Text as a Universal Interface for Transferable Personalization",
      "abstract": "We study the problem of personalization in large language models (LLMs). Prior work predominantly represents user preferences as implicit, model-specific vectors or parameters, yielding opaque ``black-box'' profiles that are difficult to interpret and transfer across models and tasks. In contrast, we advocate natural language as a universal, model- and task-agnostic interface for preference representation. The formulation leads to interpretable and reusable preference descriptions, while naturally supporting continual evolution as new interactions are observed. To learn such representations, we introduce a two-stage training framework that combines supervised fine-tuning on high-quality synthesized data with reinforcement learning to optimize long-term utility and cross-task transferability. Based on this framework, we develop AlignXplore+, a universal preference reasoning model that generates textual preference summaries. Experiments on nine benchmarks show that our 8B model achieves state-of-the-art performanc -- outperforming substantially larger open-source models -- while exhibiting strong transferability across tasks, model families, and interaction formats.",
      "authors": [
        "Yuting Liu",
        "Jian Guan",
        "Jia-Nan Li",
        "Wei Wu",
        "Jiang-Ming Yang",
        "Jianzhe Zhao",
        "Guibing Guo"
      ],
      "published": "2026-01-08T14:09:17Z",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "abs_url": "https://arxiv.org/abs/2601.04963v1",
      "pdf_url": "https://arxiv.org/pdf/2601.04963v1.pdf"
    },
    {
      "id": "2601.04956v1",
      "title": "TEA: Temporal Adaptive Satellite Image Semantic Segmentation",
      "abstract": "Crop mapping based on satellite images time-series (SITS) holds substantial economic value in agricultural production settings, in which parcel segmentation is an essential step. Existing approaches have achieved notable advancements in SITS segmentation with predetermined sequence lengths. However, we found that these approaches overlooked the generalization capability of models across scenarios with varying temporal length, leading to markedly poor segmentation results in such cases. To address this issue, we propose TEA, a TEmporal Adaptive SITS semantic segmentation method to enhance the model's resilience under varying sequence lengths. We introduce a teacher model that encapsulates the global sequence knowledge to guide a student model with adaptive temporal input lengths. Specifically, teacher shapes the student's feature space via intermediate embedding, prototypes and soft label perspectives to realize knowledge transfer, while dynamically aggregating student model to mitigate knowledge forgetting. Finally, we introduce full-sequence reconstruction as an auxiliary task to further enhance the quality of representations across inputs of varying temporal lengths. Through extensive experiments, we demonstrate that our method brings remarkable improvements across inputs of different temporal lengths on common benchmarks. Our code will be publicly available.",
      "authors": [
        "Juyuan Kang",
        "Hao Zhu",
        "Yan Zhu",
        "Wei Zhang",
        "Jianing Chen",
        "Tianxiang Xiao",
        "Yike Ma",
        "Hao Jiang",
        "Feng Dai"
      ],
      "published": "2026-01-08T14:02:28Z",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "abs_url": "https://arxiv.org/abs/2601.04956v1",
      "pdf_url": "https://arxiv.org/pdf/2601.04956v1.pdf"
    },
    {
      "id": "2601.04948v1",
      "title": "SKATER: Synthesized Kinematics for Advanced Traversing Efficiency on a Humanoid Robot via Roller Skate Swizzles",
      "abstract": "Although recent years have seen significant progress of humanoid robots in walking and running, the frequent foot strikes with ground during these locomotion gaits inevitably generate high instantaneous impact forces, which leads to exacerbated joint wear and poor energy utilization. Roller skating, as a sport with substantial biomechanical value, can achieve fast and continuous sliding through rational utilization of body inertia, featuring minimal kinetic energy loss. Therefore, this study proposes a novel humanoid robot with each foot equipped with a row of four passive wheels for roller skating. A deep reinforcement learning control framework is also developed for the swizzle gait with the reward function design based on the intrinsic characteristics of roller skating. The learned policy is first analyzed in simulation and then deployed on the physical robot to demonstrate the smoothness and efficiency of the swizzle gait over traditional bipedal walking gait in terms of Impact Intensity and Cost of Transport during locomotion. A reduction of $75.86\\%$ and $63.34\\%$ of these two metrics indicate roller skating as a superior locomotion mode for enhanced energy efficiency and joint longevity.",
      "authors": [
        "Junchi Gu",
        "Feiyang Yuan",
        "Weize Shi",
        "Tianchen Huang",
        "Haopeng Zhang",
        "Xiaohu Zhang",
        "Yu Wang",
        "Wei Gao",
        "Shiwu Zhang"
      ],
      "published": "2026-01-08T13:54:22Z",
      "categories": [
        "cs.RO"
      ],
      "primary_category": "cs.RO",
      "abs_url": "https://arxiv.org/abs/2601.04948v1",
      "pdf_url": "https://arxiv.org/pdf/2601.04948v1.pdf"
    },
    {
      "id": "2601.04946v1",
      "title": "Prototypicality Bias Reveals Blindspots in Multimodal Evaluation Metrics",
      "abstract": "Automatic metrics are now central to evaluating text-to-image models, often substituting for human judgment in benchmarking and large-scale filtering. However, it remains unclear whether these metrics truly prioritize semantic correctness or instead favor visually and socially prototypical images learned from biased data distributions. We identify and study \\emph{prototypicality bias} as a systematic failure mode in multimodal evaluation. We introduce a controlled contrastive benchmark \\textsc{\\textbf{ProtoBias}} (\\textit{\\textbf{Proto}typical \\textbf{Bias}}), spanning Animals, Objects, and Demography images, where semantically correct but non-prototypical images are paired with subtly incorrect yet prototypical adversarial counterparts. This setup enables a directional evaluation of whether metrics follow textual semantics or default to prototypes. Our results show that widely used metrics, including CLIPScore, PickScore, and VQA-based scores, frequently misrank these pairs, while even LLM-as-Judge systems exhibit uneven robustness in socially grounded cases. Human evaluations consistently favour semantic correctness with larger decision margins. Motivated by these findings, we propose \\textbf{\\textsc{ProtoScore}}, a robust 7B-parameter metric that substantially reduces failure rates and suppresses misranking, while running at orders of magnitude faster than the inference time of GPT-5, approaching the robustness of much larger closed-source judges.",
      "authors": [
        "Subhadeep Roy",
        "Gagan Bhatia",
        "Steffen Eger"
      ],
      "published": "2026-01-08T13:49:14Z",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "abs_url": "https://arxiv.org/abs/2601.04946v1",
      "pdf_url": "https://arxiv.org/pdf/2601.04946v1.pdf"
    },
    {
      "id": "2601.04912v1",
      "title": "Decentralized Privacy-Preserving Federal Learning of Computer Vision Models on Edge Devices",
      "abstract": "Collaborative training of a machine learning model comes with a risk of sharing sensitive or private data. Federated learning offers a way of collectively training a single global model without the need to share client data, by sharing only the updated parameters from each client's local model. A central server is then used to aggregate parameters from all clients and redistribute the aggregated model back to the clients. Recent findings have shown that even in this scenario, private data can be reconstructed only using information about model parameters. Current efforts to mitigate this are mainly focused on reducing privacy risks on the server side, assuming that other clients will not act maliciously. In this work, we analyzed various methods for improving the privacy of client data concerning both the server and other clients for neural networks. Some of these methods include homomorphic encryption, gradient compression, gradient noising, and discussion on possible usage of modified federated learning systems such as split learning, swarm learning or fully encrypted models. We have analyzed the negative effects of gradient compression and gradient noising on the accuracy of convolutional neural networks used for classification. We have shown the difficulty of data reconstruction in the case of segmentation networks. We have also implemented a proof of concept on the NVIDIA Jetson TX2 module used in edge devices and simulated a federated learning process.",
      "authors": [
        "Damian Harenčák",
        "Lukáš Gajdošech",
        "Martin Madaras"
      ],
      "published": "2026-01-08T13:10:33Z",
      "categories": [
        "cs.CR",
        "cs.CV"
      ],
      "primary_category": "cs.CR",
      "abs_url": "https://arxiv.org/abs/2601.04912v1",
      "pdf_url": "https://arxiv.org/pdf/2601.04912v1.pdf"
    },
    {
      "id": "2601.04899v1",
      "title": "Rotation-Robust Regression with Convolutional Model Trees",
      "abstract": "We study rotation-robust learning for image inputs using Convolutional Model Trees (CMTs) [1], whose split and leaf coefficients can be structured on the image grid and transformed geometrically at deployment time. In a controlled MNIST setting with a rotation-invariant regression target, we introduce three geometry-aware inductive biases for split directions -- convolutional smoothing, a tilt dominance constraint, and importance-based pruning -- and quantify their impact on robustness under in-plane rotations. We further evaluate a deployment-time orientation search that selects a discrete rotation maximizing a forest-level confidence proxy without updating model parameters. Orientation search improves robustness under severe rotations but can be harmful near the canonical orientation when confidence is misaligned with correctness. Finally, we observe consistent trends on MNIST digit recognition implemented as one-vs-rest regression, highlighting both the promise and limitations of confidence-based orientation selection for model-tree ensembles.",
      "authors": [
        "Hongyi Li",
        "William Ward Armstrong",
        "Jun Xu"
      ],
      "published": "2026-01-08T12:53:33Z",
      "categories": [
        "cs.CV",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "abs_url": "https://arxiv.org/abs/2601.04899v1",
      "pdf_url": "https://arxiv.org/pdf/2601.04899v1.pdf"
    },
    {
      "id": "2601.04897v1",
      "title": "V-FAT: Benchmarking Visual Fidelity Against Text-bias",
      "abstract": "Recent advancements in Multimodal Large Language Models (MLLMs) have demonstrated impressive performance on standard visual reasoning benchmarks. However, there is growing concern that these models rely excessively on linguistic shortcuts rather than genuine visual grounding, a phenomenon we term Text Bias. In this paper, we investigate the fundamental tension between visual perception and linguistic priors. We decouple the sources of this bias into two dimensions: Internal Corpus Bias, stemming from statistical correlations in pretraining, and External Instruction Bias, arising from the alignment-induced tendency toward sycophancy. To quantify this effect, we introduce V-FAT (Visual Fidelity Against Text-bias), a diagnostic benchmark comprising 4,026 VQA instances across six semantic domains. V-FAT employs a Three-Level Evaluation Framework that systematically increases the conflict between visual evidence and textual information: (L1) internal bias from atypical images, (L2) external bias from misleading instructions, and (L3) synergistic bias where both coincide. We introduce the Visual Robustness Score (VRS), a metric designed to penalize \"lucky\" linguistic guesses and reward true visual fidelity. Our evaluation of 12 frontier MLLMs reveals that while models excel in existing benchmarks, they experience significant visual collapse under high linguistic dominance.",
      "authors": [
        "Ziteng Wang",
        "Yujie He",
        "Guanliang Li",
        "Siqi Yang",
        "Jiaqi Xiong",
        "Songxiang Liu"
      ],
      "published": "2026-01-08T12:50:14Z",
      "categories": [
        "cs.CL",
        "cs.CV",
        "cs.LG",
        "cs.MM"
      ],
      "primary_category": "cs.CL",
      "abs_url": "https://arxiv.org/abs/2601.04897v1",
      "pdf_url": "https://arxiv.org/pdf/2601.04897v1.pdf"
    },
    {
      "id": "2601.04891v1",
      "title": "Scaling Vision Language Models for Pharmaceutical Long Form Video Reasoning on Industrial GenAI Platform",
      "abstract": "Vision Language Models (VLMs) have shown strong performance on multimodal reasoning tasks, yet most evaluations focus on short videos and assume unconstrained computational resources. In industrial settings such as pharmaceutical content understanding, practitioners must process long-form videos under strict GPU, latency, and cost constraints, where many existing approaches fail to scale. In this work, we present an industrial GenAI framework that processes over 200,000 PDFs, 25,326 videos across eight formats (e.g., MP4, M4V, etc.), and 888 multilingual audio files in more than 20 languages. Our study makes three contributions: (i) an industrial large-scale architecture for multimodal reasoning in pharmaceutical domains; (ii) empirical analysis of over 40 VLMs on two leading benchmarks (Video-MME and MMBench) and proprietary dataset of 25,326 videos across 14 disease areas; and (iii) four findings relevant to long-form video reasoning: the role of multimodality, attention mechanism trade-offs, temporal reasoning limits, and challenges of video splitting under GPU constraints. Results show 3-8 times efficiency gains with SDPA attention on commodity GPUs, multimodality improving up to 8/12 task domains (especially length-dependent tasks), and clear bottlenecks in temporal alignment and keyframe detection across open- and closed-source VLMs. Rather than proposing a new \"A+B\" model, this paper characterizes practical limits, trade-offs, and failure patterns of current VLMs under realistic deployment constraints, and provide actionable guidance for both researchers and practitioners designing scalable multimodal systems for long-form video understanding in industrial domains.",
      "authors": [
        "Suyash Mishra",
        "Qiang Li",
        "Srikanth Patil",
        "Satyanarayan Pati",
        "Baddu Narendra"
      ],
      "published": "2026-01-08T12:42:17Z",
      "categories": [
        "cs.CV",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "abs_url": "https://arxiv.org/abs/2601.04891v1",
      "pdf_url": "https://arxiv.org/pdf/2601.04891v1.pdf"
    },
    {
      "id": "2601.04881v1",
      "title": "Zero Wrench Control via Wrench Disturbance Observer for Learning-free Peg-in-hole Assembly",
      "abstract": "This paper proposes a Dynamic Wrench Disturbance Observer (DW-DOB) designed to achieve highly sensitive zero-wrench control in contact-rich manipulation. By embedding task-space inertia into the observer nominal model, DW-DOB cleanly separates intrinsic dynamic reactions from true external wrenches. This preserves sensitivity to small forces and moments while ensuring robust regulation of contact wrenches. A passivity-based analysis further demonstrates that DW-DOB guarantees stable interactions under dynamic conditions, addressing the shortcomings of conventional observers that fail to compensate for inertial effects. Peg-in-hole experiments at industrial tolerances (H7/h6) validate the approach, yielding deeper and more compliant insertions with minimal residual wrenches and outperforming a conventional wrench disturbance observer and a PD baseline. These results highlight DW-DOB as a practical learning-free solution for high-precision zero-wrench control in contact-rich tasks.",
      "authors": [
        "Kiyoung Choi",
        "Juwon Jeong",
        "Sehoon Oh"
      ],
      "published": "2026-01-08T12:29:21Z",
      "categories": [
        "cs.RO"
      ],
      "primary_category": "cs.RO",
      "abs_url": "https://arxiv.org/abs/2601.04881v1",
      "pdf_url": "https://arxiv.org/pdf/2601.04881v1.pdf"
    },
    {
      "id": "2601.04699v1",
      "title": "SeqWalker: Sequential-Horizon Vision-and-Language Navigation with Hierarchical Planning",
      "abstract": "Sequential-Horizon Vision-and-Language Navigation (SH-VLN) presents a challenging scenario where agents should sequentially execute multi-task navigation guided by complex, long-horizon language instructions. Current vision-and-language navigation models exhibit significant performance degradation with such multi-task instructions, as information overload impairs the agent's ability to attend to observationally relevant details. To address this problem, we propose SeqWalker, a navigation model built on a hierarchical planning framework. Our SeqWalker features: i) A High-Level Planner that dynamically selects global instructions into contextually relevant sub-instructions based on the agent's current visual observations, thus reducing cognitive load; ii) A Low-Level Planner incorporating an Exploration-Verification strategy that leverages the inherent logical structure of instructions for trajectory error correction. To evaluate SH-VLN performance, we also extend the IVLN dataset and establish a new benchmark. Extensive experiments are performed to demonstrate the superiority of the proposed SeqWalker.",
      "authors": [
        "Zebin Han",
        "Xudong Wang",
        "Baichen Liu",
        "Qi Lyu",
        "Zhenduo Shang",
        "Jiahua Dong",
        "Lianqing Liu",
        "Zhi Han"
      ],
      "published": "2026-01-08T08:09:24Z",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "abs_url": "https://arxiv.org/abs/2601.04699v1",
      "pdf_url": "https://arxiv.org/pdf/2601.04699v1.pdf"
    },
    {
      "id": "2601.04686v1",
      "title": "Nightmare Dreamer: Dreaming About Unsafe States And Planning Ahead",
      "abstract": "Reinforcement Learning (RL) has shown remarkable success in real-world applications, particularly in robotics control. However, RL adoption remains limited due to insufficient safety guarantees. We introduce Nightmare Dreamer, a model-based Safe RL algorithm that addresses safety concerns by leveraging a learned world model to predict potential safety violations and plan actions accordingly. Nightmare Dreamer achieves nearly zero safety violations while maximizing rewards. Nightmare Dreamer outperforms model-free baselines on Safety Gymnasium tasks using only image observations, achieving nearly a 20x improvement in efficiency.",
      "authors": [
        "Oluwatosin Oseni",
        "Shengjie Wang",
        "Jun Zhu",
        "Micah Corah"
      ],
      "published": "2026-01-08T07:55:07Z",
      "categories": [
        "cs.LG",
        "cs.RO"
      ],
      "primary_category": "cs.LG",
      "abs_url": "https://arxiv.org/abs/2601.04686v1",
      "pdf_url": "https://arxiv.org/pdf/2601.04686v1.pdf"
    },
    {
      "id": "2601.04668v1",
      "title": "Optimizing Path Planning using Deep Reinforcement Learning for UGVs in Precision Agriculture",
      "abstract": "This study focuses on optimizing path planning for unmanned ground vehicles (UGVs) in precision agriculture using deep reinforcement learning (DRL) techniques in continuous action spaces. The research begins with a review of traditional grid-based methods, such as A* and Dijkstra's algorithms, and discusses their limitations in dynamic agricultural environments, highlighting the need for adaptive learning strategies. The study then explores DRL approaches, including Deep Q-Networks (DQN), which demonstrate improved adaptability and performance in two-dimensional simulations. Enhancements such as Double Q-Networks and Dueling Networks are evaluated to further improve decision-making. Building on these results, the focus shifts to continuous action space models, specifically Deep Deterministic Policy Gradient (DDPG) and Twin Delayed Deep Deterministic Policy Gradient (TD3), which are tested in increasingly complex environments. Experiments conducted in a three-dimensional environment using ROS and Gazebo demonstrate the effectiveness of continuous DRL algorithms in navigating dynamic agricultural scenarios. Notably, the pretrained TD3 agent achieves a 95 percent success rate in dynamic environments, demonstrating the robustness of the proposed approach in handling moving obstacles while ensuring safety for both crops and the robot.",
      "authors": [
        "Laukik Patade",
        "Rohan Rane",
        "Sandeep Pillai"
      ],
      "published": "2026-01-08T07:28:11Z",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "abs_url": "https://arxiv.org/abs/2601.04668v1",
      "pdf_url": "https://arxiv.org/pdf/2601.04668v1.pdf"
    },
    {
      "id": "2601.04657v1",
      "title": "Model of Spatial Human-Agent Interaction with Consideration for Others",
      "abstract": "Communication robots often need to initiate conversations with people in public spaces. At the same time, such robots must not disturb pedestrians. To handle these two requirements, an agent needs to estimate the communication desires of others based on their behavior and then adjust its own communication activities accordingly. In this study, we construct a computational spatial interaction model that considers others. Consideration is expressed as a quantitative parameter: the amount of adjustment of one's internal state to the estimated internal state of the other. To validate the model, we experimented with a human and a virtual robot interacting in a VR environment. The results show that when the participant moves to the target, a virtual robot with a low consideration value inhibits the participant's movement, while a robot with a higher consideration value did not inhibit the participant's movement. When the participant approached the robot, the robot also exhibited approaching behavior, regardless of the consideration value, thus decreasing the participant's movement. These results appear to verify the proposed model's ability to clarify interactions with consideration for others.",
      "authors": [
        "Takafumi Sakamoto",
        "Yugo Takeuchi"
      ],
      "published": "2026-01-08T07:05:18Z",
      "categories": [
        "cs.RO",
        "cs.HC"
      ],
      "primary_category": "cs.RO",
      "abs_url": "https://arxiv.org/abs/2601.04657v1",
      "pdf_url": "https://arxiv.org/pdf/2601.04657v1.pdf"
    },
    {
      "id": "2601.04629v1",
      "title": "UniBiDex: A Unified Teleoperation Framework for Robotic Bimanual Dexterous Manipulation",
      "abstract": "We present UniBiDex a unified teleoperation framework for robotic bimanual dexterous manipulation that supports both VRbased and leaderfollower input modalities UniBiDex enables realtime contactrich dualarm teleoperation by integrating heterogeneous input devices into a shared control stack with consistent kinematic treatment and safety guarantees The framework employs nullspace control to optimize bimanual configurations ensuring smooth collisionfree and singularityaware motion across tasks We validate UniBiDex on a longhorizon kitchentidying task involving five sequential manipulation subtasks demonstrating higher task success rates smoother trajectories and improved robustness compared to strong baselines By releasing all hardware and software components as opensource we aim to lower the barrier to collecting largescale highquality human demonstration datasets and accelerate progress in robot learning.",
      "authors": [
        "Zhongxuan Li",
        "Zeliang Guo",
        "Jun Hu",
        "David Navarro-Alarcon",
        "Jia Pan",
        "Hongmin Wu",
        "Peng Zhou"
      ],
      "published": "2026-01-08T06:00:24Z",
      "categories": [
        "cs.RO"
      ],
      "primary_category": "cs.RO",
      "abs_url": "https://arxiv.org/abs/2601.04629v1",
      "pdf_url": "https://arxiv.org/pdf/2601.04629v1.pdf"
    },
    {
      "id": "2601.04551v1",
      "title": "Discrete Fourier Transform-based Point Cloud Compression for Efficient SLAM in Featureless Terrain",
      "abstract": "Simultaneous Localization and Mapping (SLAM) is an essential technology for the efficiency and reliability of unmanned robotic exploration missions. While the onboard computational capability and communication bandwidth are critically limited, the point cloud data handled by SLAM is large in size, attracting attention to data compression methods. To address such a problem, in this paper, we propose a new method for compressing point cloud maps by exploiting the Discrete Fourier Transform (DFT). The proposed technique converts the Digital Elevation Model (DEM) to the frequency-domain 2D image and omits its high-frequency components, focusing on the exploration of gradual terrains such as planets and deserts. Unlike terrains with detailed structures such as artificial environments, high-frequency components contribute little to the representation of gradual terrains. Thus, this method is effective in compressing data size without significant degradation of the point cloud. We evaluated the method in terms of compression rate and accuracy using camera sequences of two terrains with different elevation profiles.",
      "authors": [
        "Riku Suzuki",
        "Ayumi Umemura",
        "Shreya Santra",
        "Kentaro Uno",
        "Kazuya Yoshida"
      ],
      "published": "2026-01-08T03:28:56Z",
      "categories": [
        "cs.RO"
      ],
      "primary_category": "cs.RO",
      "abs_url": "https://arxiv.org/abs/2601.04551v1",
      "pdf_url": "https://arxiv.org/pdf/2601.04551v1.pdf"
    },
    {
      "id": "2601.04547v1",
      "title": "Data-Driven Terramechanics Approach Towards a Realistic Real-Time Simulator for Lunar Rovers",
      "abstract": "High-fidelity simulators for the lunar surface provide a digital environment for extensive testing of rover operations and mission planning. However, current simulators focus on either visual realism or physical accuracy, which limits their capability to replicate lunar conditions comprehensively. This work addresses that gap by combining high visual fidelity with realistic terrain interaction for a realistic representation of rovers on the lunar surface. Because direct simulation of wheel-soil interactions is computationally expensive, a data-driven approach was adopted, using regression models for slip and sinkage from data collected in both full-rover and single-wheel experiments and simulations. The resulting regression-based terramechanics model accurately reproduced steady-state and dynamic slip, as well as sinkage behavior, on flat terrain and slopes up to 20 degrees, with validation against field test results. Additionally, improvements were made to enhance the realism of terrain deformation and wheel trace visualization. This method supports real-time applications that require physically plausible terrain response alongside high visual fidelity.",
      "authors": [
        "Jakob M. Kern",
        "James M. Hurrell",
        "Shreya Santra",
        "Keisuke Takehana",
        "Kentaro Uno",
        "Kazuya Yoshida"
      ],
      "published": "2026-01-08T03:23:31Z",
      "categories": [
        "cs.RO"
      ],
      "primary_category": "cs.RO",
      "abs_url": "https://arxiv.org/abs/2601.04547v1",
      "pdf_url": "https://arxiv.org/pdf/2601.04547v1.pdf"
    },
    {
      "id": "2601.04541v1",
      "title": "Design and Development of Modular Limbs for Reconfigurable Robots on the Moon",
      "abstract": "In this paper, we present the development of 4-DOF robot limbs, which we call Moonbots, designed to connect in various configurations with each other and wheel modules, enabling adaptation to different environments and tasks. These modular components are intended primarily for robotic systems in space exploration and construction on the Moon in our Moonshot project. Such modular robots add flexibility and versatility for space missions where resources are constrained. Each module is driven by a common actuator characterized by a high torque-to-speed ratio, supporting both precise control and dynamic motion when required. This unified actuator design simplifies development and maintenance across the different module types. The paper describes the hardware implementation, the mechanical design of the modules, and the overall software architecture used to control and coordinate them. Additionally, we evaluate the control performance of the actuator under various load conditions to characterize its suitability for modular robot applications. To demonstrate the adaptability of the system, we introduce nine functional configurations assembled from the same set of modules: 4DOF-limb, 8DOF-limb, vehicle, dragon, minimal, quadruped, cargo, cargo-minimal, and bike. These configurations reflect different locomotion strategies and task-specific behaviors, offering a practical foundation for further research in reconfigurable robotic systems.",
      "authors": [
        "Gustavo H. Diaz",
        "A. Sejal Jain",
        "Matteo Brugnera",
        "Elian Neppel",
        "Shreya Santra",
        "Kentaro Uno",
        "Kazuya Yoshida"
      ],
      "published": "2026-01-08T03:15:31Z",
      "categories": [
        "cs.RO"
      ],
      "primary_category": "cs.RO",
      "abs_url": "https://arxiv.org/abs/2601.04541v1",
      "pdf_url": "https://arxiv.org/pdf/2601.04541v1.pdf"
    },
    {
      "id": "2601.04511v1",
      "title": "Multiagent Reinforcement Learning with Neighbor Action Estimation",
      "abstract": "Multiagent reinforcement learning, as a prominent intelligent paradigm, enables collaborative decision-making within complex systems. However, existing approaches often rely on explicit action exchange between agents to evaluate action value functions, which is frequently impractical in real-world engineering environments due to communication constraints, latency, energy consumption, and reliability requirements. From an artificial intelligence perspective, this paper proposes an enhanced multiagent reinforcement learning framework that employs action estimation neural networks to infer agent behaviors. By integrating a lightweight action estimation module, each agent infers neighboring agents' behaviors using only locally observable information, enabling collaborative policy learning without explicit action sharing. This approach is fully compatible with standard TD3 algorithms and scalable to larger multiagent systems. At the engineering application level, this framework has been implemented and validated in dual-arm robotic manipulation tasks: two robotic arms collaboratively lift objects. Experimental results demonstrate that this approach significantly enhances the robustness and deployment feasibility of real-world robotic systems while reducing dependence on information infrastructure. Overall, this research advances the development of decentralized multiagent artificial intelligence systems while enabling AI to operate effectively in dynamic, information-constrained real-world environments.",
      "authors": [
        "Zhenglong Luo",
        "Zhiyong Chen",
        "Aoxiang Liu"
      ],
      "published": "2026-01-08T02:26:57Z",
      "categories": [
        "cs.RO",
        "cs.LG"
      ],
      "primary_category": "cs.RO",
      "abs_url": "https://arxiv.org/abs/2601.04511v1",
      "pdf_url": "https://arxiv.org/pdf/2601.04511v1.pdf"
    },
    {
      "id": "2601.04493v1",
      "title": "Fast Continuum Robot Shape and External Load State Estimation on SE(3)",
      "abstract": "Previous on-manifold approaches to continuum robot state estimation have typically adopted simplified Cosserat rod models, which cannot directly account for actuation inputs or external loads. We introduce a general framework that incorporates uncertainty models for actuation (e.g., tendon tensions), applied forces and moments, process noise, boundary conditions, and arbitrary backbone measurements. By adding temporal priors across time steps, our method additionally performs joint estimation in both the spatial (arclength) and temporal domains, enabling full \\textit{spacetime} state estimation. Discretizing the arclength domain yields a factor graph representation of the continuum robot model, which can be exploited for fast batch sparse nonlinear optimization in the style of SLAM. The framework is general and applies to a broad class of continuum robots; as illustrative cases, we show (i) tendon-driven robots in simulation, where we demonstrate real-time kinematics with uncertainty, tip force sensing from position feedback, and distributed load estimation from backbone strain, and (ii) a surgical concentric tube robot in experiment, where we validate accurate kinematics and tip force estimation, highlighting potential for surgical palpation.",
      "authors": [
        "James M. Ferguson",
        "Alan Kuntz",
        "Tucker Hermans"
      ],
      "published": "2026-01-08T01:53:42Z",
      "categories": [
        "cs.RO"
      ],
      "primary_category": "cs.RO",
      "abs_url": "https://arxiv.org/abs/2601.04493v1",
      "pdf_url": "https://arxiv.org/pdf/2601.04493v1.pdf"
    },
    {
      "id": "2601.04401v1",
      "title": "Transformer-based Multi-agent Reinforcement Learning for Separation Assurance in Structured and Unstructured Airspaces",
      "abstract": "Conventional optimization-based metering depends on strict adherence to precomputed schedules, which limits the flexibility required for the stochastic operations of Advanced Air Mobility (AAM). In contrast, multi-agent reinforcement learning (MARL) offers a decentralized, adaptive framework that can better handle uncertainty, required for safe aircraft separation assurance. Despite this advantage, current MARL approaches often overfit to specific airspace structures, limiting their adaptability to new configurations. To improve generalization, we recast the MARL problem in a relative polar state space and train a transformer encoder model across diverse traffic patterns and intersection angles. The learned model provides speed advisories to resolve conflicts while maintaining aircraft near their desired cruising speeds. In our experiments, we evaluated encoder depths of 1, 2, and 3 layers in both structured and unstructured airspaces, and found that a single encoder configuration outperformed deeper variants, yielding near-zero near mid-air collision rates and shorter loss-of-separation infringements than the deeper configurations. Additionally, we showed that the same configuration outperforms a baseline model designed purely with attention. Together, our results suggest that the newly formulated state representation, novel design of neural network architecture, and proposed training strategy provide an adaptable and scalable decentralized solution for aircraft separation assurance in both structured and unstructured airspaces.",
      "authors": [
        "Arsyi Aziz",
        "Peng Wei"
      ],
      "published": "2026-01-07T21:18:28Z",
      "categories": [
        "cs.RO",
        "cs.LG",
        "cs.MA"
      ],
      "primary_category": "cs.RO",
      "abs_url": "https://arxiv.org/abs/2601.04401v1",
      "pdf_url": "https://arxiv.org/pdf/2601.04401v1.pdf"
    },
    {
      "id": "2601.04392v1",
      "title": "Enhanced-FQL($λ$), an Efficient and Interpretable RL with novel Fuzzy Eligibility Traces and Segmented Experience Replay",
      "abstract": "This paper introduces a fuzzy reinforcement learning framework, Enhanced-FQL($λ$), that integrates novel Fuzzified Eligibility Traces (FET) and Segmented Experience Replay (SER) into fuzzy Q-learning with Fuzzified Bellman Equation (FBE) for continuous control tasks. The proposed approach employs an interpretable fuzzy rule base instead of complex neural architectures, while maintaining competitive performance through two key innovations: a fuzzified Bellman equation with eligibility traces for stable multi-step credit assignment, and a memory-efficient segment-based experience replay mechanism for enhanced sample efficiency. Theoretical analysis proves the proposed method convergence under standard assumptions. Extensive evaluations in continuous control domains demonstrate that Enhanced-FQL($λ$) achieves superior sample efficiency and reduced variance compared to n-step fuzzy TD and fuzzy SARSA($λ$) baselines, while maintaining substantially lower computational complexity than deep RL alternatives such as DDPG. The framework's inherent interpretability, combined with its computational efficiency and theoretical convergence guarantees, makes it particularly suitable for safety-critical applications where transparency and resource constraints are essential.",
      "authors": [
        "Mohsen Jalaeian-Farimani"
      ],
      "published": "2026-01-07T20:59:18Z",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.RO",
        "eess.SY",
        "math.OC"
      ],
      "primary_category": "cs.LG",
      "abs_url": "https://arxiv.org/abs/2601.04392v1",
      "pdf_url": "https://arxiv.org/pdf/2601.04392v1.pdf"
    },
    {
      "id": "2601.04356v1",
      "title": "UNIC: Learning Unified Multimodal Extrinsic Contact Estimation",
      "abstract": "Contact-rich manipulation requires reliable estimation of extrinsic contacts-the interactions between a grasped object and its environment which provide essential contextual information for planning, control, and policy learning. However, existing approaches often rely on restrictive assumptions, such as predefined contact types, fixed grasp configurations, or camera calibration, that hinder generalization to novel objects and deployment in unstructured environments. In this paper, we present UNIC, a unified multimodal framework for extrinsic contact estimation that operates without any prior knowledge or camera calibration. UNIC directly encodes visual observations in the camera frame and integrates them with proprioceptive and tactile modalities in a fully data-driven manner. It introduces a unified contact representation based on scene affordance maps that captures diverse contact formations and employs a multimodal fusion mechanism with random masking, enabling robust multimodal representation learning. Extensive experiments demonstrate that UNIC performs reliably. It achieves a 9.6 mm average Chamfer distance error on unseen contact locations, performs well on unseen objects, remains robust under missing modalities, and adapts to dynamic camera viewpoints. These results establish extrinsic contact estimation as a practical and versatile capability for contact-rich manipulation.",
      "authors": [
        "Zhengtong Xu",
        "Yuki Shirai"
      ],
      "published": "2026-01-07T19:43:16Z",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.RO",
      "abs_url": "https://arxiv.org/abs/2601.04356v1",
      "pdf_url": "https://arxiv.org/pdf/2601.04356v1.pdf"
    },
    {
      "id": "2601.04334v1",
      "title": "Autonomous Reasoning for Spacecraft Control: A Large Language Model Framework with Group Relative Policy Optimization",
      "abstract": "This paper presents a learning-based guidance-and-control approach that couples a reasoning-enabled Large Language Model (LLM) with Group Relative Policy Optimization (GRPO). A two-stage procedure consisting of Supervised Fine-Tuning (SFT) to learn formatting and control primitives, followed by GRPO for interaction-driven policy improvement, trains controllers for each environment. The framework is demonstrated on four control problems spanning a gradient of dynamical complexity, from canonical linear systems through nonlinear oscillatory dynamics to three-dimensional spacecraft attitude control with gyroscopic coupling and thrust constraints. Results demonstrate that an LLM with explicit reasoning, optimized via GRPO, can synthesize feasible stabilizing policies under consistent training settings across both linear and nonlinear systems. The two-stage training methodology enables models to generate control sequences while providing human-readable explanations of their decision-making process. This work establishes a foundation for applying GRPO-based reasoning to autonomous control systems, with potential applications in aerospace and other safety-critical domains.",
      "authors": [
        "Amit Jain",
        "Richard Linares"
      ],
      "published": "2026-01-07T19:13:22Z",
      "categories": [
        "cs.RO",
        "math.OC"
      ],
      "primary_category": "cs.RO",
      "abs_url": "https://arxiv.org/abs/2601.04334v1",
      "pdf_url": "https://arxiv.org/pdf/2601.04334v1.pdf"
    },
    {
      "id": "2601.04194v1",
      "title": "Choreographing a World of Dynamic Objects",
      "abstract": "Dynamic objects in our physical 4D (3D + time) world are constantly evolving, deforming, and interacting with other objects, leading to diverse 4D scene dynamics. In this paper, we present a universal generative pipeline, CHORD, for CHOReographing Dynamic objects and scenes and synthesizing this type of phenomena. Traditional rule-based graphics pipelines to create these dynamics are based on category-specific heuristics, yet are labor-intensive and not scalable. Recent learning-based methods typically demand large-scale datasets, which may not cover all object categories in interest. Our approach instead inherits the universality from the video generative models by proposing a distillation-based pipeline to extract the rich Lagrangian motion information hidden in the Eulerian representations of 2D videos. Our method is universal, versatile, and category-agnostic. We demonstrate its effectiveness by conducting experiments to generate a diverse range of multi-body 4D dynamics, show its advantage compared to existing methods, and demonstrate its applicability in generating robotics manipulation policies. Project page: https://yanzhelyu.github.io/chord",
      "authors": [
        "Yanzhe Lyu",
        "Chen Geng",
        "Karthik Dharmarajan",
        "Yunzhi Zhang",
        "Hadi Alzayer",
        "Shangzhe Wu",
        "Jiajun Wu"
      ],
      "published": "2026-01-07T18:59:40Z",
      "categories": [
        "cs.CV",
        "cs.GR",
        "cs.RO"
      ],
      "primary_category": "cs.CV",
      "abs_url": "https://arxiv.org/abs/2601.04194v1",
      "pdf_url": "https://arxiv.org/pdf/2601.04194v1.pdf"
    },
    {
      "id": "2601.04191v1",
      "title": "Embedding Autonomous Agents in Resource-Constrained Robotic Platforms",
      "abstract": "Many embedded devices operate under resource constraints and in dynamic environments, requiring local decision-making capabilities. Enabling devices to make independent decisions in such environments can improve the responsiveness of the system and reduce the dependence on constant external control. In this work, we integrate an autonomous agent, programmed using AgentSpeak, with a small two-wheeled robot that explores a maze using its own decision-making and sensor data. Experimental results show that the agent successfully solved the maze in 59 seconds using 287 reasoning cycles, with decision phases taking less than one millisecond. These results indicate that the reasoning process is efficient enough for real-time execution on resource-constrained hardware. This integration demonstrates how high-level agent-based control can be applied to resource-constrained embedded systems for autonomous operation.",
      "authors": [
        "Negar Halakou",
        "Juan F. Gutierrez",
        "Ye Sun",
        "Han Jiang",
        "Xueming Wu",
        "Yilun Song",
        "Andres Gomez"
      ],
      "published": "2026-01-07T18:57:32Z",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "abs_url": "https://arxiv.org/abs/2601.04191v1",
      "pdf_url": "https://arxiv.org/pdf/2601.04191v1.pdf"
    },
    {
      "id": "2601.04177v2",
      "title": "Hierarchical GNN-Based Multi-Agent Learning for Dynamic Queue-Jump Lane and Emergency Vehicle Corridor Formation",
      "abstract": "Emergency vehicles require rapid passage through congested traffic, yet existing strategies fail to adapt to dynamic conditions. We propose a novel hierarchical graph neural network (GNN)-based multi-agent reinforcement learning framework to coordinate connected vehicles for emergency corridor formation. Our approach uses a high-level planner for global strategy and low-level controllers for trajectory execution, utilizing graph attention networks to scale with variable agent counts. Trained via Multi-Agent Proximal Policy Optimization (MAPPO), the system reduces emergency vehicle travel time by 28.3% compared to baselines and 44.6% compared to uncoordinated traffic in simulations. The design achieves near-zero collision rates (0.3%) while maintaining 81% of background traffic efficiency. Ablation and generalization studies confirm the framework's robustness across diverse scenarios. These results demonstrate the effectiveness of combining GNNs with hierarchical learning for intelligent transportation systems.",
      "authors": [
        "Haoran Su"
      ],
      "published": "2026-01-07T18:43:18Z",
      "categories": [
        "cs.RO",
        "eess.SY"
      ],
      "primary_category": "cs.RO",
      "abs_url": "https://arxiv.org/abs/2601.04177v2",
      "pdf_url": "https://arxiv.org/pdf/2601.04177v2.pdf"
    },
    {
      "id": "2601.04137v1",
      "title": "Wow, wo, val! A Comprehensive Embodied World Model Evaluation Turing Test",
      "abstract": "As world models gain momentum in Embodied AI, an increasing number of works explore using video foundation models as predictive world models for downstream embodied tasks like 3D prediction or interactive generation. However, before exploring these downstream tasks, video foundation models still have two critical questions unanswered: (1) whether their generative generalization is sufficient to maintain perceptual fidelity in the eyes of human observers, and (2) whether they are robust enough to serve as a universal prior for real-world embodied agents. To provide a standardized framework for answering these questions, we introduce the Embodied Turing Test benchmark: WoW-World-Eval (Wow,wo,val). Building upon 609 robot manipulation data, Wow-wo-val examines five core abilities, including perception, planning, prediction, generalization, and execution. We propose a comprehensive evaluation protocol with 22 metrics to assess the models' generation ability, which achieves a high Pearson Correlation between the overall score and human preference (>0.93) and establishes a reliable foundation for the Human Turing Test. On Wow-wo-val, models achieve only 17.27 on long-horizon planning and at best 68.02 on physical consistency, indicating limited spatiotemporal consistency and physical reasoning. For the Inverse Dynamic Model Turing Test, we first use an IDM to evaluate the video foundation models' execution accuracy in the real world. However, most models collapse to $\\approx$ 0% success, while WoW maintains a 40.74% success rate. These findings point to a noticeable gap between the generated videos and the real world, highlighting the urgency and necessity of benchmarking World Model in Embodied AI.",
      "authors": [
        "Chun-Kai Fan",
        "Xiaowei Chi",
        "Xiaozhu Ju",
        "Hao Li",
        "Yong Bao",
        "Yu-Kai Wang",
        "Lizhang Chen",
        "Zhiyuan Jiang",
        "Kuangzhi Ge",
        "Ying Li",
        "Weishi Mi",
        "Qingpo Wuwu",
        "Peidong Jia",
        "Yulin Luo",
        "Kevin Zhang",
        "Zhiyuan Qin",
        "Yong Dai",
        "Sirui Han",
        "Yike Guo",
        "Shanghang Zhang",
        "Jian Tang"
      ],
      "published": "2026-01-07T17:50:37Z",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.RO",
      "abs_url": "https://arxiv.org/abs/2601.04137v1",
      "pdf_url": "https://arxiv.org/pdf/2601.04137v1.pdf"
    },
    {
      "id": "2601.04061v1",
      "title": "CLAP: Contrastive Latent Action Pretraining for Learning Vision-Language-Action Models from Human Videos",
      "abstract": "Generalist Vision-Language-Action models are currently hindered by the scarcity of robotic data compared to the abundance of human video demonstrations. Existing Latent Action Models attempt to leverage video data but often suffer from visual entanglement, capturing noise rather than manipulation skills. To address this, we propose Contrastive Latent Action Pretraining (CLAP), a framework that aligns the visual latent space from videos with a proprioceptive latent space from robot trajectories. By employing contrastive learning, CLAP maps video transitions onto a quantized, physically executable codebook. Building on this representation, we introduce a dual-formulation VLA framework offering both CLAP-NTP, an autoregressive model excelling at instruction following and object generalization, and CLAP-RF, a Rectified Flow-based policy designed for high-frequency, precise manipulation. Furthermore, we propose a Knowledge Matching (KM) regularization strategy to mitigate catastrophic forgetting during fine-tuning. Extensive experiments demonstrate that CLAP significantly outperforms strong baselines, enabling the effective transfer of skills from human videos to robotic execution. Project page: https://lin-shan.com/CLAP/.",
      "authors": [
        "Chubin Zhang",
        "Jianan Wang",
        "Zifeng Gao",
        "Yue Su",
        "Tianru Dai",
        "Cai Zhou",
        "Jiwen Lu",
        "Yansong Tang"
      ],
      "published": "2026-01-07T16:26:33Z",
      "categories": [
        "cs.RO",
        "cs.CV"
      ],
      "primary_category": "cs.RO",
      "abs_url": "https://arxiv.org/abs/2601.04061v1",
      "pdf_url": "https://arxiv.org/pdf/2601.04061v1.pdf"
    },
    {
      "id": "2601.04052v1",
      "title": "Stable Language Guidance for Vision-Language-Action Models",
      "abstract": "Vision-Language-Action (VLA) models have demonstrated impressive capabilities in generalized robotic control; however, they remain notoriously brittle to linguistic perturbations. We identify a critical ``modality collapse'' phenomenon where strong visual priors overwhelm sparse linguistic signals, causing agents to overfit to specific instruction phrasings while ignoring the underlying semantic intent. To address this, we propose \\textbf{Residual Semantic Steering (RSS)}, a probabilistic framework that disentangles physical affordance from semantic execution. RSS introduces two theoretical innovations: (1) \\textbf{Monte Carlo Syntactic Integration}, which approximates the true semantic posterior via dense, LLM-driven distributional expansion, and (2) \\textbf{Residual Affordance Steering}, a dual-stream decoding mechanism that explicitly isolates the causal influence of language by subtracting the visual affordance prior. Theoretical analysis suggests that RSS effectively maximizes the mutual information between action and intent while suppressing visual distractors. Empirical results across diverse manipulation benchmarks demonstrate that RSS achieves state-of-the-art robustness, maintaining performance even under adversarial linguistic perturbations.",
      "authors": [
        "Zhihao Zhan",
        "Yuhao Chen",
        "Jiaying Zhou",
        "Qinhan Lv",
        "Hao Liu",
        "Keze Wang",
        "Liang Lin",
        "Guangrun Wang"
      ],
      "published": "2026-01-07T16:16:10Z",
      "categories": [
        "cs.RO",
        "cs.CL"
      ],
      "primary_category": "cs.RO",
      "abs_url": "https://arxiv.org/abs/2601.04052v1",
      "pdf_url": "https://arxiv.org/pdf/2601.04052v1.pdf"
    },
    {
      "id": "2601.04287v1",
      "title": "Online Action-Stacking Improves Reinforcement Learning Performance for Air Traffic Control",
      "abstract": "We introduce online action-stacking, an inference-time wrapper for reinforcement learning policies that produces realistic air traffic control commands while allowing training on a much smaller discrete action space. Policies are trained with simple incremental heading or level adjustments, together with an action-damping penalty that reduces instruction frequency and leads agents to issue commands in short bursts. At inference, online action-stacking compiles these bursts of primitive actions into domain-appropriate compound clearances. Using Proximal Policy Optimisation and the BluebirdDT digital twin platform, we train agents to navigate aircraft along lateral routes, manage climb and descent to target flight levels, and perform two-aircraft collision avoidance under a minimum separation constraint. In our lateral navigation experiments, action stacking greatly reduces the number of issued instructions relative to a damped baseline and achieves comparable performance to a policy trained with a 37-dimensional action space, despite operating with only five actions. These results indicate that online action-stacking helps bridge a key gap between standard reinforcement learning formulations and operational ATC requirements, and provides a simple mechanism for scaling to more complex control scenarios.",
      "authors": [
        "Ben Carvell",
        "George De Ath",
        "Eseoghene Benjamin",
        "Richard Everson"
      ],
      "published": "2026-01-07T14:28:37Z",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.MA",
        "cs.RO"
      ],
      "primary_category": "cs.LG",
      "abs_url": "https://arxiv.org/abs/2601.04287v1",
      "pdf_url": "https://arxiv.org/pdf/2601.04287v1.pdf"
    },
    {
      "id": "2601.03956v1",
      "title": "CoINS: Counterfactual Interactive Navigation via Skill-Aware VLM",
      "abstract": "Recent Vision-Language Models (VLMs) have demonstrated significant potential in robotic planning. However, they typically function as semantic reasoners, lacking an intrinsic understanding of the specific robot's physical capabilities. This limitation is particularly critical in interactive navigation, where robots must actively modify cluttered environments to create traversable paths. Existing VLM-based navigators are predominantly confined to passive obstacle avoidance, failing to reason about when and how to interact with objects to clear blocked paths. To bridge this gap, we propose Counterfactual Interactive Navigation via Skill-aware VLM (CoINS), a hierarchical framework that integrates skill-aware reasoning and robust low-level execution. Specifically, we fine-tune a VLM, named InterNav-VLM, which incorporates skill affordance and concrete constraint parameters into the input context and grounds them into a metric-scale environmental representation. By internalizing the logic of counterfactual reasoning through fine-tuning on the proposed InterNav dataset, the model learns to implicitly evaluate the causal effects of object removal on navigation connectivity, thereby determining interaction necessity and target selection. To execute the generated high-level plans, we develop a comprehensive skill library through reinforcement learning, specifically introducing traversability-oriented strategies to manipulate diverse objects for path clearance. A systematic benchmark in Isaac Sim is proposed to evaluate both the reasoning and execution aspects of interactive navigation. Extensive simulations and real-world experiments demonstrate that CoINS significantly outperforms representative baselines, achieving a 17\\% higher overall success rate and over 80\\% improvement in complex long-horizon scenarios compared to the best-performing baseline",
      "authors": [
        "Kangjie Zhou",
        "Zhejia Wen",
        "Zhiyong Zhuo",
        "Zike Yan",
        "Pengying Wu",
        "Ieng Hou U",
        "Shuaiyang Li",
        "Han Gao",
        "Kang Ding",
        "Wenhan Cao",
        "Wei Pan",
        "Chang Liu"
      ],
      "published": "2026-01-07T14:10:46Z",
      "categories": [
        "cs.RO"
      ],
      "primary_category": "cs.RO",
      "abs_url": "https://arxiv.org/abs/2601.03956v1",
      "pdf_url": "https://arxiv.org/pdf/2601.03956v1.pdf"
    },
    {
      "id": "2601.03907v1",
      "title": "An Event-Based Opto-Tactile Skin",
      "abstract": "This paper presents a neuromorphic, event-driven tactile sensing system for soft, large-area skin, based on the Dynamic Vision Sensors (DVS) integrated with a flexible silicone optical waveguide skin. Instead of repetitively scanning embedded photoreceivers, this design uses a stereo vision setup comprising two DVS cameras looking sideways through the skin. Such a design produces events as changes in brightness are detected, and estimates press positions on the 2D skin surface through triangulation, utilizing Density-Based Spatial Clustering of Applications with Noise (DBSCAN) to find the center of mass of contact events resulting from pressing actions. The system is evaluated over a 4620 mm2 probed area of the skin using a meander raster scan. Across 95 % of the presses visible to both cameras, the press localization achieved a Root-Mean-Squared Error (RMSE) of 4.66 mm. The results highlight the potential of this approach for wide-area flexible and responsive tactile sensors in soft robotics and interactive environments. Moreover, we examined how the system performs when the amount of event data is strongly reduced. Using stochastic down-sampling, the event stream was reduced to 1/1024 of its original size. Under this extreme reduction, the average localization error increased only slightly (from 4.66 mm to 9.33 mm), and the system still produced valid press localizations for 85 % of the trials. This reduction in pass rate is expected, as some presses no longer produce enough events to form a reliable cluster for triangulation. These results show that the sensing approach remains functional even with very sparse event data, which is promising for reducing power consumption and computational load in future implementations. The system exhibits a detection latency distribution with a characteristic width of 31 ms.",
      "authors": [
        "Mohammadreza Koolani",
        "Simeon Bamford",
        "Petr Trunin",
        "Simon F. Müller-Cleve",
        "Matteo Lo Preti",
        "Fulvio Mastrogiovanni",
        "Lucia Beccai",
        "Chiara Bartolozzi"
      ],
      "published": "2026-01-07T13:17:20Z",
      "categories": [
        "cs.RO"
      ],
      "primary_category": "cs.RO",
      "abs_url": "https://arxiv.org/abs/2601.03907v1",
      "pdf_url": "https://arxiv.org/pdf/2601.03907v1.pdf"
    },
    {
      "id": "2601.03904v1",
      "title": "Towards Safe Autonomous Driving: A Real-Time Motion Planning Algorithm on Embedded Hardware",
      "abstract": "Ensuring the functional safety of Autonomous Vehicles (AVs) requires motion planning modules that not only operate within strict real-time constraints but also maintain controllability in case of system faults. Existing safeguarding concepts, such as Online Verification (OV), provide safety layers that detect infeasible planning outputs. However, they lack an active mechanism to ensure safe operation in the event that the main planner fails. This paper presents a first step toward an active safety extension for fail-operational Autonomous Driving (AD). We deploy a lightweight sampling-based trajectory planner on an automotive-grade, embedded platform running a Real-Time Operating System (RTOS). The planner continuously computes trajectories under constrained computational resources, forming the foundation for future emergency planning architectures. Experimental results demonstrate deterministic timing behavior with bounded latency and minimal jitter, validating the feasibility of trajectory planning on safety-certifiable hardware. The study highlights both the potential and the remaining challenges of integrating active fallback mechanisms as an integral part of next-generation safeguarding frameworks. The code is available at: https://github.com/TUM-AVS/real-time-motion-planning",
      "authors": [
        "Korbinian Moller",
        "Glenn Johannes Tungka",
        "Lucas Jürgens",
        "Johannes Betz"
      ],
      "published": "2026-01-07T13:14:41Z",
      "categories": [
        "cs.RO"
      ],
      "primary_category": "cs.RO",
      "abs_url": "https://arxiv.org/abs/2601.03904v1",
      "pdf_url": "https://arxiv.org/pdf/2601.03904v1.pdf"
    },
    {
      "id": "2601.03869v1",
      "title": "Bayesian Monocular Depth Refinement via Neural Radiance Fields",
      "abstract": "Monocular depth estimation has applications in many fields, such as autonomous navigation and extended reality, making it an essential computer vision task. However, current methods often produce smooth depth maps that lack the fine geometric detail needed for accurate scene understanding. We propose MDENeRF, an iterative framework that refines monocular depth estimates using depth information from Neural Radiance Fields (NeRFs). MDENeRF consists of three components: (1) an initial monocular estimate for global structure, (2) a NeRF trained on perturbed viewpoints, with per-pixel uncertainty, and (3) Bayesian fusion of the noisy monocular and NeRF depths. We derive NeRF uncertainty from the volume rendering process to iteratively inject high-frequency fine details. Meanwhile, our monocular prior maintains global structure. We demonstrate superior performance on key metrics and experiments using indoor scenes from the SUN RGB-D dataset.",
      "authors": [
        "Arun Muthukkumar"
      ],
      "published": "2026-01-07T12:32:39Z",
      "categories": [
        "cs.CV",
        "cs.GR",
        "cs.LG",
        "cs.RO"
      ],
      "primary_category": "cs.CV",
      "abs_url": "https://arxiv.org/abs/2601.03869v1",
      "pdf_url": "https://arxiv.org/pdf/2601.03869v1.pdf"
    },
    {
      "id": "2601.04271v1",
      "title": "Correcting Autonomous Driving Object Detection Misclassifications with Automated Commonsense Reasoning",
      "abstract": "Autonomous Vehicle (AV) technology has been heavily researched and sought after, yet there are no SAE Level 5 AVs available today in the marketplace. We contend that over-reliance on machine learning technology is the main reason. Use of automated commonsense reasoning technology, we believe, can help achieve SAE Level 5 autonomy. In this paper, we show how automated common-sense reasoning technology can be deployed in situations where there are not enough data samples available to train a deep learning-based AV model that can handle certain abnormal road scenarios. Specifically, we consider two situations where (i) a traffic signal is malfunctioning at an intersection and (ii) all the cars ahead are slowing down and steering away due to an unexpected obstruction (e.g., animals on the road). We show that in such situations, our commonsense reasoning-based solution accurately detects traffic light colors and obstacles not correctly captured by the AV's perception model. We also provide a pathway for efficiently invoking commonsense reasoning by measuring uncertainty in the computer vision model and using commonsense reasoning to handle uncertain scenarios. We describe our experiments conducted using the CARLA simulator and the results obtained. The main contribution of our research is to show that automated commonsense reasoning effectively corrects AV-based object detection misclassifications and that hybrid models provide an effective pathway to improving AV perception.",
      "authors": [
        "Keegan Kimbrell",
        "Wang Tianhao",
        "Feng Chen",
        "Gopal Gupta"
      ],
      "published": "2026-01-07T12:01:38Z",
      "categories": [
        "cs.AI",
        "cs.RO"
      ],
      "primary_category": "cs.AI",
      "abs_url": "https://arxiv.org/abs/2601.04271v1",
      "pdf_url": "https://arxiv.org/pdf/2601.04271v1.pdf"
    },
    {
      "id": "2601.03813v1",
      "title": "Integrating Sample Inheritance into Bayesian Optimization for Evolutionary Robotics",
      "abstract": "In evolutionary robotics, robot morphologies are designed automatically using evolutionary algorithms. This creates a body-brain optimization problem, where both morphology and control must be optimized together. A common approach is to include controller optimization for each morphology, but starting from scratch for every new body may require a high controller learning budget. We address this by using Bayesian optimization for controller optimization, exploiting its sample efficiency and strong exploration capabilities, and using sample inheritance as a form of Lamarckian inheritance. Under a deliberately low controller learning budget for each morphology, we investigate two types of sample inheritance: (1) transferring all the parent's samples to the offspring to be used as prior without evaluating them, and (2) reevaluating the parent's best samples on the offspring. Both are compared to a baseline without inheritance. Our results show that reevaluation performs best, with prior-based inheritance also outperforming no inheritance. Analysis reveals that while the learning budget is too low for a single morphology, generational inheritance compensates for this by accumulating learned adaptations across generations. Furthermore, inheritance mainly benefits offspring morphologies that are similar to their parents. Finally, we demonstrate the critical role of the environment, with more challenging environments resulting in more stable walking gaits. Our findings highlight that inheritance mechanisms can boost performance in evolutionary robotics without needing large learning budgets, offering an efficient path toward more capable robot design.",
      "authors": [
        "K. Ege de Bruin",
        "Kyrre Glette",
        "Kai Olav Ellefsen"
      ],
      "published": "2026-01-07T11:18:29Z",
      "categories": [
        "cs.RO"
      ],
      "primary_category": "cs.RO",
      "abs_url": "https://arxiv.org/abs/2601.03813v1",
      "pdf_url": "https://arxiv.org/pdf/2601.03813v1.pdf"
    },
    {
      "id": "2601.03807v1",
      "title": "Generational Replacement and Learning for High-Performing and Diverse Populations in Evolvable Robots",
      "abstract": "Evolutionary Robotics offers the possibility to design robots to solve a specific task automatically by optimizing their morphology and control together. However, this co-optimization of body and control is challenging, because controllers need some time to adapt to the evolving morphology - which may make it difficult for new and promising designs to enter the evolving population. A solution to this is to add intra-life learning, defined as an additional controller optimization loop, to each individual in the evolving population. A related problem is the lack of diversity often seen in evolving populations as evolution narrows the search down to a few promising designs too quickly. This problem can be mitigated by implementing full generational replacement, where offspring robots replace the whole population. This solution for increasing diversity usually comes at the cost of lower performance compared to using elitism. In this work, we show that combining such generational replacement with intra-life learning can increase diversity while retaining performance. We also highlight the importance of performance metrics when studying learning in morphologically evolving robots, showing that evaluating according to function evaluations versus according to generations of evolution can give different conclusions.",
      "authors": [
        "K. Ege de Bruin",
        "Kyrre Glette",
        "Kai Olav Ellefsen"
      ],
      "published": "2026-01-07T11:11:57Z",
      "categories": [
        "cs.RO"
      ],
      "primary_category": "cs.RO",
      "abs_url": "https://arxiv.org/abs/2601.03807v1",
      "pdf_url": "https://arxiv.org/pdf/2601.03807v1.pdf"
    },
    {
      "id": "2601.03782v1",
      "title": "PointWorld: Scaling 3D World Models for In-The-Wild Robotic Manipulation",
      "abstract": "Humans anticipate, from a glance and a contemplated action of their bodies, how the 3D world will respond, a capability that is equally vital for robotic manipulation. We introduce PointWorld, a large pre-trained 3D world model that unifies state and action in a shared 3D space as 3D point flows: given one or few RGB-D images and a sequence of low-level robot action commands, PointWorld forecasts per-pixel displacements in 3D that respond to the given actions. By representing actions as 3D point flows instead of embodiment-specific action spaces (e.g., joint positions), this formulation directly conditions on physical geometries of robots while seamlessly integrating learning across embodiments. To train our 3D world model, we curate a large-scale dataset spanning real and simulated robotic manipulation in open-world environments, enabled by recent advances in 3D vision and simulated environments, totaling about 2M trajectories and 500 hours across a single-arm Franka and a bimanual humanoid. Through rigorous, large-scale empirical studies of backbones, action representations, learning objectives, partial observability, data mixtures, domain transfers, and scaling, we distill design principles for large-scale 3D world modeling. With a real-time (0.1s) inference speed, PointWorld can be efficiently integrated in the model-predictive control (MPC) framework for manipulation. We demonstrate that a single pre-trained checkpoint enables a real-world Franka robot to perform rigid-body pushing, deformable and articulated object manipulation, and tool use, without requiring any demonstrations or post-training and all from a single image captured in-the-wild. Project website at https://point-world.github.io/.",
      "authors": [
        "Wenlong Huang",
        "Yu-Wei Chao",
        "Arsalan Mousavian",
        "Ming-Yu Liu",
        "Dieter Fox",
        "Kaichun Mo",
        "Li Fei-Fei"
      ],
      "published": "2026-01-07T10:29:12Z",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.RO",
      "abs_url": "https://arxiv.org/abs/2601.03782v1",
      "pdf_url": "https://arxiv.org/pdf/2601.03782v1.pdf"
    },
    {
      "id": "2601.03686v1",
      "title": "Dual-Attention Heterogeneous GNN for Multi-robot Collaborative Area Search via Deep Reinforcement Learning",
      "abstract": "In multi-robot collaborative area search, a key challenge is to dynamically balance the two objectives of exploring unknown areas and covering specific targets to be rescued. Existing methods are often constrained by homogeneous graph representations, thus failing to model and balance these distinct tasks. To address this problem, we propose a Dual-Attention Heterogeneous Graph Neural Network (DA-HGNN) trained using deep reinforcement learning. Our method constructs a heterogeneous graph that incorporates three entity types: robot nodes, frontier nodes, and interesting nodes, as well as their historical states. The dual-attention mechanism comprises the relational-aware attention and type-aware attention operations. The relational-aware attention captures the complex spatio-temporal relationships among robots and candidate goals. Building on this relational-aware heterogeneous graph, the type-aware attention separately computes the relevance between robots and each goal type (frontiers vs. points of interest), thereby decoupling the exploration and coverage from the unified tasks. Extensive experiments conducted in interactive 3D scenarios within the iGibson simulator, leveraging the Gibson and MatterPort3D datasets, validate the superior scalability and generalization capability of the proposed approach.",
      "authors": [
        "Lina Zhu",
        "Jiyu Cheng",
        "Yuehu Liu",
        "Wei Zhang"
      ],
      "published": "2026-01-07T08:18:49Z",
      "categories": [
        "cs.RO"
      ],
      "primary_category": "cs.RO",
      "abs_url": "https://arxiv.org/abs/2601.03686v1",
      "pdf_url": "https://arxiv.org/pdf/2601.03686v1.pdf"
    },
    {
      "id": "2601.03617v1",
      "title": "Systematic Evaluation of Depth Backbones and Semantic Cues for Monocular Pseudo-LiDAR 3D Detection",
      "abstract": "Monocular 3D object detection offers a low-cost alternative to LiDAR, yet remains less accurate due to the difficulty of estimating metric depth from a single image. We systematically evaluate how depth backbones and feature engineering affect a monocular Pseudo-LiDAR pipeline on the KITTI validation split. Specifically, we compare NeWCRFs (supervised metric depth) against Depth Anything V2 Metric-Outdoor (Base) under an identical pseudo-LiDAR generation and PointRCNN detection protocol. NeWCRFs yields stronger downstream 3D detection, achieving 10.50\\% AP$_{3D}$ at IoU$=0.7$ on the Moderate split using grayscale intensity (Exp~2). We further test point-cloud augmentations using appearance cues (grayscale intensity) and semantic cues (instance segmentation confidence). Contrary to the expectation that semantics would substantially close the gap, these features provide only marginal gains, and mask-based sampling can degrade performance by removing contextual geometry. Finally, we report a depth-accuracy-versus-distance diagnostic using ground-truth 2D boxes (including Ped/Cyc), highlighting that coarse depth correctness does not fully predict strict 3D IoU. Overall, under an off-the-shelf LiDAR detector, depth-backbone choice and geometric fidelity dominate performance, outweighing secondary feature injection.",
      "authors": [
        "Samson Oseiwe Ajadalu"
      ],
      "published": "2026-01-07T05:57:19Z",
      "categories": [
        "cs.CV",
        "cs.LG",
        "cs.RO"
      ],
      "primary_category": "cs.CV",
      "abs_url": "https://arxiv.org/abs/2601.03617v1",
      "pdf_url": "https://arxiv.org/pdf/2601.03617v1.pdf"
    },
    {
      "id": "2601.03607v1",
      "title": "Locomotion Beyond Feet",
      "abstract": "Most locomotion methods for humanoid robots focus on leg-based gaits, yet natural bipeds frequently rely on hands, knees, and elbows to establish additional contacts for stability and support in complex environments. This paper introduces Locomotion Beyond Feet, a comprehensive system for whole-body humanoid locomotion across extremely challenging terrains, including low-clearance spaces under chairs, knee-high walls, knee-high platforms, and steep ascending and descending stairs. Our approach addresses two key challenges: contact-rich motion planning and generalization across diverse terrains. To this end, we combine physics-grounded keyframe animation with reinforcement learning. Keyframes encode human knowledge of motor skills, are embodiment-specific, and can be readily validated in simulation or on hardware, while reinforcement learning transforms these references into robust, physically accurate motions. We further employ a hierarchical framework consisting of terrain-specific motion-tracking policies, failure recovery mechanisms, and a vision-based skill planner. Real-world experiments demonstrate that Locomotion Beyond Feet achieves robust whole-body locomotion and generalizes across obstacle sizes, obstacle instances, and terrain sequences.",
      "authors": [
        "Tae Hoon Yang",
        "Haochen Shi",
        "Jiacheng Hu",
        "Zhicong Zhang",
        "Daniel Jiang",
        "Weizhuo Wang",
        "Yao He",
        "Zhen Wu",
        "Yuming Chen",
        "Yifan Hou",
        "Monroe Kennedy",
        "Shuran Song",
        "C. Karen Liu"
      ],
      "published": "2026-01-07T05:36:39Z",
      "categories": [
        "cs.RO"
      ],
      "primary_category": "cs.RO",
      "abs_url": "https://arxiv.org/abs/2601.03607v1",
      "pdf_url": "https://arxiv.org/pdf/2601.03607v1.pdf"
    },
    {
      "id": "2601.03562v1",
      "title": "From Score to Sound: An End-to-End MIDI-to-Motion Pipeline for Robotic Cello Performance",
      "abstract": "Robot musicians require precise control to obtain proper note accuracy, sound quality, and musical expression. Performance of string instruments, such as violin and cello, presents a significant challenge due to the precise control required over bow angle and pressure to produce the desired sound. While prior robotic cellists focus on accurate bowing trajectories, these works often rely on expensive motion capture techniques, and fail to sightread music in a human-like way.\n  We propose a novel end-to-end MIDI score to robotic motion pipeline which converts musical input directly into collision-aware bowing motions for a UR5e robot cellist. Through use of Universal Robot Freedrive feature, our robotic musician can achieve human-like sound without the need for motion capture. Additionally, this work records live joint data via Real-Time Data Exchange (RTDE) as the robot plays, providing labeled robotic playing data from a collection of five standard pieces to the research community. To demonstrate the effectiveness of our method in comparison to human performers, we introduce the Musical Turing Test, in which a collection of 132 human participants evaluate our robot's performance against a human baseline. Human reference recordings are also released, enabling direct comparison for future studies. This evaluation technique establishes the first benchmark for robotic cello performance. Finally, we outline a residual reinforcement learning methodology to improve upon baseline robotic controls, highlighting future opportunities for improved string-crossing efficiency and sound quality.",
      "authors": [
        "Samantha Sudhoff",
        "Pranesh Velmurugan",
        "Jiashu Liu",
        "Vincent Zhao",
        "Yung-Hsiang Lu",
        "Kristen Yeon-Ji Yun"
      ],
      "published": "2026-01-07T04:11:00Z",
      "categories": [
        "cs.RO"
      ],
      "primary_category": "cs.RO",
      "abs_url": "https://arxiv.org/abs/2601.03562v1",
      "pdf_url": "https://arxiv.org/pdf/2601.03562v1.pdf"
    },
    {
      "id": "2601.03520v1",
      "title": "A Reinforcement Learning-Based Model for Mapping and Goal-Directed Navigation Using Multiscale Place Fields",
      "abstract": "Autonomous navigation in complex and partially observable environments remains a central challenge in robotics. Several bio-inspired models of mapping and navigation based on place cells in the mammalian hippocampus have been proposed. This paper introduces a new robust model that employs parallel layers of place fields at multiple spatial scales, a replay-based reward mechanism, and dynamic scale fusion. Simulations show that the model improves path efficiency and accelerates learning compared to single-scale baselines, highlighting the value of multiscale spatial representations for adaptive robot navigation.",
      "authors": [
        "Bekarys Dukenbaev",
        "Andrew Gerstenslager",
        "Alexander Johnson",
        "Ali A. Minai"
      ],
      "published": "2026-01-07T02:10:52Z",
      "categories": [
        "cs.NE",
        "cs.AI",
        "cs.RO"
      ],
      "primary_category": "cs.NE",
      "abs_url": "https://arxiv.org/abs/2601.03520v1",
      "pdf_url": "https://arxiv.org/pdf/2601.03520v1.pdf"
    },
    {
      "id": "2601.03519v1",
      "title": "A Vision-Language-Action Model with Visual Prompt for OFF-Road Autonomous Driving",
      "abstract": "Efficient trajectory planning in off-road terrains presents a formidable challenge for autonomous vehicles, often necessitating complex multi-step pipelines. However, traditional approaches exhibit limited adaptability in dynamic environments. To address these limitations, this paper proposes OFF-EMMA, a novel end-to-end multimodal framework designed to overcome the deficiencies of insufficient spatial perception and unstable reasoning in visual-language-action (VLA) models for off-road autonomous driving scenarios. The framework explicitly annotates input images through the design of a visual prompt block and introduces a chain-of-thought with self-consistency (COT-SC) reasoning strategy to enhance the accuracy and robustness of trajectory planning. The visual prompt block utilizes semantic segmentation masks as visual prompts, enhancing the spatial understanding ability of pre-trained visual-language models for complex terrains. The COT- SC strategy effectively mitigates the error impact of outliers on planning performance through a multi-path reasoning mechanism. Experimental results on the RELLIS-3D off-road dataset demonstrate that OFF-EMMA significantly outperforms existing methods, reducing the average L2 error of the Qwen backbone model by 13.3% and decreasing the failure rate from 16.52% to 6.56%.",
      "authors": [
        "Liangdong Zhang",
        "Yiming Nie",
        "Haoyang Li",
        "Fanjie Kong",
        "Baobao Zhang",
        "Shunxin Huang",
        "Kai Fu",
        "Chen Min",
        "Liang Xiao"
      ],
      "published": "2026-01-07T02:08:18Z",
      "categories": [
        "cs.RO"
      ],
      "primary_category": "cs.RO",
      "abs_url": "https://arxiv.org/abs/2601.03519v1",
      "pdf_url": "https://arxiv.org/pdf/2601.03519v1.pdf"
    },
    {
      "id": "2601.03449v1",
      "title": "FIRE-VLM: A Vision-Language-Driven Reinforcement Learning Framework for UAV Wildfire Tracking in a Physics-Grounded Fire Digital Twin",
      "abstract": "Wildfire monitoring demands autonomous systems capable of reasoning under extreme visual degradation, rapidly evolving physical dynamics, and scarce real-world training data. Existing UAV navigation approaches rely on simplified simulators and supervised perception pipelines, and lack embodied agents interacting with physically realistic fire environments. We introduce FIRE-VLM, the first end-to-end vision-language model (VLM) guided reinforcement learning (RL) framework trained entirely within a high-fidelity, physics-grounded wildfire digital twin. Built from USGS Digital Elevation Model (DEM) terrain, LANDFIRE fuel inventories, and semi-physical fire-spread solvers, this twin captures terrain-induced runs, wind-driven acceleration, smoke plume occlusion, and dynamic fuel consumption. Within this environment, a PPO agent with dual-view UAV sensing is guided by a CLIP-style VLM. Wildfire-specific semantic alignment scores, derived from a single prompt describing active fire and smoke plumes, are integrated as potential-based reward shaping signals. Our contributions are: (1) a GIS-to-simulation pipeline for constructing wildfire digital twins; (2) a VLM-guided RL agent for UAV firefront tracking; and (3) a wildfire-aware reward design that combines physical terms with VLM semantics. Across five digital-twin evaluation tasks, our VLM-guided policy reduces time-to-detection by up to 6 times, increases time-in-FOV, and is, to our knowledge, the first RL-based UAV wildfire monitoring system demonstrated in kilometer-scale, physics-grounded digital-twin fires.",
      "authors": [
        "Chris Webb",
        "Mobin Habibpour",
        "Mayamin Hamid Raha",
        "Ali Reza Tavakkoli",
        "Janice Coen",
        "Fatemeh Afghah"
      ],
      "published": "2026-01-06T22:31:57Z",
      "categories": [
        "cs.RO"
      ],
      "primary_category": "cs.RO",
      "abs_url": "https://arxiv.org/abs/2601.03449v1",
      "pdf_url": "https://arxiv.org/pdf/2601.03449v1.pdf"
    },
    {
      "id": "2601.03447v1",
      "title": "Cost-Effective Radar Sensors for Field-Based Water Level Monitoring with Sub-Centimeter Accuracy",
      "abstract": "Water level monitoring is critical for flood management, water resource allocation, and ecological assessment, yet traditional methods remain costly and limited in coverage. This work explores radar-based sensing as a low-cost alternative for water level estimation, leveraging its non-contact nature and robustness to environmental conditions. Commercial radar sensors are evaluated in real-world field tests, applying statistical filtering techniques to improve accuracy. Results show that a single radar sensor can achieve centimeter-scale precision with minimal calibration, making it a practical solution for autonomous water monitoring using drones and robotic platforms.",
      "authors": [
        "Anna Zavei-Boroda",
        "J. Toby Minear",
        "Kyle Harlow",
        "Dusty Woods",
        "Christoffer Heckman"
      ],
      "published": "2026-01-06T22:25:19Z",
      "categories": [
        "cs.RO"
      ],
      "primary_category": "cs.RO",
      "abs_url": "https://arxiv.org/abs/2601.03447v1",
      "pdf_url": "https://arxiv.org/pdf/2601.03447v1.pdf"
    },
    {
      "id": "2601.03398v1",
      "title": "Towards Zero-Knowledge Task Planning via a Language-based Approach",
      "abstract": "In this work, we introduce and formalize the Zero-Knowledge Task Planning (ZKTP) problem, i.e., formulating a sequence of actions to achieve some goal without task-specific knowledge. Additionally, we present a first investigation and approach for ZKTP that leverages a large language model (LLM) to decompose natural language instructions into subtasks and generate behavior trees (BTs) for execution. If errors arise during task execution, the approach also uses an LLM to adjust the BTs on-the-fly in a refinement loop. Experimental validation in the AI2-THOR simulator demonstrate our approach's effectiveness in improving overall task performance compared to alternative approaches that leverage task-specific knowledge. Our work demonstrates the potential of LLMs to effectively address several aspects of the ZKTP problem, providing a robust framework for automated behavior generation with no task-specific setup.",
      "authors": [
        "Liam Merz Hoffmeister",
        "Brian Scassellati",
        "Daniel Rakita"
      ],
      "published": "2026-01-06T20:18:15Z",
      "categories": [
        "cs.RO"
      ],
      "primary_category": "cs.RO",
      "abs_url": "https://arxiv.org/abs/2601.03398v1",
      "pdf_url": "https://arxiv.org/pdf/2601.03398v1.pdf"
    },
    {
      "id": "2601.03386v1",
      "title": "Modeling and Control for UAV with Off-center Slung Load",
      "abstract": "Unmanned aerial vehicle (UAV) with slung load system is a classic air transportation system. In practical applications, the suspension point of the slung load does not always align with the center of mass (CoM) of the UAV due to mission requirements or mechanical interference. This offset creates coupling in the system's nonlinear dynamics which leads to a complicated motion control problem. In existing research, modeling of the system are performed about the UAV's CoM. In this work we use the point of suspension instead. Based on the new model, a cascade control strategy is developed. In the middle-loop controller, the acceleration of the suspension point is used to regulate the swing angle of the slung load without the need for considering the coupling between the slung load and the UAV. Using the off-center reference frame, an inner-loop controller is designed to track the UAV's attitude without the need of simplification on the coupling effects. We prove local exponential stability of the closed-loop using Lyapunov approach. Finally, simulations and experiments are conducted to validate the proposed control system.",
      "authors": [
        "Zongyang Lv",
        "Yanmei Jia",
        "Yongqing Liu",
        "Alan F. Lynch",
        "Qing Zhao",
        "Yuhu Wu"
      ],
      "published": "2026-01-06T19:48:15Z",
      "categories": [
        "eess.SY",
        "cs.RO"
      ],
      "primary_category": "eess.SY",
      "abs_url": "https://arxiv.org/abs/2601.03386v1",
      "pdf_url": "https://arxiv.org/pdf/2601.03386v1.pdf"
    },
    {
      "id": "2601.03371v1",
      "title": "Lunar Rover Cargo Transport: Mission Concept and Field Test",
      "abstract": "In future operations on the lunar surface, automated vehicles will be required to transport cargo between known locations. Such vehicles must be able to navigate precisely in safe regions to avoid natural hazards, human-constructed infrastructure, and dangerous dark shadows. Rovers must be able to park their cargo autonomously within a small tolerance to achieve a successful pickup and delivery. In this field test, Lidar Teach and Repeat provides an ideal autonomy solution for transporting cargo in this way. A one-tonne path-to-flight rover was driven in a semi-autonomous remote-control mode to create a network of safe paths. Once the route was taught, the rover immediately repeated the entire network of paths autonomously while carrying cargo. The closed-loop performance is accurate enough to align the vehicle to the cargo and pick it up. This field report describes a two-week deployment at the Canadian Space Agency's Analogue Terrain, culminating in a simulated lunar operation to evaluate the system's capabilities. Successful cargo collection and delivery were demonstrated in harsh environmental conditions.",
      "authors": [
        "Alexander Krawciw",
        "Nicolas Olmedo",
        "Faizan Rehmatullah",
        "Maxime Desjardins-Goulet",
        "Pascal Toupin",
        "Timothy D. Barfoot"
      ],
      "published": "2026-01-06T19:16:55Z",
      "categories": [
        "cs.RO"
      ],
      "primary_category": "cs.RO",
      "abs_url": "https://arxiv.org/abs/2601.03371v1",
      "pdf_url": "https://arxiv.org/pdf/2601.03371v1.pdf"
    },
    {
      "id": "2601.03360v1",
      "title": "Revisiting Continuous-Time Trajectory Estimation via Gaussian Processes and the Magnus Expansion",
      "abstract": "Continuous-time state estimation has been shown to be an effective means of (i) handling asynchronous and high-rate measurements, (ii) introducing smoothness to the estimate, (iii) post hoc querying the estimate at times other than those of the measurements, and (iv) addressing certain observability issues related to scanning-while-moving sensors. A popular means of representing the trajectory in continuous time is via a Gaussian process (GP) prior, with the prior's mean and covariance functions generated by a linear time-varying (LTV) stochastic differential equation (SDE) driven by white noise. When the state comprises elements of Lie groups, previous works have resorted to a patchwork of local GPs each with a linear time-invariant SDE kernel, which while effective in practice, lacks theoretical elegance. Here we revisit the full LTV GP approach to continuous-time trajectory estimation, deriving a global GP prior on Lie groups via the Magnus expansion, which offers a more elegant and general solution. We provide a numerical comparison between the two approaches and discuss their relative merits.",
      "authors": [
        "Timothy Barfoot",
        "Cedric Le Gentil",
        "Sven Lilge"
      ],
      "published": "2026-01-06T19:02:19Z",
      "categories": [
        "cs.RO",
        "eess.SY"
      ],
      "primary_category": "cs.RO",
      "abs_url": "https://arxiv.org/abs/2601.03360v1",
      "pdf_url": "https://arxiv.org/pdf/2601.03360v1.pdf"
    },
    {
      "id": "2601.03247v1",
      "title": "Nonlinear Spectral Modeling and Control of Soft-Robotic Muscles from Data",
      "abstract": "Artificial muscles are essential for compliant musculoskeletal robotics but complicate control due to nonlinear multiphysics dynamics. Hydraulically amplified electrostatic (HASEL) actuators, a class of soft artificial muscles, offer high performance but exhibit memory effects and hysteresis. Here we present a data-driven reduction and control strategy grounded in spectral submanifold (SSM) theory. In the adiabatic regime, where inputs vary slowly relative to intrinsic transients, trajectories rapidly converge to a low-dimensional slow manifold. We learn an explicit input-to-output map on this manifold from forced-response trajectories alone, avoiding decay experiments that can trigger hysteresis. We deploy the SSM-based model for real-time control of an antagonistic HASEL-clutch joint. This approach yields a substantial reduction in tracking error compared to feedback-only and feedforward-only baselines under identical settings. This record-and-control workflow enables rapid characterization and high-performance control of soft muscles and muscle-driven joints without detailed physics-based modeling.",
      "authors": [
        "Leonardo Bettini",
        "Amirhossein Kazemipour",
        "Robert K. Katzschmann",
        "George Haller"
      ],
      "published": "2026-01-06T18:43:49Z",
      "categories": [
        "math.DS",
        "cs.CE",
        "cs.RO",
        "eess.SY",
        "math.OC"
      ],
      "primary_category": "math.DS",
      "abs_url": "https://arxiv.org/abs/2601.03247v1",
      "pdf_url": "https://arxiv.org/pdf/2601.03247v1.pdf"
    },
    {
      "id": "2601.03200v1",
      "title": "A High-Fidelity Digital Twin for Robotic Manipulation Based on 3D Gaussian Splatting",
      "abstract": "Developing high-fidelity, interactive digital twins is crucial for enabling closed-loop motion planning and reliable real-world robot execution, which are essential to advancing sim-to-real transfer. However, existing approaches often suffer from slow reconstruction, limited visual fidelity, and difficulties in converting photorealistic models into planning-ready collision geometry. We present a practical framework that constructs high-quality digital twins within minutes from sparse RGB inputs. Our system employs 3D Gaussian Splatting (3DGS) for fast, photorealistic reconstruction as a unified scene representation. We enhance 3DGS with visibility-aware semantic fusion for accurate 3D labelling and introduce an efficient, filter-based geometry conversion method to produce collision-ready models seamlessly integrated with a Unity-ROS2-MoveIt physics engine. In experiments with a Franka Emika Panda robot performing pick-and-place tasks, we demonstrate that this enhanced geometric accuracy effectively supports robust manipulation in real-world trials. These results demonstrate that 3DGS-based digital twins, enriched with semantic and geometric consistency, offer a fast, reliable, and scalable path from perception to manipulation in unstructured environments.",
      "authors": [
        "Ziyang Sun",
        "Lingfan Bao",
        "Tianhu Peng",
        "Jingcheng Sun",
        "Chengxu Zhou"
      ],
      "published": "2026-01-06T17:29:10Z",
      "categories": [
        "cs.RO"
      ],
      "primary_category": "cs.RO",
      "abs_url": "https://arxiv.org/abs/2601.03200v1",
      "pdf_url": "https://arxiv.org/pdf/2601.03200v1.pdf"
    },
    {
      "id": "2601.03136v1",
      "title": "Limited Linguistic Diversity in Embodied AI Datasets",
      "abstract": "Language plays a critical role in Vision-Language-Action (VLA) models, yet the linguistic characteristics of the datasets used to train and evaluate these systems remain poorly documented. In this work, we present a systematic dataset audit of several widely used VLA corpora, aiming to characterize what kinds of instructions these datasets actually contain and how much linguistic variety they provide. We quantify instruction language along complementary dimensions-including lexical variety, duplication and overlap, semantic similarity, and syntactic complexity. Our analysis shows that many datasets rely on highly repetitive, template-like commands with limited structural variation, yielding a narrow distribution of instruction forms. We position these findings as descriptive documentation of the language signal available in current VLA training and evaluation data, intended to support more detailed dataset reporting, more principled dataset selection, and targeted curation or augmentation strategies that broaden language coverage.",
      "authors": [
        "Selma Wanna",
        "Agnes Luhtaru",
        "Jonathan Salfity",
        "Ryan Barron",
        "Juston Moore",
        "Cynthia Matuszek",
        "Mitch Pryor"
      ],
      "published": "2026-01-06T16:06:47Z",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.RO"
      ],
      "primary_category": "cs.CL",
      "abs_url": "https://arxiv.org/abs/2601.03136v1",
      "pdf_url": "https://arxiv.org/pdf/2601.03136v1.pdf"
    },
    {
      "id": "2601.03097v1",
      "title": "Dual-quaternion learning control for autonomous vehicle trajectory tracking with safety guarantees",
      "abstract": "We propose a learning-based trajectory tracking controller for autonomous robotic platforms whose motion can be described kinematically on $\\mathrm{SE}(3)$. The controller is formulated in the dual quaternion framework and operates at the velocity level, assuming direct command of angular and linear velocities, as is standard in many aerial vehicles and omnidirectional mobile robots. Gaussian Process (GP) regression is integrated into a geometric feedback law to learn and compensate online for unknown, state-dependent disturbances and modeling imperfections affecting both attitude and position, while preserving the algebraic structure and coupling properties inherent to rigid-body motion.\n  The proposed approach does not rely on explicit parametric models of the unknown effects, making it well-suited for robotic systems subject to sensor-induced disturbances, unmodeled actuation couplings, and environmental uncertainties. A Lyapunov-based analysis establishes probabilistic ultimate boundedness of the pose tracking error under bounded GP uncertainty, providing formal stability guarantees for the learning-based controller.\n  Simulation results demonstrate accurate and smooth trajectory tracking in the presence of realistic, localized disturbances, including correlated rotational and translational effects arising from magnetometer perturbations. These results illustrate the potential of combining geometric modeling and probabilistic learning to achieve robust, data-efficient pose control for autonomous robotic systems.",
      "authors": [
        "Omayra Yago Nieto",
        "Alexandre Anahory Simoes",
        "Juan I. Giribet",
        "Leonardo Colombo"
      ],
      "published": "2026-01-06T15:30:02Z",
      "categories": [
        "cs.RO",
        "eess.SY",
        "math.OC"
      ],
      "primary_category": "cs.RO",
      "abs_url": "https://arxiv.org/abs/2601.03097v1",
      "pdf_url": "https://arxiv.org/pdf/2601.03097v1.pdf"
    },
    {
      "id": "2601.03070v1",
      "title": "HEXAR: a Hierarchical Explainability Architecture for Robots",
      "abstract": "As robotic systems become increasingly complex, the need for explainable decision-making becomes critical. Existing explainability approaches in robotics typically either focus on individual modules, which can be difficult to query from the perspective of high-level behaviour, or employ monolithic approaches, which do not exploit the modularity of robotic architectures. We present HEXAR (Hierarchical EXplainability Architecture for Robots), a novel framework that provides a plug-in, hierarchical approach to generate explanations about robotic systems. HEXAR consists of specialised component explainers using diverse explanation techniques (e.g., LLM-based reasoning, causal models, feature importance, etc) tailored to specific robot modules, orchestrated by an explainer selector that chooses the most appropriate one for a given query. We implement and evaluate HEXAR on a TIAGo robot performing assistive tasks in a home environment, comparing it against end-to-end and aggregated baseline approaches across 180 scenario-query variations. We observe that HEXAR significantly outperforms baselines in root cause identification, incorrect information exclusion, and runtime, offering a promising direction for transparent autonomous systems.",
      "authors": [
        "Tamlin Love",
        "Ferran Gebellí",
        "Pradip Pramanick",
        "Antonio Andriella",
        "Guillem Alenyà",
        "Anais Garrell",
        "Raquel Ros",
        "Silvia Rossi"
      ],
      "published": "2026-01-06T14:55:16Z",
      "categories": [
        "cs.RO"
      ],
      "primary_category": "cs.RO",
      "abs_url": "https://arxiv.org/abs/2601.03070v1",
      "pdf_url": "https://arxiv.org/pdf/2601.03070v1.pdf"
    },
    {
      "id": "2601.03055v1",
      "title": "A Fast Semidefinite Convex Relaxation for Optimal Control Problems With Spatio-Temporal Constraints",
      "abstract": "Solving optimal control problems (OCPs) of autonomous agents operating under spatial and temporal constraints fast and accurately is essential in applications ranging from eco-driving of autonomous vehicles to quadrotor navigation. However, the nonlinear programs approximating the OCPs are inherently nonconvex due to the coupling between the dynamics and the event timing, and therefore, they are challenging to solve. Most approaches address this challenge by predefining waypoint times or just using nonconvex trajectory optimization, which simplifies the problem but often yields suboptimal solutions. To significantly improve the numerical properties, we propose a formulation with a time-scaling direct multiple shooting scheme that partitions the prediction horizon into segments aligned with characteristic time constraints. Moreover, we develop a fast semidefinite-programming-based convex relaxation that exploits the sparsity pattern of the lifted formulation. Comprehensive simulation studies demonstrate the solution optimality and computational efficiency. Furthermore, real-world experiments on a quadrotor waypoint flight task with constrained open time windows validate the practical applicability of the approach in complex environments.",
      "authors": [
        "Shiying Dong",
        "Zhipeng Shen",
        "Rudolf Reiter",
        "Hailong Huang",
        "Bingzhao Gao",
        "Hong Chen",
        "Wen-Hua Chen"
      ],
      "published": "2026-01-06T14:38:19Z",
      "categories": [
        "cs.RO"
      ],
      "primary_category": "cs.RO",
      "abs_url": "https://arxiv.org/abs/2601.03055v1",
      "pdf_url": "https://arxiv.org/pdf/2601.03055v1.pdf"
    },
    {
      "id": "2601.03044v1",
      "title": "SOP: A Scalable Online Post-Training System for Vision-Language-Action Models",
      "abstract": "Vision-language-action (VLA) models achieve strong generalization through large-scale pre-training, but real-world deployment requires expert-level task proficiency in addition to broad generality. Existing post-training approaches for VLA models are typically offline, single-robot, or task-specific, limiting effective on-policy adaptation and scalable learning from real-world interaction. We introduce a Scalable Online Post-training (SOP) system that enables online, distributed, multi-task post-training of generalist VLA models directly in the physical world. SOP tightly couples execution and learning through a closed-loop architecture in which a fleet of robots continuously streams on-policy experience and human intervention signals to a centralized cloud learner, and asynchronously receives updated policies. This design supports prompt on-policy correction, scales experience collection through parallel deployment, and preserves generality during adaptation. SOP is agnostic to the choice of post-training algorithm; we instantiate it with both interactive imitation learning (HG-DAgger) and reinforcement learning (RECAP). Across a range of real-world manipulation tasks including cloth folding, box assembly, and grocery restocking, we show that SOP substantially improves the performance of large pretrained VLA models while maintaining a single shared policy across tasks. Effective post-training can be achieved within hours of real-world interaction, and performance scales near-linearly with the number of robots in the fleet. These results suggest that tightly coupling online learning with fleet-scale deployment is instrumental to enabling efficient, reliable, and scalable post-training of generalist robot policies in the physical world.",
      "authors": [
        "Mingjie Pan",
        "Siyuan Feng",
        "Qinglin Zhang",
        "Xinchen Li",
        "Jianheng Song",
        "Chendi Qu",
        "Yi Wang",
        "Chuankang Li",
        "Ziyu Xiong",
        "Zhi Chen",
        "Yi Liu",
        "Jianlan Luo"
      ],
      "published": "2026-01-06T14:25:11Z",
      "categories": [
        "cs.RO"
      ],
      "primary_category": "cs.RO",
      "abs_url": "https://arxiv.org/abs/2601.03044v1",
      "pdf_url": "https://arxiv.org/pdf/2601.03044v1.pdf"
    },
    {
      "id": "2601.03040v1",
      "title": "PiDR: Physics-Informed Inertial Dead Reckoning for Autonomous Platforms",
      "abstract": "A fundamental requirement for full autonomy is the ability to sustain accurate navigation in the absence of external data, such as GNSS signals or visual information. In these challenging environments, the platform must rely exclusively on inertial sensors, leading to pure inertial navigation. However, the inherent noise and other error terms of the inertial sensors in such real-world scenarios will cause the navigation solution to drift over time. Although conventional deep-learning models have emerged as a possible approach to inertial navigation, they are inherently black-box in nature. Furthermore, they struggle to learn effectively with limited supervised sensor data and often fail to preserve physical principles. To address these limitations, we propose PiDR, a physics-informed inertial dead-reckoning framework for autonomous platforms in situations of pure inertial navigation. PiDR offers transparency by explicitly integrating inertial navigation principles into the network training process through the physics-informed residual component. PiDR plays a crucial role in mitigating abrupt trajectory deviations even under limited or sparse supervision. We evaluated PiDR on real-world datasets collected by a mobile robot and an autonomous underwater vehicle. We obtained more than 29% positioning improvement in both datasets, demonstrating the ability of PiDR to generalize different platforms operating in various environments and dynamics. Thus, PiDR offers a robust, lightweight, yet effective architecture and can be deployed on resource-constrained platforms, enabling real-time pure inertial navigation in adverse scenarios.",
      "authors": [
        "Arup Kumar Sahoo",
        "Itzik Klein"
      ],
      "published": "2026-01-06T14:19:50Z",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.RO",
      "abs_url": "https://arxiv.org/abs/2601.03040v1",
      "pdf_url": "https://arxiv.org/pdf/2601.03040v1.pdf"
    },
    {
      "id": "2601.03038v1",
      "title": "Validating Generalist Robots with Situation Calculus and STL Falsification",
      "abstract": "Generalist robots are becoming a reality, capable of interpreting natural language instructions and executing diverse operations. However, their validation remains challenging because each task induces its own operational context and correctness specification, exceeding the assumptions of traditional validation methods. We propose a two-layer validation framework that combines abstract reasoning with concrete system falsification. At the abstract layer, situation calculus models the world and derives weakest preconditions, enabling constraint-aware combinatorial testing to systematically generate diverse, semantically valid world-task configurations with controllable coverage strength. At the concrete layer, these configurations are instantiated for simulation-based falsification with STL monitoring. Experiments on tabletop manipulation tasks show that our framework effectively uncovers failure cases in the NVIDIA GR00T controller, demonstrating its promise for validating general-purpose robot autonomy.",
      "authors": [
        "Changwen Li",
        "Rongjie Yan",
        "Chih-Hong Cheng",
        "Jian Zhang"
      ],
      "published": "2026-01-06T14:13:33Z",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "abs_url": "https://arxiv.org/abs/2601.03038v1",
      "pdf_url": "https://arxiv.org/pdf/2601.03038v1.pdf"
    },
    {
      "id": "2601.03037v1",
      "title": "A Bi-directional Adaptive Framework for Agile UAV Landing",
      "abstract": "Autonomous landing on mobile platforms is crucial for extending quadcopter operational flexibility, yet conventional methods are often too inefficient for highly dynamic scenarios. The core limitation lies in the prevalent ``track-then-descend'' paradigm, which treats the platform as a passive target and forces the quadcopter to perform complex, sequential maneuvers. This paper challenges that paradigm by introducing a bi-directional cooperative landing framework that redefines the roles of the vehicle and the platform. The essential innovation is transforming the problem from a single-agent tracking challenge into a coupled system optimization. Our key insight is that the mobile platform is not merely a target, but an active agent in the landing process. It proactively tilts its surface to create an optimal, stable terminal attitude for the approaching quadcopter. This active cooperation fundamentally breaks the sequential model by parallelizing the alignment and descent phases. Concurrently, the quadcopter's planning pipeline focuses on generating a time-optimal and dynamically feasible trajectory that minimizes energy consumption. This bi-directional coordination allows the system to execute the recovery in an agile manner, characterized by aggressive trajectory tracking and rapid state synchronization within transient windows. The framework's effectiveness, validated in dynamic scenarios, significantly improves the efficiency, precision, and robustness of autonomous quadrotor recovery in complex and time-constrained missions.",
      "authors": [
        "Chunhui Zhao",
        "Xirui Kao",
        "Yilin Lu",
        "Yang Lyu"
      ],
      "published": "2026-01-06T14:10:06Z",
      "categories": [
        "cs.RO"
      ],
      "primary_category": "cs.RO",
      "abs_url": "https://arxiv.org/abs/2601.03037v1",
      "pdf_url": "https://arxiv.org/pdf/2601.03037v1.pdf"
    },
    {
      "id": "2601.02994v1",
      "title": "Learning to Act Robustly with View-Invariant Latent Actions",
      "abstract": "Vision-based robotic policies often struggle with even minor viewpoint changes, underscoring the need for view-invariant visual representations. This challenge becomes more pronounced in real-world settings, where viewpoint variability is unavoidable and can significantly disrupt policy performance. Existing methods typically learn invariance from multi-view observations at the scene level, but such approaches rely on visual appearance and fail to incorporate the physical dynamics essential for robust generalization. We propose View-Invariant Latent Action (VILA), which models a latent action capturing transition patterns across trajectories to learn view-invariant representations grounded in physical dynamics. VILA aligns these latent actions across viewpoints using an action-guided objective based on ground-truth action sequences. Experiments in both simulation and the real world show that VILA-based policies generalize effectively to unseen viewpoints and transfer well to new tasks, establishing VILA as a strong pretraining framework that improves robustness and downstream learning performance.",
      "authors": [
        "Youngjoon Jeong",
        "Junha Chun",
        "Taesup Kim"
      ],
      "published": "2026-01-06T13:14:01Z",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.RO",
      "abs_url": "https://arxiv.org/abs/2601.02994v1",
      "pdf_url": "https://arxiv.org/pdf/2601.02994v1.pdf"
    },
    {
      "id": "2601.02948v1",
      "title": "Parameter-Robust MPPI for Safe Online Learning of Unknown Parameters",
      "abstract": "Robots deployed in dynamic environments must remain safe even when key physical parameters are uncertain or change over time. We propose Parameter-Robust Model Predictive Path Integral (PRMPPI) control, a framework that integrates online parameter learning with probabilistic safety constraints. PRMPPI maintains a particle-based belief over parameters via Stein Variational Gradient Descent, evaluates safety constraints using Conformal Prediction, and optimizes both a nominal performance-driven and a safety-focused backup trajectory in parallel. This yields a controller that is cautious at first, improves performance as parameters are learned, and ensures safety throughout. Simulation and hardware experiments demonstrate higher success rates, lower tracking error, and more accurate parameter estimates than baselines.",
      "authors": [
        "Matti Vahs",
        "Jaeyoun Choi",
        "Niklas Schmid",
        "Jana Tumova",
        "Chuchu Fan"
      ],
      "published": "2026-01-06T11:44:11Z",
      "categories": [
        "cs.RO"
      ],
      "primary_category": "cs.RO",
      "abs_url": "https://arxiv.org/abs/2601.02948v1",
      "pdf_url": "https://arxiv.org/pdf/2601.02948v1.pdf"
    },
    {
      "id": "2601.02905v1",
      "title": "LOST-3DSG: Lightweight Open-Vocabulary 3D Scene Graphs with Semantic Tracking in Dynamic Environments",
      "abstract": "Tracking objects that move within dynamic environments is a core challenge in robotics. Recent research has advanced this topic significantly; however, many existing approaches remain inefficient due to their reliance on heavy foundation models. To address this limitation, we propose LOST-3DSG, a lightweight open-vocabulary 3D scene graph designed to track dynamic objects in real-world environments. Our method adopts a semantic approach to entity tracking based on word2vec and sentence embeddings, enabling an open-vocabulary representation while avoiding the necessity of storing dense CLIP visual features. As a result, LOST-3DSG achieves superior performance compared to approaches that rely on high-dimensional visual embeddings. We evaluate our method through qualitative and quantitative experiments conducted in a real 3D environment using a TIAGo robot. The results demonstrate the effectiveness and efficiency of LOST-3DSG in dynamic object tracking. Code and supplementary material are publicly available on the project website at https://lab-rococo-sapienza.github.io/lost-3dsg/.",
      "authors": [
        "Sara Micol Ferraina",
        "Michele Brienza",
        "Francesco Argenziano",
        "Emanuele Musumeci",
        "Vincenzo Suriani",
        "Domenico D. Bloisi",
        "Daniele Nardi"
      ],
      "published": "2026-01-06T10:44:19Z",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "abs_url": "https://arxiv.org/abs/2601.02905v1",
      "pdf_url": "https://arxiv.org/pdf/2601.02905v1.pdf"
    },
    {
      "id": "2601.02873v1",
      "title": "Warm-Starting Collision-Free Model Predictive Control With Object-Centric Diffusion",
      "abstract": "Acting in cluttered environments requires predicting and avoiding collisions while still achieving precise control. Conventional optimization-based controllers can enforce physical constraints, but they struggle to produce feasible solutions quickly when many obstacles are present. Diffusion models can generate diverse trajectories around obstacles, yet prior approaches lacked a general and efficient way to condition them on scene structure. In this paper, we show that combining diffusion-based warm-starting conditioned with a latent object-centric representation of the scene and with a collision-aware model predictive controller (MPC) yields reliable and efficient motion generation under strict time limits. Our approach conditions a diffusion transformer on the system state, task, and surroundings, using an object-centric slot attention mechanism to provide a compact obstacle representation suitable for control. The sampled trajectories are refined by an optimal control problem that enforces rigid-body dynamics and signed-distance collision constraints, producing feasible motions in real time. On benchmark tasks, this hybrid method achieved markedly higher success rates and lower latency than sampling-based planners or either component alone. Real-robot experiments with a torque-controlled Panda confirm reliable and safe execution with MPC.",
      "authors": [
        "Arthur Haffemayer",
        "Alexandre Chapin",
        "Armand Jordana",
        "Krzysztof Wojciechowski",
        "Florent Lamiraux",
        "Nicolas Mansard",
        "Vladimir Petrik"
      ],
      "published": "2026-01-06T10:02:54Z",
      "categories": [
        "cs.RO"
      ],
      "primary_category": "cs.RO",
      "abs_url": "https://arxiv.org/abs/2601.02873v1",
      "pdf_url": "https://arxiv.org/pdf/2601.02873v1.pdf"
    },
    {
      "id": "2601.02857v1",
      "title": "Soft Responsive Materials Enhance Humanoid Safety",
      "abstract": "Humanoid robots are envisioned as general-purpose platforms in human-centered environments, yet their deployment is limited by vulnerability to falls and the risks posed by rigid metal-plastic structures to people and surroundings. We introduce a soft-rigid co-design framework that leverages non-Newtonian fluid-based soft responsive materials to enhance humanoid safety. The material remains compliant during normal interaction but rapidly stiffens under impact, absorbing and dissipating fall-induced forces. Physics-based simulations guide protector placement and thickness and enable learning of active fall policies. Applied to a 42 kg life-size humanoid, the protector markedly reduces peak impact and allows repeated falls without hardware damage, including drops from 3 m and tumbles down long staircases. Across diverse scenarios, the approach improves robot robustness and environmental safety. By uniting responsive materials, structural co-design, and learning-based control, this work advances interact-safe, industry-ready humanoid robots.",
      "authors": [
        "Chunzheng Wang",
        "Yiyuan Zhang",
        "Annan Tang",
        "Ziqiu Zeng",
        "Haoran Chen",
        "Quan Gao",
        "Zixuan Zhuang",
        "Boyu Li",
        "Zhilin Xiong",
        "Aoqian Zhang",
        "Ce Hao",
        "Siyuan Luo",
        "Tongyang Zhao",
        "Cecilia Laschi",
        "Fan Shi"
      ],
      "published": "2026-01-06T09:36:44Z",
      "categories": [
        "cs.RO"
      ],
      "primary_category": "cs.RO",
      "abs_url": "https://arxiv.org/abs/2601.02857v1",
      "pdf_url": "https://arxiv.org/pdf/2601.02857v1.pdf"
    },
    {
      "id": "2601.02798v1",
      "title": "Reinforcement Learning for Follow-the-Leader Robotic Endoscopic Navigation via Synthetic Data",
      "abstract": "Autonomous navigation is crucial for both medical and industrial endoscopic robots, enabling safe and efficient exploration of narrow tubular environments without continuous human intervention, where avoiding contact with the inner walls has been a longstanding challenge for prior approaches. We present a follow-the-leader endoscopic robot based on a flexible continuum structure designed to minimize contact between the endoscope body and intestinal walls, thereby reducing patient discomfort. To achieve this objective, we propose a vision-based deep reinforcement learning framework guided by monocular depth estimation. A realistic intestinal simulation environment was constructed in \\textit{NVIDIA Omniverse} to train and evaluate autonomous navigation strategies. Furthermore, thousands of synthetic intraluminal images were generated using NVIDIA Replicator to fine-tune the Depth Anything model, enabling dense three-dimensional perception of the intestinal environment with a single monocular camera. Subsequently, we introduce a geometry-aware reward and penalty mechanism to enable accurate lumen tracking. Compared with the original Depth Anything model, our method improves $δ_{1}$ depth accuracy by 39.2% and reduces the navigation J-index by 0.67 relative to the second-best method, demonstrating the robustness and effectiveness of the proposed approach.",
      "authors": [
        "Sicong Gao",
        "Chen Qian",
        "Laurence Xian",
        "Liao Wu",
        "Maurice Pagnucco",
        "Yang Song"
      ],
      "published": "2026-01-06T08:15:53Z",
      "categories": [
        "cs.RO"
      ],
      "primary_category": "cs.RO",
      "abs_url": "https://arxiv.org/abs/2601.02798v1",
      "pdf_url": "https://arxiv.org/pdf/2601.02798v1.pdf"
    },
    {
      "id": "2601.02778v2",
      "title": "Closing the Reality Gap: Zero-Shot Sim-to-Real Deployment for Dexterous Force-Based Grasping and Manipulation",
      "abstract": "Human-like dexterous hands with multiple fingers offer human-level manipulation capabilities, but training control policies that can directly deploy on real hardware remains difficult due to contact-rich physics and imperfect actuation. We close this gap with a practical sim-to-real reinforcement learning (RL) framework that utilizes dense tactile feedback combined with joint torque sensing to explicitly regulate physical interactions. To enable effective sim-to-real transfer, we introduce (i) a computationally fast tactile simulation that computes distances between dense virtual tactile units and the object via parallel forward kinematics, providing high-rate, high-resolution touch signals needed by RL; (ii) a current-to-torque calibration that eliminates the need for torque sensors on dexterous hands by mapping motor current to joint torque; and (iii) actuator dynamics modeling to bridge the actuation gaps with randomization of non-ideal effects such as backlash, torque-speed saturation. Using an asymmetric actor-critic PPO pipeline trained entirely in simulation, our policies deploy directly to a five-finger hand. The resulting policies demonstrated two essential skills: (1) command-based, controllable grasp force tracking, and (2) reorientation of objects in the hand, both of which were robustly executed without fine-tuning on the robot. By combining tactile and torque in the observation space with effective sensing/actuation modeling, our system provides a practical solution to achieve reliable dexterous manipulation. To our knowledge, this is the first demonstration of controllable grasping on a multi-finger dexterous hand trained entirely in simulation and transferred zero-shot on real hardware.",
      "authors": [
        "Zhe Zhao",
        "Haoyu Dong",
        "Zhengmao He",
        "Yang Li",
        "Xinyu Yi",
        "Zhibin Li"
      ],
      "published": "2026-01-06T07:26:39Z",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "abs_url": "https://arxiv.org/abs/2601.02778v2",
      "pdf_url": "https://arxiv.org/pdf/2601.02778v2.pdf"
    },
    {
      "id": "2601.02777v1",
      "title": "M-SEVIQ: A Multi-band Stereo Event Visual-Inertial Quadruped-based Dataset for Perception under Rapid Motion and Challenging Illumination",
      "abstract": "Agile locomotion in legged robots poses significant challenges for visual perception. Traditional frame-based cameras often fail in these scenarios for producing blurred images, particularly under low-light conditions. In contrast, event cameras capture changes in brightness asynchronously, offering low latency, high temporal resolution, and high dynamic range. These advantages make them suitable for robust perception during rapid motion and under challenging illumination. However, existing event camera datasets exhibit limitations in stereo configurations and multi-band sensing domains under various illumination conditions. To address this gap, we present M-SEVIQ, a multi-band stereo event visual and inertial quadruped dataset collected using a Unitree Go2 equipped with stereo event cameras, a frame-based camera, an inertial measurement unit (IMU), and joint encoders. This dataset contains more than 30 real-world sequences captured across different velocity levels, illumination wavelengths, and lighting conditions. In addition, comprehensive calibration data, including intrinsic, extrinsic, and temporal alignments, are provided to facilitate accurate sensor fusion and benchmarking. Our M-SEVIQ can be used to support research in agile robot perception, sensor fusion, semantic segmentation and multi-modal vision in challenging environments.",
      "authors": [
        "Jingcheng Cao",
        "Chaoran Xiong",
        "Jianmin Song",
        "Shang Yan",
        "Jiachen Liu",
        "Ling Pei"
      ],
      "published": "2026-01-06T07:23:34Z",
      "categories": [
        "cs.RO"
      ],
      "primary_category": "cs.RO",
      "abs_url": "https://arxiv.org/abs/2601.02777v1",
      "pdf_url": "https://arxiv.org/pdf/2601.02777v1.pdf"
    },
    {
      "id": "2601.02766v1",
      "title": "Advancing Assistive Robotics: Multi-Modal Navigation and Biophysical Monitoring for Next-Generation Wheelchairs",
      "abstract": "Assistive electric-powered wheelchairs (EPWs) have become essential mobility aids for people with disabilities such as amyotrophic lateral sclerosis (ALS), post-stroke hemiplegia, and dementia-related mobility impairment. This work presents a novel multi-modal EPW control system designed to prioritize patient needs while allowing seamless switching between control modes. Four complementary interfaces, namely joystick, speech, hand gesture, and electrooculography (EOG), are integrated with a continuous vital sign monitoring framework measuring heart rate variability, oxygen saturation (SpO2), and skin temperature. This combination enables greater patient independence while allowing caregivers to maintain real-time supervision and early intervention capability.\n  Two-point calibration of the biophysical sensors against clinical reference devices resulted in root mean square errors of at most 2 bpm for heart rate, 0.5 degree Celsius for skin temperature, and 1 percent for SpO2. Experimental evaluation involved twenty participants with mobility impairments executing a total of 500 indoor navigation commands. The achieved command recognition accuracies were 99 percent for joystick control, 97 percent plus or minus 2 percent for speech, and 95 percent plus or minus 3 percent for hand gesture, with an average closed-loop latency of 20 plus or minus 0.5 milliseconds. Caregivers receive real-time alerts through an Android application following encrypted cloud transmission of physiological data. By integrating multi-modal mobility control with cloud-enabled health monitoring and reporting latency and energy budgets, the proposed prototype addresses key challenges in assistive robotics, contributes toward compliance with ISO 7176-31 and IEC 80601-2-78 safety standards, and establishes a foundation for future adaptive machine learning enhancements.",
      "authors": [
        "Md. Anowar Hossain",
        "Mohd. Ehsanul Hoque"
      ],
      "published": "2026-01-06T06:58:26Z",
      "categories": [
        "cs.RO",
        "cs.AR"
      ],
      "primary_category": "cs.RO",
      "abs_url": "https://arxiv.org/abs/2601.02766v1",
      "pdf_url": "https://arxiv.org/pdf/2601.02766v1.pdf"
    },
    {
      "id": "2601.02762v1",
      "title": "Unified Meta-Representation and Feedback Calibration for General Disturbance Estimation",
      "abstract": "Precise control in modern robotic applications is always an open issue due to unknown time-varying disturbances. Existing meta-learning-based approaches require a shared representation of environmental structures, which lack flexibility for realistic non-structural disturbances. Besides, representation error and the distribution shifts can lead to heavy degradation in prediction accuracy. This work presents a generalizable disturbance estimation framework that builds on meta-learning and feedback-calibrated online adaptation. By extracting features from a finite time window of past observations, a unified representation that effectively captures general non-structural disturbances can be learned without predefined structural assumptions. The online adaptation process is subsequently calibrated by a state-feedback mechanism to attenuate the learning residual originating from the representation and generalizability limitations. Theoretical analysis shows that simultaneous convergence of both the online learning error and the disturbance estimation error can be achieved. Through the unified meta-representation, our framework effectively estimates multiple rapidly changing disturbances, as demonstrated by quadrotor flight experiments. See the project page for video, supplementary material and code: https://nonstructural-metalearn.github.io.",
      "authors": [
        "Zihan Yang",
        "Jindou Jia",
        "Meng Wang",
        "Yuhang Liu",
        "Kexin Guo",
        "Xiang Yu"
      ],
      "published": "2026-01-06T06:53:19Z",
      "categories": [
        "cs.RO"
      ],
      "primary_category": "cs.RO",
      "abs_url": "https://arxiv.org/abs/2601.02762v1",
      "pdf_url": "https://arxiv.org/pdf/2601.02762v1.pdf"
    },
    {
      "id": "2601.02759v1",
      "title": "Towards Zero-Shot Point Cloud Registration Across Diverse Scales, Scenes, and Sensor Setups",
      "abstract": "Some deep learning-based point cloud registration methods struggle with zero-shot generalization, often requiring dataset-specific hyperparameter tuning or retraining for new environments. We identify three critical limitations: (a) fixed user-defined parameters (e.g., voxel size, search radius) that fail to generalize across varying scales, (b) learned keypoint detectors exhibit poor cross-domain transferability, and (c) absolute coordinates amplify scale mismatches between datasets. To address these three issues, we present BUFFER-X, a training-free registration framework that achieves zero-shot generalization through: (a) geometric bootstrapping for automatic hyperparameter estimation, (b) distribution-aware farthest point sampling to replace learned detectors, and (c) patch-level coordinate normalization to ensure scale consistency. Our approach employs hierarchical multi-scale matching to extract correspondences across local, middle, and global receptive fields, enabling robust registration in diverse environments. For efficiency-critical applications, we introduce BUFFER-X-Lite, which reduces total computation time by 43% (relative to BUFFER-X) through early exit strategies and fast pose solvers while preserving accuracy. We evaluate on a comprehensive benchmark comprising 12 datasets spanning object-scale, indoor, and outdoor scenes, including cross-sensor registration between heterogeneous LiDAR configurations. Results demonstrate that our approach generalizes effectively without manual tuning or prior knowledge of test domains. Code: https://github.com/MIT-SPARK/BUFFER-X.",
      "authors": [
        "Hyungtae Lim",
        "Minkyun Seo",
        "Luca Carlone",
        "Jaesik Park"
      ],
      "published": "2026-01-06T06:51:24Z",
      "categories": [
        "cs.CV",
        "cs.RO"
      ],
      "primary_category": "cs.CV",
      "abs_url": "https://arxiv.org/abs/2601.02759v1",
      "pdf_url": "https://arxiv.org/pdf/2601.02759v1.pdf"
    },
    {
      "id": "2601.02738v1",
      "title": "Optimizing Control-Friendly Trajectories with Self-Supervised Residual Learning",
      "abstract": "Real-world physics can only be analytically modeled with a certain level of precision for modern intricate robotic systems. As a result, tracking aggressive trajectories accurately could be challenging due to the existence of residual physics during controller synthesis. This paper presents a self-supervised residual learning and trajectory optimization framework to address the aforementioned challenges. At first, unknown dynamic effects on the closed-loop model are learned and treated as residuals of the nominal dynamics, jointly forming a hybrid model. We show that learning with analytic gradients can be achieved using only trajectory-level data while enjoying accurate long-horizon prediction with an arbitrary integration step size. Subsequently, a trajectory optimizer is developed to compute the optimal reference trajectory with the residual physics along it minimized. It ends up with trajectories that are friendly to the following control level. The agile flight of quadrotors illustrates that by utilizing the hybrid dynamics, the proposed optimizer outputs aggressive motions that can be precisely tracked.",
      "authors": [
        "Kexin Guo",
        "Zihan Yang",
        "Yuhang Liu",
        "Jindou Jia",
        "Xiang Yu"
      ],
      "published": "2026-01-06T06:01:58Z",
      "categories": [
        "cs.RO"
      ],
      "primary_category": "cs.RO",
      "abs_url": "https://arxiv.org/abs/2601.02738v1",
      "pdf_url": "https://arxiv.org/pdf/2601.02738v1.pdf"
    },
    {
      "id": "2601.02723v1",
      "title": "Loop Closure using AnyLoc Visual Place Recognition in DPV-SLAM",
      "abstract": "Loop closure is crucial for maintaining the accuracy and consistency of visual SLAM. We propose a method to improve loop closure performance in DPV-SLAM. Our approach integrates AnyLoc, a learning-based visual place recognition technique, as a replacement for the classical Bag of Visual Words (BoVW) loop detection method. In contrast to BoVW, which relies on handcrafted features, AnyLoc utilizes deep feature representations, enabling more robust image retrieval across diverse viewpoints and lighting conditions. Furthermore, we propose an adaptive mechanism that dynamically adjusts similarity threshold based on environmental conditions, removing the need for manual tuning. Experiments on both indoor and outdoor datasets demonstrate that our method significantly outperforms the original DPV-SLAM in terms of loop closure accuracy and robustness. The proposed method offers a practical and scalable solution for enhancing loop closure performance in modern SLAM systems.",
      "authors": [
        "Wenzheng Zhang",
        "Kazuki Adachi",
        "Yoshitaka Hara",
        "Sousuke Nakamura"
      ],
      "published": "2026-01-06T05:32:08Z",
      "categories": [
        "cs.RO",
        "cs.CV"
      ],
      "primary_category": "cs.RO",
      "abs_url": "https://arxiv.org/abs/2601.02723v1",
      "pdf_url": "https://arxiv.org/pdf/2601.02723v1.pdf"
    },
    {
      "id": "2601.02704v1",
      "title": "Analysis of Various Manipulator Configurations Based on Multi-Objective Black-Box Optimization",
      "abstract": "Various 6-degree-of-freedom (DOF) and 7-DOF manipulators have been developed to date. Over a long history, their joint configurations and link length ratios have been determined empirically. In recent years, the development of robotic foundation models has become increasingly active, leading to the continuous proposal of various manipulators to support these models. However, none of these manipulators share exactly the same structure, as the order of joints and the ratio of link lengths differ among robots. Therefore, in order to discuss the optimal structure of a manipulator, we performed multi-objective optimization from the perspectives of end-effector reachability and joint torque. We analyze where existing manipulator structures stand within the sampling results of the optimization and provide insights for future manipulator design.",
      "authors": [
        "Kento Kawaharazuka",
        "Keita Yoneda",
        "Takahiro Hattori",
        "Shintaro Inoue",
        "Kei Okada"
      ],
      "published": "2026-01-06T04:29:40Z",
      "categories": [
        "cs.RO"
      ],
      "primary_category": "cs.RO",
      "abs_url": "https://arxiv.org/abs/2601.02704v1",
      "pdf_url": "https://arxiv.org/pdf/2601.02704v1.pdf"
    },
    {
      "id": "2601.02686v1",
      "title": "Learning to Nudge: A Scalable Barrier Function Framework for Safe Robot Interaction in Dense Clutter",
      "abstract": "Robots operating in everyday environments must navigate and manipulate within densely cluttered spaces, where physical contact with surrounding objects is unavoidable. Traditional safety frameworks treat contact as unsafe, restricting robots to collision avoidance and limiting their ability to function in dense, everyday settings. As the number of objects grows, model-based approaches for safe manipulation become computationally intractable; meanwhile, learned methods typically tie safety to the task at hand, making them hard to transfer to new tasks without retraining. In this work we introduce Dense Contact Barrier Functions(DCBF). Our approach bypasses the computational complexity of explicitly modeling multi-object dynamics by instead learning a composable, object-centric function that implicitly captures the safety constraints arising from physical interactions. Trained offline on interactions with a few objects, the learned DCBFcomposes across arbitrary object sets at runtime, producing a single global safety filter that scales linearly and transfers across tasks without retraining. We validate our approach through simulated experiments in dense clutter, demonstrating its ability to enable collision-free navigation and safe, contact-rich interaction in suitable settings.",
      "authors": [
        "Haixin Jin",
        "Nikhil Uday Shinde",
        "Soofiyan Atar",
        "Hongzhan Yu",
        "Dylan Hirsch",
        "Sicun Gao",
        "Michael C. Yip",
        "Sylvia Herbert"
      ],
      "published": "2026-01-06T03:42:09Z",
      "categories": [
        "cs.RO"
      ],
      "primary_category": "cs.RO",
      "abs_url": "https://arxiv.org/abs/2601.02686v1",
      "pdf_url": "https://arxiv.org/pdf/2601.02686v1.pdf"
    },
    {
      "id": "2601.03302v1",
      "title": "CageDroneRF: A Large-Scale RF Benchmark and Toolkit for Drone Perception",
      "abstract": "We present CageDroneRF (CDRF), a large-scale benchmark for Radio-Frequency (RF) drone detection and identification built from real-world captures and systematically generated synthetic variants. CDRF addresses the scarcity and limited diversity of existing RF datasets by coupling extensive raw recordings with a principled augmentation pipeline that (i) precisely controls Signal-to-Noise Ratio (SNR), (ii) injects interfering emitters, and (iii) applies frequency shifts with label-consistent bounding-box transformations for detection. This dataset spans a wide range of contemporary drone models, many unavailable in current public datasets, and acquisition conditions, derived from data collected at the Rowan University campus and within a controlled RF-cage facility. CDRF is released with interoperable open-source tools for data generation, preprocessing, augmentation, and evaluation that also operate on existing public benchmarks. CDRF enables standardized benchmarking for classification, open-set recognition, and object detection, supporting rigorous comparisons and reproducible pipelines. By releasing this comprehensive benchmark and tooling, CDRF aims to accelerate progress toward robust, generalizable RF perception models.",
      "authors": [
        "Mohammad Rostami",
        "Atik Faysal",
        "Hongtao Xia",
        "Hadi Kasasbeh",
        "Ziang Gao",
        "Huaxia Wang"
      ],
      "published": "2026-01-06T03:39:59Z",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.RO"
      ],
      "primary_category": "cs.CV",
      "abs_url": "https://arxiv.org/abs/2601.03302v1",
      "pdf_url": "https://arxiv.org/pdf/2601.03302v1.pdf"
    },
    {
      "id": "2601.02649v1",
      "title": "Effective Online 3D Bin Packing with Lookahead Parcels Using Monte Carlo Tree Search",
      "abstract": "Online 3D Bin Packing (3D-BP) with robotic arms is crucial for reducing transportation and labor costs in modern logistics. While Deep Reinforcement Learning (DRL) has shown strong performance, it often fails to adapt to real-world short-term distribution shifts, which arise as different batches of goods arrive sequentially, causing performance drops. We argue that the short-term lookahead information available in modern logistics systems is key to mitigating this issue, especially during distribution shifts. We formulate online 3D-BP with lookahead parcels as a Model Predictive Control (MPC) problem and adapt the Monte Carlo Tree Search (MCTS) framework to solve it. Our framework employs a dynamic exploration prior that automatically balances a learned RL policy and a robust random policy based on the lookahead characteristics. Additionally, we design an auxiliary reward to penalize long-term spatial waste from individual placements. Extensive experiments on real-world datasets show that our method consistently outperforms state-of-the-art baselines, achieving over 10\\% gains under distributional shifts, 4\\% average improvement in online deployment, and up to more than 8\\% in the best case--demonstrating the effectiveness of our framework.",
      "authors": [
        "Jiangyi Fang",
        "Bowen Zhou",
        "Haotian Wang",
        "Xin Zhu",
        "Leye Wang"
      ],
      "published": "2026-01-06T01:51:11Z",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "abs_url": "https://arxiv.org/abs/2601.02649v1",
      "pdf_url": "https://arxiv.org/pdf/2601.02649v1.pdf"
    },
    {
      "id": "2601.02645v1",
      "title": "Making Infeasible Tasks Feasible: Planning to Reconfigure Disconnected 3D Environments with Movable Objects",
      "abstract": "Several planners have been developed to compute dynamically feasible, collision-free robot paths from an initial to a goal configuration. A key assumption in these works is that the goal region is reachable; an assumption that often fails in practice when environments are disconnected. Motivated by this limitation, we consider known 3D environments comprising objects, also called blocks, that form distinct navigable support surfaces (planes), and that are either non-movable (e.g., tables) or movable (e.g., boxes). These surfaces may be mutually disconnected due to height differences, holes, or lateral separations. Our focus is on tasks where the robot must reach a goal region residing on an elevated plane that is unreachable. Rather than declaring such tasks infeasible, an effective strategy is to enable the robot to interact with the environment, rearranging movable objects to create new traversable connections; a problem known as Navigation Among Movable Objects (NAMO). Existing NAMO planners typically address 2D environments, where obstacles are pushed aside to clear a path. These methods cannot directly handle the considered 3D setting; in such cases, obstacles must be placed strategically to bridge these physical disconnections. We address this challenge by developing BRiDGE (Block-based Reconfiguration in Disconnected 3D Geometric Environments), a sampling-based planner that incrementally builds trees over robot and object configurations to compute feasible plans specifying which objects to move, where to place them, and in what order, while accounting for a limited number of movable objects. To accelerate planning, we introduce non-uniform sampling strategies. We show that our method is probabilistically complete and we provide extensive numerical and hardware experiments validating its effectiveness.",
      "authors": [
        "Samarth Kalluraya",
        "Yiannis Kantaros"
      ],
      "published": "2026-01-06T01:36:09Z",
      "categories": [
        "cs.RO"
      ],
      "primary_category": "cs.RO",
      "abs_url": "https://arxiv.org/abs/2601.02645v1",
      "pdf_url": "https://arxiv.org/pdf/2601.02645v1.pdf"
    },
    {
      "id": "2601.02560v1",
      "title": "AMC26: High-performance DOb for robust position control",
      "abstract": "This paper presents a new HPDOb that significantly improves disturbance estimation accuracy and robustness in motion control systems, surpassing the capabilities of conventional DObs. The proposed observer is analysed and synthesised in the discrete-time domain, providing a realistic representation of their dynamic behaviour and enabling enhanced controller design for practical applications. The core contribution of the HPDOb is a novel synthesis method that incorporates higher-order truncation error dynamics into disturbance estimation. Unlike conventional DObs, which are limited to zero-order truncation error, the HPDOb achieves first-order truncation error, yielding markedly improved estimation accuracy and robustness against disturbances in motion control systems. Simulation and experiments verify the stability and performance of HPDOb.",
      "authors": [
        "Emre Sariyildiz"
      ],
      "published": "2026-01-05T21:24:32Z",
      "categories": [
        "eess.SY",
        "cs.RO"
      ],
      "primary_category": "eess.SY",
      "abs_url": "https://arxiv.org/abs/2601.02560v1",
      "pdf_url": "https://arxiv.org/pdf/2601.02560v1.pdf"
    },
    {
      "id": "2601.02505v1",
      "title": "Learning and Optimizing the Efficacy of Spatio-Temporal Task Allocation under Temporal and Resource Constraints",
      "abstract": "Complex multi-robot missions often require heterogeneous teams to jointly optimize task allocation, scheduling, and path planning to improve team performance under strict constraints. We formalize these complexities into a new class of problems, dubbed Spatio-Temporal Efficacy-optimized Allocation for Multi-robot systems (STEAM). STEAM builds upon trait-based frameworks that model robots using their capabilities (e.g., payload and speed), but goes beyond the typical binary success-failure model by explicitly modeling the efficacy of allocations as trait-efficacy maps. These maps encode how the aggregated capabilities assigned to a task determine performance. Further, STEAM accommodates spatio-temporal constraints, including a user-specified time budget (i.e., maximum makespan). To solve STEAM problems, we contribute a novel algorithm named Efficacy-optimized Incremental Task Allocation Graph Search (E-ITAGS) that simultaneously optimizes task performance and respects time budgets by interleaving task allocation, scheduling, and path planning. Motivated by the fact that trait-efficacy maps are difficult, if not impossible, to specify, E-ITAGS efficiently learns them using a realizability-aware active learning module. Our approach is realizability-aware since it explicitly accounts for the fact that not all combinations of traits are realizable by the robots available during learning. Further, we derive experimentally-validated bounds on E-ITAGS' suboptimality with respect to efficacy. Detailed numerical simulations and experiments using an emergency response domain demonstrate that E-ITAGS generates allocations of higher efficacy compared to baselines, while respecting resource and spatio-temporal constraints. We also show that our active learning approach is sample efficient and establishes a principled tradeoff between data and computational efficiency.",
      "authors": [
        "Jiazhen Liu",
        "Glen Neville",
        "Jinwoo Park",
        "Sonia Chernova",
        "Harish Ravichandar"
      ],
      "published": "2026-01-05T19:21:56Z",
      "categories": [
        "cs.RO"
      ],
      "primary_category": "cs.RO",
      "abs_url": "https://arxiv.org/abs/2601.02505v1",
      "pdf_url": "https://arxiv.org/pdf/2601.02505v1.pdf"
    },
    {
      "id": "2601.02456v1",
      "title": "InternVLA-A1: Unifying Understanding, Generation and Action for Robotic Manipulation",
      "abstract": "Prevalent Vision-Language-Action (VLA) models are typically built upon Multimodal Large Language Models (MLLMs) and demonstrate exceptional proficiency in semantic understanding, but they inherently lack the capability to deduce physical world dynamics. Consequently, recent approaches have shifted toward World Models, typically formulated via video prediction; however, these methods often suffer from a lack of semantic grounding and exhibit brittleness when handling prediction errors. To synergize semantic understanding with dynamic predictive capabilities, we present InternVLA-A1. This model employs a unified Mixture-of-Transformers architecture, coordinating three experts for scene understanding, visual foresight generation, and action execution. These components interact seamlessly through a unified masked self-attention mechanism. Building upon InternVL3 and Qwen3-VL, we instantiate InternVLA-A1 at 2B and 3B parameter scales. We pre-train these models on hybrid synthetic-real datasets spanning InternData-A1 and Agibot-World, covering over 533M frames. This hybrid training strategy effectively harnesses the diversity of synthetic simulation data while minimizing the sim-to-real gap. We evaluated InternVLA-A1 across 12 real-world robotic tasks and simulation benchmark. It significantly outperforms leading models like pi0 and GR00T N1.5, achieving a 14.5\\% improvement in daily tasks and a 40\\%-73.3\\% boost in dynamic settings, such as conveyor belt sorting.",
      "authors": [
        "Junhao Cai",
        "Zetao Cai",
        "Jiafei Cao",
        "Yilun Chen",
        "Zeyu He",
        "Lei Jiang",
        "Hang Li",
        "Hengjie Li",
        "Yang Li",
        "Yufei Liu",
        "Yanan Lu",
        "Qi Lv",
        "Haoxiang Ma",
        "Jiangmiao Pang",
        "Yu Qiao",
        "Zherui Qiu",
        "Yanqing Shen",
        "Xu Shi",
        "Yang Tian",
        "Bolun Wang",
        "Hanqing Wang",
        "Jiaheng Wang",
        "Tai Wang",
        "Xueyuan Wei",
        "Chao Wu",
        "Yiman Xie",
        "Boyang Xing",
        "Yuqiang Yang",
        "Yuyin Yang",
        "Qiaojun Yu",
        "Feng Yuan",
        "Jia Zeng",
        "Jingjing Zhang",
        "Shenghan Zhang",
        "Shi Zhang",
        "Zhuoma Zhaxi",
        "Bowen Zhou",
        "Yuanzhen Zhou",
        "Yunsong Zhou",
        "Hongrui Zhu",
        "Yangkun Zhu",
        "Yuchen Zhu"
      ],
      "published": "2026-01-05T18:54:29Z",
      "categories": [
        "cs.RO"
      ],
      "primary_category": "cs.RO",
      "abs_url": "https://arxiv.org/abs/2601.02456v1",
      "pdf_url": "https://arxiv.org/pdf/2601.02456v1.pdf"
    },
    {
      "id": "2601.02295v1",
      "title": "CycleVLA: Proactive Self-Correcting Vision-Language-Action Models via Subtask Backtracking and Minimum Bayes Risk Decoding",
      "abstract": "Current work on robot failure detection and correction typically operate in a post hoc manner, analyzing errors and applying corrections only after failures occur. This work introduces CycleVLA, a system that equips Vision-Language-Action models (VLAs) with proactive self-correction, the capability to anticipate incipient failures and recover before they fully manifest during execution. CycleVLA achieves this by integrating a progress-aware VLA that flags critical subtask transition points where failures most frequently occur, a VLM-based failure predictor and planner that triggers subtask backtracking upon predicted failure, and a test-time scaling strategy based on Minimum Bayes Risk (MBR) decoding to improve retry success after backtracking. Extensive experiments show that CycleVLA improves performance for both well-trained and under-trained VLAs, and that MBR serves as an effective zero-shot test-time scaling strategy for VLAs. Project Page: https://dannymcy.github.io/cyclevla/",
      "authors": [
        "Chenyang Ma",
        "Guangyu Yang",
        "Kai Lu",
        "Shitong Xu",
        "Bill Byrne",
        "Niki Trigoni",
        "Andrew Markham"
      ],
      "published": "2026-01-05T17:31:01Z",
      "categories": [
        "cs.RO"
      ],
      "primary_category": "cs.RO",
      "abs_url": "https://arxiv.org/abs/2601.02295v1",
      "pdf_url": "https://arxiv.org/pdf/2601.02295v1.pdf"
    }
  ]
}