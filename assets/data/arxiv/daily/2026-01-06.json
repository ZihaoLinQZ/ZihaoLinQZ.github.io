{
  "date": "2026-01-06",
  "fetched_at": "2026-01-06T06:09:32.565517+00:00",
  "categories": [
    "cs.CV",
    "cs.CL",
    "cs.LG",
    "cs.AI",
    "cs.RO"
  ],
  "total_count": 409,
  "papers": [
    {
      "id": "2601.02360v1",
      "title": "Heterogeneous Low-Bandwidth Pre-Training of LLMs",
      "abstract": "Pre-training large language models (LLMs) increasingly requires distributed compute, yet bandwidth constraints make it difficult to scale beyond well-provisioned datacenters-especially when model parallelism forces frequent, large inter-device communications. We study whether SparseLoCo, a low-communication data parallel method based on infrequent synchronization and sparse pseudo-gradient exchange, can be combined with low-bandwidth pipeline model parallelism via activation and activation-gradient compression. We introduce a heterogeneous distributed training framework where some participants host full replicas on high-bandwidth interconnects, while resource-limited participants are grouped to jointly instantiate a replica using pipeline parallelism with subspace-projected inter-stage communication. To make the recently introduced subspace pipeline compression compatible with SparseLoCo, we study a number of adaptations. Across large-scale language modeling experiments (178M-1B parameters) on standard pretraining corpora, we find that activation compression composes with SparseLoCo at modest cost, while selective (heterogeneous) compression consistently improves the loss-communication tradeoff relative to compressing all replicas-especially at aggressive compression ratios. These results suggest a practical path to incorporating low-bandwidth model parallelism and heterogeneous participants into LLM pre-training.",
      "authors": [
        "Yazan Obeidi",
        "Amir Sarfi",
        "Joel Lidin",
        "Paul Janson",
        "Eugene Belilovsky"
      ],
      "published": "2026-01-05T18:59:57Z",
      "categories": [
        "cs.LG"
      ],
      "primary_category": "cs.LG",
      "abs_url": "https://arxiv.org/abs/2601.02360v1",
      "pdf_url": "https://arxiv.org/pdf/2601.02360v1.pdf"
    },
    {
      "id": "2601.02359v1",
      "title": "ExposeAnyone: Personalized Audio-to-Expression Diffusion Models Are Robust Zero-Shot Face Forgery Detectors",
      "abstract": "Detecting unknown deepfake manipulations remains one of the most challenging problems in face forgery detection. Current state-of-the-art approaches fail to generalize to unseen manipulations, as they primarily rely on supervised training with existing deepfakes or pseudo-fakes, which leads to overfitting to specific forgery patterns. In contrast, self-supervised methods offer greater potential for generalization, but existing work struggles to learn discriminative representations only from self-supervision. In this paper, we propose ExposeAnyone, a fully self-supervised approach based on a diffusion model that generates expression sequences from audio. The key idea is, once the model is personalized to specific subjects using reference sets, it can compute the identity distances between suspected videos and personalized subjects via diffusion reconstruction errors, enabling person-of-interest face forgery detection. Extensive experiments demonstrate that 1) our method outperforms the previous state-of-the-art method by 4.22 percentage points in the average AUC on DF-TIMIT, DFDCP, KoDF, and IDForge datasets, 2) our model is also capable of detecting Sora2-generated videos, where the previous approaches perform poorly, and 3) our method is highly robust to corruptions such as blur and compression, highlighting the applicability in real-world face forgery detection.",
      "authors": [
        "Kaede Shiohara",
        "Toshihiko Yamasaki",
        "Vladislav Golyanik"
      ],
      "published": "2026-01-05T18:59:54Z",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "abs_url": "https://arxiv.org/abs/2601.02359v1",
      "pdf_url": "https://arxiv.org/pdf/2601.02359v1.pdf"
    },
    {
      "id": "2601.02358v1",
      "title": "VINO: A Unified Visual Generator with Interleaved OmniModal Context",
      "abstract": "We present VINO, a unified visual generator that performs image and video generation and editing within a single framework. Instead of relying on task-specific models or independent modules for each modality, VINO uses a shared diffusion backbone that conditions on text, images and videos, enabling a broad range of visual creation and editing tasks under one model. Specifically, VINO couples a vision-language model (VLM) with a Multimodal Diffusion Transformer (MMDiT), where multimodal inputs are encoded as interleaved conditioning tokens, and then used to guide the diffusion process. This design supports multi-reference grounding, long-form instruction following, and coherent identity preservation across static and dynamic content, while avoiding modality-specific architectural components. To train such a unified system, we introduce a multi-stage training pipeline that progressively expands a video generation base model into a unified, multi-task generator capable of both image and video input and output. Across diverse generation and editing benchmarks, VINO demonstrates strong visual quality, faithful instruction following, improved reference and attribute preservation, and more controllable multi-identity edits. Our results highlight a practical path toward scalable unified visual generation, and the promise of interleaved, in-context computation as a foundation for general-purpose visual creation.",
      "authors": [
        "Junyi Chen",
        "Tong He",
        "Zhoujie Fu",
        "Pengfei Wan",
        "Kun Gai",
        "Weicai Ye"
      ],
      "published": "2026-01-05T18:56:34Z",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "abs_url": "https://arxiv.org/abs/2601.02358v1",
      "pdf_url": "https://arxiv.org/pdf/2601.02358v1.pdf"
    },
    {
      "id": "2601.02357v1",
      "title": "DARC: Drum accompaniment generation with fine-grained rhythm control",
      "abstract": "In music creation, rapid prototyping is essential for exploring and refining ideas, yet existing generative tools often fall short when users require both structural control and stylistic flexibility. Prior approaches in stem-to-stem generation can condition on other musical stems but offer limited control over rhythm, and timbre-transfer methods allow users to specify specific rhythms, but cannot condition on musical context. We introduce DARC, a generative drum accompaniment model that conditions both on musical context from other stems and explicit rhythm prompts such as beatboxing or tapping tracks. Using parameter-efficient fine-tuning, we augment STAGE, a state-of-the-art drum stem generator, with fine-grained rhythm control while maintaining musical context awareness.",
      "authors": [
        "Trey Brosnan"
      ],
      "published": "2026-01-05T18:55:43Z",
      "categories": [
        "cs.SD",
        "cs.AI",
        "eess.AS"
      ],
      "primary_category": "cs.SD",
      "abs_url": "https://arxiv.org/abs/2601.02357v1",
      "pdf_url": "https://arxiv.org/pdf/2601.02357v1.pdf"
    },
    {
      "id": "2601.02356v1",
      "title": "Talk2Move: Reinforcement Learning for Text-Instructed Object-Level Geometric Transformation in Scenes",
      "abstract": "We introduce Talk2Move, a reinforcement learning (RL) based diffusion framework for text-instructed spatial transformation of objects within scenes. Spatially manipulating objects in a scene through natural language poses a challenge for multimodal generation systems. While existing text-based manipulation methods can adjust appearance or style, they struggle to perform object-level geometric transformations-such as translating, rotating, or resizing objects-due to scarce paired supervision and pixel-level optimization limits. Talk2Move employs Group Relative Policy Optimization (GRPO) to explore geometric actions through diverse rollouts generated from input images and lightweight textual variations, removing the need for costly paired data. A spatial reward guided model aligns geometric transformations with linguistic description, while off-policy step evaluation and active step sampling improve learning efficiency by focusing on informative transformation stages. Furthermore, we design object-centric spatial rewards that evaluate displacement, rotation, and scaling behaviors directly, enabling interpretable and coherent transformations. Experiments on curated benchmarks demonstrate that Talk2Move achieves precise, consistent, and semantically faithful object transformations, outperforming existing text-guided editing approaches in both spatial accuracy and scene coherence.",
      "authors": [
        "Jing Tan",
        "Zhaoyang Zhang",
        "Yantao Shen",
        "Jiarui Cai",
        "Shuo Yang",
        "Jiajun Wu",
        "Wei Xia",
        "Zhuowen Tu",
        "Stefano Soatto"
      ],
      "published": "2026-01-05T18:55:32Z",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "abs_url": "https://arxiv.org/abs/2601.02356v1",
      "pdf_url": "https://arxiv.org/pdf/2601.02356v1.pdf"
    },
    {
      "id": "2601.02353v1",
      "title": "Meta-Learning Guided Pruning for Few-Shot Plant Pathology on Edge Devices",
      "abstract": "Farmers in remote areas need quick and reliable methods for identifying plant diseases, yet they often lack access to laboratories or high-performance computing resources. Deep learning models can detect diseases from leaf images with high accuracy, but these models are typically too large and computationally expensive to run on low-cost edge devices such as Raspberry Pi. Furthermore, collecting thousands of labeled disease images for training is both expensive and time-consuming. This paper addresses both challenges by combining neural network pruning -- removing unnecessary parts of the model -- with few-shot learning, which enables the model to learn from limited examples. This paper proposes Disease-Aware Channel Importance Scoring (DACIS), a method that identifies which parts of the neural network are most important for distinguishing between different plant diseases, integrated into a three-stage Prune-then-Meta-Learn-then-Prune (PMP) pipeline. Experiments on PlantVillage and PlantDoc datasets demonstrate that the proposed approach reduces model size by 78\\% while maintaining 92.3\\% of the original accuracy, with the compressed model running at 7 frames per second on a Raspberry Pi 4, making real-time field diagnosis practical for smallholder farmers.",
      "authors": [
        "Shahnawaz Alam",
        "Mohammed Mudassir Uddin",
        "Mohammed Kaif Pasha"
      ],
      "published": "2026-01-05T18:55:05Z",
      "categories": [
        "cs.CV",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "abs_url": "https://arxiv.org/abs/2601.02353v1",
      "pdf_url": "https://arxiv.org/pdf/2601.02353v1.pdf"
    },
    {
      "id": "2601.02346v1",
      "title": "Falcon-H1R: Pushing the Reasoning Frontiers with a Hybrid Model for Efficient Test-Time Scaling",
      "abstract": "This work introduces Falcon-H1R, a 7B-parameter reasoning-optimized model that establishes the feasibility of achieving competitive reasoning performance with small language models (SLMs). Falcon-H1R stands out for its parameter efficiency, consistently matching or outperforming SOTA reasoning models that are $2\\times$ to $7\\times$ larger across a variety of reasoning-intensive benchmarks. These results underscore the importance of careful data curation and targeted training strategies (via both efficient SFT and RL scaling) in delivering significant performance gains without increasing model size. Furthermore, Falcon-H1R advances the 3D limits of reasoning efficiency by combining faster inference (through its hybrid-parallel architecture design), token efficiency, and higher accuracy. This unique blend makes Falcon-H1R-7B a practical backbone for scaling advanced reasoning systems, particularly in scenarios requiring extensive chain-of-thoughts generation and parallel test-time scaling. Leveraging the recently introduced DeepConf approach, Falcon-H1R achieves state-of-the-art test-time scaling efficiency, offering substantial improvements in both accuracy and computational cost. As a result, Falcon-H1R demonstrates that compact models, through targeted model training and architectural choices, can deliver robust and scalable reasoning performance.",
      "authors": [
        " Falcon LLM Team",
        "Iheb Chaabane",
        "Puneesh Khanna",
        "Suhail Mohmad",
        "Slim Frikha",
        "Shi Hu",
        "Abdalgader Abubaker",
        "Reda Alami",
        "Mikhail Lubinets",
        "Mohamed El Amine Seddik",
        "Hakim Hacid"
      ],
      "published": "2026-01-05T18:44:27Z",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "abs_url": "https://arxiv.org/abs/2601.02346v1",
      "pdf_url": "https://arxiv.org/pdf/2601.02346v1.pdf"
    },
    {
      "id": "2601.02339v1",
      "title": "Joint Semantic and Rendering Enhancements in 3D Gaussian Modeling with Anisotropic Local Encoding",
      "abstract": "Recent works propose extending 3DGS with semantic feature vectors for simultaneous semantic segmentation and image rendering. However, these methods often treat the semantic and rendering branches separately, relying solely on 2D supervision while ignoring the 3D Gaussian geometry. Moreover, current adaptive strategies adapt the Gaussian set depending solely on rendering gradients, which can be insufficient in subtle or textureless regions. In this work, we propose a joint enhancement framework for 3D semantic Gaussian modeling that synergizes both semantic and rendering branches. Firstly, unlike conventional point cloud shape encoding, we introduce an anisotropic 3D Gaussian Chebyshev descriptor using the Laplace-Beltrami operator to capture fine-grained 3D shape details, thereby distinguishing objects with similar appearances and reducing reliance on potentially noisy 2D guidance. In addition, without relying solely on rendering gradient, we adaptively adjust Gaussian allocation and spherical harmonics with local semantic and shape signals, enhancing rendering efficiency through selective resource allocation. Finally, we employ a cross-scene knowledge transfer module to continuously update learned shape patterns, enabling faster convergence and robust representations without relearning shape information from scratch for each new scene. Experiments on multiple datasets demonstrate improvements in segmentation accuracy and rendering quality while maintaining high rendering frame rates.",
      "authors": [
        "Jingming He",
        "Chongyi Li",
        "Shiqi Wang",
        "Sam Kwong"
      ],
      "published": "2026-01-05T18:33:50Z",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "abs_url": "https://arxiv.org/abs/2601.02339v1",
      "pdf_url": "https://arxiv.org/pdf/2601.02339v1.pdf"
    },
    {
      "id": "2601.02337v1",
      "title": "Robust Persona-Aware Toxicity Detection with Prompt Optimization and Learned Ensembling",
      "abstract": "Toxicity detection is inherently subjective, shaped by the diverse perspectives and social priors of different demographic groups. While ``pluralistic'' modeling as used in economics and the social sciences aims to capture perspective differences across contexts, current Large Language Model (LLM) prompting techniques have different results across different personas and base models. In this work, we conduct a systematic evaluation of persona-aware toxicity detection, showing that no single prompting method, including our proposed automated prompt optimization strategy, uniformly dominates across all model-persona pairs. To exploit complementary errors, we explore ensembling four prompting variants and propose a lightweight meta-ensemble: an SVM over the 4-bit vector of prompt predictions. Our results demonstrate that the proposed SVM ensemble consistently outperforms individual prompting methods and traditional majority-voting techniques, achieving the strongest overall performance across diverse personas. This work provides one of the first systematic comparisons of persona-conditioned prompting for toxicity detection and offers a robust method for pluralistic evaluation in subjective NLP tasks.",
      "authors": [
        "Berk Atil",
        "Rebecca J. Passonneau",
        "Ninareh Mehrabi"
      ],
      "published": "2026-01-05T18:32:45Z",
      "categories": [
        "cs.CL"
      ],
      "primary_category": "cs.CL",
      "abs_url": "https://arxiv.org/abs/2601.02337v1",
      "pdf_url": "https://arxiv.org/pdf/2601.02337v1.pdf"
    },
    {
      "id": "2601.02329v1",
      "title": "BEDS: Bayesian Emergent Dissipative Structures",
      "abstract": "We present BEDS (Bayesian Emergent Dissipative Structures), a theoretical framework that unifies concepts from non-equilibrium thermodynamics, Bayesian inference, information geometry, and machine learning. The central thesis proposes that learning, across physical, biological, and computational systems, fundamentally constitutes the conversion of flux into structure through entropy export. Building on Prigogine's theory of dissipative structures, we establish a formal isomorphism between thermodynamic processes and Bayesian updating, demonstrating that sustainable learning systems must follow dissipative patterns where crystallized posteriors become priors for subsequent levels of emergence.\n  We derive fundamental mathematical constants (e, π, φ) as fixed points of Bayesian inference under minimal axioms, suggesting these constants emerge necessarily from any system capable of representing and updating uncertainty. Furthermore, we propose a conjecture linking Gödel's incompleteness theorems to thermodynamic constraints, hypothesizing that pathologies of formal systems (incompleteness, undecidability) are structurally analogous to dissipation deficits in physical systems.\n  As practical validation, we present a peer-to-peer network architecture implementing BEDS principles, achieving six orders of magnitude improvement in energy efficiency compared to existing distributed consensus systems while enabling continuous learning. This work bridges fundamental physics, mathematical logic, and practical system design, offering both theoretical insights into the nature of learning and computation, and a concrete pathway toward sustainable artificial intelligence.",
      "authors": [
        "Laurent Caraffa"
      ],
      "published": "2026-01-05T18:21:02Z",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "abs_url": "https://arxiv.org/abs/2601.02329v1",
      "pdf_url": "https://arxiv.org/pdf/2601.02329v1.pdf"
    },
    {
      "id": "2601.02324v1",
      "title": "Hunting for \"Oddballs\" with Machine Learning: Detecting Anomalous Exoplanets Using a Deep-Learned Low-Dimensional Representation of Transit Spectra with Autoencoders",
      "abstract": "This study explores the application of autoencoder-based machine learning techniques for anomaly detection to identify exoplanet atmospheres with unconventional chemical signatures using a low-dimensional data representation. We use the Atmospheric Big Challenge (ABC) database, a publicly available dataset with over 100,000 simulated exoplanet spectra, to construct an anomaly detection scenario by defining CO2-rich atmospheres as anomalies and CO2-poor atmospheres as the normal class. We benchmarked four different anomaly detection strategies: Autoencoder Reconstruction Loss, One-Class Support Vector Machine (1 class-SVM), K-means Clustering, and Local Outlier Factor (LOF). Each method was evaluated in both the original spectral space and the autoencoder's latent space using Receiver Operating Characteristic (ROC) curves and Area Under the Curve (AUC) metrics. To test the performance of the different methods under realistic conditions, we introduced Gaussian noise levels ranging from 10 to 50 ppm. Our results indicate that anomaly detection is consistently more effective when performed within the latent space across all noise levels. Specifically, K-means clustering in the latent space emerged as a stable and high-performing method. We demonstrate that this anomaly detection approach is robust to noise levels up to 30 ppm (consistent with realistic space-based observations) and remains viable even at 50 ppm when leveraging latent space representations. On the other hand, the performance of the anomaly detection methods applied directly in the raw spectral space degrades significantly with increasing the level of noise. This suggests that autoencoder-driven dimensionality reduction offers a robust methodology for flagging chemically anomalous targets in large-scale surveys where exhaustive retrievals are computationally prohibitive.",
      "authors": [
        "Alexander Roman",
        "Emilie Panek",
        "Roy T. Forestano",
        "Eyup B. Unlu",
        "Katia Matcheva",
        "Konstantin T. Matchev"
      ],
      "published": "2026-01-05T18:15:53Z",
      "categories": [
        "astro-ph.EP",
        "astro-ph.IM",
        "cs.LG"
      ],
      "primary_category": "astro-ph.EP",
      "abs_url": "https://arxiv.org/abs/2601.02324v1",
      "pdf_url": "https://arxiv.org/pdf/2601.02324v1.pdf"
    },
    {
      "id": "2601.02322v1",
      "title": "Environment-Adaptive Covariate Selection: Learning When to Use Spurious Correlations for Out-of-Distribution Prediction",
      "abstract": "Out-of-distribution (OOD) prediction is often approached by restricting models to causal or invariant covariates, avoiding non-causal spurious associations that may be unstable across environments. Despite its theoretical appeal, this strategy frequently underperforms empirical risk minimization (ERM) in practice. We investigate the source of this gap and show that such failures naturally arise when only a subset of the true causes of the outcome is observed. In these settings, non-causal spurious covariates can serve as informative proxies for unobserved causes and substantially improve prediction, except under distribution shifts that break these proxy relationships. Consequently, the optimal set of predictive covariates is neither universal nor necessarily exhibits invariant relationships with the outcome across all environments, but instead depends on the specific type of shift encountered. Crucially, we observe that different covariate shifts induce distinct, observable signatures in the covariate distribution itself. Moreover, these signatures can be extracted from unlabeled data in the target OOD environment and used to assess when proxy covariates remain reliable and when they fail. Building on this observation, we propose an environment-adaptive covariate selection (EACS) algorithm that maps environment-level covariate summaries to environment-specific covariate sets, while allowing the incorporation of prior causal knowledge as constraints. Across simulations and applied datasets, EACS consistently outperforms static causal, invariant, and ERM-based predictors under diverse distribution shifts.",
      "authors": [
        "Shuozhi Zuo",
        "Yixin Wang"
      ],
      "published": "2026-01-05T18:13:02Z",
      "categories": [
        "stat.ME",
        "cs.LG"
      ],
      "primary_category": "stat.ME",
      "abs_url": "https://arxiv.org/abs/2601.02322v1",
      "pdf_url": "https://arxiv.org/pdf/2601.02322v1.pdf"
    },
    {
      "id": "2601.02320v1",
      "title": "Estimating Text Temperature",
      "abstract": "Autoregressive language models typically use temperature parameter at inference to shape the probability distribution and control the randomness of the text generated. After the text was generated, this parameter can be estimated using maximum likelihood approach. Following it, we propose a procedure to estimate the temperature of any text, including ones written by humans, with respect to a given language model. We evaluate the temperature estimation capability of a wide selection of small-to-medium LLMs. We then use the best-performing Qwen3 14B to estimate temperatures of popular corpora.",
      "authors": [
        "Nikolay Mikhaylovskiy"
      ],
      "published": "2026-01-05T18:09:41Z",
      "categories": [
        "cs.CL"
      ],
      "primary_category": "cs.CL",
      "abs_url": "https://arxiv.org/abs/2601.02320v1",
      "pdf_url": "https://arxiv.org/pdf/2601.02320v1.pdf"
    },
    {
      "id": "2601.02318v1",
      "title": "Fusion2Print: Deep Flash-Non-Flash Fusion for Contactless Fingerprint Matching",
      "abstract": "Contactless fingerprint recognition offers a hygienic and convenient alternative to contact-based systems, enabling rapid acquisition without latent prints, pressure artifacts, or hygiene risks. However, contactless images often show degraded ridge clarity due to illumination variation, subcutaneous skin discoloration, and specular reflections. Flash captures preserve ridge detail but introduce noise, whereas non-flash captures reduce noise but lower ridge contrast. We propose Fusion2Print (F2P), the first framework to systematically capture and fuse paired flash-non-flash contactless fingerprints. We construct a custom paired dataset, FNF Database, and perform manual flash-non-flash subtraction to isolate ridge-preserving signals. A lightweight attention-based fusion network also integrates both modalities, emphasizing informative channels and suppressing noise, and then a U-Net enhancement module produces an optimally weighted grayscale image. Finally, a deep embedding model with cross-domain compatibility, generates discriminative and robust representations in a unified embedding space compatible with both contactless and contact-based fingerprints for verification. F2P enhances ridge clarity and achieves superior recognition performance (AUC=0.999, EER=1.12%) over single-capture baselines (Verifinger, DeepPrint).",
      "authors": [
        "Roja Sahoo",
        "Anoop Namboodiri"
      ],
      "published": "2026-01-05T18:09:27Z",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "abs_url": "https://arxiv.org/abs/2601.02318v1",
      "pdf_url": "https://arxiv.org/pdf/2601.02318v1.pdf"
    },
    {
      "id": "2601.02316v1",
      "title": "DatBench: Discriminative, Faithful, and Efficient VLM Evaluations",
      "abstract": "Empirical evaluation serves as the primary compass guiding research progress in foundation models. Despite a large body of work focused on training frontier vision-language models (VLMs), approaches to their evaluation remain nascent. To guide their maturation, we propose three desiderata that evaluations should satisfy: (1) faithfulness to the modality and application, (2) discriminability between models of varying quality, and (3) efficiency in compute. Through this lens, we identify critical failure modes that violate faithfulness and discriminability, misrepresenting model capabilities: (i) multiple-choice formats reward guessing, poorly reflect downstream use cases, and saturate early as models improve; (ii) blindly solvable questions, which can be answered without images, constitute up to 70% of some evaluations; and (iii) mislabeled or ambiguous samples compromise up to 42% of examples in certain datasets. Regarding efficiency, the computational burden of evaluating frontier models has become prohibitive: by some accounts, nearly 20% of development compute is devoted to evaluation alone. Rather than discarding existing benchmarks, we curate them via transformation and filtering to maximize fidelity and discriminability. We find that converting multiple-choice questions to generative tasks reveals sharp capability drops of up to 35%. In addition, filtering blindly solvable and mislabeled samples improves discriminative power while simultaneously reducing computational cost. We release DatBench-Full, a cleaned evaluation suite of 33 datasets spanning nine VLM capabilities, and DatBench, a discriminative subset that achieves 13x average speedup (up to 50x) while closely matching the discriminative power of the original datasets. Our work outlines a path toward evaluation practices that are both rigorous and sustainable as VLMs continue to scale.",
      "authors": [
        "Siddharth Joshi",
        "Haoli Yin",
        "Rishabh Adiga",
        "Ricardo Monti",
        "Aldo Carranza",
        "Alex Fang",
        "Alvin Deng",
        "Amro Abbas",
        "Brett Larsen",
        "Cody Blakeney",
        "Darren Teh",
        "David Schwab",
        "Fan Pan",
        "Haakon Mongstad",
        "Jack Urbanek",
        "Jason Lee",
        "Jason Telanoff",
        "Josh Wills",
        "Kaleigh Mentzer",
        "Luke Merrick",
        "Parth Doshi",
        "Paul Burstein",
        "Pratyush Maini",
        "Scott Loftin",
        "Spandan Das",
        "Tony Jiang",
        "Vineeth Dorna",
        "Zhengping Wang",
        "Bogdan Gaza",
        "Ari Morcos",
        "Matthew Leavitt"
      ],
      "published": "2026-01-05T18:07:51Z",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "abs_url": "https://arxiv.org/abs/2601.02316v1",
      "pdf_url": "https://arxiv.org/pdf/2601.02316v1.pdf"
    },
    {
      "id": "2601.02315v1",
      "title": "Prithvi-Complimentary Adaptive Fusion Encoder (CAFE): unlocking full-potential for flood inundation mapping",
      "abstract": "Geo-Foundation Models (GFMs), have proven effective in diverse downstream applications, including semantic segmentation, classification, and regression tasks. However, in case of flood mapping using Sen1Flood11 dataset as a downstream task, GFMs struggles to outperform the baseline U-Net, highlighting model's limitation in capturing critical local nuances. To address this, we present the Prithvi-Complementary Adaptive Fusion Encoder (CAFE), which integrate Prithvi GFM pretrained encoder with a parallel CNN residual branch enhanced by Convolutional Attention Modules (CAM). Prithvi-CAFE enables fast and efficient fine-tuning through adapters in Prithvi and performs multi-scale, multi-level fusion with CNN features, capturing critical local details while preserving long-range dependencies. We achieve state-of-the-art results on two comprehensive flood mapping datasets: Sen1Flood11 and FloodPlanet. On Sen1Flood11 test data, Prithvi-CAFE (IoU 83.41) outperforms the original Prithvi (IoU 82.50) and other major GFMs (TerraMind 82.90, DOFA 81.54, spectralGPT: 81.02). The improvement is even more pronounced on the hold-out test site, where Prithvi-CAFE achieves an IoU of 81.37 compared to the baseline U-Net (70.57) and original Prithvi (72.42). On FloodPlanet, Prithvi-CAFE also surpasses the baseline U-Net and other GFMs, achieving an IoU of 64.70 compared to U-Net (60.14), Terramind (62.33), DOFA (59.15) and Prithvi 2.0 (61.91). Our proposed simple yet effective Prithvi-CAFE demonstrates strong potential for improving segmentation tasks where multi-channel and multi-modal data provide complementary information and local details are critical. The code is released on \\href{https://github.com/Sk-2103/Prithvi-CAFE}{Prithvi-CAFE Github}",
      "authors": [
        "Saurabh Kaushik",
        "Lalit Maurya",
        "Beth Tellman"
      ],
      "published": "2026-01-05T18:07:21Z",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "abs_url": "https://arxiv.org/abs/2601.02315v1",
      "pdf_url": "https://arxiv.org/pdf/2601.02315v1.pdf"
    },
    {
      "id": "2601.02314v1",
      "title": "Project Ariadne: A Structural Causal Framework for Auditing Faithfulness in LLM Agents",
      "abstract": "As Large Language Model (LLM) agents are increasingly tasked with high-stakes autonomous decision-making, the transparency of their reasoning processes has become a critical safety concern. While \\textit{Chain-of-Thought} (CoT) prompting allows agents to generate human-readable reasoning traces, it remains unclear whether these traces are \\textbf{faithful} generative drivers of the model's output or merely \\textbf{post-hoc rationalizations}. We introduce \\textbf{Project Ariadne}, a novel XAI framework that utilizes Structural Causal Models (SCMs) and counterfactual logic to audit the causal integrity of agentic reasoning. Unlike existing interpretability methods that rely on surface-level textual similarity, Project Ariadne performs \\textbf{hard interventions} ($do$-calculus) on intermediate reasoning nodes -- systematically inverting logic, negating premises, and reversing factual claims -- to measure the \\textbf{Causal Sensitivity} ($φ$) of the terminal answer. Our empirical evaluation of state-of-the-art models reveals a persistent \\textit{Faithfulness Gap}. We define and detect a widespread failure mode termed \\textbf{Causal Decoupling}, where agents exhibit a violation density ($ρ$) of up to $0.77$ in factual and scientific domains. In these instances, agents arrive at identical conclusions despite contradictory internal logic, proving that their reasoning traces function as \"Reasoning Theater\" while decision-making is governed by latent parametric priors. Our findings suggest that current agentic architectures are inherently prone to unfaithful explanation, and we propose the Ariadne Score as a new benchmark for aligning stated logic with model action.",
      "authors": [
        "Sourena Khanzadeh"
      ],
      "published": "2026-01-05T18:05:29Z",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "abs_url": "https://arxiv.org/abs/2601.02314v1",
      "pdf_url": "https://arxiv.org/pdf/2601.02314v1.pdf"
    },
    {
      "id": "2601.02313v1",
      "title": "Game of Coding: Coding Theory in the Presence of Rational Adversaries, Motivated by Decentralized Machine Learning",
      "abstract": "Coding theory plays a crucial role in enabling reliable communication, storage, and computation. Classical approaches assume a worst-case adversarial model and ensure error correction and data recovery only when the number of honest nodes exceeds the number of adversarial ones by some margin. However, in some emerging decentralized applications, particularly in decentralized machine learning (DeML), participating nodes are rewarded for accepted contributions. This incentive structure naturally gives rise to rational adversaries who act strategically rather than behaving in purely malicious ways.\n  In this paper, we first motivate the need for coding in the presence of rational adversaries, particularly in the context of outsourced computation in decentralized systems. We contrast this need with existing approaches and highlight their limitations. We then introduce the game of coding, a novel game-theoretic framework that extends coding theory to trust-minimized settings where honest nodes are not in the majority. Focusing on repetition coding, we highlight two key features of this framework: (1) the ability to achieve a non-zero probability of data recovery even when adversarial nodes are in the majority, and (2) Sybil resistance, i.e., the equilibrium remains unchanged even as the number of adversarial nodes increases. Finally, we explore scenarios in which the adversary's strategy is unknown and outline several open problems for future research.",
      "authors": [
        "Hanzaleh Akbari Nodehi",
        "Viveck R. Cadambe",
        "Mohammad Ali Maddah-Ali"
      ],
      "published": "2026-01-05T18:04:32Z",
      "categories": [
        "cs.LG"
      ],
      "primary_category": "cs.LG",
      "abs_url": "https://arxiv.org/abs/2601.02313v1",
      "pdf_url": "https://arxiv.org/pdf/2601.02313v1.pdf"
    },
    {
      "id": "2601.02311v1",
      "title": "Placement Semantics for Distributed Deep Learning: A Systematic Framework for Analyzing Parallelism Strategies",
      "abstract": "Training large language models requires distributing computation across many accelerators, yet practitioners select parallelism strategies (data, tensor, pipeline, ZeRO) through trial and error because no unified systematic framework predicts their behavior. We introduce placement semantics: each strategy is specified by how it places four training states (parameters, optimizer, gradients, activations) across devices using five modes (replicated, sharded, sharded-with-gather, materialized, offloaded). From placement alone, without implementation details, we derive memory consumption and communication volume. Our predictions match published results exactly: ZeRO-3 uses 8x less memory than data parallelism at 1.5x communication cost, as reported in the original paper. We prove two conditions (gradient integrity, state consistency) are necessary and sufficient for distributed training to match single-device results, and provide composition rules for combining strategies safely. The framework unifies ZeRO Stages 1-3, Fully Sharded Data Parallel (FSDP), tensor parallelism, and pipeline parallelism as instances with different placement choices.",
      "authors": [
        "Deep Pankajbhai Mehta"
      ],
      "published": "2026-01-05T18:01:38Z",
      "categories": [
        "cs.DC",
        "cs.AI"
      ],
      "primary_category": "cs.DC",
      "abs_url": "https://arxiv.org/abs/2601.02311v1",
      "pdf_url": "https://arxiv.org/pdf/2601.02311v1.pdf"
    },
    {
      "id": "2601.02310v1",
      "title": "Temporal Kolmogorov-Arnold Networks (T-KAN) for High-Frequency Limit Order Book Forecasting: Efficiency, Interpretability, and Alpha Decay",
      "abstract": "High-Frequency trading (HFT) environments are characterised by large volumes of limit order book (LOB) data, which is notoriously noisy and non-linear. Alpha decay represents a significant challenge, with traditional models such as DeepLOB losing predictive power as the time horizon (k) increases. In this paper, using data from the FI-2010 dataset, we introduce Temporal Kolmogorov-Arnold Networks (T-KAN) to replace the fixed, linear weights of standard LSTMs with learnable B-spline activation functions. This allows the model to learn the 'shape' of market signals as opposed to just their magnitude. This resulted in a 19.1% relative improvement in the F1-score at the k = 100 horizon. The efficacy of T-KAN networks cannot be understated, producing a 132.48% return compared to the -82.76% DeepLOB drawdown under 1.0 bps transaction costs. In addition to this, the T-KAN model proves quite interpretable, with the 'dead-zones' being clearly visible in the splines. The T-KAN architecture is also uniquely optimized for low-latency FPGA implementation via High level Synthesis (HLS). The code for the experiments in this project can be found at https://github.com/AhmadMak/Temporal-Kolmogorov-Arnold-Networks-T-KAN-for-High-Frequency-Limit-Order-Book-Forecasting.",
      "authors": [
        "Ahmad Makinde"
      ],
      "published": "2026-01-05T17:59:42Z",
      "categories": [
        "cs.LG"
      ],
      "primary_category": "cs.LG",
      "abs_url": "https://arxiv.org/abs/2601.02310v1",
      "pdf_url": "https://arxiv.org/pdf/2601.02310v1.pdf"
    },
    {
      "id": "2601.02309v1",
      "title": "360DVO: Deep Visual Odometry for Monocular 360-Degree Camera",
      "abstract": "Monocular omnidirectional visual odometry (OVO) systems leverage 360-degree cameras to overcome field-of-view limitations of perspective VO systems. However, existing methods, reliant on handcrafted features or photometric objectives, often lack robustness in challenging scenarios, such as aggressive motion and varying illumination. To address this, we present 360DVO, the first deep learning-based OVO framework. Our approach introduces a distortion-aware spherical feature extractor (DAS-Feat) that adaptively learns distortion-resistant features from 360-degree images. These sparse feature patches are then used to establish constraints for effective pose estimation within a novel omnidirectional differentiable bundle adjustment (ODBA) module. To facilitate evaluation in realistic settings, we also contribute a new real-world OVO benchmark. Extensive experiments on this benchmark and public synthetic datasets (TartanAir V2 and 360VO) demonstrate that 360DVO surpasses state-of-the-art baselines (including 360VO and OpenVSLAM), improving robustness by 50% and accuracy by 37.5%. Homepage: https://chris1004336379.github.io/360DVO-homepage",
      "authors": [
        "Xiaopeng Guo",
        "Yinzhe Xu",
        "Huajian Huang",
        "Sai-Kit Yeung"
      ],
      "published": "2026-01-05T17:52:50Z",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "abs_url": "https://arxiv.org/abs/2601.02309v1",
      "pdf_url": "https://arxiv.org/pdf/2601.02309v1.pdf"
    },
    {
      "id": "2601.02307v1",
      "title": "Differential Privacy for Transformer Embeddings of Text with Nonparametric Variational Information Bottleneck",
      "abstract": "We propose a privacy-preserving method for sharing text data by sharing noisy versions of their transformer embeddings. It has been shown that hidden representations learned by deep models can encode sensitive information from the input, making it possible for adversaries to recover the input data with considerable accuracy. This problem is exacerbated in transformer embeddings because they consist of multiple vectors, one per token. To mitigate this risk, we propose Nonparametric Variational Differential Privacy (NVDP), which ensures both useful data sharing and strong privacy protection. We take a differential privacy approach, integrating a Nonparametric Variational Information Bottleneck (NVIB) layer into the transformer architecture to inject noise into its multi-vector embeddings and thereby hide information, and measuring privacy protection with Rényi divergence and its corresponding Bayesian Differential Privacy (BDP) guarantee. Training the NVIB layer calibrates the noise level according to utility. We test NVDP on the GLUE benchmark and show that varying the noise level gives us a useful tradeoff between privacy and accuracy. With lower noise levels, our model maintains high accuracy while offering strong privacy guarantees, effectively balancing privacy and utility.",
      "authors": [
        "Dina El Zein",
        "James Henderson"
      ],
      "published": "2026-01-05T17:49:39Z",
      "categories": [
        "cs.LG"
      ],
      "primary_category": "cs.LG",
      "abs_url": "https://arxiv.org/abs/2601.02307v1",
      "pdf_url": "https://arxiv.org/pdf/2601.02307v1.pdf"
    },
    {
      "id": "2601.02303v1",
      "title": "Classifying several dialectal Nawatl varieties",
      "abstract": "Mexico is a country with a large number of indigenous languages, among which the most widely spoken is Nawatl, with more than two million people currently speaking it (mainly in North and Central America). Despite its rich cultural heritage, which dates back to the 15th century, Nawatl is a language with few computer resources. The problem is compounded when it comes to its dialectal varieties, with approximately 30 varieties recognised, not counting the different spellings in the written forms of the language. In this research work, we addressed the problem of classifying Nawatl varieties using Machine Learning and Neural Networks.",
      "authors": [
        "Juan-José Guzmán-Landa",
        "Juan-Manuel Torres-Moreno",
        "Miguel Figueroa-Saavedra",
        "Carlos-Emiliano González-Gallardo",
        "Graham Ranger",
        "Martha Lorena-Avendaño-Garrido"
      ],
      "published": "2026-01-05T17:38:55Z",
      "categories": [
        "cs.CL"
      ],
      "primary_category": "cs.CL",
      "abs_url": "https://arxiv.org/abs/2601.02303v1",
      "pdf_url": "https://arxiv.org/pdf/2601.02303v1.pdf"
    },
    {
      "id": "2601.02299v1",
      "title": "SortWaste: A Densely Annotated Dataset for Object Detection in Industrial Waste Sorting",
      "abstract": "The increasing production of waste, driven by population growth, has created challenges in managing and recycling materials effectively. Manual waste sorting is a common practice; however, it remains inefficient for handling large-scale waste streams and presents health risks for workers. On the other hand, existing automated sorting approaches still struggle with the high variability, clutter, and visual complexity of real-world waste streams. The lack of real-world datasets for waste sorting is a major reason automated systems for this problem are underdeveloped. Accordingly, we introduce SortWaste, a densely annotated object detection dataset collected from a Material Recovery Facility. Additionally, we contribute to standardizing waste detection in sorting lines by proposing ClutterScore, an objective metric that gauges the scene's hardness level using a set of proxies that affect visual complexity (e.g., object count, class and size entropy, and spatial overlap). In addition to these contributions, we provide an extensive benchmark of state-of-the-art object detection models, detailing their results with respect to the hardness level assessed by the proposed metric. Despite achieving promising results (mAP of 59.7% in the plastic-only detection task), performance significantly decreases in highly cluttered scenes. This highlights the need for novel and more challenging datasets on the topic.",
      "authors": [
        "Sara Inácio",
        "Hugo Proença",
        "João C. Neves"
      ],
      "published": "2026-01-05T17:34:50Z",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "abs_url": "https://arxiv.org/abs/2601.02299v1",
      "pdf_url": "https://arxiv.org/pdf/2601.02299v1.pdf"
    },
    {
      "id": "2601.02298v1",
      "title": "Power-of-Two Quantization-Aware-Training (PoT-QAT) in Large Language Models (LLMs)",
      "abstract": "In Large Language Models (LLMs), the number of parameters has grown exponentially in the past few years, e.g., from 1.5 billion parameters in GPT-2 to 175 billion in GPT-3 to possibly more than trillion in higher versions. This raises a significant challenge for implementation, especially for Edge devices. Unlike cloud computing, memory and processing power for Edge devices are very limited, which necessitates developing novel ideas to make such applications feasible. In this work, we investigate compressing weights with a special quantization that limits numbers to only power-of-two (PoT). This helps save a huge amount of memory as only exponents need to be stored, more importantly, it significantly reduces processing power by replacing costly multiplication with low cost bit shifting. To overcome performance loss due to this strict quantization, we investigate Quantization Aware Training (QAT) to enhance performance through additional training. Results on GPT-2 124M show a major enhancement for quantized PoT model after additional training, with a perplexity enhancement of 66% and BERT-Score loss to baseline GPT-2 of 1%. The memory saving is estimated to be 87.5% while the inference speed is expected to be 3-10x faster with PoT quantization versus full-precision.",
      "authors": [
        "Mahmoud Elgenedy"
      ],
      "published": "2026-01-05T17:33:16Z",
      "categories": [
        "cs.CL",
        "eess.SP"
      ],
      "primary_category": "cs.CL",
      "abs_url": "https://arxiv.org/abs/2601.02298v1",
      "pdf_url": "https://arxiv.org/pdf/2601.02298v1.pdf"
    },
    {
      "id": "2601.02295v1",
      "title": "CycleVLA: Proactive Self-Correcting Vision-Language-Action Models via Subtask Backtracking and Minimum Bayes Risk Decoding",
      "abstract": "Current work on robot failure detection and correction typically operate in a post hoc manner, analyzing errors and applying corrections only after failures occur. This work introduces CycleVLA, a system that equips Vision-Language-Action models (VLAs) with proactive self-correction, the capability to anticipate incipient failures and recover before they fully manifest during execution. CycleVLA achieves this by integrating a progress-aware VLA that flags critical subtask transition points where failures most frequently occur, a VLM-based failure predictor and planner that triggers subtask backtracking upon predicted failure, and a test-time scaling strategy based on Minimum Bayes Risk (MBR) decoding to improve retry success after backtracking. Extensive experiments show that CycleVLA improves performance for both well-trained and under-trained VLAs, and that MBR serves as an effective zero-shot test-time scaling strategy for VLAs. Project Page: https://dannymcy.github.io/cyclevla/",
      "authors": [
        "Chenyang Ma",
        "Guangyu Yang",
        "Kai Lu",
        "Shitong Xu",
        "Bill Byrne",
        "Niki Trigoni",
        "Andrew Markham"
      ],
      "published": "2026-01-05T17:31:01Z",
      "categories": [
        "cs.RO"
      ],
      "primary_category": "cs.RO",
      "abs_url": "https://arxiv.org/abs/2601.02295v1",
      "pdf_url": "https://arxiv.org/pdf/2601.02295v1.pdf"
    },
    {
      "id": "2601.02289v1",
      "title": "Rank-based Geographical Regularization: Revisiting Contrastive Self-Supervised Learning for Multispectral Remote Sensing Imagery",
      "abstract": "Self-supervised learning (SSL) has become a powerful paradigm for learning from large, unlabeled datasets, particularly in computer vision (CV). However, applying SSL to multispectral remote sensing (RS) images presents unique challenges and opportunities due to the geographical and temporal variability of the data. In this paper, we introduce GeoRank, a novel regularization method for contrastive SSL that improves upon prior techniques by directly optimizing spherical distances to embed geographical relationships into the learned feature space. GeoRank outperforms or matches prior methods that integrate geographical metadata and consistently improves diverse contrastive SSL algorithms (e.g., BYOL, DINO). Beyond this, we present a systematic investigation of key adaptations of contrastive SSL for multispectral RS images, including the effectiveness of data augmentations, the impact of dataset cardinality and image size on performance, and the task dependency of temporal views. Code is available at https://github.com/tomburgert/georank.",
      "authors": [
        "Tom Burgert",
        "Leonard Hackel",
        "Paolo Rota",
        "Begüm Demir"
      ],
      "published": "2026-01-05T17:24:50Z",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "abs_url": "https://arxiv.org/abs/2601.02289v1",
      "pdf_url": "https://arxiv.org/pdf/2601.02289v1.pdf"
    },
    {
      "id": "2601.02285v1",
      "title": "pdfQA: Diverse, Challenging, and Realistic Question Answering over PDFs",
      "abstract": "PDFs are the second-most used document type on the internet (after HTML). Yet, existing QA datasets commonly start from text sources or only address specific domains. In this paper, we present pdfQA, a multi-domain 2K human-annotated (real-pdfQA) and 2K synthetic dataset (syn-pdfQA) differentiating QA pairs in ten complexity dimensions (e.g., file type, source modality, source position, answer type). We apply and evaluate quality and difficulty filters on both datasets, obtaining valid and challenging QA pairs. We answer the questions with open-source LLMs, revealing existing challenges that correlate with our complexity dimensions. pdfQA presents a basis for end-to-end QA pipeline evaluation, testing diverse skill sets and local optimizations (e.g., in information retrieval or parsing).",
      "authors": [
        "Tobias Schimanski",
        "Imene Kolli",
        "Jingwei Ni",
        "Yu Fan",
        "Ario Saeid Vaghefi",
        "Elliott Ash",
        "Markus Leippold"
      ],
      "published": "2026-01-05T17:15:26Z",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "abs_url": "https://arxiv.org/abs/2601.02285v1",
      "pdf_url": "https://arxiv.org/pdf/2601.02285v1.pdf"
    },
    {
      "id": "2601.02281v1",
      "title": "InfiniteVGGT: Visual Geometry Grounded Transformer for Endless Streams",
      "abstract": "The grand vision of enabling persistent, large-scale 3D visual geometry understanding is shackled by the irreconcilable demands of scalability and long-term stability. While offline models like VGGT achieve inspiring geometry capability, their batch-based nature renders them irrelevant for live systems. Streaming architectures, though the intended solution for live operation, have proven inadequate. Existing methods either fail to support truly infinite-horizon inputs or suffer from catastrophic drift over long sequences. We shatter this long-standing dilemma with InfiniteVGGT, a causal visual geometry transformer that operationalizes the concept of a rolling memory through a bounded yet adaptive and perpetually expressive KV cache. Capitalizing on this, we devise a training-free, attention-agnostic pruning strategy that intelligently discards obsolete information, effectively ``rolling'' the memory forward with each new frame. Fully compatible with FlashAttention, InfiniteVGGT finally alleviates the compromise, enabling infinite-horizon streaming while outperforming existing streaming methods in long-term stability. The ultimate test for such a system is its performance over a truly infinite horizon, a capability that has been impossible to rigorously validate due to the lack of extremely long-term, continuous benchmarks. To address this critical gap, we introduce the Long3D benchmark, which, for the first time, enables a rigorous evaluation of continuous 3D geometry estimation on sequences about 10,000 frames. This provides the definitive evaluation platform for future research in long-term 3D geometry understanding. Code is available at: https://github.com/AutoLab-SAI-SJTU/InfiniteVGGT",
      "authors": [
        "Shuai Yuan",
        "Yantai Yang",
        "Xiaotian Yang",
        "Xupeng Zhang",
        "Zhonghao Zhao",
        "Lingming Zhang",
        "Zhipeng Zhang"
      ],
      "published": "2026-01-05T17:11:00Z",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "abs_url": "https://arxiv.org/abs/2601.02281v1",
      "pdf_url": "https://arxiv.org/pdf/2601.02281v1.pdf"
    },
    {
      "id": "2601.02273v1",
      "title": "TopoLoRA-SAM: Topology-Aware Parameter-Efficient Adaptation of Foundation Segmenters for Thin-Structure and Cross-Domain Binary Semantic Segmentation",
      "abstract": "Foundation segmentation models such as the Segment Anything Model (SAM) exhibit strong zero-shot generalization through large-scale pretraining, but adapting them to domain-specific semantic segmentation remains challenging, particularly for thin structures (e.g., retinal vessels) and noisy modalities (e.g., SAR imagery). Full fine-tuning is computationally expensive and risks catastrophic forgetting. We propose \\textbf{TopoLoRA-SAM}, a topology-aware and parameter-efficient adaptation framework for binary semantic segmentation. TopoLoRA-SAM injects Low-Rank Adaptation (LoRA) into the frozen ViT encoder, augmented with a lightweight spatial convolutional adapter and optional topology-aware supervision via differentiable clDice. We evaluate our approach on five benchmarks spanning retinal vessel segmentation (DRIVE, STARE, CHASE\\_DB1), polyp segmentation (Kvasir-SEG), and SAR sea/land segmentation (SL-SSDD), comparing against U-Net, DeepLabV3+, SegFormer, and Mask2Former. TopoLoRA-SAM achieves the best retina-average Dice and the best overall average Dice across datasets, while training only \\textbf{5.2\\%} of model parameters ($\\sim$4.9M). On the challenging CHASE\\_DB1 dataset, our method substantially improves segmentation accuracy and robustness, demonstrating that topology-aware parameter-efficient adaptation can match or exceed fully fine-tuned specialist models. Code is available at : https://github.com/salimkhazem/Seglab.git",
      "authors": [
        "Salim Khazem"
      ],
      "published": "2026-01-05T17:03:45Z",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "abs_url": "https://arxiv.org/abs/2601.02273v1",
      "pdf_url": "https://arxiv.org/pdf/2601.02273v1.pdf"
    },
    {
      "id": "2601.02267v1",
      "title": "DiffProxy: Multi-View Human Mesh Recovery via Diffusion-Generated Dense Proxies",
      "abstract": "Human mesh recovery from multi-view images faces a fundamental challenge: real-world datasets contain imperfect ground-truth annotations that bias the models' training, while synthetic data with precise supervision suffers from domain gap. In this paper, we propose DiffProxy, a novel framework that generates multi-view consistent human proxies for mesh recovery. Central to DiffProxy is leveraging the diffusion-based generative priors to bridge the synthetic training and real-world generalization. Its key innovations include: (1) a multi-conditional mechanism for generating multi-view consistent, pixel-aligned human proxies; (2) a hand refinement module that incorporates flexible visual prompts to enhance local details; and (3) an uncertainty-aware test-time scaling method that increases robustness to challenging cases during optimization. These designs ensure that the mesh recovery process effectively benefits from the precise synthetic ground truth and generative advantages of the diffusion-based pipeline. Trained entirely on synthetic data, DiffProxy achieves state-of-the-art performance across five real-world benchmarks, demonstrating strong zero-shot generalization particularly on challenging scenarios with occlusions and partial views. Project page: https://wrk226.github.io/DiffProxy.html",
      "authors": [
        "Renke Wang",
        "Zhenyu Zhang",
        "Ying Tai",
        "Jian Yang"
      ],
      "published": "2026-01-05T16:51:45Z",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "abs_url": "https://arxiv.org/abs/2601.02267v1",
      "pdf_url": "https://arxiv.org/pdf/2601.02267v1.pdf"
    },
    {
      "id": "2601.02265v1",
      "title": "Predicting Early and Complete Drug Release from Long-Acting Injectables Using Explainable Machine Learning",
      "abstract": "Polymer-based long-acting injectables (LAIs) have transformed the treatment of chronic diseases by enabling controlled drug delivery, thus reducing dosing frequency and extending therapeutic duration. Achieving controlled drug release from LAIs requires extensive optimization of the complex underlying physicochemical properties. Machine learning (ML) can accelerate LAI development by modeling the complex relationships between LAI properties and drug release. However, recent ML studies have provided limited information on key properties that modulate drug release, due to the lack of custom modeling and analysis tailored to LAI data. This paper presents a novel data transformation and explainable ML approach to synthesize actionable information from 321 LAI formulations by predicting early drug release at 24, 48, and 72 hours, classification of release profile types, and prediction of complete release profiles. These three experiments investigate the contribution and control of LAI material characteristics in early and complete drug release profiles. A strong correlation (>0.65) is observed between the true and predicted drug release in 72 hours, while a 0.87 F1-score is obtained in classifying release profile types. A time-independent ML framework predicts delayed biphasic and triphasic curves with better performance than current time-dependent approaches. Shapley additive explanations reveal the relative influence of material characteristics during early and for complete release which fill several gaps in previous in-vitro and ML-based studies. The novel approach and findings can provide a quantitative strategy and recommendations for scientists to optimize the drug-release dynamics of LAI. The source code for the model implementation is publicly available.",
      "authors": [
        "Karla N. Robles",
        "Manar D. Samad"
      ],
      "published": "2026-01-05T16:49:17Z",
      "categories": [
        "q-bio.BM",
        "cs.LG"
      ],
      "primary_category": "q-bio.BM",
      "abs_url": "https://arxiv.org/abs/2601.02265v1",
      "pdf_url": "https://arxiv.org/pdf/2601.02265v1.pdf"
    },
    {
      "id": "2601.02264v1",
      "title": "POSEIDON: Physics-Optimized Seismic Energy Inference and Detection Operating Network",
      "abstract": "Earthquake prediction and seismic hazard assessment remain fundamental challenges in geophysics, with existing machine learning approaches often operating as black boxes that ignore established physical laws. We introduce POSEIDON (Physics-Optimized Seismic Energy Inference and Detection Operating Network), a physics-informed energy-based model for unified multi-task seismic event prediction, alongside the Poseidon dataset -- the largest open-source global earthquake catalog comprising 2.8 million events spanning 30 years. POSEIDON embeds fundamental seismological principles, including the Gutenberg-Richter magnitude-frequency relationship and Omori-Utsu aftershock decay law, as learnable constraints within an energy-based modeling framework. The architecture simultaneously addresses three interconnected prediction tasks: aftershock sequence identification, tsunami generation potential, and foreshock detection. Extensive experiments demonstrate that POSEIDON achieves state-of-the-art performance across all tasks, outperforming gradient boosting, random forest, and CNN baselines with the highest average F1 score among all compared methods. Crucially, the learned physics parameters converge to scientifically interpretable values -- Gutenberg-Richter b-value of 0.752 and Omori-Utsu parameters p=0.835, c=0.1948 days -- falling within established seismological ranges while enhancing rather than compromising predictive accuracy. The Poseidon dataset is publicly available at https://huggingface.co/datasets/BorisKriuk/Poseidon, providing pre-computed energy features, spatial grid indices, and standardized quality metrics to advance physics-informed seismic research.",
      "authors": [
        "Boris Kriuk",
        "Fedor Kriuk"
      ],
      "published": "2026-01-05T16:46:34Z",
      "categories": [
        "cs.LG"
      ],
      "primary_category": "cs.LG",
      "abs_url": "https://arxiv.org/abs/2601.02264v1",
      "pdf_url": "https://arxiv.org/pdf/2601.02264v1.pdf"
    },
    {
      "id": "2601.02257v1",
      "title": "Improved Accuracy for Private Continual Cardinality Estimation in Fully Dynamic Streams via Matrix Factorization",
      "abstract": "We study differentially-private statistics in the fully dynamic continual observation model, where many updates can arrive at each time step and updates to a stream can involve both insertions and deletions of an item. Earlier work (e.g., Jain et al., NeurIPS 2023 for counting distinct elements; Raskhodnikova & Steiner, PODS 2025 for triangle counting with edge updates) reduced the respective cardinality estimation problem to continual counting on the difference stream associated with the true function values on the input stream. In such reductions, a change in the original stream can cause many changes in the difference stream, this poses a challenge for applying private continual counting algorithms to obtain optimal error bounds. We improve the accuracy of several such reductions by studying the associated $\\ell_p$-sensitivity vectors of the resulting difference streams and isolating their properties.\n  We demonstrate that our framework gives improved bounds for counting distinct elements, estimating degree histograms, and estimating triangle counts (under a slightly relaxed privacy model), thus offering a general approach to private continual cardinality estimation in streaming settings. Our improved accuracy stems from tight analysis of known factorization mechanisms for the counting matrix in this setting; the key technical challenge is arguing that one can use state-of-the-art factorizations for sensitivity vector sets with the properties we isolate. Empirically and analytically, we demonstrate that our improved error bounds offer a substantial improvement in accuracy for cardinality estimation problems over a large range of parameters.",
      "authors": [
        "Joel Daniel Andersson",
        "Palak Jain",
        "Satchit Sivakumar"
      ],
      "published": "2026-01-05T16:36:59Z",
      "categories": [
        "cs.CR",
        "cs.DS",
        "cs.LG"
      ],
      "primary_category": "cs.CR",
      "abs_url": "https://arxiv.org/abs/2601.02257v1",
      "pdf_url": "https://arxiv.org/pdf/2601.02257v1.pdf"
    },
    {
      "id": "2601.02256v1",
      "title": "VAR RL Done Right: Tackling Asynchronous Policy Conflicts in Visual Autoregressive Generation",
      "abstract": "Visual generation is dominated by three paradigms: AutoRegressive (AR), diffusion, and Visual AutoRegressive (VAR) models. Unlike AR and diffusion, VARs operate on heterogeneous input structures across their generation steps, which creates severe asynchronous policy conflicts. This issue becomes particularly acute in reinforcement learning (RL) scenarios, leading to unstable training and suboptimal alignment. To resolve this, we propose a novel framework to enhance Group Relative Policy Optimization (GRPO) by explicitly managing these conflicts. Our method integrates three synergistic components: 1) a stabilizing intermediate reward to guide early-stage generation; 2) a dynamic time-step reweighting scheme for precise credit assignment; and 3) a novel mask propagation algorithm, derived from principles of Reward Feedback Learning (ReFL), designed to isolate optimization effects both spatially and temporally. Our approach demonstrates significant improvements in sample quality and objective alignment over the vanilla GRPO baseline, enabling robust and effective optimization for VAR models.",
      "authors": [
        "Shikun Sun",
        "Liao Qu",
        "Huichao Zhang",
        "Yiheng Liu",
        "Yangyang Song",
        "Xian Li",
        "Xu Wang",
        "Yi Jiang",
        "Daniel K. Du",
        "Xinglong Wu",
        "Jia Jia"
      ],
      "published": "2026-01-05T16:36:40Z",
      "categories": [
        "cs.CV",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "abs_url": "https://arxiv.org/abs/2601.02256v1",
      "pdf_url": "https://arxiv.org/pdf/2601.02256v1.pdf"
    },
    {
      "id": "2601.02253v1",
      "title": "Neuro-Channel Networks: A Multiplication-Free Architecture by Biological Signal Transmission",
      "abstract": "The rapid proliferation of Deep Learning is increasingly constrained by its heavy reliance on high-performance hardware, particularly Graphics Processing Units (GPUs). These specialized accelerators are not only prohibitively expensive and energy-intensive but also suffer from significant supply scarcity, limiting the ubiquity of Artificial Intelligence (AI) deployment on edge devices. The core of this inefficiency stems from the standard artificial perceptron's dependence on intensive matrix multiplications. However, biological nervous systems achieve unparalleled efficiency without such arithmetic intensity; synaptic signal transmission is regulated by physical ion channel limits and chemical neurotransmitter levels rather than a process that can be analogous to arithmetic multiplication. Inspired by this biological mechanism, we propose Neuro-Channel Networks (NCN), a novel multiplication-free architecture designed to decouple AI from expensive hardware dependencies. In our model, weights are replaced with Channel Widths that physically limit the signal magnitude, while a secondary parameter acts as a Neurotransmitter to regulate Signal Transmission based on sign logic. The forward pass relies exclusively on addition, subtraction, and bitwise operations (minimum, sign), eliminating floating-point multiplication entirely. In this proof-of-concept study, we demonstrate that NCNs can solve non-linearly separable problems like XOR and the Majority function with 100% accuracy using standard backpropagation, proving their capability to form complex decision boundaries without multiplicative weights. This architecture offers a highly efficient alternative for next-generation neuromorphic hardware, paving the way for running complex models on commodity CPUs or ultra-low-power chips without relying on costly GPU clusters.",
      "authors": [
        "Emrah Mete",
        "Emin Erkan Korkmaz"
      ],
      "published": "2026-01-05T16:33:13Z",
      "categories": [
        "cs.LG",
        "cs.AR",
        "cs.CV"
      ],
      "primary_category": "cs.LG",
      "abs_url": "https://arxiv.org/abs/2601.02253v1",
      "pdf_url": "https://arxiv.org/pdf/2601.02253v1.pdf"
    },
    {
      "id": "2601.02249v1",
      "title": "SLGNet: Synergizing Structural Priors and Language-Guided Modulation for Multimodal Object Detection",
      "abstract": "Multimodal object detection leveraging RGB and Infrared (IR) images is pivotal for robust perception in all-weather scenarios. While recent adapter-based approaches efficiently transfer RGB-pretrained foundation models to this task, they often prioritize model efficiency at the expense of cross-modal structural consistency. Consequently, critical structural cues are frequently lost when significant domain gaps arise, such as in high-contrast or nighttime environments. Moreover, conventional static multimodal fusion mechanisms typically lack environmental awareness, resulting in suboptimal adaptation and constrained detection performance under complex, dynamic scene variations. To address these limitations, we propose SLGNet, a parameter-efficient framework that synergizes hierarchical structural priors and language-guided modulation within a frozen Vision Transformer (ViT)-based foundation model. Specifically, we design a Structure-Aware Adapter to extract hierarchical structural representations from both modalities and dynamically inject them into the ViT to compensate for structural degradation inherent in ViT-based backbones. Furthermore, we propose a Language-Guided Modulation module that exploits VLM-driven structured captions to dynamically recalibrate visual features, thereby endowing the model with robust environmental awareness. Extensive experiments on the LLVIP, FLIR, KAIST, and DroneVehicle datasets demonstrate that SLGNet establishes new state-of-the-art performance. Notably, on the LLVIP benchmark, our method achieves an mAP of 66.1, while reducing trainable parameters by approximately 87% compared to traditional full fine-tuning. This confirms SLGNet as a robust and efficient solution for multimodal perception.",
      "authors": [
        "Xiantai Xiang",
        "Guangyao Zhou",
        "Zixiao Wen",
        "Wenshuai Li",
        "Ben Niu",
        "Feng Wang",
        "Lijia Huang",
        "Qiantong Wang",
        "Yuhan Liu",
        "Zongxu Pan",
        "Yuxin Hu"
      ],
      "published": "2026-01-05T16:31:41Z",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "abs_url": "https://arxiv.org/abs/2601.02249v1",
      "pdf_url": "https://arxiv.org/pdf/2601.02249v1.pdf"
    },
    {
      "id": "2601.02246v1",
      "title": "A Comparative Study of Custom CNNs, Pre-trained Models, and Transfer Learning Across Multiple Visual Datasets",
      "abstract": "Convolutional Neural Networks (CNNs) are a standard approach for visual recognition due to their capacity to learn hierarchical representations from raw pixels. In practice, practitioners often choose among (i) training a compact custom CNN from scratch, (ii) using a large pre-trained CNN as a fixed feature extractor, and (iii) performing transfer learning via partial or full fine-tuning of a pre-trained backbone. This report presents a controlled comparison of these three paradigms across five real-world image classification datasets spanning road-surface defect recognition, agricultural variety identification, fruit/leaf disease recognition, pedestrian walkway encroachment recognition, and unauthorized vehicle recognition. Models are evaluated using accuracy and macro F1-score, complemented by efficiency metrics including training time per epoch and parameter counts. The results show that transfer learning consistently yields the strongest predictive performance, while the custom CNN provides an attractive efficiency--accuracy trade-off, especially when compute and memory budgets are constrained.",
      "authors": [
        "Annoor Sharara Akhand"
      ],
      "published": "2026-01-05T16:26:32Z",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "abs_url": "https://arxiv.org/abs/2601.02246v1",
      "pdf_url": "https://arxiv.org/pdf/2601.02246v1.pdf"
    },
    {
      "id": "2601.02242v1",
      "title": "VIBE: Visual Instruction Based Editor",
      "abstract": "Instruction-based image editing is among the fastest developing areas in generative AI. Over the past year, the field has reached a new level, with dozens of open-source models released alongside highly capable commercial systems. However, only a limited number of open-source approaches currently achieve real-world quality. In addition, diffusion backbones, the dominant choice for these pipelines, are often large and computationally expensive for many deployments and research settings, with widely used variants typically containing 6B to 20B parameters. This paper presents a compact, high-throughput instruction-based image editing pipeline that uses a modern 2B-parameter Qwen3-VL model to guide the editing process and the 1.6B-parameter diffusion model Sana1.5 for image generation. Our design decisions across architecture, data processing, training configuration, and evaluation target low-cost inference and strict source consistency while maintaining high quality across the major edit categories feasible at this scale. Evaluated on the ImgEdit and GEdit benchmarks, the proposed method matches or exceeds the performance of substantially heavier baselines, including models with several times as many parameters and higher inference cost, and is particularly strong on edits that require preserving the input image, such as an attribute adjustment, object removal, background edits, and targeted replacement. The model fits within 24 GB of GPU memory and generates edited images at up to 2K resolution in approximately 4 seconds on an NVIDIA H100 in BF16, without additional inference optimizations or distillation.",
      "authors": [
        "Grigorii Alekseenko",
        "Aleksandr Gordeev",
        "Irina Tolstykh",
        "Bulat Suleimanov",
        "Vladimir Dokholyan",
        "Georgii Fedorov",
        "Sergey Yakubson",
        "Aleksandra Tsybina",
        "Mikhail Chernyshov",
        "Maksim Kuprashevich"
      ],
      "published": "2026-01-05T16:17:20Z",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "abs_url": "https://arxiv.org/abs/2601.02242v1",
      "pdf_url": "https://arxiv.org/pdf/2601.02242v1.pdf"
    },
    {
      "id": "2601.02241v1",
      "title": "From Mice to Trains: Amortized Bayesian Inference on Graph Data",
      "abstract": "Graphs arise across diverse domains, from biology and chemistry to social and information networks, as well as in transportation and logistics. Inference on graph-structured data requires methods that are permutation-invariant, scalable across varying sizes and sparsities, and capable of capturing complex long-range dependencies, making posterior estimation on graph parameters particularly challenging. Amortized Bayesian Inference (ABI) is a simulation-based framework that employs generative neural networks to enable fast, likelihood-free posterior inference. We adapt ABI to graph data to address these challenges to perform inference on node-, edge-, and graph-level parameters. Our approach couples permutation-invariant graph encoders with flexible neural posterior estimators in a two-module pipeline: a summary network maps attributed graphs to fixed-length representations, and an inference network approximates the posterior over parameters. In this setting, several neural architectures can serve as the summary network. In this work we evaluate multiple architectures and assess their performance on controlled synthetic settings and two real-world domains - biology and logistics - in terms of recovery and calibration.",
      "authors": [
        "Svenja Jedhoff",
        "Elizaveta Semenova",
        "Aura Raulo",
        "Anne Meyer",
        "Paul-Christian Bürkner"
      ],
      "published": "2026-01-05T16:16:28Z",
      "categories": [
        "stat.ML",
        "cs.LG"
      ],
      "primary_category": "stat.ML",
      "abs_url": "https://arxiv.org/abs/2601.02241v1",
      "pdf_url": "https://arxiv.org/pdf/2601.02241v1.pdf"
    },
    {
      "id": "2601.02236v1",
      "title": "CD4LM: Consistency Distillation and aDaptive Decoding for Diffusion Language Models",
      "abstract": "Autoregressive large language models achieve strong results on many benchmarks, but decoding remains fundamentally latency-limited by sequential dependence on previously generated tokens. Diffusion language models (DLMs) promise parallel generation but suffer from a fundamental static-to-dynamic misalignment: Training optimizes local transitions under fixed schedules, whereas efficient inference requires adaptive \"long-jump\" refinements through unseen states. Our goal is to enable highly parallel decoding for DLMs with low number of function evaluations while preserving generation quality. To achieve this, we propose CD4LM, a framework that decouples training from inference via Discrete-Space Consistency Distillation (DSCD) and Confidence-Adaptive Decoding (CAD). Unlike standard objectives, DSCD trains a student to be trajectory-invariant, mapping diverse noisy states directly to the clean distribution. This intrinsic robustness enables CAD to dynamically allocate compute resources based on token confidence, aggressively skipping steps without the quality collapse typical of heuristic acceleration. On GSM8K, CD4LM matches the LLaDA baseline with a 5.18x wall-clock speedup; across code and math benchmarks, it strictly dominates the accuracy-efficiency Pareto frontier, achieving a 3.62x mean speedup while improving average accuracy. Code is available at https://github.com/yihao-liang/CDLM",
      "authors": [
        "Yihao Liang",
        "Ze Wang",
        "Hao Chen",
        "Ximeng Sun",
        "Jialian Wu",
        "Xiaodong Yu",
        "Jiang Liu",
        "Emad Barsoum",
        "Zicheng Liu",
        "Niraj K. Jha"
      ],
      "published": "2026-01-05T16:09:22Z",
      "categories": [
        "cs.CL"
      ],
      "primary_category": "cs.CL",
      "abs_url": "https://arxiv.org/abs/2601.02236v1",
      "pdf_url": "https://arxiv.org/pdf/2601.02236v1.pdf"
    },
    {
      "id": "2601.02232v1",
      "title": "ELLA: Efficient Lifelong Learning for Adapters in Large Language Models",
      "abstract": "Large Language Models (LLMs) suffer severe catastrophic forgetting when adapted sequentially to new tasks in a continual learning (CL) setting. Existing approaches are fundamentally limited: replay-based methods are impractical and privacy-violating, while strict orthogonality-based methods collapse under scale: each new task is projected onto an orthogonal complement, progressively reducing the residual degrees of freedom and eliminating forward transfer by forbidding overlap in shared representations. In this work, we introduce ELLA, a training framework built on the principle of selective subspace de-correlation. Rather than forbidding all overlap, ELLA explicitly characterizes the structure of past updates and penalizes alignments along their high-energy, task-specific directions, while preserving freedom in the low-energy residual subspaces to enable transfer. Formally, this is realized via a lightweight regularizer on a single aggregated update matrix. We prove this mechanism corresponds to an anisotropic shrinkage operator that bounds interference, yielding a penalty that is both memory- and compute-constant regardless of task sequence length. ELLA requires no data replay, no architectural expansion, and negligible storage. Empirically, it achieves state-of-the-art CL performance on three popular benchmarks, with relative accuracy gains of up to $9.6\\%$ and a $35\\times$ smaller memory footprint. Further, ELLA scales robustly across architectures and actively enhances the model's zero-shot generalization performance on unseen tasks, establishing a principled and scalable solution for constructive lifelong LLM adaptation.",
      "authors": [
        "Shristi Das Biswas",
        "Yue Zhang",
        "Anwesan Pal",
        "Radhika Bhargava",
        "Kaushik Roy"
      ],
      "published": "2026-01-05T15:58:08Z",
      "categories": [
        "cs.LG"
      ],
      "primary_category": "cs.LG",
      "abs_url": "https://arxiv.org/abs/2601.02232v1",
      "pdf_url": "https://arxiv.org/pdf/2601.02232v1.pdf"
    },
    {
      "id": "2601.02228v1",
      "title": "FMVP: Masked Flow Matching for Adversarial Video Purification",
      "abstract": "Video recognition models remain vulnerable to adversarial attacks, while existing diffusion-based purification methods suffer from inefficient sampling and curved trajectories. Directly regressing clean videos from adversarial inputs often fails to recover faithful content due to the subtle nature of perturbations; this necessitates physically shattering the adversarial structure. Therefore, we propose Flow Matching for Adversarial Video Purification FMVP. FMVP physically shatters global adversarial structures via a masking strategy and reconstructs clean video dynamics using Conditional Flow Matching (CFM) with an inpainting objective. To further decouple semantic content from adversarial noise, we design a Frequency-Gated Loss (FGL) that explicitly suppresses high-frequency adversarial residuals while preserving low-frequency fidelity. We design Attack-Aware and Generalist training paradigms to handle known and unknown threats, respectively. Extensive experiments on UCF-101 and HMDB-51 demonstrate that FMVP outperforms state-of-the-art methods (DiffPure, Defense Patterns (DP), Temporal Shuffling (TS) and FlowPure), achieving robust accuracy exceeding 87% against PGD and 89% against CW attacks. Furthermore, FMVP demonstrates superior robustness against adaptive attacks (DiffHammer) and functions as a zero-shot adversarial detector, attaining detection accuracies of 98% for PGD and 79% for highly imperceptible CW attacks.",
      "authors": [
        "Duoxun Tang",
        "Xueyi Zhang",
        "Chak Hin Wang",
        "Xi Xiao",
        "Dasen Dai",
        "Xinhang Jiang",
        "Wentao Shi",
        "Rui Li",
        "Qing Li"
      ],
      "published": "2026-01-05T15:55:46Z",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "abs_url": "https://arxiv.org/abs/2601.02228v1",
      "pdf_url": "https://arxiv.org/pdf/2601.02228v1.pdf"
    },
    {
      "id": "2601.02224v1",
      "title": "From XAI to Stories: A Factorial Study of LLM-Generated Explanation Quality",
      "abstract": "Explainable AI (XAI) methods like SHAP and LIME produce numerical feature attributions that remain inaccessible to non expert users. Prior work has shown that Large Language Models (LLMs) can transform these outputs into natural language explanations (NLEs), but it remains unclear which factors contribute to high-quality explanations. We present a systematic factorial study investigating how Forecasting model choice, XAI method, LLM selection, and prompting strategy affect NLE quality. Our design spans four models (XGBoost (XGB), Random Forest (RF), Multilayer Perceptron (MLP), and SARIMAX - comparing black-box Machine-Learning (ML) against classical time-series approaches), three XAI conditions (SHAP, LIME, and a no-XAI baseline), three LLMs (GPT-4o, Llama-3-8B, DeepSeek-R1), and eight prompting strategies. Using G-Eval, an LLM-as-a-judge evaluation method, with dual LLM judges and four evaluation criteria, we evaluate 660 explanations for time-series forecasting. Our results suggest that: (1) XAI provides only small improvements over no-XAI baselines, and only for expert audiences; (2) LLM choice dominates all other factors, with DeepSeek-R1 outperforming GPT-4o and Llama-3; (3) we observe an interpretability paradox: in our setting, SARIMAX yielded lower NLE quality than ML models despite higher prediction accuracy; (4) zero-shot prompting is competitive with self-consistency at 7-times lower cost; and (5) chain-of-thought hurts rather than helps.",
      "authors": [
        "Fabian Lukassen",
        "Jan Herrmann",
        "Christoph Weisser",
        "Benjamin Saefken",
        "Thomas Kneib"
      ],
      "published": "2026-01-05T15:52:20Z",
      "categories": [
        "cs.CL"
      ],
      "primary_category": "cs.CL",
      "abs_url": "https://arxiv.org/abs/2601.02224v1",
      "pdf_url": "https://arxiv.org/pdf/2601.02224v1.pdf"
    },
    {
      "id": "2601.02215v1",
      "title": "LLM-Empowered Functional Safety and Security by Design in Automotive Systems",
      "abstract": "This paper presents LLM-empowered workflow to support Software Defined Vehicle (SDV) software development, covering the aspects of security-aware system topology design, as well as event-driven decision-making code analysis. For code analysis we adopt event chains model which provides formal foundations to systematic validation of functional safety, taking into account the semantic validity of messages exchanged between key components, including both CAN and Vehicle Signal Specification (VSS). Analysis of security aspects for topology relies on synergy with Model-Driven Engineering (MDE) approach and Object Constraint Language (OCL) rules. Both locally deployable and proprietary solution are taken into account for evaluation within Advanced Driver-Assistance Systems (ADAS)-related scenarios.",
      "authors": [
        "Nenad Petrovic",
        "Vahid Zolfaghari",
        "Fengjunjie Pan",
        "Alois Knoll"
      ],
      "published": "2026-01-05T15:37:08Z",
      "categories": [
        "cs.SE",
        "cs.AI"
      ],
      "primary_category": "cs.SE",
      "abs_url": "https://arxiv.org/abs/2601.02215v1",
      "pdf_url": "https://arxiv.org/pdf/2601.02215v1.pdf"
    },
    {
      "id": "2601.02213v1",
      "title": "Quantized SO(3)-Equivariant Graph Neural Networks for Efficient Molecular Property Prediction",
      "abstract": "Deploying 3D graph neural networks (GNNs) that are equivariant to 3D rotations (the group SO(3)) on edge devices is challenging due to their high computational cost. This paper addresses the problem by compressing and accelerating an SO(3)-equivariant GNN using low-bit quantization techniques. Specifically, we introduce three innovations for quantized equivariant transformers: (1) a magnitude-direction decoupled quantization scheme that separately quantizes the norm and orientation of equivariant (vector) features, (2) a branch-separated quantization-aware training strategy that treats invariant and equivariant feature channels differently in an attention-based $SO(3)$-GNN, and (3) a robustness-enhancing attention normalization mechanism that stabilizes low-precision attention computations. Experiments on the QM9 and rMD17 molecular benchmarks demonstrate that our 8-bit models achieve accuracy on energy and force predictions comparable to full-precision baselines with markedly improved efficiency. We also conduct ablation studies to quantify the contribution of each component to maintain accuracy and equivariance under quantization, using the Local error of equivariance (LEE) metric. The proposed techniques enable the deployment of symmetry-aware GNNs in practical chemistry applications with 2.37--2.73x faster inference and 4x smaller model size, without sacrificing accuracy or physical symmetry.",
      "authors": [
        "Haoyu Zhou",
        "Ping Xue",
        "Tianfan Fu",
        "Hao Zhang"
      ],
      "published": "2026-01-05T15:36:04Z",
      "categories": [
        "cs.LG"
      ],
      "primary_category": "cs.LG",
      "abs_url": "https://arxiv.org/abs/2601.02213v1",
      "pdf_url": "https://arxiv.org/pdf/2601.02213v1.pdf"
    },
    {
      "id": "2601.02212v1",
      "title": "Prior-Guided DETR for Ultrasound Nodule Detection",
      "abstract": "Accurate detection of ultrasound nodules is essential for the early diagnosis and treatment of thyroid and breast cancers. However, this task remains challenging due to irregular nodule shapes, indistinct boundaries, substantial scale variations, and the presence of speckle noise that degrades structural visibility. To address these challenges, we propose a prior-guided DETR framework specifically designed for ultrasound nodule detection. Instead of relying on purely data-driven feature learning, the proposed framework progressively incorporates different prior knowledge at multiple stages of the network. First, a Spatially-adaptive Deformable FFN with Prior Regularization (SDFPR) is embedded into the CNN backbone to inject geometric priors into deformable sampling, stabilizing feature extraction for irregular and blurred nodules. Second, a Multi-scale Spatial-Frequency Feature Mixer (MSFFM) is designed to extract multi-scale structural priors, where spatial-domain processing emphasizes contour continuity and boundary cues, while frequency-domain modeling captures global morphology and suppresses speckle noise. Furthermore, a Dense Feature Interaction (DFI) mechanism propagates and exploits these prior-modulated features across all encoder layers, enabling the decoder to enhance query refinement under consistent geometric and structural guidance. Experiments conducted on two clinically collected thyroid ultrasound datasets (Thyroid I and Thyroid II) and two public benchmarks (TN3K and BUSI) for thyroid and breast nodules demonstrate that the proposed method achieves superior accuracy compared with 18 detection methods, particularly in detecting morphologically complex nodules.The source code is publicly available at https://github.com/wjj1wjj/Ultrasound-DETR.",
      "authors": [
        "Jingjing Wang",
        "Zhuo Xiao",
        "Xinning Yao",
        "Bo Liu",
        "Lijuan Niu",
        "Xiangzhi Bai",
        "Fugen Zhou"
      ],
      "published": "2026-01-05T15:32:58Z",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "abs_url": "https://arxiv.org/abs/2601.02212v1",
      "pdf_url": "https://arxiv.org/pdf/2601.02212v1.pdf"
    },
    {
      "id": "2601.02211v1",
      "title": "Unraveling MMDiT Blocks: Training-free Analysis and Enhancement of Text-conditioned Diffusion",
      "abstract": "Recent breakthroughs of transformer-based diffusion models, particularly with Multimodal Diffusion Transformers (MMDiT) driven models like FLUX and Qwen Image, have facilitated thrilling experiences in text-to-image generation and editing. To understand the internal mechanism of MMDiT-based models, existing methods tried to analyze the effect of specific components like positional encoding and attention layers. Yet, a comprehensive understanding of how different blocks and their interactions with textual conditions contribute to the synthesis process remains elusive. In this paper, we first develop a systematic pipeline to comprehensively investigate each block's functionality by removing, disabling and enhancing textual hidden-states at corresponding blocks. Our analysis reveals that 1) semantic information appears in earlier blocks and finer details are rendered in later blocks, 2) removing specific blocks is usually less disruptive than disabling text conditions, and 3) enhancing textual conditions in selective blocks improves semantic attributes. Building on these observations, we further propose novel training-free strategies for improved text alignment, precise editing, and acceleration. Extensive experiments demonstrated that our method outperforms various baselines and remains flexible across text-to-image generation, image editing, and inference acceleration. Our method improves T2I-Combench++ from 56.92% to 63.00% and GenEval from 66.42% to 71.63% on SD3.5, without sacrificing synthesis quality. These results advance understanding of MMDiT models and provide valuable insights to unlock new possibilities for further improvements.",
      "authors": [
        "Binglei Li",
        "Mengping Yang",
        "Zhiyu Tan",
        "Junping Zhang",
        "Hao Li"
      ],
      "published": "2026-01-05T15:32:53Z",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "abs_url": "https://arxiv.org/abs/2601.02211v1",
      "pdf_url": "https://arxiv.org/pdf/2601.02211v1.pdf"
    },
    {
      "id": "2601.02209v1",
      "title": "ARCADE: A City-Scale Corpus for Fine-Grained Arabic Dialect Tagging",
      "abstract": "The Arabic language is characterized by a rich tapestry of regional dialects that differ substantially in phonetics and lexicon, reflecting the geographic and cultural diversity of its speakers. Despite the availability of many multi-dialect datasets, mapping speech to fine-grained dialect sources, such as cities, remains underexplored. We present ARCADE (Arabic Radio Corpus for Audio Dialect Evaluation), the first Arabic speech dataset designed explicitly with city-level dialect granularity. The corpus comprises Arabic radio speech collected from streaming services across the Arab world. Our data pipeline captures 30-second segments from verified radio streams, encompassing both Modern Standard Arabic (MSA) and diverse dialectal speech. To ensure reliability, each clip was annotated by one to three native Arabic reviewers who assigned rich metadata, including emotion, speech type, dialect category, and a validity flag for dialect identification tasks. The resulting corpus comprises 6,907 annotations and 3,790 unique audio segments spanning 58 cities across 19 countries. These fine-grained annotations enable robust multi-task learning, serving as a benchmark for city-level dialect tagging. We detail the data collection methodology, assess audio quality, and provide a comprehensive analysis of label distributions. The dataset is available on: https://huggingface.co/datasets/riotu-lab/ARCADE-full",
      "authors": [
        "Omer Nacar",
        "Serry Sibaee",
        "Adel Ammar",
        "Yasser Alhabashi",
        "Nadia Samer Sibai",
        "Yara Farouk Ahmed",
        "Ahmed Saud Alqusaiyer",
        "Sulieman Mahmoud AlMahmoud",
        "Abdulrhman Mamdoh Mukhaniq",
        "Lubaba Raed",
        "Sulaiman Mohammed Alatwah",
        "Waad Nasser Alqahtani",
        "Yousif Abdulmajeed Alnasser",
        "Mohamed Aziz Khadraoui",
        "Wadii Boulila"
      ],
      "published": "2026-01-05T15:32:17Z",
      "categories": [
        "cs.CL",
        "cs.CY",
        "cs.SD"
      ],
      "primary_category": "cs.CL",
      "abs_url": "https://arxiv.org/abs/2601.02209v1",
      "pdf_url": "https://arxiv.org/pdf/2601.02209v1.pdf"
    },
    {
      "id": "2601.02206v1",
      "title": "Seeing the Unseen: Zooming in the Dark with Event Cameras",
      "abstract": "This paper addresses low-light video super-resolution (LVSR), aiming to restore high-resolution videos from low-light, low-resolution (LR) inputs. Existing LVSR methods often struggle to recover fine details due to limited contrast and insufficient high-frequency information. To overcome these challenges, we present RetinexEVSR, the first event-driven LVSR framework that leverages high-contrast event signals and Retinex-inspired priors to enhance video quality under low-light scenarios. Unlike previous approaches that directly fuse degraded signals, RetinexEVSR introduces a novel bidirectional cross-modal fusion strategy to extract and integrate meaningful cues from noisy event data and degraded RGB frames. Specifically, an illumination-guided event enhancement module is designed to progressively refine event features using illumination maps derived from the Retinex model, thereby suppressing low-light artifacts while preserving high-contrast details. Furthermore, we propose an event-guided reflectance enhancement module that utilizes the enhanced event features to dynamically recover reflectance details via a multi-scale fusion mechanism. Experimental results show that our RetinexEVSR achieves state-of-the-art performance on three datasets. Notably, on the SDSD benchmark, our method can get up to 2.95 dB gain while reducing runtime by 65% compared to prior event-based methods. Code: https://github.com/DachunKai/RetinexEVSR.",
      "authors": [
        "Dachun Kai",
        "Zeyu Xiao",
        "Huyue Zhu",
        "Jiaxiao Wang",
        "Yueyi Zhang",
        "Xiaoyan Sun"
      ],
      "published": "2026-01-05T15:31:07Z",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "abs_url": "https://arxiv.org/abs/2601.02206v1",
      "pdf_url": "https://arxiv.org/pdf/2601.02206v1.pdf"
    },
    {
      "id": "2601.02204v1",
      "title": "NextFlow: Unified Sequential Modeling Activates Multimodal Understanding and Generation",
      "abstract": "We present NextFlow, a unified decoder-only autoregressive transformer trained on 6 trillion interleaved text-image discrete tokens. By leveraging a unified vision representation within a unified autoregressive architecture, NextFlow natively activates multimodal understanding and generation capabilities, unlocking abilities of image editing, interleaved content and video generation. Motivated by the distinct nature of modalities - where text is strictly sequential and images are inherently hierarchical - we retain next-token prediction for text but adopt next-scale prediction for visual generation. This departs from traditional raster-scan methods, enabling the generation of 1024x1024 images in just 5 seconds - orders of magnitude faster than comparable AR models. We address the instabilities of multi-scale generation through a robust training recipe. Furthermore, we introduce a prefix-tuning strategy for reinforcement learning. Experiments demonstrate that NextFlow achieves state-of-the-art performance among unified models and rivals specialized diffusion baselines in visual quality.",
      "authors": [
        "Huichao Zhang",
        "Liao Qu",
        "Yiheng Liu",
        "Hang Chen",
        "Yangyang Song",
        "Yongsheng Dong",
        "Shikun Sun",
        "Xian Li",
        "Xu Wang",
        "Yi Jiang",
        "Hu Ye",
        "Bo Chen",
        "Yiming Gao",
        "Peng Liu",
        "Akide Liu",
        "Zhipeng Yang",
        "Qili Deng",
        "Linjie Xing",
        "Jiyang Liu",
        "Zhao Wang",
        "Yang Zhou",
        "Mingcong Liu",
        "Yi Zhang",
        "Qian He",
        "Xiwei Hu",
        "Zhongqi Qi",
        "Jie Shao",
        "Zhiye Fu",
        "Shuai Wang",
        "Fangmin Chen",
        "Xuezhi Chai",
        "Zhihua Wu",
        "Yitong Wang",
        "Zehuan Yuan",
        "Daniel K. Du",
        "Xinglong Wu"
      ],
      "published": "2026-01-05T15:27:04Z",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "abs_url": "https://arxiv.org/abs/2601.02204v1",
      "pdf_url": "https://arxiv.org/pdf/2601.02204v1.pdf"
    },
    {
      "id": "2601.02203v1",
      "title": "Parameter-Efficient Domain Adaption for CSI Crowd-Counting via Self-Supervised Learning with Adapter Modules",
      "abstract": "Device-free crowd-counting using WiFi Channel State Information (CSI) is a key enabling technology for a new generation of privacy-preserving Internet of Things (IoT) applications. However, practical deployment is severely hampered by the domain shift problem, where models trained in one environment fail to generalise to another. To overcome this, we propose a novel two-stage framework centred on a CSI-ResNet-A architecture. This model is pre-trained via self-supervised contrastive learning to learn domain-invariant representations and leverages lightweight Adapter modules for highly efficient fine-tuning. The resulting event sequence is then processed by a stateful counting machine to produce a final, stable occupancy estimate. We validate our framework extensively. On our WiFlow dataset, our unsupervised approach excels in a 10-shot learning scenario, achieving a final Mean Absolute Error (MAE) of just 0.44--a task where supervised baselines fail. To formally quantify robustness, we introduce the Generalisation Index (GI), on which our model scores near-perfectly, confirming its ability to generalise. Furthermore, our framework sets a new state-of-the-art public WiAR benchmark with 98.8\\% accuracy. Our ablation studies reveal the core strength of our design: adapter-based fine-tuning achieves performance within 1\\% of a full fine-tune (98.84\\% vs. 99.67\\%) while training 97.2\\% fewer parameters. Our work provides a practical and scalable solution for developing robust sensing systems ready for real-world IoT deployments.",
      "authors": [
        "Oliver Custance",
        "Saad Khan",
        "Simon Parkinson",
        "Quan Z. Sheng"
      ],
      "published": "2026-01-05T15:27:04Z",
      "categories": [
        "cs.CV",
        "cs.CR"
      ],
      "primary_category": "cs.CV",
      "abs_url": "https://arxiv.org/abs/2601.02203v1",
      "pdf_url": "https://arxiv.org/pdf/2601.02203v1.pdf"
    },
    {
      "id": "2601.02201v1",
      "title": "CORE: Code-based Inverse Self-Training Framework with Graph Expansion for Virtual Agents",
      "abstract": "The development of Multimodal Virtual Agents has made significant progress through the integration of Multimodal Large Language Models. However, mainstream training paradigms face key challenges: Behavior Cloning is simple and effective through imitation but suffers from low behavioral diversity, while Reinforcement Learning is capable of discovering novel strategies through exploration but heavily relies on manually designed reward functions. To address the conflict between these two methods, we present CORE, a Code-based Inverse Self-Training Framework with Graph Expansion that bridges imitation and exploration, offering a novel training framework that promotes behavioral diversity while eliminating the reliance on manually reward design. Specifically, we introduce Semantic Code Abstraction to automatically infers reward functions from expert demonstrations without manual design. The inferred reward function, referred to as the Label Function, is executable code that verifies one key step within a task. Building on this, we propose Strategy Graph Expansion to enhance in-domain behavioral diversity, which constructs a multi-path graph called Strategy Graph that captures diverse valid solutions beyond expert demonstrations. Furthermore, we introduce Trajectory-Guided Extrapolation, which enriches out-of-domain behavioral diversity by utilizing both successful and failed trajectories to expand the task space. Experiments on Web and Android platforms demonstrate that CORE significantly improves both overall performance and generalization, highlighting its potential as a robust and generalizable training paradigm for building powerful virtual agents.",
      "authors": [
        "Keyu Wang",
        "Bingchen Miao",
        "Wendong Bu",
        "Yu Wu",
        "Juncheng Li",
        "Shengyu Zhang",
        "Wenqiao Zhang",
        "Siliang Tang",
        "Jun Xiao",
        "Yueting Zhuang"
      ],
      "published": "2026-01-05T15:24:05Z",
      "categories": [
        "cs.LG",
        "cs.CV"
      ],
      "primary_category": "cs.LG",
      "abs_url": "https://arxiv.org/abs/2601.02201v1",
      "pdf_url": "https://arxiv.org/pdf/2601.02201v1.pdf"
    },
    {
      "id": "2601.02200v1",
      "title": "Code for Machines, Not Just Humans: Quantifying AI-Friendliness with Code Health Metrics",
      "abstract": "We are entering a hybrid era in which human developers and AI coding agents work in the same codebases. While industry practice has long optimized code for human comprehension, it is increasingly important to ensure that LLMs with different capabilities can edit code reliably. In this study, we investigate the concept of ``AI-friendly code'' via LLM-based refactoring on a dataset of 5,000 Python files from competitive programming. We find a meaningful association between CodeHealth, a quality metric calibrated for human comprehension, and semantic preservation after AI refactoring. Our findings confirm that human-friendly code is also more compatible with AI tooling. These results suggest that organizations can use CodeHealth to guide where AI interventions are lower risk and where additional human oversight is warranted. Investing in maintainability not only helps humans; it also prepares for large-scale AI adoption.",
      "authors": [
        "Markus Borg",
        "Nadim Hagatulah",
        "Adam Tornhill",
        "Emma Söderberg"
      ],
      "published": "2026-01-05T15:23:55Z",
      "categories": [
        "cs.SE",
        "cs.AI"
      ],
      "primary_category": "cs.SE",
      "abs_url": "https://arxiv.org/abs/2601.02200v1",
      "pdf_url": "https://arxiv.org/pdf/2601.02200v1.pdf"
    },
    {
      "id": "2601.02198v1",
      "title": "Mind the Gap: Continuous Magnification Sampling for Pathology Foundation Models",
      "abstract": "In histopathology, pathologists examine both tissue architecture at low magnification and fine-grained morphology at high magnification. Yet, the performance of pathology foundation models across magnifications and the effect of magnification sampling during training remain poorly understood. We model magnification sampling as a multi-source domain adaptation problem and develop a simple theoretical framework that reveals systematic trade-offs between sampling strategies. We show that the widely used discrete uniform sampling of magnifications (0.25, 0.5, 1.0, 2.0 mpp) leads to degradation at intermediate magnifications. We introduce continuous magnification sampling, which removes gaps in magnification coverage while preserving performance at standard scales. Further, we derive sampling distributions that optimize representation quality across magnification scales. To evaluate these strategies, we introduce two new benchmarks (TCGA-MS, BRACS-MS) with appropriate metrics. Our experiments show that continuous sampling substantially improves over discrete sampling at intermediate magnifications, with gains of up to 4 percentage points in balanced classification accuracy, and that optimized distributions can further improve performance. Finally, we evaluate current histopathology foundation models, finding that magnification is a primary driver of performance variation across models. Our work paves the way towards future pathology foundation models that perform reliably across magnifications.",
      "authors": [
        "Alexander Möllers",
        "Julius Hense",
        "Florian Schulz",
        "Timo Milbich",
        "Maximilian Alber",
        "Lukas Ruff"
      ],
      "published": "2026-01-05T15:19:59Z",
      "categories": [
        "cs.CV",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "abs_url": "https://arxiv.org/abs/2601.02198v1",
      "pdf_url": "https://arxiv.org/pdf/2601.02198v1.pdf"
    },
    {
      "id": "2601.02196v1",
      "title": "ACDZero: Graph-Embedding-Based Tree Search for Mastering Automated Cyber Defense",
      "abstract": "Automated cyber defense (ACD) seeks to protect computer networks with minimal or no human intervention, reacting to intrusions by taking corrective actions such as isolating hosts, resetting services, deploying decoys, or updating access controls. However, existing approaches for ACD, such as deep reinforcement learning (RL), often face difficult exploration in complex networks with large decision/state spaces and thus require an expensive amount of samples. Inspired by the need to learn sample-efficient defense policies, we frame ACD in CAGE Challenge 4 (CAGE-4 / CC4) as a context-based partially observable Markov decision problem and propose a planning-centric defense policy based on Monte Carlo Tree Search (MCTS). It explicitly models the exploration-exploitation tradeoff in ACD and uses statistical sampling to guide exploration and decision making. We make novel use of graph neural networks (GNNs) to embed observations from the network as attributed graphs, to enable permutation-invariant reasoning over hosts and their relationships. To make our solution practical in complex search spaces, we guide MCTS with learned graph embeddings and priors over graph-edit actions, combining model-free generalization and policy distillation with look-ahead planning. We evaluate the resulting agent on CC4 scenarios involving diverse network structures and adversary behaviors, and show that our search-guided, graph-embedding-based planning improves defense reward and robustness relative to state-of-the-art RL baselines.",
      "authors": [
        "Yu Li",
        "Sizhe Tang",
        "Rongqian Chen",
        "Fei Xu Yu",
        "Guangyu Jiang",
        "Mahdi Imani",
        "Nathaniel D. Bastian",
        "Tian Lan"
      ],
      "published": "2026-01-05T15:18:54Z",
      "categories": [
        "cs.LG"
      ],
      "primary_category": "cs.LG",
      "abs_url": "https://arxiv.org/abs/2601.02196v1",
      "pdf_url": "https://arxiv.org/pdf/2601.02196v1.pdf"
    },
    {
      "id": "2601.02193v1",
      "title": "Learning with Monotone Adversarial Corruptions",
      "abstract": "We study the extent to which standard machine learning algorithms rely on exchangeability and independence of data by introducing a monotone adversarial corruption model. In this model, an adversary, upon looking at a \"clean\" i.i.d. dataset, inserts additional \"corrupted\" points of their choice into the dataset. These added points are constrained to be monotone corruptions, in that they get labeled according to the ground-truth target function. Perhaps surprisingly, we demonstrate that in this setting, all known optimal learning algorithms for binary classification can be made to achieve suboptimal expected error on a new independent test point drawn from the same distribution as the clean dataset. On the other hand, we show that uniform convergence-based algorithms do not degrade in their guarantees. Our results showcase how optimal learning algorithms break down in the face of seemingly helpful monotone corruptions, exposing their overreliance on exchangeability.",
      "authors": [
        "Kasper Green Larsen",
        "Chirag Pabbaraju",
        "Abhishek Shetty"
      ],
      "published": "2026-01-05T15:16:26Z",
      "categories": [
        "cs.LG",
        "cs.DS",
        "stat.ML"
      ],
      "primary_category": "cs.LG",
      "abs_url": "https://arxiv.org/abs/2601.02193v1",
      "pdf_url": "https://arxiv.org/pdf/2601.02193v1.pdf"
    },
    {
      "id": "2601.02189v1",
      "title": "QuIC: A Quantum-Inspired Interaction Classifier for Revitalizing Shallow CNNs in Fine-Grained Recognition",
      "abstract": "Deploying deep learning models for Fine-Grained Visual Classification (FGVC) on resource-constrained edge devices remains a significant challenge. While deep architectures achieve high accuracy on benchmarks like CUB-200-2011, their computational cost is often prohibitive. Conversely, shallow networks (e.g., AlexNet, VGG) offer efficiency but fail to distinguish visually similar sub-categories. This is because standard Global Average Pooling (GAP) heads capture only first-order statistics, missing the subtle high-order feature interactions required for FGVC. While Bilinear CNNs address this, they suffer from high feature dimensionality and instability during training. To bridge this gap, we propose the Quantum-inspired Interaction Classifier (QuIC). Drawing inspiration from quantum mechanics, QuIC models feature channels as interacting quantum states and captures second-order feature covariance via a learnable observable operator. Designed as a lightweight, plug-and-play module, QuIC supports stable, single-stage end-to-end training without exploding feature dimensions. Experimental results demonstrate that QuIC significantly revitalizes shallow backbones: it boosts the Top-1 accuracy of VGG16 by nearly 20% and outperforms state-of-the-art attention mechanisms (SE-Block) on ResNet18. Qualitative analysis, including t-SNE visualization, further confirms that QuIC resolves ambiguous cases by explicitly attending to fine-grained discriminative features and enforcing compact intra-class clustering.",
      "authors": [
        "Cheng Ying Wu",
        "Yen Jui Chang"
      ],
      "published": "2026-01-05T15:09:18Z",
      "categories": [
        "cs.CV",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "abs_url": "https://arxiv.org/abs/2601.02189v1",
      "pdf_url": "https://arxiv.org/pdf/2601.02189v1.pdf"
    },
    {
      "id": "2601.02186v1",
      "title": "Toward Global Large Language Models in Medicine",
      "abstract": "Despite continuous advances in medical technology, the global distribution of health care resources remains uneven. The development of large language models (LLMs) has transformed the landscape of medicine and holds promise for improving health care quality and expanding access to medical information globally. However, existing LLMs are primarily trained on high-resource languages, limiting their applicability in global medical scenarios. To address this gap, we constructed GlobMed, a large multilingual medical dataset, containing over 500,000 entries spanning 12 languages, including four low-resource languages. Building on this, we established GlobMed-Bench, which systematically assesses 56 state-of-the-art proprietary and open-weight LLMs across multiple multilingual medical tasks, revealing significant performance disparities across languages, particularly for low-resource languages. Additionally, we introduced GlobMed-LLMs, a suite of multilingual medical LLMs trained on GlobMed, with parameters ranging from 1.7B to 8B. GlobMed-LLMs achieved an average performance improvement of over 40% relative to baseline models, with a more than threefold increase in performance on low-resource languages. Together, these resources provide an important foundation for advancing the equitable development and application of LLMs globally, enabling broader language communities to benefit from technological advances.",
      "authors": [
        "Rui Yang",
        "Huitao Li",
        "Weihao Xuan",
        "Heli Qi",
        "Xin Li",
        "Kunyu Yu",
        "Yingjian Chen",
        "Rongrong Wang",
        "Jacques Behmoaras",
        "Tianxi Cai",
        "Bibhas Chakraborty",
        "Qingyu Chen",
        "Lionel Tim-Ee Cheng",
        "Marie-Louise Damwanza",
        "Chido Dzinotyiwei",
        "Aosong Feng",
        "Chuan Hong",
        "Yusuke Iwasawa",
        "Yuhe Ke",
        "Linah Kitala",
        "Taehoon Ko",
        "Jisan Lee",
        "Irene Li",
        "Jonathan Chong Kai Liew",
        "Hongfang Liu",
        "Lian Leng Low",
        "Edison Marrese-Taylor",
        "Yutaka Matsuo",
        "Isheanesu Misi",
        "Yilin Ning",
        "Jasmine Chiat Ling Ong",
        "Marcus Eng Hock Ong",
        "Enrico Petretto",
        "Hossein Rouhizadeh",
        "Abiram Sandralegar",
        "Oren Schreier",
        "Iain Bee Huat Tan",
        "Patrick Tan",
        "Daniel Shu Wei Ting",
        "Junjue Wang",
        "Chunhua Weng",
        "Matthew Yu Heng Wong",
        "Fang Wu",
        "Yunze Xiao",
        "Xuhai Xu",
        "Qingcheng Zeng",
        "Zhuo Zheng",
        "Yifan Peng",
        "Douglas Teodoro",
        "Nan Liu"
      ],
      "published": "2026-01-05T15:05:49Z",
      "categories": [
        "cs.CL"
      ],
      "primary_category": "cs.CL",
      "abs_url": "https://arxiv.org/abs/2601.02186v1",
      "pdf_url": "https://arxiv.org/pdf/2601.02186v1.pdf"
    },
    {
      "id": "2601.02184v1",
      "title": "Differential Barometric Altimetry for Submeter Vertical Localization and Floor Recognition Indoors",
      "abstract": "Accurate altitude estimation and reliable floor recognition are critical for mobile robot localization and navigation within complex multi-storey environments. In this paper, we present a robust, low-cost vertical estimation framework leveraging differential barometric sensing integrated within a fully ROS-compliant software package. Our system simultaneously publishes real-time altitude data from both a stationary base station and a mobile sensor, enabling precise and drift-free vertical localization. Empirical evaluations conducted in challenging scenarios -- such as fully enclosed stairwells and elevators, demonstrate that our proposed barometric pipeline achieves sub-meter vertical accuracy (RMSE: 0.29 m) and perfect (100%) floor-level identification. In contrast, our results confirm that standalone height estimates, obtained solely from visual- or LiDAR-based SLAM odometry, are insufficient for reliable vertical localization. The proposed ROS-compatible barometric module thus provides a practical and cost-effective solution for robust vertical awareness in real-world robotic deployments. The implementation of our method is released as open source at https://github.com/witsir/differential-barometric.",
      "authors": [
        "Yuhang Zhang",
        "Sören Schwertfeger"
      ],
      "published": "2026-01-05T15:03:55Z",
      "categories": [
        "cs.RO"
      ],
      "primary_category": "cs.RO",
      "abs_url": "https://arxiv.org/abs/2601.02184v1",
      "pdf_url": "https://arxiv.org/pdf/2601.02184v1.pdf"
    },
    {
      "id": "2601.02179v1",
      "title": "Confidence Estimation for LLMs in Multi-turn Interactions",
      "abstract": "While confidence estimation is a promising direction for mitigating hallucinations in Large Language Models (LLMs), current research dominantly focuses on single-turn settings. The dynamics of model confidence in multi-turn conversations, where context accumulates and ambiguity is progressively resolved, remain largely unexplored. Reliable confidence estimation in multi-turn settings is critical for many downstream applications, such as autonomous agents and human-in-the-loop systems. This work presents the first systematic study of confidence estimation in multi-turn interactions, establishing a formal evaluation framework grounded in two key desiderata: per-turn calibration and monotonicity of confidence as more information becomes available. To facilitate this, we introduce novel metrics, including a length-normalized Expected Calibration Error (InfoECE), and a new \"Hinter-Guesser\" paradigm for generating controlled evaluation datasets. Our experiments reveal that widely-used confidence techniques struggle with calibration and monotonicity in multi-turn dialogues. We propose P(Sufficient), a logit-based probe that achieves comparatively better performance, although the task remains far from solved. Our work provides a foundational methodology for developing more reliable and trustworthy conversational agents.",
      "authors": [
        "Caiqi Zhang",
        "Ruihan Yang",
        "Xiaochen Zhu",
        "Chengzu Li",
        "Tiancheng Hu",
        "Yijiang River Dong",
        "Deqing Yang",
        "Nigel Collier"
      ],
      "published": "2026-01-05T14:58:04Z",
      "categories": [
        "cs.CL"
      ],
      "primary_category": "cs.CL",
      "abs_url": "https://arxiv.org/abs/2601.02179v1",
      "pdf_url": "https://arxiv.org/pdf/2601.02179v1.pdf"
    },
    {
      "id": "2601.02177v1",
      "title": "Why Commodity WiFi Sensors Fail at Multi-Person Gait Identification: A Systematic Analysis Using ESP32",
      "abstract": "WiFi Channel State Information (CSI) has shown promise for single-person gait identification, with numerous studies reporting high accuracy. However, multi-person identification remains largely unexplored, with the limited existing work relying on complex, expensive setups requiring modified firmware. A critical question remains unanswered: is poor multi-person performance an algorithmic limitation or a fundamental hardware constraint? We systematically evaluate six diverse signal separation methods (FastICA, SOBI, PCA, NMF, Wavelet, Tensor Decomposition) across seven scenarios with 1-10 people using commodity ESP32 WiFi sensors--a simple, low-cost, off-the-shelf solution. Through novel diagnostic metrics (intra-subject variability, inter-subject distinguishability, performance degradation rate), we reveal that all methods achieve similarly low accuracy (45-56\\%, $σ$=3.74\\%) with statistically insignificant differences (p $>$ 0.05). Even the best-performing method, NMF, achieves only 56\\% accuracy. Our analysis reveals high intra-subject variability, low inter-subject distinguishability, and severe performance degradation as person count increases, indicating that commodity ESP32 sensors cannot provide sufficient signal quality for reliable multi-person separation.",
      "authors": [
        "Oliver Custance",
        "Saad Khan",
        "Simon Parkinson"
      ],
      "published": "2026-01-05T14:55:38Z",
      "categories": [
        "cs.CV",
        "cs.CR"
      ],
      "primary_category": "cs.CV",
      "abs_url": "https://arxiv.org/abs/2601.02177v1",
      "pdf_url": "https://arxiv.org/pdf/2601.02177v1.pdf"
    },
    {
      "id": "2601.02170v1",
      "title": "Streaming Hallucination Detection in Long Chain-of-Thought Reasoning",
      "abstract": "Long chain-of-thought (CoT) reasoning improves the performance of large language models, yet hallucinations in such settings often emerge subtly and propagate across reasoning steps. We suggest that hallucination in long CoT reasoning is better understood as an evolving latent state rather than a one-off erroneous event. Accordingly, we treat step-level hallucination judgments as local observations and introduce a cumulative prefix-level hallucination signal that tracks the global evolution of the reasoning state over the entire trajectory. Overall, our approach enables streaming hallucination detection in long CoT reasoning, providing real-time, interpretable evidence.",
      "authors": [
        "Haolang Lu",
        "Minghui Pan",
        "Ripeng Li",
        "Guoshun Nan",
        "Jialin Zhuang",
        "Zijie Zhao",
        "Zhongxiang Sun",
        "Kun Wang",
        "Yang Liu"
      ],
      "published": "2026-01-05T14:47:41Z",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "abs_url": "https://arxiv.org/abs/2601.02170v1",
      "pdf_url": "https://arxiv.org/pdf/2601.02170v1.pdf"
    },
    {
      "id": "2601.02163v1",
      "title": "EverMemOS: A Self-Organizing Memory Operating System for Structured Long-Horizon Reasoning",
      "abstract": "Large Language Models (LLMs) are increasingly deployed as long-term interactive agents, yet their limited context windows make it difficult to sustain coherent behavior over extended interactions. Existing memory systems often store isolated records and retrieve fragments, limiting their ability to consolidate evolving user states and resolve conflicts. We introduce EverMemOS, a self-organizing memory operating system that implements an engram-inspired lifecycle for computational memory. Episodic Trace Formation converts dialogue streams into MemCells that capture episodic traces, atomic facts, and time-bounded Foresight signals. Semantic Consolidation organizes MemCells into thematic MemScenes, distilling stable semantic structures and updating user profiles. Reconstructive Recollection performs MemScene-guided agentic retrieval to compose the necessary and sufficient context for downstream reasoning. Experiments on LoCoMo and LongMemEval show that EverMemOS achieves state-of-the-art performance on memory-augmented reasoning tasks. We further report a profile study on PersonaMem v2 and qualitative case studies illustrating chat-oriented capabilities such as user profiling and Foresight. Code is available at https://github.com/EverMind-AI/EverMemOS.",
      "authors": [
        "Chuanrui Hu",
        "Xingze Gao",
        "Zuyi Zhou",
        "Dannong Xu",
        "Yi Bai",
        "Xintong Li",
        "Hui Zhang",
        "Tong Li",
        "Chong Zhang",
        "Lidong Bing",
        "Yafeng Deng"
      ],
      "published": "2026-01-05T14:39:43Z",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.AI",
      "abs_url": "https://arxiv.org/abs/2601.02163v1",
      "pdf_url": "https://arxiv.org/pdf/2601.02163v1.pdf"
    },
    {
      "id": "2601.02158v1",
      "title": "FormationEval, an open multiple-choice benchmark for petroleum geoscience",
      "abstract": "This paper presents FormationEval, an open multiple-choice question benchmark for evaluating language models on petroleum geoscience and subsurface disciplines. The dataset contains 505 questions across seven domains including petrophysics, petroleum geology and reservoir engineering, derived from three authoritative sources using a reasoning model with detailed instructions and a concept-based approach that avoids verbatim copying of copyrighted text. Each question includes source metadata to support traceability and audit. The evaluation covers 72 models from major providers including OpenAI, Anthropic, Google, Meta and open-weight alternatives. The top performers achieve over 97\\% accuracy, with Gemini 3 Pro Preview reaching 99.8\\%, while tier and domain gaps persist. Among open-weight models, GLM-4.7 leads at 98.6\\%, with several DeepSeek, Llama, Qwen and Mistral models also exceeding 93\\%. The performance gap between open-weight and closed models is narrower than expected, with several lower-cost open-weight models exceeding 90\\% accuracy. Petrophysics emerges as the most challenging domain across all models, while smaller models show wider performance variance. Residual length bias in the dataset (correct answers tend to be longer) is documented along with bias mitigation strategies applied during construction. The benchmark, evaluation code and results are publicly available.",
      "authors": [
        "Almaz Ermilov"
      ],
      "published": "2026-01-05T14:36:02Z",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG",
        "physics.geo-ph"
      ],
      "primary_category": "cs.CL",
      "abs_url": "https://arxiv.org/abs/2601.02158v1",
      "pdf_url": "https://arxiv.org/pdf/2601.02158v1.pdf"
    },
    {
      "id": "2601.02151v1",
      "title": "Entropy-Adaptive Fine-Tuning: Resolving Confident Conflicts to Mitigate Forgetting",
      "abstract": "Supervised Fine-Tuning (SFT) is the standard paradigm for domain adaptation, yet it frequently incurs the cost of catastrophic forgetting. In sharp contrast, on-policy Reinforcement Learning (RL) effectively preserves general capabilities. We investigate this discrepancy and identify a fundamental distributional gap: while RL aligns with the model's internal belief, SFT forces the model to fit external supervision. This mismatch often manifests as \"Confident Conflicts\" tokens characterized by low probability but low entropy. In these instances, the model is highly confident in its own prediction but is forced to learn a divergent ground truth, triggering destructive gradient updates. To address this, we propose Entropy-Adaptive Fine-Tuning (EAFT). Unlike methods relying solely on prediction probability, EAFT utilizes token-level entropy as a gating mechanism to distinguish between epistemic uncertainty and knowledge conflict. This allows the model to learn from uncertain samples while suppressing gradients on conflicting data. Extensive experiments on Qwen and GLM series (ranging from 4B to 32B parameters) across mathematical, medical, and agentic domains confirm our hypothesis. EAFT consistently matches the downstream performance of standard SFT while significantly mitigating the degradation of general capabilities.",
      "authors": [
        "Muxi Diao",
        "Lele Yang",
        "Wuxuan Gong",
        "Yutong Zhang",
        "Zhonghao Yan",
        "Yufei Han",
        "Kongming Liang",
        "Weiran Xu",
        "Zhanyu Ma"
      ],
      "published": "2026-01-05T14:28:17Z",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.LG",
      "abs_url": "https://arxiv.org/abs/2601.02151v1",
      "pdf_url": "https://arxiv.org/pdf/2601.02151v1.pdf"
    },
    {
      "id": "2601.02149v1",
      "title": "AI-enhanced tuning of quantum dot Hamiltonians toward Majorana modes",
      "abstract": "We propose a neural network-based model capable of learning the broad landscape of working regimes in quantum dot simulators, and using this knowledge to autotune these devices - based on transport measurements - toward obtaining Majorana modes in the structure. The model is trained in an unsupervised manner on synthetic data in the form of conductance maps, using a physics-informed loss that incorporates key properties of Majorana zero modes. We show that, with appropriate training, a deep vision-transformer network can efficiently memorize relation between Hamiltonian parameters and structures on conductance maps and use it to propose parameters update for a quantum dot chain that drive the system toward topological phase. Starting from a broad range of initial detunings in parameter space, a single update step is sufficient to generate nontrivial zero modes. Moreover, by enabling an iterative tuning procedure - where the system acquires updated conductance maps at each step - we demonstrate that the method can address a much larger region of the parameter space.",
      "authors": [
        "Mateusz Krawczyk",
        "Jarosław Pawłowski"
      ],
      "published": "2026-01-05T14:25:49Z",
      "categories": [
        "cond-mat.mes-hall",
        "cond-mat.dis-nn",
        "cs.AI"
      ],
      "primary_category": "cond-mat.mes-hall",
      "abs_url": "https://arxiv.org/abs/2601.02149v1",
      "pdf_url": "https://arxiv.org/pdf/2601.02149v1.pdf"
    },
    {
      "id": "2601.02147v1",
      "title": "BiPrompt: Bilateral Prompt Optimization for Visual and Textual Debiasing in Vision-Language Models",
      "abstract": "Vision language foundation models such as CLIP exhibit impressive zero-shot generalization yet remain vulnerable to spurious correlations across visual and textual modalities. Existing debiasing approaches often address a single modality either visual or textual leading to partial robustness and unstable adaptation under distribution shifts. We propose a bilateral prompt optimization framework (BiPrompt) that simultaneously mitigates non-causal feature reliance in both modalities during test-time adaptation. On the visual side, it employs structured attention-guided erasure to suppress background activations and enforce orthogonal prediction consistency between causal and spurious regions. On the textual side, it introduces balanced prompt normalization, a learnable re-centering mechanism that aligns class embeddings toward an isotropic semantic space. Together, these modules jointly minimize conditional mutual information between spurious cues and predictions, steering the model toward causal, domain invariant reasoning without retraining or domain supervision. Extensive evaluations on real-world and synthetic bias benchmarks demonstrate consistent improvements in both average and worst-group accuracies over prior test-time debiasing methods, establishing a lightweight yet effective path toward trustworthy and causally grounded vision-language adaptation.",
      "authors": [
        "Sunny Gupta",
        "Shounak Das",
        "Amit Sethi"
      ],
      "published": "2026-01-05T14:22:20Z",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "abs_url": "https://arxiv.org/abs/2601.02147v1",
      "pdf_url": "https://arxiv.org/pdf/2601.02147v1.pdf"
    },
    {
      "id": "2601.02145v1",
      "title": "Feature-based Inversion of 2.5D Controlled Source Electromagnetic Data using Generative Priors",
      "abstract": "In this study, we investigate feature-based 2.5D controlled source marine electromagnetic (mCSEM) data inversion using generative priors. Two-and-half dimensional modeling using finite difference method (FDM) is adopted to compute the response of horizontal electric dipole (HED) excitation. Rather than using a neural network to approximate the entire inverse mapping in a black-box manner, we adopt a plug-andplay strategy in which a variational autoencoder (VAE) is used solely to learn prior information on conductivity distributions. During the inversion process, the conductivity model is iteratively updated using the Gauss Newton method, while the model space is constrained by projections onto the learned VAE decoder. This framework preserves explicit control over data misfit and enables flexible adaptation to different survey configurations. Numerical and field experiments demonstrate that the proposed approach effectively incorporates prior information, improves reconstruction accuracy, and exhibits good generalization performance.",
      "authors": [
        "Hongyu Zhou",
        "Haoran Sun",
        "Rui Guo",
        "Maokun Li",
        "Fan Yang",
        "Shenheng Xu"
      ],
      "published": "2026-01-05T14:18:14Z",
      "categories": [
        "physics.geo-ph",
        "cs.LG"
      ],
      "primary_category": "physics.geo-ph",
      "abs_url": "https://arxiv.org/abs/2601.02145v1",
      "pdf_url": "https://arxiv.org/pdf/2601.02145v1.pdf"
    },
    {
      "id": "2601.02144v1",
      "title": "Routing by Analogy: kNN-Augmented Expert Assignment for Mixture-of-Experts",
      "abstract": "Mixture-of-Experts (MoE) architectures scale large language models efficiently by employing a parametric \"router\" to dispatch tokens to a sparse subset of experts. Typically, this router is trained once and then frozen, rendering routing decisions brittle under distribution shifts. We address this limitation by introducing kNN-MoE, a retrieval-augmented routing framework that reuses optimal expert assignments from a memory of similar past cases. This memory is constructed offline by directly optimizing token-wise routing logits to maximize the likelihood on a reference set. Crucially, we use the aggregate similarity of retrieved neighbors as a confidence-driven mixing coefficient, thus allowing the method to fall back to the frozen router when no relevant cases are found. Experiments show kNN-MoE outperforms zero-shot baselines and rivals computationally expensive supervised fine-tuning.",
      "authors": [
        "Boxuan Lyu",
        "Soichiro Murakami",
        "Hidetaka Kamigaito",
        "Peinan Zhang"
      ],
      "published": "2026-01-05T14:16:11Z",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "abs_url": "https://arxiv.org/abs/2601.02144v1",
      "pdf_url": "https://arxiv.org/pdf/2601.02144v1.pdf"
    },
    {
      "id": "2601.02141v1",
      "title": "Efficient Unrolled Networks for Large-Scale 3D Inverse Problems",
      "abstract": "Deep learning-based methods have revolutionized the field of imaging inverse problems, yielding state-of-the-art performance across various imaging domains. The best performing networks incorporate the imaging operator within the network architecture, typically in the form of deep unrolling. However, in large-scale problems, such as 3D imaging, most existing methods fail to incorporate the operator in the architecture due to the prohibitive amount of memory required by global forward operators, which hinder typical patching strategies. In this work, we present a domain partitioning strategy and normal operator approximations that enable the training of end-to-end reconstruction models incorporating forward operators of arbitrarily large problems into their architecture. The proposed method achieves state-of-the-art performance on 3D X-ray cone-beam tomography and 3D multi-coil accelerated MRI, while requiring only a single GPU for both training and inference.",
      "authors": [
        "Romain Vo",
        "Julián Tachella"
      ],
      "published": "2026-01-05T14:12:43Z",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "abs_url": "https://arxiv.org/abs/2601.02141v1",
      "pdf_url": "https://arxiv.org/pdf/2601.02141v1.pdf"
    },
    {
      "id": "2601.02139v1",
      "title": "Beyond Segmentation: An Oil Spill Change Detection Framework Using Synthetic SAR Imagery",
      "abstract": "Marine oil spills are urgent environmental hazards that demand rapid and reliable detection to minimise ecological and economic damage. While Synthetic Aperture Radar (SAR) imagery has become a key tool for large-scale oil spill monitoring, most existing detection methods rely on deep learning-based segmentation applied to single SAR images. These static approaches struggle to distinguish true oil spills from visually similar oceanic features (e.g., biogenic slicks or low-wind zones), leading to high false positive rates and limited generalizability, especially under data-scarce conditions. To overcome these limitations, we introduce Oil Spill Change Detection (OSCD), a new bi-temporal task that focuses on identifying changes between pre- and post-spill SAR images. As real co-registered pre-spill imagery is not always available, we propose the Temporal-Aware Hybrid Inpainting (TAHI) framework, which generates synthetic pre-spill images from post-spill SAR data. TAHI integrates two key components: High-Fidelity Hybrid Inpainting for oil-free reconstruction, and Temporal Realism Enhancement for radiometric and sea-state consistency. Using TAHI, we construct the first OSCD dataset and benchmark several state-of-the-art change detection models. Results show that OSCD significantly reduces false positives and improves detection accuracy compared to conventional segmentation, demonstrating the value of temporally-aware methods for reliable, scalable oil spill monitoring in real-world scenarios.",
      "authors": [
        "Chenyang Lai",
        "Shuaiyu Chen",
        "Tianjin Huang",
        "Siyang Song",
        "Guangliang Cheng",
        "Chunbo Luo",
        "Zeyu Fu"
      ],
      "published": "2026-01-05T14:10:13Z",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "abs_url": "https://arxiv.org/abs/2601.02139v1",
      "pdf_url": "https://arxiv.org/pdf/2601.02139v1.pdf"
    },
    {
      "id": "2601.02138v1",
      "title": "Edge-aware GAT-based protein binding site prediction",
      "abstract": "Accurate identification of protein binding sites is crucial for understanding biomolecular interaction mechanisms and for the rational design of drug targets. Traditional predictive methods often struggle to balance prediction accuracy with computational efficiency when capturing complex spatial conformations. To address this challenge, we propose an Edge-aware Graph Attention Network (Edge-aware GAT) model for the fine-grained prediction of binding sites across various biomolecules, including proteins, DNA/RNA, ions, ligands, and lipids. Our method constructs atom-level graphs and integrates multidimensional structural features, including geometric descriptors, DSSP-derived secondary structure, and relative solvent accessibility (RSA), to generate spatially aware embedding vectors. By incorporating interatomic distances and directional vectors as edge features within the attention mechanism, the model significantly enhances its representation capacity. On benchmark datasets, our model achieves an ROC-AUC of 0.93 for protein-protein binding site prediction, outperforming several state-of-the-art methods. The use of directional tensor propagation and residue-level attention pooling further improves both binding site localization and the capture of local structural details. Visualizations using PyMOL confirm the model's practical utility and interpretability. To facilitate community access and application, we have deployed a publicly accessible web server at http://119.45.201.89:5000/. In summary, our approach offers a novel and efficient solution that balances prediction accuracy, generalization, and interpretability for identifying functional sites in proteins.",
      "authors": [
        "Weisen Yang",
        "Hanqing Zhang",
        "Wangren Qiu",
        "Xuan Xiao",
        "Weizhong Lin"
      ],
      "published": "2026-01-05T14:09:57Z",
      "categories": [
        "cs.LG",
        "q-bio.QM"
      ],
      "primary_category": "cs.LG",
      "abs_url": "https://arxiv.org/abs/2601.02138v1",
      "pdf_url": "https://arxiv.org/pdf/2601.02138v1.pdf"
    },
    {
      "id": "2601.02128v1",
      "title": "Towards Multi-Level Transcript Segmentation: LoRA Fine-Tuning for Table-of-Contents Generation",
      "abstract": "Segmenting speech transcripts into thematic sections benefits both downstream processing and users who depend on written text for accessibility. We introduce a novel approach to hierarchical topic segmentation in transcripts, generating multi-level tables of contents that capture both topic and subtopic boundaries. We compare zero-shot prompting and LoRA fine-tuning on large language models, while also exploring the integration of high-level speech pause features. Evaluations on English meeting recordings and multilingual lecture transcripts (Portuguese, German) show significant improvements over established topic segmentation baselines. Additionally, we adapt a common evaluation measure for multi-level segmentation, taking into account all hierarchical levels within one metric.",
      "authors": [
        "Steffen Freisinger",
        "Philipp Seeberger",
        "Thomas Ranzenberger",
        "Tobias Bocklet",
        "Korbinian Riedhammer"
      ],
      "published": "2026-01-05T14:00:48Z",
      "categories": [
        "cs.CL",
        "eess.AS"
      ],
      "primary_category": "cs.CL",
      "abs_url": "https://arxiv.org/abs/2601.02128v1",
      "pdf_url": "https://arxiv.org/pdf/2601.02128v1.pdf"
    },
    {
      "id": "2601.02126v1",
      "title": "Remote Sensing Change Detection via Weak Temporal Supervision",
      "abstract": "Semantic change detection in remote sensing aims to identify land cover changes between bi-temporal image pairs. Progress in this area has been limited by the scarcity of annotated datasets, as pixel-level annotation is costly and time-consuming. To address this, recent methods leverage synthetic data or generate artificial change pairs, but out-of-domain generalization remains limited. In this work, we introduce a weak temporal supervision strategy that leverages additional temporal observations of existing single-temporal datasets, without requiring any new annotations. Specifically, we extend single-date remote sensing datasets with new observations acquired at different times and train a change detection model by assuming that real bi-temporal pairs mostly contain no change, while pairing images from different locations to generate change examples. To handle the inherent noise in these weak labels, we employ an object-aware change map generation and an iterative refinement process. We validate our approach on extended versions of the FLAIR and IAILD aerial datasets, achieving strong zero-shot and low-data regime performance across different benchmarks. Lastly, we showcase results over large areas in France, highlighting the scalability potential of our method.",
      "authors": [
        "Xavier Bou",
        "Elliot Vincent",
        "Gabriele Facciolo",
        "Rafael Grompone von Gioi",
        "Jean-Michel Morel",
        "Thibaud Ehret"
      ],
      "published": "2026-01-05T13:57:02Z",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "abs_url": "https://arxiv.org/abs/2601.02126v1",
      "pdf_url": "https://arxiv.org/pdf/2601.02126v1.pdf"
    },
    {
      "id": "2601.02125v1",
      "title": "SingingBot: An Avatar-Driven System for Robotic Face Singing Performance",
      "abstract": "Equipping robotic faces with singing capabilities is crucial for empathetic Human-Robot Interaction. However, existing robotic face driving research primarily focuses on conversations or mimicking static expressions, struggling to meet the high demands for continuous emotional expression and coherence in singing. To address this, we propose a novel avatar-driven framework for appealing robotic singing. We first leverage portrait video generation models embedded with extensive human priors to synthesize vivid singing avatars, providing reliable expression and emotion guidance. Subsequently, these facial features are transferred to the robot via semantic-oriented mapping functions that span a wide expression space. Furthermore, to quantitatively evaluate the emotional richness of robotic singing, we propose the Emotion Dynamic Range metric to measure the emotional breadth within the Valence-Arousal space, revealing that a broad emotional spectrum is crucial for appealing performances. Comprehensive experiments prove that our method achieves rich emotional expressions while maintaining lip-audio synchronization, significantly outperforming existing approaches.",
      "authors": [
        "Zhuoxiong Xu",
        "Xuanchen Li",
        "Yuhao Cheng",
        "Fei Xu",
        "Yichao Yan",
        "Xiaokang Yang"
      ],
      "published": "2026-01-05T13:56:36Z",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "abs_url": "https://arxiv.org/abs/2601.02125v1",
      "pdf_url": "https://arxiv.org/pdf/2601.02125v1.pdf"
    },
    {
      "id": "2601.02123v1",
      "title": "DeCode: Decoupling Content and Delivery for Medical QA",
      "abstract": "Large language models (LLMs) exhibit strong medical knowledge and can generate factually accurate responses. However, existing models often fail to account for individual patient contexts, producing answers that are clinically correct yet poorly aligned with patients' needs. In this work, we introduce DeCode, a training-free, model-agnostic framework that adapts existing LLMs to produce contextualized answers in clinical settings. We evaluate DeCode on OpenAI HealthBench, a comprehensive and challenging benchmark designed to assess clinical relevance and validity of LLM responses. DeCode improves the previous state of the art from $28.4\\%$ to $49.8\\%$, corresponding to a $75\\%$ relative improvement. Experimental results suggest the effectiveness of DeCode in improving clinical question answering of LLMs.",
      "authors": [
        "Po-Jen Ko",
        "Chen-Han Tsai",
        "Yu-Shao Peng"
      ],
      "published": "2026-01-05T13:54:38Z",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "abs_url": "https://arxiv.org/abs/2601.02123v1",
      "pdf_url": "https://arxiv.org/pdf/2601.02123v1.pdf"
    },
    {
      "id": "2601.02121v1",
      "title": "Inferring Network Evolutionary History via Structure-State Coupled Learning",
      "abstract": "Inferring a network's evolutionary history from a single final snapshot with limited temporal annotations is fundamental yet challenging. Existing approaches predominantly rely on topology alone, which often provides insufficient and noisy cues. This paper leverages network steady-state dynamics -- converged node states under a given dynamical process -- as an additional and widely accessible observation for network evolution history inference. We propose CS$^2$, which explicitly models structure-state coupling to capture how topology modulates steady states and how the two signals jointly improve edge discrimination for formation-order recovery. Experiments on six real temporal networks, evaluated under multiple dynamical processes, show that CS$^2$ consistently outperforms strong baselines, improving pairwise edge precedence accuracy by 4.0% on average and global ordering consistency (Spearman-$ρ$) by 7.7% on average. CS$^2$ also more faithfully recovers macroscopic evolution trajectories such as clustering formation, degree heterogeneity, and hub growth. Moreover, a steady-state-only variant remains competitive when reliable topology is limited, highlighting steady states as an independent signal for evolution inference.",
      "authors": [
        "En Xu",
        "Shihe Zhou",
        "Huandong Wang",
        "Jingtao Ding",
        "Yong Li"
      ],
      "published": "2026-01-05T13:53:44Z",
      "categories": [
        "cs.SI",
        "cs.AI"
      ],
      "primary_category": "cs.SI",
      "abs_url": "https://arxiv.org/abs/2601.02121v1",
      "pdf_url": "https://arxiv.org/pdf/2601.02121v1.pdf"
    },
    {
      "id": "2601.02112v1",
      "title": "Car Drag Coefficient Prediction from 3D Point Clouds Using a Slice-Based Surrogate Model",
      "abstract": "The automotive industry's pursuit of enhanced fuel economy and performance necessitates efficient aerodynamic design. However, traditional evaluation methods such as computational fluid dynamics (CFD) and wind tunnel testing are resource intensive, hindering rapid iteration in the early design stages. Machine learning-based surrogate models offer a promising alternative, yet many existing approaches suffer from high computational complexity, limited interpretability, or insufficient accuracy for detailed geometric inputs. This paper introduces a novel lightweight surrogate model for the prediction of the aerodynamic drag coefficient (Cd) based on a sequential slice-wise processing of the geometry of the 3D vehicle. Inspired by medical imaging, 3D point clouds of vehicles are decomposed into an ordered sequence of 2D cross-sectional slices along the stream-wise axis. Each slice is encoded by a lightweight PointNet2D module, and the sequence of slice embeddings is processed by a bidirectional LSTM to capture longitudinal geometric evolution. The model, trained and evaluated on the DrivAerNet++ dataset, achieves a high coefficient of determination (R^2 > 0.9528) and a low mean absolute error (MAE approx 6.046 x 10^{-3}) in Cd prediction. With an inference time of approximately 0.025 seconds per sample on a consumer-grade GPU, our approach provides fast, accurate, and interpretable aerodynamic feedback, facilitating more agile and informed automotive design exploration.",
      "authors": [
        "Utkarsh Singh",
        "Absaar Ali",
        "Adarsh Roy"
      ],
      "published": "2026-01-05T13:41:20Z",
      "categories": [
        "cs.CV",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "abs_url": "https://arxiv.org/abs/2601.02112v1",
      "pdf_url": "https://arxiv.org/pdf/2601.02112v1.pdf"
    },
    {
      "id": "2601.02107v1",
      "title": "MagicFight: Personalized Martial Arts Combat Video Generation",
      "abstract": "Amid the surge in generic text-to-video generation, the field of personalized human video generation has witnessed notable advancements, primarily concentrated on single-person scenarios. However, to our knowledge, the domain of two-person interactions, particularly in the context of martial arts combat, remains uncharted. We identify a significant gap: existing models for single-person dancing generation prove insufficient for capturing the subtleties and complexities of two engaged fighters, resulting in challenges such as identity confusion, anomalous limbs, and action mismatches. To address this, we introduce a pioneering new task, Personalized Martial Arts Combat Video Generation. Our approach, MagicFight, is specifically crafted to overcome these hurdles. Given this pioneering task, we face a lack of appropriate datasets. Thus, we generate a bespoke dataset using the game physics engine Unity, meticulously crafting a multitude of 3D characters, martial arts moves, and scenes designed to represent the diversity of combat. MagicFight refines and adapts existing models and strategies to generate high-fidelity two-person combat videos that maintain individual identities and ensure seamless, coherent action sequences, thereby laying the groundwork for future innovations in the realm of interactive video content creation.\n  Website: https://MingfuYAN.github.io/MagicFight/\n  Dataset: https://huggingface.co/datasets/MingfuYAN/KungFu-Fiesta",
      "authors": [
        "Jiancheng Huang",
        "Mingfu Yan",
        "Songyan Chen",
        "Yi Huang",
        "Shifeng Chen"
      ],
      "published": "2026-01-05T13:34:17Z",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "abs_url": "https://arxiv.org/abs/2601.02107v1",
      "pdf_url": "https://arxiv.org/pdf/2601.02107v1.pdf"
    },
    {
      "id": "2601.02106v1",
      "title": "Prototype-Based Learning for Healthcare: A Demonstration of Interpretable AI",
      "abstract": "Despite recent advances in machine learning and explainable AI, a gap remains in personalized preventive healthcare: predictions, interventions, and recommendations should be both understandable and verifiable for all stakeholders in the healthcare sector. We present a demonstration of how prototype-based learning can address these needs. Our proposed framework, ProtoPal, features both front- and back-end modes; it achieves superior quantitative performance while also providing an intuitive presentation of interventions and their simulated outcomes.",
      "authors": [
        "Ashish Rana",
        "Ammar Shaker",
        "Sascha Saralajew",
        "Takashi Suzuki",
        "Kosuke Yasuda",
        "Shintaro Kato",
        "Toshikazu Wada",
        "Toshiyuki Fujikawa",
        "Toru Kikutsuji"
      ],
      "published": "2026-01-05T13:34:01Z",
      "categories": [
        "cs.LG"
      ],
      "primary_category": "cs.LG",
      "abs_url": "https://arxiv.org/abs/2601.02106v1",
      "pdf_url": "https://arxiv.org/pdf/2601.02106v1.pdf"
    },
    {
      "id": "2601.02105v1",
      "title": "LION-DG: Layer-Informed Initialization with Deep Gradient Protocols for Accelerated Neural Network Training",
      "abstract": "Weight initialization remains decisive for neural network optimization, yet existing methods are largely layer-agnostic. We study initialization for deeply-supervised architectures with auxiliary classifiers, where untrained auxiliary heads can destabilize early training through gradient interference.\n  We propose LION-DG, a layer-informed initialization that zero-initializes auxiliary classifier heads while applying standard He-initialization to the backbone. We prove that this implements Gradient Awakening: auxiliary gradients are exactly zero at initialization, then phase in naturally as weights grow -- providing an implicit warmup without hyperparameters.\n  Experiments on CIFAR-10 and CIFAR-100 with DenseNet-DS and ResNet-DS architectures demonstrate: (1) DenseNet-DS: +8.3% faster convergence on CIFAR-10 with comparable accuracy, (2) Hybrid approach: Combining LSUV with LION-DG achieves best accuracy (81.92% on CIFAR-10), (3) ResNet-DS: Positive speedup on CIFAR-100 (+11.3%) with side-tap auxiliary design.\n  We identify architecture-specific trade-offs and provide clear guidelines for practitioners. LION-DG is simple, requires zero hyperparameters, and adds no computational overhead.",
      "authors": [
        "Hyunjun Kim"
      ],
      "published": "2026-01-05T13:33:09Z",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "abs_url": "https://arxiv.org/abs/2601.02105v1",
      "pdf_url": "https://arxiv.org/pdf/2601.02105v1.pdf"
    },
    {
      "id": "2601.02103v1",
      "title": "HeadLighter: Disentangling Illumination in Generative 3D Gaussian Heads via Lightstage Captures",
      "abstract": "Recent 3D-aware head generative models based on 3D Gaussian Splatting achieve real-time, photorealistic and view-consistent head synthesis. However, a fundamental limitation persists: the deep entanglement of illumination and intrinsic appearance prevents controllable relighting. Existing disentanglement methods rely on strong assumptions to enable weakly supervised learning, which restricts their capacity for complex illumination. To address this challenge, we introduce HeadLighter, a novel supervised framework that learns a physically plausible decomposition of appearance and illumination in head generative models. Specifically, we design a dual-branch architecture that separately models lighting-invariant head attributes and physically grounded rendering components. A progressive disentanglement training is employed to gradually inject head appearance priors into the generative architecture, supervised by multi-view images captured under controlled light conditions with a light stage setup. We further introduce a distillation strategy to generate high-quality normals for realistic rendering. Experiments demonstrate that our method preserves high-quality generation and real-time rendering, while simultaneously supporting explicit lighting and viewpoint editing. We will publicly release our code and dataset.",
      "authors": [
        "Yating Wang",
        "Yuan Sun",
        "Xuan Wang",
        "Ran Yi",
        "Boyao Zhou",
        "Yipengjing Sun",
        "Hongyu Liu",
        "Yinuo Wang",
        "Lizhuang Ma"
      ],
      "published": "2026-01-05T13:32:37Z",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "abs_url": "https://arxiv.org/abs/2601.02103v1",
      "pdf_url": "https://arxiv.org/pdf/2601.02103v1.pdf"
    },
    {
      "id": "2601.02102v1",
      "title": "360-GeoGS: Geometrically Consistent Feed-Forward 3D Gaussian Splatting Reconstruction for 360 Images",
      "abstract": "3D scene reconstruction is fundamental for spatial intelligence applications such as AR, robotics, and digital twins. Traditional multi-view stereo struggles with sparse viewpoints or low-texture regions, while neural rendering approaches, though capable of producing high-quality results, require per-scene optimization and lack real-time efficiency. Explicit 3D Gaussian Splatting (3DGS) enables efficient rendering, but most feed-forward variants focus on visual quality rather than geometric consistency, limiting accurate surface reconstruction and overall reliability in spatial perception tasks. This paper presents a novel feed-forward 3DGS framework for 360 images, capable of generating geometrically consistent Gaussian primitives while maintaining high rendering quality. A Depth-Normal geometric regularization is introduced to couple rendered depth gradients with normal information, supervising Gaussian rotation, scale, and position to improve point cloud and surface accuracy. Experimental results show that the proposed method maintains high rendering quality while significantly improving geometric consistency, providing an effective solution for 3D reconstruction in spatial perception tasks.",
      "authors": [
        "Jiaqi Yao",
        "Zhongmiao Yan",
        "Jingyi Xu",
        "Songpengcheng Xia",
        "Yan Xiang",
        "Ling Pei"
      ],
      "published": "2026-01-05T13:28:28Z",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "abs_url": "https://arxiv.org/abs/2601.02102v1",
      "pdf_url": "https://arxiv.org/pdf/2601.02102v1.pdf"
    },
    {
      "id": "2601.02098v1",
      "title": "InpaintHuman: Reconstructing Occluded Humans with Multi-Scale UV Mapping and Identity-Preserving Diffusion Inpainting",
      "abstract": "Reconstructing complete and animatable 3D human avatars from monocular videos remains challenging, particularly under severe occlusions. While 3D Gaussian Splatting has enabled photorealistic human rendering, existing methods struggle with incomplete observations, often producing corrupted geometry and temporal inconsistencies. We present InpaintHuman, a novel method for generating high-fidelity, complete, and animatable avatars from occluded monocular videos. Our approach introduces two key innovations: (i) a multi-scale UV-parameterized representation with hierarchical coarse-to-fine feature interpolation, enabling robust reconstruction of occluded regions while preserving geometric details; and (ii) an identity-preserving diffusion inpainting module that integrates textual inversion with semantic-conditioned guidance for subject-specific, temporally coherent completion. Unlike SDS-based methods, our approach employs direct pixel-level supervision to ensure identity fidelity. Experiments on synthetic benchmarks (PeopleSnapshot, ZJU-MoCap) and real-world scenarios (OcMotion) demonstrate competitive performance with consistent improvements in reconstruction quality across diverse poses and viewpoints.",
      "authors": [
        "Jinlong Fan",
        "Shanshan Zhao",
        "Liang Zheng",
        "Jing Zhang",
        "Yuxiang Yang",
        "Mingming Gong"
      ],
      "published": "2026-01-05T13:26:02Z",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "abs_url": "https://arxiv.org/abs/2601.02098v1",
      "pdf_url": "https://arxiv.org/pdf/2601.02098v1.pdf"
    },
    {
      "id": "2601.02096v1",
      "title": "Dancing Points: Synthesizing Ballroom Dancing with Three-Point Inputs",
      "abstract": "Ballroom dancing is a structured yet expressive motion category. Its highly diverse movement and complex interactions between leader and follower dancers make the understanding and synthesis challenging. We demonstrate that the three-point trajectory available from a virtual reality (VR) device can effectively serve as a dancer's motion descriptor, simplifying the modeling and synthesis of interplay between dancers' full-body motions down to sparse trajectories. Thanks to the low dimensionality, we can employ an efficient MLP network to predict the follower's three-point trajectory directly from the leader's three-point input for certain types of ballroom dancing, addressing the challenge of modeling high-dimensional full-body interaction. It also prevents our method from overfitting thanks to its compact yet explicit representation. By leveraging the inherent structure of the movements and carefully planning the autoregressive procedure, we show a deterministic neural network is able to translate three-point trajectories into a virtual embodied avatar, which is typically considered under-constrained and requires generative models for common motions. In addition, we demonstrate this deterministic approach generalizes beyond small, structured datasets like ballroom dancing, and performs robustly on larger, more diverse datasets such as LaFAN. Our method provides a computationally- and data-efficient solution, opening new possibilities for immersive paired dancing applications. Code and pre-trained models for this paper are available at https://peizhuoli.github.io/dancing-points.",
      "authors": [
        "Peizhuo Li",
        "Sebastian Starke",
        "Yuting Ye",
        "Olga Sorkine-Hornung"
      ],
      "published": "2026-01-05T13:24:12Z",
      "categories": [
        "cs.GR",
        "cs.CV"
      ],
      "primary_category": "cs.GR",
      "abs_url": "https://arxiv.org/abs/2601.02096v1",
      "pdf_url": "https://arxiv.org/pdf/2601.02096v1.pdf"
    },
    {
      "id": "2601.02094v1",
      "title": "Horizon Activation Mapping for Neural Networks in Time Series Forecasting",
      "abstract": "Neural networks for time series forecasting have relied on error metrics and architecture-specific interpretability approaches for model selection that don't apply across models of different families. To interpret forecasting models agnostic to the types of layers across state-of-the-art model families, we introduce Horizon Activation Mapping (HAM), a visual interpretability technique inspired by grad-CAM that uses gradient norm averages to study the horizon's subseries where grad-CAM studies attention maps over image data. We introduce causal and anti-causal modes to calculate gradient update norm averages across subseries at every timestep and lines of proportionality signifying uniform distributions of the norm averages. Optimization landscape studies with respect to changes in batch sizes, early stopping, train-val-test splits, univariate forecasting and dropouts are studied with respect to performances and subseries in HAM. Interestingly, batch size based differences in activities seem to indicate potential for existence of an exponential approximation across them per epoch relative to each other. Multivariate forecasting models including MLP-based CycleNet, N-Linear, N-HITS, self attention-based FEDformer, Pyraformer, SSM-based SpaceTime and diffusion-based Multi-Resolution DDPM over different horizon sizes trained over the ETTm2 dataset are used for HAM plots in this study. NHITS' neural approximation theorem and SpaceTime's exponential autoregressive activities have been attributed to trends in HAM plots over their training, validation and test sets. In general, HAM can be used for granular model selection, validation set choices and comparisons across different neural network model families.",
      "authors": [
        "Hans Krupakar",
        "V A Kandappan"
      ],
      "published": "2026-01-05T13:21:30Z",
      "categories": [
        "cs.LG",
        "math.FA"
      ],
      "primary_category": "cs.LG",
      "abs_url": "https://arxiv.org/abs/2601.02094v1",
      "pdf_url": "https://arxiv.org/pdf/2601.02094v1.pdf"
    },
    {
      "id": "2601.02091v1",
      "title": "MCD-Net: A Lightweight Deep Learning Baseline for Optical-Only Moraine Segmentation",
      "abstract": "Glacial segmentation is essential for reconstructing past glacier dynamics and evaluating climate-driven landscape change. However, weak optical contrast and the limited availability of high-resolution DEMs hinder automated mapping. This study introduces the first large-scale optical-only moraine segmentation dataset, comprising 3,340 manually annotated high-resolution images from Google Earth covering glaciated regions of Sichuan and Yunnan, China. We develop MCD-Net, a lightweight baseline that integrates a MobileNetV2 encoder, a Convolutional Block Attention Module (CBAM), and a DeepLabV3+ decoder. Benchmarking against deeper backbones (ResNet152, Xception) shows that MCD-Net achieves 62.3\\% mean Intersection over Union (mIoU) and 72.8\\% Dice coefficient while reducing computational cost by more than 60\\%. Although ridge delineation remains constrained by sub-pixel width and spectral ambiguity, the results demonstrate that optical imagery alone can provide reliable moraine-body segmentation. The dataset and code are publicly available at https://github.com/Lyra-alpha/MCD-Net, establishing a reproducible benchmark for moraine-specific segmentation and offering a deployable baseline for high-altitude glacial monitoring.",
      "authors": [
        "Zhehuan Cao",
        "Fiseha Berhanu Tesema",
        "Ping Fu",
        "Jianfeng Ren",
        "Ahmed Nasr"
      ],
      "published": "2026-01-05T13:18:11Z",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "abs_url": "https://arxiv.org/abs/2601.02091v1",
      "pdf_url": "https://arxiv.org/pdf/2601.02091v1.pdf"
    },
    {
      "id": "2601.02088v1",
      "title": "PhysSFI-Net: Physics-informed Geometric Learning of Skeletal and Facial Interactions for Orthognathic Surgical Outcome Prediction",
      "abstract": "Orthognathic surgery repositions jaw bones to restore occlusion and enhance facial aesthetics. Accurate simulation of postoperative facial morphology is essential for preoperative planning. However, traditional biomechanical models are computationally expensive, while geometric deep learning approaches often lack interpretability. In this study, we develop and validate a physics-informed geometric deep learning framework named PhysSFI-Net for precise prediction of soft tissue deformation following orthognathic surgery. PhysSFI-Net consists of three components: a hierarchical graph module with craniofacial and surgical plan encoders combined with attention mechanisms to extract skeletal-facial interaction features; a Long Short-Term Memory (LSTM)-based sequential predictor for incremental soft tissue deformation; and a biomechanics-inspired module for high-resolution facial surface reconstruction. Model performance was assessed using point cloud shape error (Hausdorff distance), surface deviation error, and landmark localization error (Euclidean distances of craniomaxillofacial landmarks) between predicted facial shapes and corresponding ground truths. A total of 135 patients who underwent combined orthodontic and orthognathic treatment were included for model training and validation. Quantitative analysis demonstrated that PhysSFI-Net achieved a point cloud shape error of 1.070 +/- 0.088 mm, a surface deviation error of 1.296 +/- 0.349 mm, and a landmark localization error of 2.445 +/- 1.326 mm. Comparative experiments indicated that PhysSFI-Net outperformed the state-of-the-art method ACMT-Net in prediction accuracy. In conclusion, PhysSFI-Net enables interpretable, high-resolution prediction of postoperative facial morphology with superior accuracy, showing strong potential for clinical application in orthognathic surgical planning and simulation.",
      "authors": [
        "Jiahao Bao",
        "Huazhen Liu",
        "Yu Zhuang",
        "Leran Tao",
        "Xinyu Xu",
        "Yongtao Shi",
        "Mengjia Cheng",
        "Yiming Wang",
        "Congshuang Ku",
        "Ting Zeng",
        "Yilang Du",
        "Siyi Chen",
        "Shunyao Shen",
        "Suncheng Xiang",
        "Hongbo Yu"
      ],
      "published": "2026-01-05T13:14:19Z",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "abs_url": "https://arxiv.org/abs/2601.02088v1",
      "pdf_url": "https://arxiv.org/pdf/2601.02088v1.pdf"
    },
    {
      "id": "2601.02085v1",
      "title": "Vision-Based Early Fault Diagnosis and Self-Recovery for Strawberry Harvesting Robots",
      "abstract": "Strawberry harvesting robots faced persistent challenges such as low integration of visual perception, fruit-gripper misalignment, empty grasping, and strawberry slippage from the gripper due to insufficient gripping force, all of which compromised harvesting stability and efficiency in orchard environments. To overcome these issues, this paper proposed a visual fault diagnosis and self-recovery framework that integrated multi-task perception with corrective control strategies. At the core of this framework was SRR-Net, an end-to-end multi-task perception model that simultaneously performed strawberry detection, segmentation, and ripeness estimation, thereby unifying visual perception with fault diagnosis. Based on this integrated perception, a relative error compensation method based on the simultaneous target-gripper detection was designed to address positional misalignment, correcting deviations when error exceeded the tolerance threshold. To mitigate empty grasping and fruit-slippage faults, an early abort strategy was implemented. A micro-optical camera embedded in the end-effector provided real-time visual feedback, enabling grasp detection during the deflating stage and strawberry slip prediction during snap-off through MobileNet V3-Small classifier and a time-series LSTM classifier. Experiments demonstrated that SRR-Net maintained high perception accuracy. For detection, it achieved a precision of 0.895 and recall of 0.813 on strawberries, and 0.972/0.958 on hands. In segmentation, it yielded a precision of 0.887 and recall of 0.747 for strawberries, and 0.974/0.947 for hands. For ripeness estimation, SRR-Net attained a mean absolute error of 0.035, while simultaneously supporting multi-task perception and sustaining a competitive inference speed of 163.35 FPS.",
      "authors": [
        "Meili Sun",
        "Chunjiang Zhao",
        "Lichao Yang",
        "Hao Liu",
        "Shimin Hu",
        "Ya Xiong"
      ],
      "published": "2026-01-05T13:12:42Z",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "abs_url": "https://arxiv.org/abs/2601.02085v1",
      "pdf_url": "https://arxiv.org/pdf/2601.02085v1.pdf"
    },
    {
      "id": "2601.02082v1",
      "title": "Realistic adversarial scenario generation via human-like pedestrian model for autonomous vehicle control parameter optimisation",
      "abstract": "Autonomous vehicles (AVs) are rapidly advancing and are expected to play a central role in future mobility. Ensuring their safe deployment requires reliable interaction with other road users, not least pedestrians. Direct testing on public roads is costly and unsafe for rare but critical interactions, making simulation a practical alternative. Within simulation-based testing, adversarial scenarios are widely used to probe safety limits, but many prioritise difficulty over realism, producing exaggerated behaviours which may result in AV controllers that are overly conservative. We propose an alternative method, instead using a cognitively inspired pedestrian model featuring both inter-individual and intra-individual variability to generate behaviourally plausible adversarial scenarios. We provide a proof of concept demonstration of this method's potential for AV control optimisation, in closed-loop testing and tuning of an AV controller. Our results show that replacing the rule-based CARLA pedestrian with the human-like model yields more realistic gap acceptance patterns and smoother vehicle decelerations. Unsafe interactions occur only for certain pedestrian individuals and conditions, underscoring the importance of human variability in AV testing. Adversarial scenarios generated by this model can be used to optimise AV control towards safer and more efficient behaviour. Overall, this work illustrates how incorporating human-like road user models into simulation-based adversarial testing can enhance the credibility of AV evaluation and provide a practical basis to behaviourally informed controller optimisation.",
      "authors": [
        "Yueyang Wang",
        "Mehmet Dogar",
        "Gustav Markkula"
      ],
      "published": "2026-01-05T13:10:32Z",
      "categories": [
        "cs.HC",
        "cs.RO"
      ],
      "primary_category": "cs.HC",
      "abs_url": "https://arxiv.org/abs/2601.02082v1",
      "pdf_url": "https://arxiv.org/pdf/2601.02082v1.pdf"
    },
    {
      "id": "2601.02081v1",
      "title": "A Differentiable Adversarial Framework for Task-Aware Data Subsampling",
      "abstract": "The proliferation of large-scale datasets poses a major computational challenge to model training. The traditional data subsampling method works as a static, task independent preprocessing step which usually discards information that is critical to downstream prediction. In this paper, we introduces the antagonistic soft selection subsampling (ASSS) framework as is a novel paradigm that reconstructs data reduction into a differentiable end-to-end learning problem. ASSS uses the adversarial game between selector network and task network, and selector network learning assigns continuous importance weights to samples. This direct optimization implemented by Gumbel-Softmax relaxation allows the selector to identify and retain samples with the maximum amount of information for a specific task target under the guidance of the loss function that balances the fidelity and sparsity of the prediction. Theoretical analysis links this framework with the information bottleneck principle. Comprehensive experiments on four large-scale real world datasets show that ASSS has always been better than heuristic subsampling baselines such as clustering and nearest neighbor thinning in maintaining model performance. It is worth noting that ASSS can not only match, but also sometimes exceed the training performance of the entire dataset, showcasing the effect of intelligent denoising. This work establishes task aware data subsampling as a learnable component, providing a principled solution for effective large-scale data learning.",
      "authors": [
        "Jiacheng Lyu",
        "Bihua Bao"
      ],
      "published": "2026-01-05T13:10:09Z",
      "categories": [
        "cs.LG"
      ],
      "primary_category": "cs.LG",
      "abs_url": "https://arxiv.org/abs/2601.02081v1",
      "pdf_url": "https://arxiv.org/pdf/2601.02081v1.pdf"
    },
    {
      "id": "2601.02080v1",
      "title": "The Homogeneity Trap: Spectral Collapse in Doubly-Stochastic Deep Networks",
      "abstract": "Doubly-stochastic matrices (DSM) are increasingly utilized in structure-preserving deep architectures -- such as Optimal Transport layers and Sinkhorn-based attention -- to enforce numerical stability and probabilistic interpretability. In this work, we identify a critical spectral degradation phenomenon inherent to these constraints, termed the Homogeneity Trap. We demonstrate that the maximum-entropy bias, typical of Sinkhorn-based projections, drives the mixing operator towards the uniform barycenter, thereby suppressing the subdominant singular value σ_2 and filtering out high-frequency feature components. We derive a spectral bound linking σ_2 to the network's effective depth, showing that high-entropy constraints restrict feature transformation to a shallow effective receptive field. Furthermore, we formally demonstrate that Layer Normalization fails to mitigate this collapse in noise-dominated regimes; specifically, when spectral filtering degrades the Signal-to-Noise Ratio (SNR) below a critical threshold, geometric structure is irreversibly lost to noise-induced orthogonal collapse. Our findings highlight a fundamental trade-off between entropic stability and spectral expressivity in DSM-constrained networks.",
      "authors": [
        "Yizhi Liu"
      ],
      "published": "2026-01-05T13:09:42Z",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "abs_url": "https://arxiv.org/abs/2601.02080v1",
      "pdf_url": "https://arxiv.org/pdf/2601.02080v1.pdf"
    },
    {
      "id": "2601.02078v1",
      "title": "Genie Sim 3.0 : A High-Fidelity Comprehensive Simulation Platform for Humanoid Robot",
      "abstract": "The development of robust and generalizable robot learning models is critically contingent upon the availability of large-scale, diverse training data and reliable evaluation benchmarks. Collecting data in the physical world poses prohibitive costs and scalability challenges, and prevailing simulation benchmarks frequently suffer from fragmentation, narrow scope, or insufficient fidelity to enable effective sim-to-real transfer. To address these challenges, we introduce Genie Sim 3.0, a unified simulation platform for robotic manipulation. We present Genie Sim Generator, a large language model (LLM)-powered tool that constructs high-fidelity scenes from natural language instructions. Its principal strength resides in rapid and multi-dimensional generalization, facilitating the synthesis of diverse environments to support scalable data collection and robust policy evaluation. We introduce the first benchmark that pioneers the application of LLM for automated evaluation. It leverages LLM to mass-generate evaluation scenarios and employs Vision-Language Model (VLM) to establish an automated assessment pipeline. We also release an open-source dataset comprising more than 10,000 hours of synthetic data across over 200 tasks. Through systematic experimentation, we validate the robust zero-shot sim-to-real transfer capability of our open-source dataset, demonstrating that synthetic data can server as an effective substitute for real-world data under controlled conditions for scalable policy training. For code and dataset details, please refer to: https://github.com/AgibotTech/genie_sim.",
      "authors": [
        "Chenghao Yin",
        "Da Huang",
        "Di Yang",
        "Jichao Wang",
        "Nanshu Zhao",
        "Chen Xu",
        "Wenjun Sun",
        "Linjie Hou",
        "Zhijun Li",
        "Junhui Wu",
        "Zhaobo Liu",
        "Zhen Xiao",
        "Sheng Zhang",
        "Lei Bao",
        "Rui Feng",
        "Zhenquan Pang",
        "Jiayu Li",
        "Qian Wang",
        "Maoqing Yao"
      ],
      "published": "2026-01-05T12:59:39Z",
      "categories": [
        "cs.RO"
      ],
      "primary_category": "cs.RO",
      "abs_url": "https://arxiv.org/abs/2601.02078v1",
      "pdf_url": "https://arxiv.org/pdf/2601.02078v1.pdf"
    },
    {
      "id": "2601.02076v1",
      "title": "Deferred Commitment Decoding for Diffusion Language Models with Confidence-Aware Sliding Windows",
      "abstract": "Diffusion language models (DLMs) have recently emerged as a strong alternative to autoregressive models by enabling parallel text generation. To improve inference efficiency and KV-cache compatibility, prior work commonly adopts block-based diffusion, decoding tokens block by block. However, this paradigm suffers from a structural limitation that we term Boundary-Induced Context Truncation (BICT): undecoded tokens near block boundaries are forced to commit without access to nearby future context, even when such context could substantially reduce uncertainty. This limitation degrades decoding confidence and generation quality, especially for tasks requiring precise reasoning, such as mathematical problem solving and code generation. We propose Deferred Commitment Decoding (DCD), a novel, training-free decoding strategy that mitigates this issue. DCD maintains a confidence-aware sliding window over masked tokens, resolving low-uncertainty tokens early while deferring high-uncertainty tokens until sufficient contextual evidence becomes available. This design enables effective bidirectional information flow within the decoding window without sacrificing efficiency. Extensive experiments across multiple diffusion language models, benchmarks, and caching configurations show that DCD improves generation accuracy by 1.39% with comparable time on average compared to fixed block-based diffusion methods, with the most significant improvement reaching 9.0%. These results demonstrate that deferring token commitment based on uncertainty is a simple yet effective principle for improving both the quality and efficiency of diffusion language model decoding.",
      "authors": [
        "Yingte Shu",
        "Yuchuan Tian",
        "Chao Xu",
        "Yunhe Wang",
        "Hanting Chen"
      ],
      "published": "2026-01-05T12:57:33Z",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "abs_url": "https://arxiv.org/abs/2601.02076v1",
      "pdf_url": "https://arxiv.org/pdf/2601.02076v1.pdf"
    },
    {
      "id": "2601.02075v1",
      "title": "MDAgent2: Large Language Model for Code Generation and Knowledge Q&A in Molecular Dynamics",
      "abstract": "Molecular dynamics (MD) simulations are essential for understanding atomic-scale behaviors in materials science, yet writing LAMMPS scripts remains highly specialized and time-consuming tasks. Although LLMs show promise in code generation and domain-specific question answering, their performance in MD scenarios is limited by scarce domain data, the high deployment cost of state-of-the-art LLMs, and low code executability. Building upon our prior MDAgent, we present MDAgent2, the first end-to-end framework capable of performing both knowledge Q&A and code generation within the MD domain. We construct a domain-specific data-construction pipeline that yields three high-quality datasets spanning MD knowledge, question answering, and code generation. Based on these datasets, we adopt a three stage post-training strategy--continued pre-training (CPT), supervised fine-tuning (SFT), and reinforcement learning (RL)--to train two domain-adapted models, MD-Instruct and MD-Code. Furthermore, we introduce MD-GRPO, a closed-loop RL method that leverages simulation outcomes as reward signals and recycles low-reward trajectories for continual refinement. We further build MDAgent2-RUNTIME, a deployable multi-agent system that integrates code generation, execution, evaluation, and self-correction. Together with MD-EvalBench proposed in this work, the first benchmark for LAMMPS code generation and question answering, our models and system achieve performance surpassing several strong baselines.This work systematically demonstrates the adaptability and generalization capability of large language models in industrial simulation tasks, laying a methodological foundation for automatic code generation in AI for Science and industrial-scale simulations. URL: https://github.com/FredericVAN/PKU_MDAgent2",
      "authors": [
        "Zhuofan Shi",
        "Hubao A",
        "Yufei Shao",
        "Mengyan Dai",
        "Yadong Yu",
        "Pan Xiang",
        "Dongliang Huang",
        "Hongxu An",
        "Chunxiao Xin",
        "Haiyang Shen",
        "Zhenyu Wang",
        "Yunshan Na",
        "Gang Huang",
        "Xiang Jing"
      ],
      "published": "2026-01-05T12:56:51Z",
      "categories": [
        "cs.CE",
        "cs.LG"
      ],
      "primary_category": "cs.CE",
      "abs_url": "https://arxiv.org/abs/2601.02075v1",
      "pdf_url": "https://arxiv.org/pdf/2601.02075v1.pdf"
    },
    {
      "id": "2601.02072v1",
      "title": "SketchRodGS: Sketch-based Extraction of Slender Geometries for Animating Gaussian Splatting Scenes",
      "abstract": "Physics simulation of slender elastic objects often requires discretization as a polyline. However, constructing a polyline from Gaussian splatting is challenging as Gaussian splatting lacks connectivity information and the configuration of Gaussian primitives contains much noise. This paper presents a method to extract a polyline representation of the slender part of the objects in a Gaussian splatting scene from the user's sketching input. Our method robustly constructs a polyline mesh that represents the slender parts using the screen-space shortest path analysis that can be efficiently solved using dynamic programming. We demonstrate the effectiveness of our approach in several in-the-wild examples.",
      "authors": [
        "Haato Watanabe",
        "Nobuyuki Umetani"
      ],
      "published": "2026-01-05T12:51:12Z",
      "categories": [
        "cs.GR",
        "cs.CV"
      ],
      "primary_category": "cs.GR",
      "abs_url": "https://arxiv.org/abs/2601.02072v1",
      "pdf_url": "https://arxiv.org/pdf/2601.02072v1.pdf"
    },
    {
      "id": "2601.02071v1",
      "title": "FormuLLA: A Large Language Model Approach to Generating Novel 3D Printable Formulations",
      "abstract": "Pharmaceutical three-dimensional (3D) printing is an advanced fabrication technology with the potential to enable truly personalised dosage forms. Recent studies have integrated artificial intelligence (AI) to accelerate formulation and process development, drastically transforming current approaches to pharmaceutical 3D printing. To date, most AI-driven efforts remain narrowly focused, while failing to account for the broader formulation challenges inherent to the technology. Recent advances in AI have introduced artificial general intelligence concepts, wherein systems extend beyond conventional predictive modelling toward more generalised, human-like reasoning. In this work, we investigate the application of large language models (LLMs), fine-tuned on a fused deposition modelling (FDM) dataset comprising over 1400 formulations, to recommend suitable excipients based on active pharmaceutical ingredient (API) dose, and predict filament mechanical properties. Four LLM architectures were fine-tuned, with systematic evaluation of both fine-tuning and generative parameter configurations. Our results demonstrate that Llama2 was best suited for recommending excipients for FDM formulations. Additionally, model selection and parameterisation significantly influence performance, with smaller LLMs exhibiting instances of catastrophic forgetting. Furthermore, we demonstrate: (i) even with relatively small dataset of over 1400 formulations, it can lead to model catastrophic forgetting; (ii) standard LLM metrics only evaluate linguistic performance but not formulation processability; and (iii) LLMs trained on biomedically-related data do not always produce the best results. Addressing these challenges is essential to advancing LLMs beyond linguistic proficiency and toward reliable systems for pharmaceutical formulation development.",
      "authors": [
        "Adeshola Okubena",
        "Yusuf Ali Mohammed",
        "Moe Elbadawi"
      ],
      "published": "2026-01-05T12:50:50Z",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "abs_url": "https://arxiv.org/abs/2601.02071v1",
      "pdf_url": "https://arxiv.org/pdf/2601.02071v1.pdf"
    },
    {
      "id": "2601.02065v1",
      "title": "Cost-Efficient Cross-Lingual Retrieval-Augmented Generation for Low-Resource Languages: A Case Study in Bengali Agricultural Advisory",
      "abstract": "Access to reliable agricultural advisory remains limited in many developing regions due to a persistent language barrier: authoritative agricultural manuals are predominantly written in English, while farmers primarily communicate in low-resource local languages such as Bengali. Although recent advances in Large Language Models (LLMs) enable natural language interaction, direct generation in low-resource languages often exhibits poor fluency and factual inconsistency, while cloud-based solutions remain cost-prohibitive. This paper presents a cost-efficient, cross-lingual Retrieval-Augmented Generation (RAG) framework for Bengali agricultural advisory that emphasizes factual grounding and practical deployability. The proposed system adopts a translation-centric architecture in which Bengali user queries are translated into English, enriched through domain-specific keyword injection to align colloquial farmer terminology with scientific nomenclature, and answered via dense vector retrieval over a curated corpus of English agricultural manuals (FAO, IRRI). The generated English response is subsequently translated back into Bengali to ensure accessibility. The system is implemented entirely using open-source models and operates on consumer-grade hardware without reliance on paid APIs. Experimental evaluation demonstrates reliable source-grounded responses, robust rejection of out-of-domain queries, and an average end-to-end latency below 20 seconds. The results indicate that cross-lingual retrieval combined with controlled translation offers a practical and scalable solution for agricultural knowledge access in low-resource language settings",
      "authors": [
        "Md. Asif Hossain",
        "Nabil Subhan",
        "Mantasha Rahman Mahi",
        "Jannatul Ferdous Nabila"
      ],
      "published": "2026-01-05T12:41:44Z",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "abs_url": "https://arxiv.org/abs/2601.02065v1",
      "pdf_url": "https://arxiv.org/pdf/2601.02065v1.pdf"
    },
    {
      "id": "2601.02061v1",
      "title": "Higher-Order Action Regularization in Deep Reinforcement Learning: From Continuous Control to Building Energy Management",
      "abstract": "Deep reinforcement learning agents often exhibit erratic, high-frequency control behaviors that hinder real-world deployment due to excessive energy consumption and mechanical wear. We systematically investigate action smoothness regularization through higher-order derivative penalties, progressing from theoretical understanding in continuous control benchmarks to practical validation in building energy management. Our comprehensive evaluation across four continuous control environments demonstrates that third-order derivative penalties (jerk minimization) consistently achieve superior smoothness while maintaining competitive performance. We extend these findings to HVAC control systems where smooth policies reduce equipment switching by 60%, translating to significant operational benefits. Our work establishes higher-order action regularization as an effective bridge between RL optimization and operational constraints in energy-critical applications.",
      "authors": [
        "Faizan Ahmed",
        "Aniket Dixit",
        "James Brusey"
      ],
      "published": "2026-01-05T12:35:33Z",
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "abs_url": "https://arxiv.org/abs/2601.02061v1",
      "pdf_url": "https://arxiv.org/pdf/2601.02061v1.pdf"
    },
    {
      "id": "2601.02060v1",
      "title": "Perish or Flourish? A Holistic Evaluation of Large Language Models for Code Generation in Functional Programming",
      "abstract": "Functional programming provides strong foundations for developing reliable and secure software systems, yet its adoption remains not widespread due to the steep learning curve. Recent advances in Large Language Models (LLMs) for code generation present new opportunities to lower these barriers. However, extensive evaluations of LLMs largely focus on imperative programming languages, and their capabilities in functional programming languages (FP) remain underexplored. To address this gap, we introduce FPEval, a holistic evaluation framework built on FPBench, a new benchmark of 721 programming tasks across three difficulty levels on three mainstream FP languages: Haskell, Ocaml and Scala. FPEval provides compehensive evaluation infrastructures with both test validations with comprehensive test suites and static analysis tools to assess both functional correctness and code style and maintainability. Using this framework, we evaluate state-of-the-art LLMs, including GPT-3.5, GPT-4o, and GPT-5, for code generation in functional programming languages and Java as an imperative baseline. Our results demonstrate that LLM performance in functional programming improves substantially with model advancement; however, error rates remain significantly higher in purely functional languages (Haskell and OCaml) than in hybrid (Scala) or imperative (Java) languages. Moreover, LLMs frequently generate non-idiomatic functional code that follows imperative patterns, raising concerns about code style and long-term maintainability. Finally, we show that LLMs can partially self-repair both correctness and quality issues when provided with static analysis feedback and hand-crafted instructions for common types of issues.",
      "authors": [
        "Nguyet-Anh H. Lang",
        "Eric Lang",
        "Thanh Le-Cong",
        "Bach Le",
        "Quyet-Thang Huynh"
      ],
      "published": "2026-01-05T12:33:37Z",
      "categories": [
        "cs.PL",
        "cs.AI",
        "cs.SE"
      ],
      "primary_category": "cs.PL",
      "abs_url": "https://arxiv.org/abs/2601.02060v1",
      "pdf_url": "https://arxiv.org/pdf/2601.02060v1.pdf"
    },
    {
      "id": "2601.02050v1",
      "title": "Explore the Ideology of Deep Learning in ENSO Forecasts",
      "abstract": "The El Ni{~n}o-Southern Oscillation (ENSO) exerts profound influence on global climate variability, yet its prediction remains a grand challenge. Recent advances in deep learning have significantly improved forecasting skill, but the opacity of these models hampers scientific trust and operational deployment. Here, we introduce a mathematically grounded interpretability framework based on bounded variation function. By rescuing the \"dead\" neurons from the saturation zone of the activation function, we enhance the model's expressive capacity. Our analysis reveals that ENSO predictability emerges dominantly from the tropical Pacific, with contributions from the Indian and Atlantic Oceans, consistent with physical understanding. Controlled experiments affirm the robustness of our method and its alignment with established predictors. Notably, we probe the persistent Spring Predictability Barrier (SPB), finding that despite expanded sensitivity during spring, predictive performance declines-likely due to suboptimal variable selection. These results suggest that incorporating additional ocean-atmosphere variables may help transcend SPB limitations and advance long-range ENSO prediction.",
      "authors": [
        "Yanhai Gan",
        "Yipeng Chen",
        "Ning Li",
        "Xingguo Liu",
        "Junyu Dong",
        "Xianyao Chen"
      ],
      "published": "2026-01-05T12:15:39Z",
      "categories": [
        "cs.LG"
      ],
      "primary_category": "cs.LG",
      "abs_url": "https://arxiv.org/abs/2601.02050v1",
      "pdf_url": "https://arxiv.org/pdf/2601.02050v1.pdf"
    },
    {
      "id": "2601.02046v1",
      "title": "Agentic Retoucher for Text-To-Image Generation",
      "abstract": "Text-to-image (T2I) diffusion models such as SDXL and FLUX have achieved impressive photorealism, yet small-scale distortions remain pervasive in limbs, face, text and so on. Existing refinement approaches either perform costly iterative re-generation or rely on vision-language models (VLMs) with weak spatial grounding, leading to semantic drift and unreliable local edits. To close this gap, we propose Agentic Retoucher, a hierarchical decision-driven framework that reformulates post-generation correction as a human-like perception-reasoning-action loop. Specifically, we design (1) a perception agent that learns contextual saliency for fine-grained distortion localization under text-image consistency cues, (2) a reasoning agent that performs human-aligned inferential diagnosis via progressive preference alignment, and (3) an action agent that adaptively plans localized inpainting guided by user preference. This design integrates perceptual evidence, linguistic reasoning, and controllable correction into a unified, self-corrective decision process. To enable fine-grained supervision and quantitative evaluation, we further construct GenBlemish-27K, a dataset of 6K T2I images with 27K annotated artifact regions across 12 categories. Extensive experiments demonstrate that Agentic Retoucher consistently outperforms state-of-the-art methods in perceptual quality, distortion localization and human preference alignment, establishing a new paradigm for self-corrective and perceptually reliable T2I generation.",
      "authors": [
        "Shaocheng Shen",
        "Jianfeng Liang. Chunlei Cai",
        "Cong Geng",
        "Huiyu Duan",
        "Xiaoyun Zhang",
        "Qiang Hu",
        "Guangtao Zhai"
      ],
      "published": "2026-01-05T12:06:43Z",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "abs_url": "https://arxiv.org/abs/2601.02046v1",
      "pdf_url": "https://arxiv.org/pdf/2601.02046v1.pdf"
    },
    {
      "id": "2601.02045v1",
      "title": "The New Compiler Stack: A Survey on the Synergy of LLMs and Compilers",
      "abstract": "This survey has provided a systematic overview of the emerging field of LLM-enabled compilation by addressing several key research questions. We first answered how LLMs are being integrated by proposing a comprehensive, multi-dimensional taxonomy that categorizes works based on their Design Philosophy (Selector, Translator, Generator), LLM Methodology, their operational Level of Code Abstraction, and the specific Task Type they address. In answering what advancements these approaches offer, we identified three primary benefits: the democratization of compiler development, the discovery of novel optimization strategies, and the broadening of the compiler's traditional scope. Finally, in addressing the field's challenges and opportunities, we highlighted the critical hurdles of ensuring correctness and achieving scalability, while identifying the development of hybrid systems as the most promising path forward. By providing these answers, this survey serves as a foundational roadmap for researchers and practitioners, charting the course for a new generation of LLM-powered, intelligent, adaptive and synergistic compilation tools.",
      "authors": [
        "Shuoming Zhang",
        "Jiacheng Zhao",
        "Qiuchu Yu",
        "Chunwei Xia",
        "Zheng Wang",
        "Xiaobing Feng",
        "Huimin Cui"
      ],
      "published": "2026-01-05T12:02:57Z",
      "categories": [
        "cs.PL",
        "cs.AI",
        "cs.SE"
      ],
      "primary_category": "cs.PL",
      "abs_url": "https://arxiv.org/abs/2601.02045v1",
      "pdf_url": "https://arxiv.org/pdf/2601.02045v1.pdf"
    },
    {
      "id": "2601.02043v1",
      "title": "Simulated Reasoning is Reasoning",
      "abstract": "Reasoning has long been understood as a pathway between stages of understanding. Proper reasoning leads to understanding of a given subject. This reasoning was conceptualized as a process of understanding in a particular way, i.e., \"symbolic reasoning\". Foundational Models (FM) demonstrate that this is not a necessary condition for many reasoning tasks: they can \"reason\" by way of imitating the process of \"thinking out loud\", testing the produced pathways, and iterating on these pathways on their own. This leads to some form of reasoning that can solve problems on its own or with few-shot learning, but appears fundamentally different from human reasoning due to its lack of grounding and common sense, leading to brittleness of the reasoning process. These insights promise to substantially alter our assessment of reasoning and its necessary conditions, but also inform the approaches to safety and robust defences against this brittleness of FMs. This paper offers and discusses several philosophical interpretations of this phenomenon, argues that the previously apt metaphor of the \"stochastic parrot\" has lost its relevance and thus should be abandoned, and reflects on different normative elements in the safety- and appropriateness-considerations emerging from these reasoning models and their growing capacity.",
      "authors": [
        "Hendrik Kempt",
        "Alon Lavie"
      ],
      "published": "2026-01-05T12:00:04Z",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.AI",
      "abs_url": "https://arxiv.org/abs/2601.02043v1",
      "pdf_url": "https://arxiv.org/pdf/2601.02043v1.pdf"
    },
    {
      "id": "2601.02038v1",
      "title": "AlignVTOFF: Texture-Spatial Feature Alignment for High-Fidelity Virtual Try-Off",
      "abstract": "Virtual Try-Off (VTOFF) is a challenging multimodal image generation task that aims to synthesize high-fidelity flat-lay garments under complex geometric deformation and rich high-frequency textures. Existing methods often rely on lightweight modules for fast feature extraction, which struggles to preserve structured patterns and fine-grained details, leading to texture attenuation during generation.To address these issues, we propose AlignVTOFF, a novel parallel U-Net framework built upon a Reference U-Net and Texture-Spatial Feature Alignment (TSFA). The Reference U-Net performs multi-scale feature extraction and enhances geometric fidelity, enabling robust modeling of deformation while retaining complex structured patterns. TSFA then injects the reference garment features into a frozen denoising U-Net via a hybrid attention design, consisting of a trainable cross-attention module and a frozen self-attention module. This design explicitly aligns texture and spatial cues and alleviates the loss of high-frequency information during the denoising process.Extensive experiments across multiple settings demonstrate that AlignVTOFF consistently outperforms state-of-the-art methods, producing flat-lay garment results with improved structural realism and high-frequency detail fidelity.",
      "authors": [
        "Yihan Zhu",
        "Mengying Ge"
      ],
      "published": "2026-01-05T11:50:02Z",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "abs_url": "https://arxiv.org/abs/2601.02038v1",
      "pdf_url": "https://arxiv.org/pdf/2601.02038v1.pdf"
    },
    {
      "id": "2601.02037v1",
      "title": "Multivariate Time-series Anomaly Detection via Dynamic Model Pool & Ensembling",
      "abstract": "Multivariate time-series (MTS) anomaly detection is critical in domains such as service monitor, IoT, and network security. While multi-model methods based on selection or ensembling outperform single-model ones, they still face limitations: (i) selection methods rely on a single chosen model and are sensitive to the strategy; (ii) ensembling methods often combine all models or are restricted to univariate data; and (iii) most methods depend on fixed data dimensionality, limiting scalability. To address these, we propose DMPEAD, a Dynamic Model Pool and Ensembling framework for MTS Anomaly Detection. The framework first (i) constructs a diverse model pool via parameter transfer and diversity metric, then (ii) updates it with a meta-model and similarity-based strategy for adaptive pool expansion, subset selection, and pool merging, finally (iii) ensembles top-ranked models through proxy metric ranking and top-k aggregation in the selected subset, outputting the final anomaly detection result. Extensive experiments on 8 real-world datasets show that our model outperforms all baselines, demonstrating superior adaptability and scalability.",
      "authors": [
        "Wei Hu",
        "Zewei Yu",
        "Jianqiu Xu"
      ],
      "published": "2026-01-05T11:48:21Z",
      "categories": [
        "cs.LG",
        "cs.DB"
      ],
      "primary_category": "cs.LG",
      "abs_url": "https://arxiv.org/abs/2601.02037v1",
      "pdf_url": "https://arxiv.org/pdf/2601.02037v1.pdf"
    },
    {
      "id": "2601.02036v1",
      "title": "GDRO: Group-level Reward Post-training Suitable for Diffusion Models",
      "abstract": "Recent advancements adopt online reinforcement learning (RL) from LLMs to text-to-image rectified flow diffusion models for reward alignment. The use of group-level rewards successfully aligns the model with the targeted reward. However, it faces challenges including low efficiency, dependency on stochastic samplers, and reward hacking. The problem is that rectified flow models are fundamentally different from LLMs: 1) For efficiency, online image sampling takes much more time and dominates the time of training. 2) For stochasticity, rectified flow is deterministic once the initial noise is fixed. Aiming at these problems and inspired by the effects of group-level rewards from LLMs, we design Group-level Direct Reward Optimization (GDRO). GDRO is a new post-training paradigm for group-level reward alignment that combines the characteristics of rectified flow models. Through rigorous theoretical analysis, we point out that GDRO supports full offline training that saves the large time cost for image rollout sampling. Also, it is diffusion-sampler-independent, which eliminates the need for the ODE-to-SDE approximation to obtain stochasticity. We also empirically study the reward hacking trap that may mislead the evaluation, and involve this factor in the evaluation using a corrected score that not only considers the original evaluation reward but also the trend of reward hacking. Extensive experiments demonstrate that GDRO effectively and efficiently improves the reward score of the diffusion model through group-wise offline optimization across the OCR and GenEval tasks, while demonstrating strong stability and robustness in mitigating reward hacking.",
      "authors": [
        "Yiyang Wang",
        "Xi Chen",
        "Xiaogang Xu",
        "Yu Liu",
        "Hengshuang Zhao"
      ],
      "published": "2026-01-05T11:47:18Z",
      "categories": [
        "cs.LG",
        "cs.CV"
      ],
      "primary_category": "cs.LG",
      "abs_url": "https://arxiv.org/abs/2601.02036v1",
      "pdf_url": "https://arxiv.org/pdf/2601.02036v1.pdf"
    },
    {
      "id": "2601.02031v1",
      "title": "Output Embedding Centering for Stable LLM Pretraining",
      "abstract": "Pretraining of large language models is not only expensive but also prone to certain training instabilities. A specific instability that often occurs for large learning rates at the end of training is output logit divergence. The most widely used mitigation strategy, z-loss, merely addresses the symptoms rather than the underlying cause of the problem. In this paper, we analyze the instability from the perspective of the output embeddings' geometry and identify its cause. Based on this, we propose output embedding centering (OEC) as a new mitigation strategy, and prove that it suppresses output logit divergence. OEC can be implemented in two different ways, as a deterministic operation called μ-centering, or a regularization method called μ-loss. Our experiments show that both variants outperform z-loss in terms of training stability and learning rate sensitivity. In particular, they ensure that training converges even for large learning rates when z-loss fails. Furthermore, we find that μ-loss is significantly less sensitive to regularization hyperparameter tuning than z-loss.",
      "authors": [
        "Felix Stollenwerk",
        "Anna Lokrantz",
        "Niclas Hertzberg"
      ],
      "published": "2026-01-05T11:44:05Z",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.LG",
      "abs_url": "https://arxiv.org/abs/2601.02031v1",
      "pdf_url": "https://arxiv.org/pdf/2601.02031v1.pdf"
    },
    {
      "id": "2601.02029v1",
      "title": "Leveraging 2D-VLM for Label-Free 3D Segmentation in Large-Scale Outdoor Scene Understanding",
      "abstract": "This paper presents a novel 3D semantic segmentation method for large-scale point cloud data that does not require annotated 3D training data or paired RGB images. The proposed approach projects 3D point clouds onto 2D images using virtual cameras and performs semantic segmentation via a foundation 2D model guided by natural language prompts. 3D segmentation is achieved by aggregating predictions from multiple viewpoints through weighted voting. Our method outperforms existing training-free approaches and achieves segmentation accuracy comparable to supervised methods. Moreover, it supports open-vocabulary recognition, enabling users to detect objects using arbitrary text queries, thus overcoming the limitations of traditional supervised approaches.",
      "authors": [
        "Toshihiko Nishimura",
        "Hirofumi Abe",
        "Kazuhiko Murasaki",
        "Taiga Yoshida",
        "Ryuichi Tanida"
      ],
      "published": "2026-01-05T11:42:49Z",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "abs_url": "https://arxiv.org/abs/2601.02029v1",
      "pdf_url": "https://arxiv.org/pdf/2601.02029v1.pdf"
    },
    {
      "id": "2601.02023v1",
      "title": "Not All Needles Are Found: How Fact Distribution and Don't Make It Up Prompts Shape Literal Extraction, Logical Inference, and Hallucination Risks in Long-Context LLMs",
      "abstract": "Large language models (LLMs) increasingly support very long input contexts. Yet it remains unclear how reliably they extract and infer information at scale. Performance varies with context length and strongly interacts with how information is distributed in real-world corpora. Motivated by these observations, we study how fact placement, corpus-level fact distributions, and Don't Make It Up prompts influence model behavior. We introduce an extended needle-in-a-haystack benchmark across four production-scale models: Gemini-2.5-flash, ChatGPT-5-mini, Claude-4.5-haiku, and Deepseek-v3.2-chat. Unlike prior work, we separately evaluate literal extraction, logical inference, and hallucination risk. Our study considers both positional effects and realistic distributions of evidence across long contexts, as well as prompts that explicitly discourage fabrication. We find that longer contexts alone do not guarantee better performance and can be detrimental when relevant evidence is diluted or widely dispersed. Performance varies substantially across models: some show severe degradation under realistic conditions, while others remain more robust at longer context lengths. Anti-hallucination (AH) instructions can make some models overly conservative, sharply reducing accuracy in literal extraction and logical inference. While we do not directly compare retrieval-augmented generation (RAG) and cache-augmented generation (CAG), our results suggest many failures stem from ineffective context utilization. Models often struggle to identify and prioritize relevant information even when it is present. These findings have direct practical implications, as enterprise workflows increasingly involve pasting large volumes of unfiltered documents into LLM prompts. Effective context length and model-specific robustness to long contexts are therefore critical for reliable LLM deployment in research and business.",
      "authors": [
        "Amirali Ebrahimzadeh",
        "Seyyed M. Salili"
      ],
      "published": "2026-01-05T11:30:56Z",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "abs_url": "https://arxiv.org/abs/2601.02023v1",
      "pdf_url": "https://arxiv.org/pdf/2601.02023v1.pdf"
    },
    {
      "id": "2601.02022v1",
      "title": "Prior Diffusiveness and Regret in the Linear-Gaussian Bandit",
      "abstract": "We prove that Thompson sampling exhibits $\\tilde{O}(σd \\sqrt{T} + d r \\sqrt{\\mathrm{Tr}(Σ_0)})$ Bayesian regret in the linear-Gaussian bandit with a $\\mathcal{N}(μ_0, Σ_0)$ prior distribution on the coefficients, where $d$ is the dimension, $T$ is the time horizon, $r$ is the maximum $\\ell_2$ norm of the actions, and $σ^2$ is the noise variance. In contrast to existing regret bounds, this shows that to within logarithmic factors, the prior-dependent ``burn-in'' term $d r \\sqrt{\\mathrm{Tr}(Σ_0)}$ decouples additively from the minimax (long run) regret $σd \\sqrt{T}$. Previous regret bounds exhibit a multiplicative dependence on these terms. We establish these results via a new ``elliptical potential'' lemma, and also provide a lower bound indicating that the burn-in term is unavoidable.",
      "authors": [
        "Yifan Zhu",
        "John C. Duchi",
        "Benjamin Van Roy"
      ],
      "published": "2026-01-05T11:30:08Z",
      "categories": [
        "cs.LG"
      ],
      "primary_category": "cs.LG",
      "abs_url": "https://arxiv.org/abs/2601.02022v1",
      "pdf_url": "https://arxiv.org/pdf/2601.02022v1.pdf"
    },
    {
      "id": "2601.02020v1",
      "title": "Adapting Depth Anything to Adverse Imaging Conditions with Events",
      "abstract": "Robust depth estimation under dynamic and adverse lighting conditions is essential for robotic systems. Currently, depth foundation models, such as Depth Anything, achieve great success in ideal scenes but remain challenging under adverse imaging conditions such as extreme illumination and motion blur. These degradations corrupt the visual signals of frame cameras, weakening the discriminative features of frame-based depths across the spatial and temporal dimensions. Typically, existing approaches incorporate event cameras to leverage their high dynamic range and temporal resolution, aiming to compensate for corrupted frame features. However, such specialized fusion models are predominantly trained from scratch on domain-specific datasets, thereby failing to inherit the open-world knowledge and robust generalization inherent to foundation models. In this work, we propose ADAE, an event-guided spatiotemporal fusion framework for Depth Anything in degraded scenes. Our design is guided by two key insights: 1) Entropy-Aware Spatial Fusion. We adaptively merge frame-based and event-based features using an information entropy strategy to indicate illumination-induced degradation. 2) Motion-Guided Temporal Correction. We resort to the event-based motion cue to recalibrate ambiguous features in blurred regions. Under our unified framework, the two components are complementary to each other and jointly enhance Depth Anything under adverse imaging conditions. Extensive experiments have been performed to verify the superiority of the proposed method. Our code will be released upon acceptance.",
      "authors": [
        "Shihan Peng",
        "Yuyang Xiong",
        "Hanyu Zhou",
        "Zhiwei Shi",
        "Haoyue Liu",
        "Gang Chen",
        "Luxin Yan",
        "Yi Chang"
      ],
      "published": "2026-01-05T11:29:49Z",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "abs_url": "https://arxiv.org/abs/2601.02020v1",
      "pdf_url": "https://arxiv.org/pdf/2601.02020v1.pdf"
    },
    {
      "id": "2601.02018v1",
      "title": "Towards Any-Quality Image Segmentation via Generative and Adaptive Latent Space Enhancement",
      "abstract": "Segment Anything Models (SAMs), known for their exceptional zero-shot segmentation performance, have garnered significant attention in the research community. Nevertheless, their performance drops significantly on severely degraded, low-quality images, limiting their effectiveness in real-world scenarios. To address this, we propose GleSAM++, which utilizes Generative Latent space Enhancement to boost robustness on low-quality images, thus enabling generalization across various image qualities. Additionally, to improve compatibility between the pre-trained diffusion model and the segmentation framework, we introduce two techniques, i.e., Feature Distribution Alignment (FDA) and Channel Replication and Expansion (CRE). However, the above components lack explicit guidance regarding the degree of degradation. The model is forced to implicitly fit a complex noise distribution that spans conditions from mild noise to severe artifacts, which substantially increases the learning burden and leads to suboptimal reconstructions. To address this issue, we further introduce a Degradation-aware Adaptive Enhancement (DAE) mechanism. The key principle of DAE is to decouple the reconstruction process for arbitrary-quality features into two stages: degradation-level prediction and degradation-aware reconstruction. Our method can be applied to pre-trained SAM and SAM2 with only minimal additional learnable parameters, allowing for efficient optimization. Extensive experiments demonstrate that GleSAM++ significantly improves segmentation robustness on complex degradations while maintaining generalization to clear images. Furthermore, GleSAM++ also performs well on unseen degradations, underscoring the versatility of our approach and dataset.",
      "authors": [
        "Guangqian Guo",
        "Aixi Ren",
        "Yong Guo",
        "Xuehui Yu",
        "Jiacheng Tian",
        "Wenli Li",
        "Yaoxing Wang",
        "Shan Gao"
      ],
      "published": "2026-01-05T11:28:58Z",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "abs_url": "https://arxiv.org/abs/2601.02018v1",
      "pdf_url": "https://arxiv.org/pdf/2601.02018v1.pdf"
    },
    {
      "id": "2601.02016v1",
      "title": "Enhancing Object Detection with Privileged Information: A Model-Agnostic Teacher-Student Approach",
      "abstract": "This paper investigates the integration of the Learning Using Privileged Information (LUPI) paradigm in object detection to exploit fine-grained, descriptive information available during training but not at inference. We introduce a general, model-agnostic methodology for injecting privileged information-such as bounding box masks, saliency maps, and depth cues-into deep learning-based object detectors through a teacher-student architecture. Experiments are conducted across five state-of-the-art object detection models and multiple public benchmarks, including UAV-based litter detection datasets and Pascal VOC 2012, to assess the impact on accuracy, generalization, and computational efficiency. Our results demonstrate that LUPI-trained students consistently outperform their baseline counterparts, achieving significant boosts in detection accuracy with no increase in inference complexity or model size. Performance improvements are especially marked for medium and large objects, while ablation studies reveal that intermediate weighting of teacher guidance optimally balances learning from privileged and standard inputs. The findings affirm that the LUPI framework provides an effective and practical strategy for advancing object detection systems in both resource-constrained and real-world settings.",
      "authors": [
        "Matthias Bartolo",
        "Dylan Seychell",
        "Gabriel Hili",
        "Matthew Montebello",
        "Carl James Debono",
        "Saviour Formosa",
        "Konstantinos Makantasis"
      ],
      "published": "2026-01-05T11:24:34Z",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.ET",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "abs_url": "https://arxiv.org/abs/2601.02016v1",
      "pdf_url": "https://arxiv.org/pdf/2601.02016v1.pdf"
    },
    {
      "id": "2601.02015v1",
      "title": "Surprisal and Metaphor Novelty: Moderate Correlations and Divergent Scaling Effects",
      "abstract": "Novel metaphor comprehension involves complex semantic processes and linguistic creativity, making it an interesting task for studying language models (LMs). This study investigates whether surprisal, a probabilistic measure of predictability in LMs, correlates with different metaphor novelty datasets. We analyse surprisal from 16 LM variants on corpus-based and synthetic metaphor novelty datasets. We explore a cloze-style surprisal method that conditions on full-sentence context. Results show that LMs yield significant moderate correlations with scores/labels of metaphor novelty. We further identify divergent scaling patterns: on corpus-based data, correlation strength decreases with model size (inverse scaling effect), whereas on synthetic data it increases (Quality-Power Hypothesis). We conclude that while surprisal can partially account for annotations of metaphor novelty, it remains a limited metric of linguistic creativity.",
      "authors": [
        "Omar Momen",
        "Emilie Sitter",
        "Berenike Herrmann",
        "Sina Zarrieß"
      ],
      "published": "2026-01-05T11:24:33Z",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.IT"
      ],
      "primary_category": "cs.CL",
      "abs_url": "https://arxiv.org/abs/2601.02015v1",
      "pdf_url": "https://arxiv.org/pdf/2601.02015v1.pdf"
    },
    {
      "id": "2601.02010v1",
      "title": "A neural network for modeling human concept formation, understanding and communication",
      "abstract": "A remarkable capability of the human brain is to form more abstract conceptual representations from sensorimotor experiences and flexibly apply them independent of direct sensory inputs. However, the computational mechanism underlying this ability remains poorly understood. Here, we present a dual-module neural network framework, the CATS Net, to bridge this gap. Our model consists of a concept-abstraction module that extracts low-dimensional conceptual representations, and a task-solving module that performs visual judgement tasks under the hierarchical gating control of the formed concepts. The system develops transferable semantic structure based on concept representations that enable cross-network knowledge transfer through conceptual communication. Model-brain fitting analyses reveal that these emergent concept spaces align with both neurocognitive semantic model and brain response structures in the human ventral occipitotemporal cortex, while the gating mechanisms mirror that in the semantic control brain network. This work establishes a unified computational framework that can offer mechanistic insights for understanding human conceptual cognition and engineering artificial systems with human-like conceptual intelligence.",
      "authors": [
        "Liangxuan Guo",
        "Haoyang Chen",
        "Yang Chen",
        "Yanchao Bi",
        "Shan Yu"
      ],
      "published": "2026-01-05T11:19:07Z",
      "categories": [
        "q-bio.NC",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "q-bio.NC",
      "abs_url": "https://arxiv.org/abs/2601.02010v1",
      "pdf_url": "https://arxiv.org/pdf/2601.02010v1.pdf"
    },
    {
      "id": "2601.02008v1",
      "title": "XAI-MeD: Explainable Knowledge Guided Neuro-Symbolic Framework for Domain Generalization and Rare Class Detection in Medical Imaging",
      "abstract": "Explainability domain generalization and rare class reliability are critical challenges in medical AI where deep models often fail under real world distribution shifts and exhibit bias against infrequent clinical conditions This paper introduces XAIMeD an explainable medical AI framework that integrates clinically accurate expert knowledge into deep learning through a unified neuro symbolic architecture XAIMeD is designed to improve robustness under distribution shift enhance rare class sensitivity and deliver transparent clinically aligned interpretations The framework encodes clinical expertise as logical connectives over atomic medical propositions transforming them into machine checkable class specific rules Their diagnostic utility is quantified through weighted feature satisfaction scores enabling a symbolic reasoning branch that complements neural predictions A confidence weighted fusion integrates symbolic and deep outputs while a Hunt inspired adaptive routing mechanism guided by Entropy Imbalance Gain EIG and Rare Class Gini mitigates class imbalance high intra class variability and uncertainty We evaluate XAIMeD across diverse modalities on four challenging tasks i Seizure Onset Zone SOZ localization from rs fMRI ii Diabetic Retinopathy grading across 6 multicenter datasets demonstrate substantial performance improvements including 6 percent gains in cross domain generalization and a 10 percent improved rare class F1 score far outperforming state of the art deep learning baselines Ablation studies confirm that the clinically grounded symbolic components act as effective regularizers ensuring robustness to distribution shifts XAIMeD thus provides a principled clinically faithful and interpretable approach to multimodal medical AI.",
      "authors": [
        "Midhat Urooj",
        "Ayan Banerjee",
        "Sandeep Gupta"
      ],
      "published": "2026-01-05T11:17:33Z",
      "categories": [
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.AI",
      "abs_url": "https://arxiv.org/abs/2601.02008v1",
      "pdf_url": "https://arxiv.org/pdf/2601.02008v1.pdf"
    },
    {
      "id": "2601.02002v1",
      "title": "Exploring Approaches for Detecting Memorization of Recommender System Data in Large Language Models",
      "abstract": "Large Language Models (LLMs) are increasingly applied in recommendation scenarios due to their strong natural language understanding and generation capabilities. However, they are trained on vast corpora whose contents are not publicly disclosed, raising concerns about data leakage. Recent work has shown that the MovieLens-1M dataset is memorized by both the LLaMA and OpenAI model families, but the extraction of such memorized data has so far relied exclusively on manual prompt engineering. In this paper, we pose three main questions: Is it possible to enhance manual prompting? Can LLM memorization be detected through methods beyond manual prompting? And can the detection of data leakage be automated? To address these questions, we evaluate three approaches: (i) jailbreak prompt engineering; (ii) unsupervised latent knowledge discovery, probing internal activations via Contrast-Consistent Search (CCS) and Cluster-Norm; and (iii) Automatic Prompt Engineering (APE), which frames prompt discovery as a meta-learning process that iteratively refines candidate instructions. Experiments on MovieLens-1M using LLaMA models show that jailbreak prompting does not improve the retrieval of memorized items and remains inconsistent; CCS reliably distinguishes genuine from fabricated movie titles but fails on numerical user and rating data; and APE retrieves item-level information with moderate success yet struggles to recover numerical interactions. These findings suggest that automatically optimizing prompts is the most promising strategy for extracting memorized samples.",
      "authors": [
        "Antonio Colacicco",
        "Vito Guida",
        "Dario Di Palma",
        "Fedelucio Narducci",
        "Tommaso Di Noia"
      ],
      "published": "2026-01-05T11:03:56Z",
      "categories": [
        "cs.IR",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.IR",
      "abs_url": "https://arxiv.org/abs/2601.02002v1",
      "pdf_url": "https://arxiv.org/pdf/2601.02002v1.pdf"
    },
    {
      "id": "2601.01998v1",
      "title": "Nighttime Hazy Image Enhancement via Progressively and Mutually Reinforcing Night-Haze Priors",
      "abstract": "Enhancing the visibility of nighttime hazy images is challenging due to the complex degradation distributions. Existing methods mainly address a single type of degradation (e.g., haze or low-light) at a time, ignoring the interplay of different degradation types and resulting in limited visibility improvement. We observe that the domain knowledge shared between low-light and haze priors can be reinforced mutually for better visibility. Based on this key insight, in this paper, we propose a novel framework that enhances visibility in nighttime hazy images by reinforcing the intrinsic consistency between haze and low-light priors mutually and progressively. In particular, our model utilizes image-, patch-, and pixel-level experts that operate across visual and frequency domains to recover global scene structure, regional patterns, and fine-grained details progressively. A frequency-aware router is further introduced to adaptively guide the contribution of each expert, ensuring robust image restoration. Extensive experiments demonstrate the superior performance of our model on nighttime dehazing benchmarks both quantitatively and qualitatively. Moreover, we showcase the generalizability of our model in daytime dehazing and low-light enhancement tasks.",
      "authors": [
        "Chen Zhu",
        "Huiwen Zhang",
        "Mu He",
        "Yujie Li",
        "Xiaotian Qiao"
      ],
      "published": "2026-01-05T10:58:02Z",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "abs_url": "https://arxiv.org/abs/2601.01998v1",
      "pdf_url": "https://arxiv.org/pdf/2601.01998v1.pdf"
    },
    {
      "id": "2601.01997v1",
      "title": "Exploring Diversity, Novelty, and Popularity Bias in ChatGPT's Recommendations",
      "abstract": "ChatGPT has emerged as a versatile tool, demonstrating capabilities across diverse domains. Given these successes, the Recommender Systems (RSs) community has begun investigating its applications within recommendation scenarios primarily focusing on accuracy. While the integration of ChatGPT into RSs has garnered significant attention, a comprehensive analysis of its performance across various dimensions remains largely unexplored. Specifically, the capabilities of providing diverse and novel recommendations or exploring potential biases such as popularity bias have not been thoroughly examined. As the use of these models continues to expand, understanding these aspects is crucial for enhancing user satisfaction and achieving long-term personalization.\n  This study investigates the recommendations provided by ChatGPT-3.5 and ChatGPT-4 by assessing ChatGPT's capabilities in terms of diversity, novelty, and popularity bias. We evaluate these models on three distinct datasets and assess their performance in Top-N recommendation and cold-start scenarios. The findings reveal that ChatGPT-4 matches or surpasses traditional recommenders, demonstrating the ability to balance novelty and diversity in recommendations. Furthermore, in the cold-start scenario, ChatGPT models exhibit superior performance in both accuracy and novelty, suggesting they can be particularly beneficial for new users. This research highlights the strengths and limitations of ChatGPT's recommendations, offering new perspectives on the capacity of these models to provide recommendations beyond accuracy-focused metrics.",
      "authors": [
        "Dario Di Palma",
        "Giovanni Maria Biancofiore",
        "Vito Walter Anelli",
        "Fedelucio Narducci",
        "Tommaso Di Noia"
      ],
      "published": "2026-01-05T10:56:01Z",
      "categories": [
        "cs.IR",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.IR",
      "abs_url": "https://arxiv.org/abs/2601.01997v1",
      "pdf_url": "https://arxiv.org/pdf/2601.01997v1.pdf"
    },
    {
      "id": "2601.01993v1",
      "title": "MindChat: A Privacy-preserving Large Language Model for Mental Health Support",
      "abstract": "Large language models (LLMs) have shown promise for mental health support, yet training such models is constrained by the scarcity and sensitivity of real counseling dialogues. In this article, we present MindChat, a privacy-preserving LLM for mental health support, together with MindCorpus, a synthetic multi-turn counseling dataset constructed via a multi-agent role-playing framework. To synthesize high-quality counseling data, the developed dialogue-construction framework employs a dual closed-loop feedback design to integrate psychological expertise and counseling techniques through role-playing: (i) turn-level critique-and-revision to improve coherence and counseling appropriateness within a session, and (ii) session-level strategy refinement to progressively enrich counselor behaviors across sessions. To mitigate privacy risks under decentralized data ownership, we fine-tune the base model using federated learning with parameter-efficient LoRA adapters and incorporate differentially private optimization to reduce membership and memorization risks. Experiments on synthetic-data quality assessment and counseling capability evaluation show that MindCorpus improves training effectiveness and that MindChat is competitive with existing general and counseling-oriented LLM baselines under both automatic LLM-judge and human evaluation protocols, while exhibiting reduced privacy leakage under membership inference attacks.",
      "authors": [
        "Dong Xue",
        "Jicheng Tu",
        "Ming Wang",
        "Xin Yan",
        "Fangzhou Liu",
        "Jie Hu"
      ],
      "published": "2026-01-05T10:54:18Z",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "abs_url": "https://arxiv.org/abs/2601.01993v1",
      "pdf_url": "https://arxiv.org/pdf/2601.01993v1.pdf"
    },
    {
      "id": "2601.01992v1",
      "title": "API: Empowering Generalizable Real-World Image Dehazing via Adaptive Patch Importance Learning",
      "abstract": "Real-world image dehazing is a fundamental yet challenging task in low-level vision. Existing learning-based methods often suffer from significant performance degradation when applied to complex real-world hazy scenes, primarily due to limited training data and the intrinsic complexity of haze density distributions.To address these challenges, we introduce a novel Adaptive Patch Importance-aware (API) framework for generalizable real-world image dehazing. Specifically, our framework consists of an Automatic Haze Generation (AHG) module and a Density-aware Haze Removal (DHR) module. AHG provides a hybrid data augmentation strategy by generating realistic and diverse hazy images as additional high-quality training data. DHR considers hazy regions with varying haze density distributions for generalizable real-world image dehazing in an adaptive patch importance-aware manner. To alleviate the ambiguity of the dehazed image details, we further introduce a new Multi-Negative Contrastive Dehazing (MNCD) loss, which fully utilizes information from multiple negative samples across both spatial and frequency domains. Extensive experiments demonstrate that our framework achieves state-of-the-art performance across multiple real-world benchmarks, delivering strong results in both quantitative metrics and qualitative visual quality, and exhibiting robust generalization across diverse haze distributions.",
      "authors": [
        "Chen Zhu",
        "Huiwen Zhang",
        "Yujie Li",
        "Mu He",
        "Xiaotian Qiao"
      ],
      "published": "2026-01-05T10:53:41Z",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "abs_url": "https://arxiv.org/abs/2601.01992v1",
      "pdf_url": "https://arxiv.org/pdf/2601.01992v1.pdf"
    },
    {
      "id": "2601.01989v1",
      "title": "VIT-Ped: Visionary Intention Transformer for Pedestrian Behavior Analysis",
      "abstract": "Pedestrian Intention prediction is one of the key technologies in the transition from level 3 to level 4 autonomous driving. To understand pedestrian crossing behaviour, several elements and features should be taken into consideration to make the roads of tomorrow safer for everybody. We introduce a transformer / video vision transformer based algorithm of different sizes which uses different data modalities .We evaluated our algorithms on popular pedestrian behaviour dataset, JAAD, and have reached SOTA performance and passed the SOTA in metrics like Accuracy, AUC and F1-score. The advantages brought by different model design choices are investigated via extensive ablation studies.",
      "authors": [
        "Aly R. Elkammar",
        "Karim M. Gamaleldin",
        "Catherine M. Elias"
      ],
      "published": "2026-01-05T10:48:12Z",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.RO"
      ],
      "primary_category": "cs.CV",
      "abs_url": "https://arxiv.org/abs/2601.01989v1",
      "pdf_url": "https://arxiv.org/pdf/2601.01989v1.pdf"
    },
    {
      "id": "2601.01984v1",
      "title": "Thinking with Blueprints: Assisting Vision-Language Models in Spatial Reasoning via Structured Object Representation",
      "abstract": "Spatial reasoning -- the ability to perceive and reason about relationships in space -- advances vision-language models (VLMs) from visual perception toward spatial semantic understanding. Existing approaches either revisit local image patches, improving fine-grained perception but weakening global spatial awareness, or mark isolated coordinates, which capture object locations but overlook their overall organization. In this work, we integrate the cognitive concept of an object-centric blueprint into VLMs to enhance spatial reasoning. Given an image and a question, the model first constructs a JSON-style blueprint that records the positions, sizes, and attributes of relevant objects, and then reasons over this structured representation to produce the final answer. To achieve this, we introduce three key techniques: (1) blueprint-embedded reasoning traces for supervised fine-tuning to elicit basic reasoning skills; (2) blueprint-aware rewards in reinforcement learning to encourage the blueprint to include an appropriate number of objects and to align final answers with this causal reasoning; and (3) anti-shortcut data augmentation that applies targeted perturbations to images and questions, discouraging reliance on superficial visual or linguistic cues. Experiments show that our method consistently outperforms existing VLMs and specialized spatial reasoning models.",
      "authors": [
        "Weijian Ma",
        "Shizhao Sun",
        "Tianyu Yu",
        "Ruiyu Wang",
        "Tat-Seng Chua",
        "Jiang Bian"
      ],
      "published": "2026-01-05T10:38:26Z",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "abs_url": "https://arxiv.org/abs/2601.01984v1",
      "pdf_url": "https://arxiv.org/pdf/2601.01984v1.pdf"
    },
    {
      "id": "2601.01982v1",
      "title": "ChaosBench-Logic: A Benchmark for Logical and Symbolic Reasoning on Chaotic Dynamical Systems",
      "abstract": "Large language models (LLMs) excel at natural language tasks but remain brittle in domains requiring precise logical and symbolic reasoning. Chaotic dynamical systems provide an especially demanding test because chaos is deterministic yet often misinterpreted as randomness or complexity. We introduce ChaosBench-Logic, a benchmark that evaluates LLM reasoning across 30 diverse dynamical systems using a unified first-order logic (FOL) ontology. Each system is annotated with truth assignments for 11 semantic predicates, and 621 questions are generated across seven reasoning categories, including multi-hop implications, cross-system analogies, counterfactual reasoning, bias probes, and multi-turn dialogues. We define metrics for logical accuracy, implication consistency, dialogue coherence, and contradiction, and we release an open-source evaluation pipeline. Initial experiments show that frontier LLMs such as GPT-4, Claude 3.5 Sonnet, Gemini 2.5 Flash, and the open-source LLaMA-3 70B achieve 91-94% per-item accuracy, yet still score 0% on compositional items and exhibit fragile global coherence. Dialogue-level accuracy ranges from 53.1% (GPT-4 CoT) to 75.5% (LLaMA-3 zero-shot). ChaosBench-Logic provides a rigorous testbed for diagnosing such failures and a foundation for developing neuro-symbolic approaches that improve scientific reasoning in LLMs.",
      "authors": [
        "Noel Thomas"
      ],
      "published": "2026-01-05T10:36:40Z",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "abs_url": "https://arxiv.org/abs/2601.01982v1",
      "pdf_url": "https://arxiv.org/pdf/2601.01982v1.pdf"
    },
    {
      "id": "2601.01979v1",
      "title": "SerpentFlow: Generative Unpaired Domain Alignment via Shared-Structure Decomposition",
      "abstract": "Domain alignment refers broadly to learning correspondences between data distributions from distinct domains. In this work, we focus on a setting where domains share underlying structural patterns despite differences in their specific realizations. The task is particularly challenging in the absence of paired observations, which removes direct supervision across domains. We introduce a generative framework, called SerpentFlow (SharEd-structuRe decomPosition for gEnerative domaiN adapTation), for unpaired domain alignment. SerpentFlow decomposes data within a latent space into a shared component common to both domains and a domain-specific one. By isolating the shared structure and replacing the domain-specific component with stochastic noise, we construct synthetic training pairs between shared representations and target-domain samples, thereby enabling the use of conditional generative models that are traditionally restricted to paired settings. We apply this approach to super-resolution tasks, where the shared component naturally corresponds to low-frequency content while high-frequency details capture domain-specific variability. The cutoff frequency separating low- and high-frequency components is determined automatically using a classifier-based criterion, ensuring a data-driven and domain-adaptive decomposition. By generating pseudo-pairs that preserve low-frequency structures while injecting stochastic high-frequency realizations, we learn the conditional distribution of the target domain given the shared representation. We implement SerpentFlow using Flow Matching as the generative pipeline, although the framework is compatible with other conditional generative approaches. Experiments on synthetic images, physical process simulations, and a climate downscaling task demonstrate that the method effectively reconstructs high-frequency structures consistent with underlying low-frequency patterns, supporting shared-structure decomposition as an effective strategy for unpaired domain alignment.",
      "authors": [
        "Julie Keisler",
        "Anastase Alexandre Charantonis",
        "Yannig Goude",
        "Boutheina Oueslati",
        "Claire Monteleoni"
      ],
      "published": "2026-01-05T10:33:48Z",
      "categories": [
        "cs.LG",
        "cs.NE"
      ],
      "primary_category": "cs.LG",
      "abs_url": "https://arxiv.org/abs/2601.01979v1",
      "pdf_url": "https://arxiv.org/pdf/2601.01979v1.pdf"
    },
    {
      "id": "2601.01976v1",
      "title": "CNC-TP: Classifier Nominal Concept Based on Top-Pertinent Attributes",
      "abstract": "Knowledge Discovery in Databases (KDD) aims to exploit the vast amounts of data generated daily across various domains of computer applications. Its objective is to extract hidden and meaningful knowledge from datasets through a structured process comprising several key steps: data selection, preprocessing, transformation, data mining, and visualization. Among the core data mining techniques are classification and clustering. Classification involves predicting the class of new instances using a classifier trained on labeled data. Several approaches have been proposed in the literature, including Decision Tree Induction, Bayesian classifiers, Nearest Neighbor search, Neural Networks, Support Vector Machines, and Formal Concept Analysis (FCA). The last one is recognized as an effective approach for interpretable and explainable learning. It is grounded in the mathematical structure of the concept lattice, which enables the generation of formal concepts and the discovery of hidden relationships among them. In this paper, we present a state-of-theart review of FCA-based classifiers. We explore various methods for computing closure operators from nominal data and introduce a novel approach for constructing a partial concept lattice that focuses on the most relevant concepts. Experimental results are provided to demonstrate the efficiency of the proposed method.",
      "authors": [
        "Yasmine Souissi",
        "Fabrice Boissier",
        "Nida Meddouri"
      ],
      "published": "2026-01-05T10:32:10Z",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "abs_url": "https://arxiv.org/abs/2601.01976v1",
      "pdf_url": "https://arxiv.org/pdf/2601.01976v1.pdf"
    },
    {
      "id": "2601.01972v1",
      "title": "Hidden State Poisoning Attacks against Mamba-based Language Models",
      "abstract": "State space models (SSMs) like Mamba offer efficient alternatives to Transformer-based language models, with linear time complexity. Yet, their adversarial robustness remains critically unexplored. This paper studies the phenomenon whereby specific short input phrases induce a partial amnesia effect in such models, by irreversibly overwriting information in their hidden states, referred to as a Hidden State Poisoning Attack (HiSPA). Our benchmark RoBench25 allows evaluating a model's information retrieval capabilities when subject to HiSPAs, and confirms the vulnerability of SSMs against such attacks. Even a recent 52B hybrid SSM-Transformer model from the Jamba family collapses on RoBench25 under optimized HiSPA triggers, whereas pure Transformers do not. We also observe that HiSPA triggers significantly weaken the Jamba model on the popular Open-Prompt-Injections benchmark, unlike pure Transformers. Finally, our interpretability study reveals patterns in Mamba's hidden layers during HiSPAs that could be used to build a HiSPA mitigation system. The full code and data to reproduce the experiments can be found at https://anonymous.4open.science/r/hispa_anonymous-5DB0.",
      "authors": [
        "Alexandre Le Mercier",
        "Chris Develder",
        "Thomas Demeester"
      ],
      "published": "2026-01-05T10:27:19Z",
      "categories": [
        "cs.CL"
      ],
      "primary_category": "cs.CL",
      "abs_url": "https://arxiv.org/abs/2601.01972v1",
      "pdf_url": "https://arxiv.org/pdf/2601.01972v1.pdf"
    },
    {
      "id": "2601.01971v1",
      "title": "Deep Robust Koopman Learning from Noisy Data",
      "abstract": "Koopman operator theory has emerged as a leading data-driven approach that relies on a judicious choice of observable functions to realize global linear representations of nonlinear systems in the lifted observable space. However, real-world data is often noisy, making it difficult to obtain an accurate and unbiased approximation of the Koopman operator. The Koopman operator generated from noisy datasets is typically corrupted by noise-induced bias that severely degrades prediction and downstream tracking performance. In order to address this drawback, this paper proposes a novel autoencoder-based neural architecture to jointly learn the appropriate lifting functions and the reduced-bias Koopman operator from noisy data. The architecture initially learns the Koopman basis functions that are consistent for both the forward and backward temporal dynamics of the system. Subsequently, by utilizing the learned forward and backward temporal dynamics, the Koopman operator is synthesized with a reduced bias making the method more robust to noise compared to existing techniques. Theoretical analysis is used to demonstrate significant bias reduction in the presence of training noise. Dynamics prediction and tracking control simulations are conducted for multiple serial manipulator arms, including performance comparisons with leading alternative designs, to demonstrate its robustness under various noise levels. Experimental studies with the Franka FR3 7-DoF manipulator arm are further used to demonstrate the effectiveness of the proposed approach in a practical setting.",
      "authors": [
        "Aditya Singh",
        "Rajpal Singh",
        "Jishnu Keshavan"
      ],
      "published": "2026-01-05T10:26:26Z",
      "categories": [
        "cs.RO"
      ],
      "primary_category": "cs.RO",
      "abs_url": "https://arxiv.org/abs/2601.01971v1",
      "pdf_url": "https://arxiv.org/pdf/2601.01971v1.pdf"
    },
    {
      "id": "2601.01970v1",
      "title": "A Multilayered Approach to Classifying Customer Responsiveness and Credit Risk",
      "abstract": "This study evaluates the performance of various classifiers in three distinct models: response, risk, and response-risk, concerning credit card mail campaigns and default prediction. In the response model, the Extra Trees classifier demonstrates the highest recall level (79.1%), emphasizing its effectiveness in identifying potential responders to targeted credit card offers. Conversely, in the risk model, the Random Forest classifier exhibits remarkable specificity of 84.1%, crucial for identifying customers least likely to default. Furthermore, in the multi-class response-risk model, the Random Forest classifier achieves the highest accuracy (83.2%), indicating its efficacy in discerning both potential responders to credit card mail campaign and low-risk credit card users. In this study, we optimized various performance metrics to solve a specific credit risk and mail responsiveness business problem.",
      "authors": [
        "Ayomide Afolabi",
        "Ebere Ogburu",
        "Symon Kimitei"
      ],
      "published": "2026-01-05T10:24:08Z",
      "categories": [
        "stat.ML",
        "cs.LG",
        "stat.AP"
      ],
      "primary_category": "stat.ML",
      "abs_url": "https://arxiv.org/abs/2601.01970v1",
      "pdf_url": "https://arxiv.org/pdf/2601.01970v1.pdf"
    },
    {
      "id": "2601.01969v1",
      "title": "What you reward is what you learn: Comparing rewards for online speech policy optimization in public HRI",
      "abstract": "Designing policies that are both efficient and acceptable for conversational service robots in open and diverse environments is non-trivial. Unlike fixed, hand-tuned parameters, online learning can adapt to non-stationary conditions. In this paper, we study how to adapt a social robot's speech policy in the wild. During a 12-day in-situ deployment with over 1,400 public encounters, we cast online policy optimization as a multi-armed bandit problem and use Thompson sampling to select among six actions defined by speech rate (slow/normal/fast) and verbosity (concise/detailed). We compare three complementary binary rewards--Ru (user rating), Rc (conversation closure), and Rt (>=2 turns)--and show that each induces distinct arm distributions and interaction behaviors. We complement the online results with offline evaluations that analyze contextual factors (e.g., crowd level, group size) using video-annotated data. Taken together, we distill ready-to-use design lessons for deploying online optimization of speech policies in real public HRI settings.",
      "authors": [
        "Sichao Song",
        "Yuki Okafuji",
        "Kaito Ariu",
        "Amy Koike"
      ],
      "published": "2026-01-05T10:22:58Z",
      "categories": [
        "cs.RO"
      ],
      "primary_category": "cs.RO",
      "abs_url": "https://arxiv.org/abs/2601.01969v1",
      "pdf_url": "https://arxiv.org/pdf/2601.01969v1.pdf"
    },
    {
      "id": "2601.01966v1",
      "title": "Refinement Provenance Inference: Detecting LLM-Refined Training Prompts from Model Behavior",
      "abstract": "Instruction tuning increasingly relies on LLM-based prompt refinement, where prompts in the training corpus are selectively rewritten by an external refiner to improve clarity and instruction alignment. This motivates an instance-level audit problem: for a fine-tuned model and a training prompt-response pair, can we infer whether the model was trained on the original prompt or its LLM-refined version within a mixed corpus? This matters for dataset governance and dispute resolution when training data are contested. However, it is non-trivial in practice: refined and raw instances are interleaved in the training corpus with unknown, source-dependent mixture ratios, making it harder to develop provenance methods that generalize across models and training setups. In this paper, we formalize this audit task as Refinement Provenance Inference (RPI) and show that prompt refinement yields stable, detectable shifts in teacher-forced token distributions, even when semantic differences are not obvious. Building on this phenomenon, we propose RePro, a logit-based provenance framework that fuses teacher-forced likelihood features with logit-ranking signals. During training, RePro learns a transferable representation via shadow fine-tuning, and uses a lightweight linear head to infer provenance on unseen victims without training-data access. Empirically, RePro consistently attains strong performance and transfers well across refiners, suggesting that it exploits refiner-agnostic distribution shifts rather than rewrite-style artifacts.",
      "authors": [
        "Bo Yin",
        "Qi Li",
        "Runpeng Yu",
        "Xinchao Wang"
      ],
      "published": "2026-01-05T10:16:41Z",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "abs_url": "https://arxiv.org/abs/2601.01966v1",
      "pdf_url": "https://arxiv.org/pdf/2601.01966v1.pdf"
    },
    {
      "id": "2601.01964v1",
      "title": "CSF: Contrastive Semantic Features for Direct Multilingual Sign Language Generation",
      "abstract": "Sign language translation systems typically require English as an intermediary language, creating barriers for non-English speakers in the global deaf community. We present Canonical Semantic Form (CSF), a language-agnostic semantic representation framework that enables direct translation from any source language to sign language without English mediation. CSF decomposes utterances into nine universal semantic slots: event, intent, time, condition, agent, object, location, purpose, and modifier. A key contribution is our comprehensive condition taxonomy comprising 35 condition types across eight semantic categories, enabling nuanced representation of conditional expressions common in everyday communication. We train a lightweight transformer-based extractor (0.74 MB) that achieves 99.03% average slot extraction accuracy across four typologically diverse languages: English, Vietnamese, Japanese, and French. The model demonstrates particularly strong performance on condition classification (99.4% accuracy) despite the 35-class complexity. With inference latency of 3.02ms on CPU, our approach enables real-time sign language generation in browser-based applications. We release our code, trained models, and multilingual dataset to support further research in accessible sign language technology.",
      "authors": [
        "Tran Sy Bao"
      ],
      "published": "2026-01-05T10:15:35Z",
      "categories": [
        "cs.CL"
      ],
      "primary_category": "cs.CL",
      "abs_url": "https://arxiv.org/abs/2601.01964v1",
      "pdf_url": "https://arxiv.org/pdf/2601.01964v1.pdf"
    },
    {
      "id": "2601.01963v1",
      "title": "Forget Less by Learning Together through Concept Consolidation",
      "abstract": "Custom Diffusion Models (CDMs) have gained significant attention due to their remarkable ability to personalize generative processes. However, existing CDMs suffer from catastrophic forgetting when continuously learning new concepts. Most prior works attempt to mitigate this issue under the sequential learning setting with a fixed order of concept inflow and neglect inter-concept interactions. In this paper, we propose a novel framework - Forget Less by Learning Together (FL2T) - that enables concurrent and order-agnostic concept learning while addressing catastrophic forgetting. Specifically, we introduce a set-invariant inter-concept learning module where proxies guide feature selection across concepts, facilitating improved knowledge retention and transfer. By leveraging inter-concept guidance, our approach preserves old concepts while efficiently incorporating new ones. Extensive experiments, across three datasets, demonstrates that our method significantly improves concept retention and mitigates catastrophic forgetting, highlighting the effectiveness of inter-concept catalytic behavior in incremental concept learning of ten tasks with at least 2% gain on average CLIP Image Alignment scores.",
      "authors": [
        "Arjun Ramesh Kaushik",
        "Naresh Kumar Devulapally",
        "Vishnu Suresh Lokhande",
        "Nalini Ratha",
        "Venu Govindaraju"
      ],
      "published": "2026-01-05T10:14:16Z",
      "categories": [
        "cs.CV",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "abs_url": "https://arxiv.org/abs/2601.01963v1",
      "pdf_url": "https://arxiv.org/pdf/2601.01963v1.pdf"
    },
    {
      "id": "2601.01957v1",
      "title": "AFTER: Mitigating the Object Hallucination of LVLM via Adaptive Factual-Guided Activation Editing",
      "abstract": "Large Vision-Language Models (LVLMs) have achieved substantial progress in cross-modal tasks. However, due to language bias, LVLMs are susceptible to object hallucination, which can be primarily divided into category, attribute, and relation hallucination, significantly impeding the trustworthy AI applications. Editing the internal activations of LVLMs has shown promising effectiveness in mitigating hallucinations with minimal cost. However, previous editing approaches neglect the effective guidance offered by factual textual semantics, thereby struggling to explicitly mitigate language bias. To address these issues, we propose Adaptive Factual-guided Visual-Textual Editing for hallucination mitigation (AFTER), which comprises Factual-Augmented Activation Steering (FAS) and Query-Adaptive Offset Optimization (QAO), to adaptively guides the original biased activations towards factual semantics. Specifically, FAS is proposed to provide factual and general guidance for activation editing, thereby explicitly modeling the precise visual-textual associations. Subsequently, QAO introduces a query-aware offset estimator to establish query-specific editing from the general steering vector, enhancing the diversity and granularity of editing. Extensive experiments on standard hallucination benchmarks across three widely adopted LVLMs validate the efficacy of the proposed AFTER, notably achieving up to a 16.3% reduction of hallucination over baseline on the AMBER benchmark. Our code and data will be released for reproducibility.",
      "authors": [
        "Tianbo Wang",
        "Yuqing Ma",
        "Kewei Liao",
        "Zhange Zhang",
        "Simin Li",
        "Jinyang Guo",
        "Xianglong Liu"
      ],
      "published": "2026-01-05T10:02:22Z",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "abs_url": "https://arxiv.org/abs/2601.01957v1",
      "pdf_url": "https://arxiv.org/pdf/2601.01957v1.pdf"
    },
    {
      "id": "2601.01955v1",
      "title": "MotionAdapter: Video Motion Transfer via Content-Aware Attention Customization",
      "abstract": "Recent advances in diffusion-based text-to-video models, particularly those built on the diffusion transformer architecture, have achieved remarkable progress in generating high-quality and temporally coherent videos. However, transferring complex motions between videos remains challenging. In this work, we present MotionAdapter, a content-aware motion transfer framework that enables robust and semantically aligned motion transfer within DiT-based T2V models. Our key insight is that effective motion transfer requires \\romannumeral1) explicit disentanglement of motion from appearance and \\romannumeral 2) adaptive customization of motion to target content. MotionAdapter first isolates motion by analyzing cross-frame attention within 3D full-attention modules to extract attention-derived motion fields. To bridge the semantic gap between reference and target videos, we further introduce a DINO-guided motion customization module that rearranges and refines motion fields based on content correspondences. The customized motion field is then used to guide the DiT denoising process, ensuring that the synthesized video inherits the reference motion while preserving target appearance and semantics. Extensive experiments demonstrate that MotionAdapter outperforms state-of-the-art methods in both qualitative and quantitative evaluations. Moreover, MotionAdapter naturally supports complex motion transfer and motion editing tasks such as zooming.",
      "authors": [
        "Zhexin Zhang",
        "Yifeng Zhu",
        "Yangyang Xu",
        "Long Chen",
        "Yong Du",
        "Shengfeng He",
        "Jun Yu"
      ],
      "published": "2026-01-05T10:01:27Z",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "abs_url": "https://arxiv.org/abs/2601.01955v1",
      "pdf_url": "https://arxiv.org/pdf/2601.01955v1.pdf"
    },
    {
      "id": "2601.01950v1",
      "title": "Face Normal Estimation from Rags to Riches",
      "abstract": "Although recent approaches to face normal estimation have achieved promising results, their effectiveness heavily depends on large-scale paired data for training. This paper concentrates on relieving this requirement via developing a coarse-to-fine normal estimator. Concretely, our method first trains a neat model from a small dataset to produce coarse face normals that perform as guidance (called exemplars) for the following refinement. A self-attention mechanism is employed to capture long-range dependencies, thus remedying severe local artifacts left in estimated coarse facial normals. Then, a refinement network is customized for the sake of mapping input face images together with corresponding exemplars to fine-grained high-quality facial normals. Such a logical function split can significantly cut the requirement of massive paired data and computational resource. Extensive experiments and ablation studies are conducted to demonstrate the efficacy of our design and reveal its superiority over state-of-the-art methods in terms of both training expense as well as estimation quality. Our code and models are open-sourced at: https://github.com/AutoHDR/FNR2R.git.",
      "authors": [
        "Meng Wang",
        "Wenjing Dai",
        "Jiawan Zhang",
        "Xiaojie Guo"
      ],
      "published": "2026-01-05T09:57:24Z",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "abs_url": "https://arxiv.org/abs/2601.01950v1",
      "pdf_url": "https://arxiv.org/pdf/2601.01950v1.pdf"
    },
    {
      "id": "2601.01948v1",
      "title": "Learning Diffusion Policy from Primitive Skills for Robot Manipulation",
      "abstract": "Diffusion policies (DP) have recently shown great promise for generating actions in robotic manipulation. However, existing approaches often rely on global instructions to produce short-term control signals, which can result in misalignment in action generation. We conjecture that the primitive skills, referred to as fine-grained, short-horizon manipulations, such as ``move up'' and ``open the gripper'', provide a more intuitive and effective interface for robot learning. To bridge this gap, we propose SDP, a skill-conditioned DP that integrates interpretable skill learning with conditional action planning. SDP abstracts eight reusable primitive skills across tasks and employs a vision-language model to extract discrete representations from visual observations and language instructions. Based on them, a lightweight router network is designed to assign a desired primitive skill for each state, which helps construct a single-skill policy to generate skill-aligned actions. By decomposing complex tasks into a sequence of primitive skills and selecting a single-skill policy, SDP ensures skill-consistent behavior across diverse tasks. Extensive experiments on two challenging simulation benchmarks and real-world robot deployments demonstrate that SDP consistently outperforms SOTA methods, providing a new paradigm for skill-based robot learning with diffusion policies.",
      "authors": [
        "Zhihao Gu",
        "Ming Yang",
        "Difan Zou",
        "Dong Xu"
      ],
      "published": "2026-01-05T09:56:24Z",
      "categories": [
        "cs.RO"
      ],
      "primary_category": "cs.RO",
      "abs_url": "https://arxiv.org/abs/2601.01948v1",
      "pdf_url": "https://arxiv.org/pdf/2601.01948v1.pdf"
    },
    {
      "id": "2601.01946v1",
      "title": "From Metrics to Meaning: Insights from a Mixed-Methods Field Experiment on Retail Robot Deployment",
      "abstract": "We report a mixed-methods field experiment of a conversational service robot deployed under everyday staffing discretion in a live bedding store. Over 12 days we alternated three conditions--Baseline (no robot), Robot-only, and Robot+Fixture--and video-annotated the service funnel from passersby to purchase. An explanatory sequential design then used six post-experiment staff interviews to interpret the quantitative patterns.\n  Quantitatively, the robot increased stopping per passerby (highest with the fixture), yet clerk-led downstream steps per stopper--clerk approach, store entry, assisted experience, and purchase--decreased. Interviews explained this divergence: clerks avoided interrupting ongoing robot-customer talk, struggled with ambiguous timing amid conversational latency, and noted child-centered attraction that often satisfied curiosity at the doorway. The fixture amplified visibility but also anchored encounters at the threshold, creating a well-defined micro-space where needs could ``close'' without moving inside.\n  We synthesize these strands into an integrative account from the initial show of interest on the part of a customer to their entering the store and derive actionable guidance. The results advance the understanding of interactions between customers, staff members, and the robot and offer practical recommendations for deploying service robots in high-touch retail.",
      "authors": [
        "Sichao Song",
        "Yuki Okafuji",
        "Takuya Iwamoto",
        "Jun Baba",
        "Hiroshi Ishiguro"
      ],
      "published": "2026-01-05T09:54:33Z",
      "categories": [
        "cs.RO",
        "cs.HC"
      ],
      "primary_category": "cs.RO",
      "abs_url": "https://arxiv.org/abs/2601.01946v1",
      "pdf_url": "https://arxiv.org/pdf/2601.01946v1.pdf"
    },
    {
      "id": "2601.01944v1",
      "title": "The Invisible Hand of AI Libraries Shaping Open Source Projects and Communities",
      "abstract": "In the early 1980s, Open Source Software emerged as a revolutionary concept amidst the dominance of proprietary software. What began as a revolutionary idea has now become the cornerstone of computer science. Amidst OSS projects, AI is increasing its presence and relevance. However, despite the growing popularity of AI, its adoption and impacts on OSS projects remain underexplored.\n  We aim to assess the adoption of AI libraries in Python and Java OSS projects and examine how they shape development, including the technical ecosystem and community engagement. To this end, we will perform a large-scale analysis on 157.7k potential OSS repositories, employing repository metrics and software metrics to compare projects adopting AI libraries against those that do not. We expect to identify measurable differences in development activity, community engagement, and code complexity between OSS projects that adopt AI libraries and those that do not, offering evidence-based insights into how AI integration reshapes software development practices.",
      "authors": [
        "Matteo Esposito",
        "Andrea Janes",
        "Valentina Lenarduzzi",
        "Davide Taibi"
      ],
      "published": "2026-01-05T09:50:37Z",
      "categories": [
        "cs.SE",
        "cs.AI",
        "cs.CL",
        "cs.IR",
        "cs.PL"
      ],
      "primary_category": "cs.SE",
      "abs_url": "https://arxiv.org/abs/2601.01944v1",
      "pdf_url": "https://arxiv.org/pdf/2601.01944v1.pdf"
    },
    {
      "id": "2601.01943v1",
      "title": "SynRXN: An Open Benchmark and Curated Dataset for Computational Reaction Modeling",
      "abstract": "We present SynRXN, a unified benchmarking framework and open-data resource for computer-aided synthesis planning (CASP). SynRXN decomposes end-to-end synthesis planning into five task families, covering reaction rebalancing, atom-to-atom mapping, reaction classification, reaction property prediction, and synthesis route design. Curated, provenance-tracked reaction corpora are assembled from heterogeneous public sources into a harmonized representation and packaged as versioned datasets for each task family, with explicit source metadata, licence tags, and machine-readable manifests that record checksums, and row counts. For every task, SynRXN provides transparent splitting functions that generate leakage-aware train, validation, and test partitions, together with standardized evaluation workflows and metric suites tailored to classification, regression, and structured prediction settings. For sensitive benchmarking, we combine public training and validation data with held-out gold-standard test sets, and contamination-prone tasks such as reaction rebalancing and atom-to-atom mapping are distributed only as evaluation sets and are explicitly not intended for model training. Scripted build recipes enable bitwise-reproducible regeneration of all corpora across machines and over time, and the entire resource is released under permissive open licences to support reuse and extension. By removing dataset heterogeneity and packaging transparent, reusable evaluation scaffolding, SynRXN enables fair longitudinal comparison of CASP methods, supports rigorous ablations and stress tests along the full reaction-informatics pipeline, and lowers the barrier for practitioners who seek robust and comparable performance estimates for real-world synthesis planning workloads.",
      "authors": [
        "Tieu-Long Phan",
        "Nhu-Ngoc Nguyen Song",
        "Peter F. Stadler"
      ],
      "published": "2026-01-05T09:49:58Z",
      "categories": [
        "cs.LG"
      ],
      "primary_category": "cs.LG",
      "abs_url": "https://arxiv.org/abs/2601.01943v1",
      "pdf_url": "https://arxiv.org/pdf/2601.01943v1.pdf"
    },
    {
      "id": "2601.01939v1",
      "title": "OpenSocInt: A Multi-modal Training Environment for Human-Aware Social Navigation",
      "abstract": "In this paper, we introduce OpenSocInt, an open-source software package providing a simulator for multi-modal social interactions and a modular architecture to train social agents. We described the software package and showcased its interest via an experimental protocol based on the task of social navigation. Our framework allows for exploring the use of different perceptual features, their encoding and fusion, as well as the use of different agents. The software is already publicly available under GPL at https://gitlab.inria.fr/robotlearn/OpenSocInt/.",
      "authors": [
        "Victor Sanchez",
        "Chris Reinke",
        "Ahamed Mohamed",
        "Xavier Alameda-Pineda"
      ],
      "published": "2026-01-05T09:48:18Z",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "abs_url": "https://arxiv.org/abs/2601.01939v1",
      "pdf_url": "https://arxiv.org/pdf/2601.01939v1.pdf"
    },
    {
      "id": "2601.01932v1",
      "title": "Visualizing the Structure of Lenia Parameter Space",
      "abstract": "Continuous cellular automata are rocketing in popularity, yet developing a theoretical understanding of their behaviour remains a challenge. In the case of Lenia, a few fundamental open problems include determining what exactly constitutes a soliton, what is the overall structure of the parameter space, and where do the solitons occur in it. In this abstract, we present a new method to automatically classify Lenia systems into four qualitatively different dynamical classes. This allows us to detect moving solitons, and to provide an interactive visualization of Lenia's parameter space structure on our website https://lenia-explorer.vercel.app/. The results shed new light on the above-mentioned questions and lead to several observations: the existence of new soliton families for parameters where they were not believed to exist, or the universality of the phase space structure across various kernels.",
      "authors": [
        "Barbora Hudcová",
        "František Dušek",
        "Marco Tuccio",
        "Clément Hongler"
      ],
      "published": "2026-01-05T09:35:06Z",
      "categories": [
        "nlin.CG",
        "cs.AI"
      ],
      "primary_category": "nlin.CG",
      "abs_url": "https://arxiv.org/abs/2601.01932v1",
      "pdf_url": "https://arxiv.org/pdf/2601.01932v1.pdf"
    },
    {
      "id": "2601.01931v1",
      "title": "DéjàQ: Open-Ended Evolution of Diverse, Learnable and Verifiable Problems",
      "abstract": "Recent advances in reasoning models have yielded impressive results in mathematics and coding. However, most approaches rely on static datasets, which have been suggested to encourage memorisation and limit generalisation. We introduce DéjàQ, a framework that departs from this paradigm by jointly evolving a diverse set of synthetic mathematical problems alongside model training. This evolutionary process adapts to the model's ability throughout training, optimising problems for learnability. We propose two LLM-driven mutation strategies in which the model itself mutates the training data, either by altering contextual details or by directly modifying problem structure. We find that the model can generate novel and meaningful problems, and that these LLM-driven mutations improve RL training. We analyse key aspects of DéjàQ, including the validity of generated problems and computational overhead. Our results underscore the potential of dynamically evolving training data to enhance mathematical reasoning and indicate broader applicability, which we will support by open-sourcing our code.",
      "authors": [
        "Willem Röpke",
        "Samuel Coward",
        "Andrei Lupu",
        "Thomas Foster",
        "Tim Rocktäschel",
        "Jakob Foerster"
      ],
      "published": "2026-01-05T09:27:49Z",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "abs_url": "https://arxiv.org/abs/2601.01931v1",
      "pdf_url": "https://arxiv.org/pdf/2601.01931v1.pdf"
    },
    {
      "id": "2601.01930v1",
      "title": "MCGI: Manifold-Consistent Graph Indexing for Billion-Scale Disk-Resident Vector Search",
      "abstract": "Graph-based Approximate Nearest Neighbor (ANN) search often suffers from performance degradation in high-dimensional spaces due to the ``Euclidean-Geodesic mismatch,'' where greedy routing diverges from the underlying data manifold. To address this, we propose Manifold-Consistent Graph Indexing (MCGI), a geometry-aware and disk-resident indexing method that leverages Local Intrinsic Dimensionality (LID) to dynamically adapt search strategies to the data's intrinsic geometry. Unlike standard algorithms that treat dimensions uniformly, MCGI modulates its beam search budget based on in situ geometric analysis, eliminating dependency on static hyperparameters. Theoretical analysis confirms that MCGI enables improved approximation guarantees by preserving manifold-consistent topological connectivity. Empirically, MCGI achieves 5.8$\\times$ higher throughput at 95\\% recall on high-dimensional GIST1M compared to state-of-the-art DiskANN. On the billion-scale SIFT1B dataset, MCGI further validates its scalability by reducing high-recall query latency by 3$\\times$, while maintaining performance parity on standard lower-dimensional datasets.",
      "authors": [
        "Dongfang Zhao"
      ],
      "published": "2026-01-05T09:23:48Z",
      "categories": [
        "cs.IR",
        "cs.AI"
      ],
      "primary_category": "cs.IR",
      "abs_url": "https://arxiv.org/abs/2601.01930v1",
      "pdf_url": "https://arxiv.org/pdf/2601.01930v1.pdf"
    },
    {
      "id": "2601.01927v1",
      "title": "Theoretical Convergence of SMOTE-Generated Samples",
      "abstract": "Imbalanced data affects a wide range of machine learning applications, from healthcare to network security. As SMOTE is one of the most popular approaches to addressing this issue, it is imperative to validate it not only empirically but also theoretically. In this paper, we provide a rigorous theoretical analysis of SMOTE's convergence properties. Concretely, we prove that the synthetic random variable Z converges in probability to the underlying random variable X. We further prove a stronger convergence in mean when X is compact. Finally, we show that lower values of the nearest neighbor rank lead to faster convergence offering actionable guidance to practitioners. The theoretical results are supported by numerical experiments using both real-life and synthetic data. Our work provides a foundational understanding that enhances data augmentation techniques beyond imbalanced data scenarios.",
      "authors": [
        "Firuz Kamalov",
        "Hana Sulieman",
        "Witold Pedrycz"
      ],
      "published": "2026-01-05T09:19:45Z",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "abs_url": "https://arxiv.org/abs/2601.01927v1",
      "pdf_url": "https://arxiv.org/pdf/2601.01927v1.pdf"
    },
    {
      "id": "2601.01926v1",
      "title": "MacVQA: Adaptive Memory Allocation and Global Noise Filtering for Continual Visual Question Answering",
      "abstract": "Visual Question Answering (VQA) requires models to reason over multimodal information, combining visual and textual data. With the development of continual learning, significant progress has been made in retaining knowledge and adapting to new information in the VQA domain. However, current methods often struggle with balancing knowledge retention, adaptation, and robust feature representation. To address these challenges, we propose a novel framework with adaptive memory allocation and global noise filtering called MacVQA for visual question answering. MacVQA fuses visual and question information while filtering noise to ensure robust representations, and employs prototype-based memory allocation to optimize feature quality and memory usage. These designs enable MacVQA to balance knowledge acquisition, retention, and compositional generalization in continual VQA learning. Experiments on ten continual VQA tasks show that MacVQA outperforms existing baselines, achieving 43.38% average accuracy and 2.32% average forgetting on standard tasks, and 42.53% average accuracy and 3.60% average forgetting on novel composition tasks.",
      "authors": [
        "Zhifei Li",
        "Yiran Wang",
        "Chenyi Xiong",
        "Yujing Xia",
        "Xiaoju Hou",
        "Yue Zhao",
        "Miao Zhang",
        "Kui Xiao",
        "Bing Yang"
      ],
      "published": "2026-01-05T09:18:09Z",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "abs_url": "https://arxiv.org/abs/2601.01926v1",
      "pdf_url": "https://arxiv.org/pdf/2601.01926v1.pdf"
    },
    {
      "id": "2601.01925v1",
      "title": "AR-MOT: Autoregressive Multi-object Tracking",
      "abstract": "As multi-object tracking (MOT) tasks continue to evolve toward more general and multi-modal scenarios, the rigid and task-specific architectures of existing MOT methods increasingly hinder their applicability across diverse tasks and limit flexibility in adapting to new tracking formulations. Most approaches rely on fixed output heads and bespoke tracking pipelines, making them difficult to extend to more complex or instruction-driven tasks. To address these limitations, we propose AR-MOT, a novel autoregressive paradigm that formulates MOT as a sequence generation task within a large language model (LLM) framework. This design enables the model to output structured results through flexible sequence construction, without requiring any task-specific heads. To enhance region-level visual perception, we introduce an Object Tokenizer based on a pretrained detector. To mitigate the misalignment between global and regional features, we propose a Region-Aware Alignment (RAA) module, and to support long-term tracking, we design a Temporal Memory Fusion (TMF) module that caches historical object tokens. AR-MOT offers strong potential for extensibility, as new modalities or instructions can be integrated by simply modifying the output sequence format without altering the model architecture. Extensive experiments on MOT17 and DanceTrack validate the feasibility of our approach, achieving performance comparable to state-of-the-art methods while laying the foundation for more general and flexible MOT systems.",
      "authors": [
        "Lianjie Jia",
        "Yuhan Wu",
        "Binghao Ran",
        "Yifan Wang",
        "Lijun Wang",
        "Huchuan Lu"
      ],
      "published": "2026-01-05T09:17:28Z",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "abs_url": "https://arxiv.org/abs/2601.01925v1",
      "pdf_url": "https://arxiv.org/pdf/2601.01925v1.pdf"
    },
    {
      "id": "2601.01922v1",
      "title": "Efficient temporal prediction of compressible flows in irregular domains using Fourier neural operators",
      "abstract": "This paper investigates the temporal evolution of high-speed compressible fluids in irregular flow fields using the Fourier Neural Operator (FNO). We reconstruct the irregular flow field point set into sequential format compatible with FNO input requirements, and then embed temporal bundling technique within a recurrent neural network (RNN) for multi-step prediction. We further employ a composite loss function to balance errors across different physical quantities. Experiments are conducted on three different types of irregular flow fields, including orthogonal and non-orthogonal grid configurations. Then we comprehensively analyze the physical component loss curves, flow field visualizations, and physical profiles. Results demonstrate that our approach significantly surpasses traditional numerical methods in computational efficiency while achieving high accuracy, with maximum relative $L_2$ errors of (0.78, 0.57, 0.35)% for ($p$, $T$, $\\mathbf{u}$) respectively. This verifies that the method can efficiently and accurately simulate the temporal evolution of high-speed compressible flows in irregular domains.",
      "authors": [
        "Yifan Nie",
        "Qiaoxin Li"
      ],
      "published": "2026-01-05T09:12:35Z",
      "categories": [
        "physics.flu-dyn",
        "cs.LG"
      ],
      "primary_category": "physics.flu-dyn",
      "abs_url": "https://arxiv.org/abs/2601.01922v1",
      "pdf_url": "https://arxiv.org/pdf/2601.01922v1.pdf"
    },
    {
      "id": "2601.01921v1",
      "title": "A Defect is Being Born: How Close Are We? A Time Sensitive Forecasting Approach",
      "abstract": "Background. Defect prediction has been a highly active topic among researchers in the Empirical Software Engineering field. Previous literature has successfully achieved the most accurate prediction of an incoming fault and identified the features and anomalies that precede it through just-in-time prediction. As software systems evolve continuously, there is a growing need for time-sensitive methods capable of forecasting defects before they manifest.\n  Aim. Our study seeks to explore the effectiveness of time-sensitive techniques for defect forecasting. Moreover, we aim to investigate the early indicators that precede the occurrence of a defect.\n  Method. We will train multiple time-sensitive forecasting techniques to forecast the future bug density of a software project, as well as identify the early symptoms preceding the occurrence of a defect.\n  Expected results. Our expected results are translated into empirical evidence on the effectiveness of our approach for early estimation of bug proneness.",
      "authors": [
        "Mikel Robredo",
        "Matteo Esposito",
        "Fabio Palomba",
        "Rafael Peñaloza",
        "Valentina Lenarduzzi"
      ],
      "published": "2026-01-05T09:11:29Z",
      "categories": [
        "cs.SE",
        "cs.AI",
        "cs.IR",
        "cs.LG"
      ],
      "primary_category": "cs.SE",
      "abs_url": "https://arxiv.org/abs/2601.01921v1",
      "pdf_url": "https://arxiv.org/pdf/2601.01921v1.pdf"
    },
    {
      "id": "2601.01917v1",
      "title": "Distorted Distributional Policy Evaluation for Offline Reinforcement Learning",
      "abstract": "While Distributional Reinforcement Learning (DRL) methods have demonstrated strong performance in online settings, its success in offline scenarios remains limited. We hypothesize that a key limitation of existing offline DRL methods lies in their approach to uniformly underestimate return quantiles. This uniform pessimism can lead to overly conservative value estimates, ultimately hindering generalization and performance. To address this, we introduce a novel concept called quantile distortion, which enables non-uniform pessimism by adjusting the degree of conservatism based on the availability of supporting data. Our approach is grounded in theoretical analysis and empirically validated, demonstrating improved performance over uniform pessimism.",
      "authors": [
        "Ryo Iwaki",
        "Takayuki Osogami"
      ],
      "published": "2026-01-05T09:04:10Z",
      "categories": [
        "cs.LG"
      ],
      "primary_category": "cs.LG",
      "abs_url": "https://arxiv.org/abs/2601.01917v1",
      "pdf_url": "https://arxiv.org/pdf/2601.01917v1.pdf"
    },
    {
      "id": "2601.01915v1",
      "title": "TalkPhoto: A Versatile Training-Free Conversational Assistant for Intelligent Image Editing",
      "abstract": "Thanks to the powerful language comprehension capabilities of Large Language Models (LLMs), existing instruction-based image editing methods have introduced Multimodal Large Language Models (MLLMs) to promote information exchange between instructions and images, ensuring the controllability and flexibility of image editing. However, these frameworks often build a multi-instruction dataset to train the model to handle multiple editing tasks, which is not only time-consuming and labor-intensive but also fails to achieve satisfactory results. In this paper, we present TalkPhoto, a versatile training-free image editing framework that facilitates precise image manipulation through conversational interaction. We instruct the open-source LLM with a specially designed prompt template to analyze user needs after receiving instructions and hierarchically invoke existing advanced editing methods, all without additional training. Moreover, we implement a plug-and-play and efficient invocation of image editing methods, allowing complex and unseen editing tasks to be integrated into the current framework, achieving stable and high-quality editing results. Extensive experiments demonstrate that our method not only provides more accurate invocation with fewer token consumption but also achieves higher editing quality across various image editing tasks.",
      "authors": [
        "Yujie Hu",
        "Zecheng Tang",
        "Xu Jiang",
        "Weiqi Li",
        "Jian Zhang"
      ],
      "published": "2026-01-05T09:00:32Z",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "abs_url": "https://arxiv.org/abs/2601.01915v1",
      "pdf_url": "https://arxiv.org/pdf/2601.01915v1.pdf"
    },
    {
      "id": "2601.01914v1",
      "title": "Learning Action Hierarchies via Hybrid Geometric Diffusion",
      "abstract": "Temporal action segmentation is a critical task in video understanding, where the goal is to assign action labels to each frame in a video. While recent advances leverage iterative refinement-based strategies, they fail to explicitly utilize the hierarchical nature of human actions. In this work, we propose HybridTAS - a novel framework that incorporates a hybrid of Euclidean and hyperbolic geometries into the denoising process of diffusion models to exploit the hierarchical structure of actions. Hyperbolic geometry naturally provides tree-like relationships between embeddings, enabling us to guide the action label denoising process in a coarse-to-fine manner: higher diffusion timesteps are influenced by abstract, high-level action categories (root nodes), while lower timesteps are refined using fine-grained action classes (leaf nodes). Extensive experiments on three benchmark datasets, GTEA, 50Salads, and Breakfast, demonstrate that our method achieves state-of-the-art performance, validating the effectiveness of hyperbolic-guided denoising for the temporal action segmentation task.",
      "authors": [
        "Arjun Ramesh Kaushik",
        "Nalini K. Ratha",
        "Venu Govindaraju"
      ],
      "published": "2026-01-05T08:59:07Z",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "abs_url": "https://arxiv.org/abs/2601.01914v1",
      "pdf_url": "https://arxiv.org/pdf/2601.01914v1.pdf"
    },
    {
      "id": "2601.01910v1",
      "title": "MMP-A*: Multimodal Perception Enhanced Incremental Heuristic Search on Path Planning",
      "abstract": "Autonomous path planning requires a synergy between global reasoning and geometric precision, especially in complex or cluttered environments. While classical A* is valued for its optimality, it incurs prohibitive computational and memory costs in large-scale scenarios. Recent attempts to mitigate these limitations by using Large Language Models for waypoint guidance remain insufficient, as they rely only on text-based reasoning without spatial grounding. As a result, such models often produce incorrect waypoints in topologically complex environments with dead ends, and lack the perceptual capacity to interpret ambiguous physical boundaries. These inconsistencies lead to costly corrective expansions and undermine the intended computational efficiency.\n  We introduce MMP-A*, a multimodal framework that integrates the spatial grounding capabilities of vision-language models with a novel adaptive decay mechanism. By anchoring high-level reasoning in physical geometry, the framework produces coherent waypoint guidance that addresses the limitations of text-only planners. The adaptive decay mechanism dynamically regulates the influence of uncertain waypoints within the heuristic, ensuring geometric validity while substantially reducing memory overhead. To evaluate robustness, we test the framework in challenging environments characterized by severe clutter and topological complexity. Experimental results show that MMP-A* achieves near-optimal trajectories with significantly reduced operational costs, demonstrating its potential as a perception-grounded and computationally efficient paradigm for autonomous navigation.",
      "authors": [
        "Minh Hieu Ha",
        "Khanh Ly Ta",
        "Hung Phan",
        "Tung Doan",
        "Tung Dao",
        "Dao Tran",
        "Huynh Thi Thanh Binh"
      ],
      "published": "2026-01-05T08:55:27Z",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "abs_url": "https://arxiv.org/abs/2601.01910v1",
      "pdf_url": "https://arxiv.org/pdf/2601.01910v1.pdf"
    },
    {
      "id": "2601.01908v1",
      "title": "Nodule-DETR: A Novel DETR Architecture with Frequency-Channel Attention for Ultrasound Thyroid Nodule Detection",
      "abstract": "Thyroid cancer is the most common endocrine malignancy, and its incidence is rising globally. While ultrasound is the preferred imaging modality for detecting thyroid nodules, its diagnostic accuracy is often limited by challenges such as low image contrast and blurred nodule boundaries. To address these issues, we propose Nodule-DETR, a novel detection transformer (DETR) architecture designed for robust thyroid nodule detection in ultrasound images. Nodule-DETR introduces three key innovations: a Multi-Spectral Frequency-domain Channel Attention (MSFCA) module that leverages frequency analysis to enhance features of low-contrast nodules; a Hierarchical Feature Fusion (HFF) module for efficient multi-scale integration; and Multi-Scale Deformable Attention (MSDA) to flexibly capture small and irregularly shaped nodules. We conducted extensive experiments on a clinical dataset of real-world thyroid ultrasound images. The results demonstrate that Nodule-DETR achieves state-of-the-art performance, outperforming the baseline model by a significant margin of 0.149 in mAP@0.5:0.95. The superior accuracy of Nodule-DETR highlights its significant potential for clinical application as an effective tool in computer-aided thyroid diagnosis. The code of work is available at https://github.com/wjj1wjj/Nodule-DETR.",
      "authors": [
        "Jingjing Wang",
        "Qianglin Liu",
        "Zhuo Xiao",
        "Xinning Yao",
        "Bo Liu",
        "Lu Li",
        "Lijuan Niu",
        "Fugen Zhou"
      ],
      "published": "2026-01-05T08:53:04Z",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "abs_url": "https://arxiv.org/abs/2601.01908v1",
      "pdf_url": "https://arxiv.org/pdf/2601.01908v1.pdf"
    },
    {
      "id": "2601.01904v1",
      "title": "Evaluating Feature Dependent Noise in Preference-based Reinforcement Learning",
      "abstract": "Learning from Preferences in Reinforcement Learning (PbRL) has gained attention recently, as it serves as a natural fit for complicated tasks where the reward function is not easily available. However, preferences often come with uncertainty and noise if they are not from perfect teachers. Much prior literature aimed to detect noise, but with limited types of noise and most being uniformly distributed with no connection to observations. In this work, we formalize the notion of targeted feature-dependent noise and propose several variants like trajectory feature noise, trajectory similarity noise, uncertainty-aware noise, and Language Model noise.\n  We evaluate feature-dependent noise, where noise is correlated with certain features in complex continuous control tasks from DMControl and Meta-world. Our experiments show that in some feature-dependent noise settings, the state-of-the-art noise-robust PbRL method's learning performance is significantly deteriorated, while PbRL method with no explicit denoising can surprisingly outperform noise-robust PbRL in majority settings.\n  We also find language model's noise exhibits similar characteristics to feature-dependent noise, thereby simulating realistic humans and call for further study in learning with feature-dependent noise robustly.",
      "authors": [
        "Yuxuan Li",
        "Harshith Reddy Kethireddy",
        "Srijita Das"
      ],
      "published": "2026-01-05T08:49:30Z",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "abs_url": "https://arxiv.org/abs/2601.01904v1",
      "pdf_url": "https://arxiv.org/pdf/2601.01904v1.pdf"
    },
    {
      "id": "2601.01903v1",
      "title": "TT-FSI: Scalable Faithful Shapley Interactions via Tensor-Train",
      "abstract": "The Faithful Shapley Interaction (FSI) index uniquely satisfies the faithfulness axiom among Shapley interaction indices, but computing FSI requires $O(d^\\ell \\cdot 2^d)$ time and existing implementations use $O(4^d)$ memory. We present TT-FSI, which exploits FSI's algebraic structure via Matrix Product Operators (MPO). Our main theoretical contribution is proving that the linear operator $v \\mapsto \\text{FSI}(v)$ admits an MPO representation with TT-rank $O(\\ell d)$, enabling an efficient sweep algorithm with $O(\\ell^2 d^3 \\cdot 2^d)$ time and $O(\\ell d^2)$ core storage an exponential improvement over existing methods. Experiments on six datasets ($d=8$ to $d=20$) demonstrate up to 280$\\times$ speedup over baseline, 85$\\times$ over SHAP-IQ, and 290$\\times$ memory reduction. TT-FSI scales to $d=20$ (1M coalitions) where all competing methods fail.",
      "authors": [
        "Ungsik Kim",
        "Suwon Lee"
      ],
      "published": "2026-01-05T08:49:25Z",
      "categories": [
        "cs.LG"
      ],
      "primary_category": "cs.LG",
      "abs_url": "https://arxiv.org/abs/2601.01903v1",
      "pdf_url": "https://arxiv.org/pdf/2601.01903v1.pdf"
    },
    {
      "id": "2601.01901v1",
      "title": "FedBiCross: A Bi-Level Optimization Framework to Tackle Non-IID Challenges in Data-Free One-Shot Federated Learning on Medical Data",
      "abstract": "Data-free knowledge distillation-based one-shot federated learning (OSFL) trains a model in a single communication round without sharing raw data, making OSFL attractive for privacy-sensitive medical applications. However, existing methods aggregate predictions from all clients to form a global teacher. Under non-IID data, conflicting predictions cancel out during averaging, yielding near-uniform soft labels that provide weak supervision for distillation. We propose FedBiCross, a personalized OSFL framework with three stages: (1) clustering clients by model output similarity to form coherent sub-ensembles, (2) bi-level cross-cluster optimization that learns adaptive weights to selectively leverage beneficial cross-cluster knowledge while suppressing negative transfer, and (3) personalized distillation for client-specific adaptation. Experiments on four medical image datasets demonstrate that FedBiCross consistently outperforms state-of-the-art baselines across different non-IID degrees.",
      "authors": [
        "Yuexuan Xia",
        "Yinghao Zhang",
        "Yalin Liu",
        "Hong-Ning Dai",
        "Yong Xia"
      ],
      "published": "2026-01-05T08:46:11Z",
      "categories": [
        "cs.LG"
      ],
      "primary_category": "cs.LG",
      "abs_url": "https://arxiv.org/abs/2601.01901v1",
      "pdf_url": "https://arxiv.org/pdf/2601.01901v1.pdf"
    },
    {
      "id": "2601.01896v1",
      "title": "Tackling the Inherent Difficulty of Noise Filtering in RAG",
      "abstract": "Retrieval-Augmented Generation (RAG) has become a widely adopted approach to enhance Large Language Models (LLMs) by incorporating external knowledge and reducing hallucinations. However, noisy or irrelevant documents are often introduced during RAG, potentially degrading performance and even causing hallucinated outputs. While various methods have been proposed to filter out such noise, we argue that identifying irrelevant information from retrieved content is inherently difficult and limited number of transformer layers can hardly solve this. Consequently, retrievers fail to filter out irrelevant documents entirely. Therefore, LLMs must be robust against such noise, but we demonstrate that standard fine-tuning approaches are often ineffective in enabling the model to selectively utilize relevant information while ignoring irrelevant content due to the structural constraints of attention patterns. To address this, we propose a novel fine-tuning method designed to enhance the model's ability to distinguish between relevant and irrelevant information within retrieved documents. Extensive experiments across multiple benchmarks show that our approach significantly improves the robustness and performance of LLMs.",
      "authors": [
        "Jingyu Liu",
        "Jiaen Lin",
        "Yong Liu"
      ],
      "published": "2026-01-05T08:40:37Z",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "abs_url": "https://arxiv.org/abs/2601.01896v1",
      "pdf_url": "https://arxiv.org/pdf/2601.01896v1.pdf"
    },
    {
      "id": "2601.01892v1",
      "title": "Forget Less by Learning from Parents Through Hierarchical Relationships",
      "abstract": "Custom Diffusion Models (CDMs) offer impressive capabilities for personalization in generative modeling, yet they remain vulnerable to catastrophic forgetting when learning new concepts sequentially. Existing approaches primarily focus on minimizing interference between concepts, often neglecting the potential for positive inter-concept interactions. In this work, we present Forget Less by Learning from Parents (FLLP), a novel framework that introduces a parent-child inter-concept learning mechanism in hyperbolic space to mitigate forgetting. By embedding concept representations within a Lorentzian manifold, naturally suited to modeling tree-like hierarchies, we define parent-child relationships in which previously learned concepts serve as guidance for adapting to new ones. Our method not only preserves prior knowledge but also supports continual integration of new concepts. We validate FLLP on three public datasets and one synthetic benchmark, showing consistent improvements in both robustness and generalization.",
      "authors": [
        "Arjun Ramesh Kaushik",
        "Naresh Kumar Devulapally",
        "Vishnu Suresh Lokhande",
        "Nalini K. Ratha",
        "Venu Govindaraju"
      ],
      "published": "2026-01-05T08:35:36Z",
      "categories": [
        "cs.CV",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "abs_url": "https://arxiv.org/abs/2601.01892v1",
      "pdf_url": "https://arxiv.org/pdf/2601.01892v1.pdf"
    },
    {
      "id": "2601.01891v1",
      "title": "Agentic AI in Remote Sensing: Foundations, Taxonomy, and Emerging Systems",
      "abstract": "The paradigm of Earth Observation analysis is shifting from static deep learning models to autonomous agentic AI. Although recent vision foundation models and multimodal large language models advance representation learning, they often lack the sequential planning and active tool orchestration required for complex geospatial workflows. This survey presents the first comprehensive review of agentic AI in remote sensing. We introduce a unified taxonomy distinguishing between single-agent copilots and multi-agent systems while analyzing architectural foundations such as planning mechanisms, retrieval-augmented generation, and memory structures. Furthermore, we review emerging benchmarks that move the evaluation from pixel-level accuracy to trajectory-aware reasoning correctness. By critically examining limitations in grounding, safety, and orchestration, this work outlines a strategic roadmap for the development of robust, autonomous geospatial intelligence.",
      "authors": [
        "Niloufar Alipour Talemi",
        "Julia Boone",
        "Fatemeh Afghah"
      ],
      "published": "2026-01-05T08:34:17Z",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "abs_url": "https://arxiv.org/abs/2601.01891v1",
      "pdf_url": "https://arxiv.org/pdf/2601.01891v1.pdf"
    },
    {
      "id": "2601.01888v1",
      "title": "SafeLoad: Efficient Admission Control Framework for Identifying Memory-Overloading Queries in Cloud Data Warehouses",
      "abstract": "Memory overload is a common form of resource exhaustion in cloud data warehouses. When database queries fail due to memory overload, it not only wastes critical resources such as CPU time but also disrupts the execution of core business processes, as memory-overloading (MO) queries are typically part of complex workflows. If such queries are identified in advance and scheduled to memory-rich serverless clusters, it can prevent resource wastage and query execution failure. Therefore, cloud data warehouses desire an admission control framework with high prediction precision, interpretability, efficiency, and adaptability to effectively identify MO queries. However, existing admission control frameworks primarily focus on scenarios like SLA satisfaction and resource isolation, with limited precision in identifying MO queries. Moreover, there is a lack of publicly available MO-labeled datasets with workloads for training and benchmarking. To tackle these challenges, we propose SafeLoad, the first query admission control framework specifically designed to identify MO queries. Alongside, we release SafeBench, an open-source, industrial-scale benchmark for this task, which includes 150 million real queries. SafeLoad first filters out memory-safe queries using the interpretable discriminative rule. It then applies a hybrid architecture that integrates both a global model and cluster-level models, supplemented by a misprediction correction module to identify MO queries. Additionally, a self-tuning quota management mechanism dynamically adjusts prediction quotas per cluster to improve precision. Experimental results show that SafeLoad achieves state-of-the-art prediction performance with low online and offline time overhead. Specifically, SafeLoad improves precision by up to 66% over the best baseline and reduces wasted CPU time by up to 8.09x compared to scenarios without SafeLoad.",
      "authors": [
        "Yifan Wu",
        "Yuhan Li",
        "Zhenhua Wang",
        "Zhongle Xie",
        "Dingyu Yang",
        "Ke Chen",
        "Lidan Shou",
        "Bo Tang",
        "Liang Lin",
        "Huan Li",
        "Gang Chen"
      ],
      "published": "2026-01-05T08:29:51Z",
      "categories": [
        "cs.DB",
        "cs.LG"
      ],
      "primary_category": "cs.DB",
      "abs_url": "https://arxiv.org/abs/2601.01888v1",
      "pdf_url": "https://arxiv.org/pdf/2601.01888v1.pdf"
    },
    {
      "id": "2601.01887v1",
      "title": "Safety at One Shot: Patching Fine-Tuned LLMs with A Single Instance",
      "abstract": "Fine-tuning safety-aligned large language models (LLMs) can substantially compromise their safety. Previous approaches require many safety samples or calibration sets, which not only incur significant computational overhead during realignment but also lead to noticeable degradation in model utility. Contrary to this belief, we show that safety alignment can be fully recovered with only a single safety example, without sacrificing utility and at minimal cost. Remarkably, this recovery is effective regardless of the number of harmful examples used in fine-tuning or the size of the underlying model, and convergence is achieved within just a few epochs. Furthermore, we uncover the low-rank structure of the safety gradient, which explains why such efficient correction is possible. We validate our findings across five safety-aligned LLMs and multiple datasets, demonstrating the generality of our approach.",
      "authors": [
        "Jiawen Zhang",
        "Lipeng He",
        "Kejia Chen",
        "Jian Lou",
        "Jian Liu",
        "Xiaohu Yang",
        "Ruoxi Jia"
      ],
      "published": "2026-01-05T08:26:34Z",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "abs_url": "https://arxiv.org/abs/2601.01887v1",
      "pdf_url": "https://arxiv.org/pdf/2601.01887v1.pdf"
    },
    {
      "id": "2601.01885v1",
      "title": "Agentic Memory: Learning Unified Long-Term and Short-Term Memory Management for Large Language Model Agents",
      "abstract": "Large language model (LLM) agents face fundamental limitations in long-horizon reasoning due to finite context windows, making effective memory management critical. Existing methods typically handle long-term memory (LTM) and short-term memory (STM) as separate components, relying on heuristics or auxiliary controllers, which limits adaptability and end-to-end optimization. In this paper, we propose Agentic Memory (AgeMem), a unified framework that integrates LTM and STM management directly into the agent's policy. AgeMem exposes memory operations as tool-based actions, enabling the LLM agent to autonomously decide what and when to store, retrieve, update, summarize, or discard information. To train such unified behaviors, we propose a three-stage progressive reinforcement learning strategy and design a step-wise GRPO to address sparse and discontinuous rewards induced by memory operations. Experiments on five long-horizon benchmarks demonstrate that AgeMem consistently outperforms strong memory-augmented baselines across multiple LLM backbones, achieving improved task performance, higher-quality long-term memory, and more efficient context usage.",
      "authors": [
        "Yi Yu",
        "Liuyi Yao",
        "Yuexiang Xie",
        "Qingquan Tan",
        "Jiaqi Feng",
        "Yaliang Li",
        "Libing Wu"
      ],
      "published": "2026-01-05T08:24:16Z",
      "categories": [
        "cs.CL"
      ],
      "primary_category": "cs.CL",
      "abs_url": "https://arxiv.org/abs/2601.01885v1",
      "pdf_url": "https://arxiv.org/pdf/2601.01885v1.pdf"
    },
    {
      "id": "2601.01878v1",
      "title": "Theory Trace Card: Theory-Driven Socio-Cognitive Evaluation of LLMs",
      "abstract": "Socio-cognitive benchmarks for large language models (LLMs) often fail to predict real-world behavior, even when models achieve high benchmark scores. Prior work has attributed this evaluation-deployment gap to problems of measurement and validity. While these critiques are insightful, we argue that they overlook a more fundamental issue: many socio-cognitive evaluations proceed without an explicit theoretical specification of the target capability, leaving the assumptions linking task performance to competence implicit. Without this theoretical grounding, benchmarks that exercise only narrow subsets of a capability are routinely misinterpreted as evidence of broad competence: a gap that creates a systemic validity illusion by masking the failure to evaluate the capability's other essential dimensions. To address this gap, we make two contributions. First, we diagnose and formalize this theory gap as a foundational failure that undermines measurement and enables systematic overgeneralization of benchmark results. Second, we introduce the Theory Trace Card (TTC), a lightweight documentation artifact designed to accompany socio-cognitive evaluations, which explicitly outlines the theoretical basis of an evaluation, the components of the target capability it exercises, its operationalization, and its limitations. We argue that TTCs enhance the interpretability and reuse of socio-cognitive evaluations by making explicit the full validity chain, which links theory, task operationalization, scoring, and limitations, without modifying benchmarks or requiring agreement on a single theory.",
      "authors": [
        "Farzan Karimi-Malekabadi",
        "Suhaib Abdurahman",
        "Zhivar Sourati",
        "Jackson Trager",
        "Morteza Dehghani"
      ],
      "published": "2026-01-05T08:06:50Z",
      "categories": [
        "cs.AI",
        "cs.CY"
      ],
      "primary_category": "cs.AI",
      "abs_url": "https://arxiv.org/abs/2601.01878v1",
      "pdf_url": "https://arxiv.org/pdf/2601.01878v1.pdf"
    },
    {
      "id": "2601.01877v1",
      "title": "Random-Matrix-Induced Simplicity Bias in Over-parameterized Variational Quantum Circuits",
      "abstract": "Over-parameterization is commonly used to increase the expressivity of variational quantum circuits (VQCs), yet deeper and more highly parameterized circuits often exhibit poor trainability and limited generalization. In this work, we provide a theoretical explanation for this phenomenon from a function-class perspective. We show that sufficiently expressive, unstructured variational ansatze enter a Haar-like universality class in which both observable expectation values and parameter gradients concentrate exponentially with system size. As a consequence, the hypothesis class induced by such circuits collapses with high probability to a narrow family of near-constant functions, a phenomenon we term simplicity bias, with barren plateaus arising as a consequence rather than the root cause. Using tools from random matrix theory and concentration of measure, we rigorously characterize this universality class and establish uniform hypothesis-class collapse over finite datasets. We further show that this collapse is not unavoidable: tensor-structured VQCs, including tensor-network-based and tensor-hypernetwork parameterizations, lie outside the Haar-like universality class. By restricting the accessible unitary ensemble through bounded tensor rank or bond dimension, these architectures prevent concentration of measure, preserve output variability for local observables, and retain non-degenerate gradient signals even in over-parameterized regimes. Together, our results unify barren plateaus, expressivity limits, and generalization collapse under a single structural mechanism rooted in random-matrix universality, highlighting the central role of architectural inductive bias in variational quantum algorithms.",
      "authors": [
        "Jun Qi",
        "Chao-Han Huck Yang",
        "Pin-Yu Chen",
        "Min-Hsiu Hsieh"
      ],
      "published": "2026-01-05T08:04:33Z",
      "categories": [
        "quant-ph",
        "cs.LG",
        "math-ph"
      ],
      "primary_category": "quant-ph",
      "abs_url": "https://arxiv.org/abs/2601.01877v1",
      "pdf_url": "https://arxiv.org/pdf/2601.01877v1.pdf"
    },
    {
      "id": "2601.01875v1",
      "title": "Toward Auditable Neuro-Symbolic Reasoning in Pathology: SQL as an Explicit Trace of Evidence",
      "abstract": "Automated pathology image analysis is central to clinical diagnosis, but clinicians still ask which slide features drive a model's decision and why. Vision-language models can produce natural language explanations, but these are often correlational and lack verifiable evidence. In this paper, we introduce an SQL-centered agentic framework that enables both feature measurement and reasoning to be auditable. Specifically, after extracting human-interpretable cellular features, Feature Reasoning Agents compose and execute SQL queries over feature tables to aggregate visual evidence into quantitative findings. A Knowledge Comparison Agent then evaluates these findings against established pathological knowledge, mirroring how pathologists justify diagnoses from measurable observations. Extensive experiments evaluated on two pathology visual question answering datasets demonstrate our method improves interpretability and decision traceability while producing executable SQL traces that link cellular measurements to diagnostic conclusions.",
      "authors": [
        "Kewen Cao",
        "Jianxu Chen",
        "Yongbing Zhang",
        "Ye Zhang",
        "Hongxiao Wang"
      ],
      "published": "2026-01-05T08:02:49Z",
      "categories": [
        "cs.AI",
        "q-bio.QM"
      ],
      "primary_category": "cs.AI",
      "abs_url": "https://arxiv.org/abs/2601.01875v1",
      "pdf_url": "https://arxiv.org/pdf/2601.01875v1.pdf"
    },
    {
      "id": "2601.01874v1",
      "title": "CogFlow: Bridging Perception and Reasoning through Knowledge Internalization for Visual Mathematical Problem Solving",
      "abstract": "Despite significant progress, multimodal large language models continue to struggle with visual mathematical problem solving. Some recent works recognize that visual perception is a bottleneck in visual mathematical reasoning, but their solutions are limited to improving the extraction and interpretation of visual inputs. Notably, they all ignore the key issue of whether the extracted visual cues are faithfully integrated and properly utilized in subsequent reasoning. Motivated by this, we present CogFlow, a novel cognitive-inspired three-stage framework that incorporates a knowledge internalization stage, explicitly simulating the hierarchical flow of human reasoning: perception$\\Rightarrow$internalization$\\Rightarrow$reasoning. Inline with this hierarchical flow, we holistically enhance all its stages. We devise Synergistic Visual Rewards to boost perception capabilities in parametric and semantic spaces, jointly improving visual information extraction from symbols and diagrams. To guarantee faithful integration of extracted visual cues into subsequent reasoning, we introduce a Knowledge Internalization Reward model in the internalization stage, bridging perception and reasoning. Moreover, we design a Visual-Gated Policy Optimization algorithm to further enforce the reasoning is grounded with the visual knowledge, preventing models seeking shortcuts that appear coherent but are visually ungrounded reasoning chains. Moreover, we contribute a new dataset MathCog for model training, which contains samples with over 120K high-quality perception-reasoning aligned annotations. Comprehensive experiments and analysis on commonly used visual mathematical reasoning benchmarks validate the superiority of the proposed CogFlow.",
      "authors": [
        "Shuhang Chen",
        "Yunqiu Xu",
        "Junjie Xie",
        "Aojun Lu",
        "Tao Feng",
        "Zeying Huang",
        "Ning Zhang",
        "Yi Sun",
        "Yi Yang",
        "Hangjie Yuan"
      ],
      "published": "2026-01-05T08:02:18Z",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "abs_url": "https://arxiv.org/abs/2601.01874v1",
      "pdf_url": "https://arxiv.org/pdf/2601.01874v1.pdf"
    },
    {
      "id": "2601.01872v1",
      "title": "CausalNav: A Long-term Embodied Navigation System for Autonomous Mobile Robots in Dynamic Outdoor Scenarios",
      "abstract": "Autonomous language-guided navigation in large-scale outdoor environments remains a key challenge in mobile robotics, due to difficulties in semantic reasoning, dynamic conditions, and long-term stability. We propose CausalNav, the first scene graph-based semantic navigation framework tailored for dynamic outdoor environments. We construct a multi-level semantic scene graph using LLMs, referred to as the Embodied Graph, that hierarchically integrates coarse-grained map data with fine-grained object entities. The constructed graph serves as a retrievable knowledge base for Retrieval-Augmented Generation (RAG), enabling semantic navigation and long-range planning under open-vocabulary queries. By fusing real-time perception with offline map data, the Embodied Graph supports robust navigation across varying spatial granularities in dynamic outdoor environments. Dynamic objects are explicitly handled in both the scene graph construction and hierarchical planning modules. The Embodied Graph is continuously updated within a temporal window to reflect environmental changes and support real-time semantic navigation. Extensive experiments in both simulation and real-world settings demonstrate superior robustness and efficiency.",
      "authors": [
        "Hongbo Duan",
        "Shangyi Luo",
        "Zhiyuan Deng",
        "Yanbo Chen",
        "Yuanhao Chiang",
        "Yi Liu",
        "Fangming Liu",
        "Xueqian Wang"
      ],
      "published": "2026-01-05T08:00:34Z",
      "categories": [
        "cs.RO"
      ],
      "primary_category": "cs.RO",
      "abs_url": "https://arxiv.org/abs/2601.01872v1",
      "pdf_url": "https://arxiv.org/pdf/2601.01872v1.pdf"
    },
    {
      "id": "2601.01870v1",
      "title": "Entity-Guided Multi-Task Learning for Infrared and Visible Image Fusion",
      "abstract": "Existing text-driven infrared and visible image fusion approaches often rely on textual information at the sentence level, which can lead to semantic noise from redundant text and fail to fully exploit the deeper semantic value of textual information. To address these issues, we propose a novel fusion approach named Entity-Guided Multi-Task learning for infrared and visible image fusion (EGMT). Our approach includes three key innovative components: (i) A principled method is proposed to extract entity-level textual information from image captions generated by large vision-language models, eliminating semantic noise from raw text while preserving critical semantic information; (ii) A parallel multi-task learning architecture is constructed, which integrates image fusion with a multi-label classification task. By using entities as pseudo-labels, the multi-label classification task provides semantic supervision, enabling the model to achieve a deeper understanding of image content and significantly improving the quality and semantic density of the fused image; (iii) An entity-guided cross-modal interactive module is also developed to facilitate the fine-grained interaction between visual and entity-level textual features, which enhances feature representation by capturing cross-modal dependencies at both inter-visual and visual-entity levels. To promote the wide application of the entity-guided image fusion framework, we release the entity-annotated version of four public datasets (i.e., TNO, RoadScene, M3FD, and MSRS). Extensive experiments demonstrate that EGMT achieves superior performance in preserving salient targets, texture details, and semantic consistency, compared to the state-of-the-art methods. The code and dataset will be publicly available at https://github.com/wyshao-01/EGMT.",
      "authors": [
        "Wenyu Shao",
        "Hongbo Liu",
        "Yunchuan Ma",
        "Ruili Wang"
      ],
      "published": "2026-01-05T08:00:03Z",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "abs_url": "https://arxiv.org/abs/2601.01870v1",
      "pdf_url": "https://arxiv.org/pdf/2601.01870v1.pdf"
    },
    {
      "id": "2601.01868v1",
      "title": "DermoGPT: Open Weights and Open Data for Morphology-Grounded Dermatological Reasoning MLLMs",
      "abstract": "Multimodal Large Language Models (MLLMs) show promise for medical applications, yet progress in dermatology lags due to limited training data, narrow task coverage, and lack of clinically-grounded supervision that mirrors expert diagnostic workflows. We present a comprehensive framework to address these gaps. First, we introduce DermoInstruct, a large-scale morphology-anchored instruction corpus comprising 211,243 images and 772,675 trajectories across five task formats, capturing the complete diagnostic pipeline from morphological observation and clinical reasoning to final diagnosis. Second, we establish DermoBench, a rigorous benchmark evaluating 11 tasks across four clinical axes: Morphology, Diagnosis, Reasoning, and Fairness, including a challenging subset of 3,600 expert-verified open-ended instances and human performance baselines. Third, we develop DermoGPT, a dermatology reasoning MLLM trained via supervised fine-tuning followed by our Morphologically-Anchored Visual-Inference-Consistent (MAVIC) reinforcement learning objective, which enforces consistency between visual observations and diagnostic conclusions. At inference, we deploy Confidence-Consistency Test-time adaptation (CCT) for robust predictions. Experiments show DermoGPT significantly outperforms 16 representative baselines across all axes, achieving state-of-the-art performance while substantially narrowing the human-AI gap. DermoInstruct, DermoBench and DermoGPT will be made publicly available at https://github.com/mendicant04/DermoGPT upon acceptance.",
      "authors": [
        "Jinghan Ru",
        "Siyuan Yan",
        "Yuguo Yin",
        "Yuexian Zou",
        "Zongyuan Ge"
      ],
      "published": "2026-01-05T07:55:36Z",
      "categories": [
        "cs.CL"
      ],
      "primary_category": "cs.CL",
      "abs_url": "https://arxiv.org/abs/2601.01868v1",
      "pdf_url": "https://arxiv.org/pdf/2601.01868v1.pdf"
    },
    {
      "id": "2601.01865v1",
      "title": "RRNet: Configurable Real-Time Video Enhancement with Arbitrary Local Lighting Variations",
      "abstract": "With the growing demand for real-time video enhancement in live applications, existing methods often struggle to balance speed and effective exposure control, particularly under uneven lighting. We introduce RRNet (Rendering Relighting Network), a lightweight and configurable framework that achieves a state-of-the-art tradeoff between visual quality and efficiency. By estimating parameters for a minimal set of virtual light sources, RRNet enables localized relighting through a depth-aware rendering module without requiring pixel-aligned training data. This object-aware formulation preserves facial identity and supports real-time, high-resolution performance using a streamlined encoder and lightweight prediction head. To facilitate training, we propose a generative AI-based dataset creation pipeline that synthesizes diverse lighting conditions at low cost. With its interpretable lighting control and efficient architecture, RRNet is well suited for practical applications such as video conferencing, AR-based portrait enhancement, and mobile photography. Experiments show that RRNet consistently outperforms prior methods in low-light enhancement, localized illumination adjustment, and glare removal.",
      "authors": [
        "Wenlong Yang",
        "Canran Jin",
        "Weihang Yuan",
        "Chao Wang",
        "Lifeng Sun"
      ],
      "published": "2026-01-05T07:50:59Z",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "abs_url": "https://arxiv.org/abs/2601.01865v1",
      "pdf_url": "https://arxiv.org/pdf/2601.01865v1.pdf"
    },
    {
      "id": "2601.01862v1",
      "title": "Judging with Personality and Confidence: A Study on Personality-Conditioned LLM Relevance Assessment",
      "abstract": "Recent studies have shown that prompting can enable large language models (LLMs) to simulate specific personality traits and produce behaviors that align with those traits. However, there is limited understanding of how these simulated personalities influence critical web search decisions, specifically relevance assessment. Moreover, few studies have examined how simulated personalities impact confidence calibration, specifically the tendencies toward overconfidence or underconfidence. This gap exists even though psychological literature suggests these biases are trait-specific, often linking high extraversion to overconfidence and high neuroticism to underconfidence. To address this gap, we conducted a comprehensive study evaluating multiple LLMs, including commercial models and open-source models, prompted to simulate Big Five personality traits. We tested these models across three test collections (TREC DL 2019, TREC DL 2020, and LLMJudge), collecting two key outputs for each query-document pair: a relevance judgment and a self-reported confidence score.\n  The findings show that personalities such as low agreeableness consistently align more closely with human labels than the unprompted condition. Additionally, low conscientiousness performs well in balancing the suppression of both overconfidence and underconfidence. We also observe that relevance scores and confidence distributions vary systematically across different personalities. Based on the above findings, we incorporate personality-conditioned scores and confidence as features in a random forest classifier. This approach achieves performance that surpasses the best single-personality condition on a new dataset (TREC DL 2021), even with limited training data. These findings highlight that personality-derived confidence offers a complementary predictive signal, paving the way for more reliable and human-aligned LLM evaluators.",
      "authors": [
        "Nuo Chen",
        "Hanpei Fang",
        "Piaohong Wang",
        "Jiqun Liu",
        "Tetsuya Sakai",
        "Xiao-Ming Wu"
      ],
      "published": "2026-01-05T07:46:29Z",
      "categories": [
        "cs.CL",
        "cs.IR"
      ],
      "primary_category": "cs.CL",
      "abs_url": "https://arxiv.org/abs/2601.01862v1",
      "pdf_url": "https://arxiv.org/pdf/2601.01862v1.pdf"
    },
    {
      "id": "2601.01860v1",
      "title": "High-Order Epistasis Detection Using Factorization Machine with Quadratic Optimization Annealing and MDR-Based Evaluation",
      "abstract": "Detecting high-order epistasis is a fundamental challenge in genetic association studies due to the combinatorial explosion of candidate locus combinations. Although multifactor dimensionality reduction (MDR) is a widely used method for evaluating epistasis, exhaustive MDR-based searches become computationally infeasible as the number of loci or the interaction order increases. In this paper, we define the epistasis detection problem as a black-box optimization problem and solve it with a factorization machine with quadratic optimization annealing (FMQA). We propose an efficient epistasis detection method based on FMQA, in which the classification error rate (CER) computed by MDR is used as a black-box objective function. Experimental evaluations were conducted using simulated case-control datasets with predefined high-order epistasis. The results demonstrate that the proposed method successfully identified ground-truth epistasis across various interaction orders and the numbers of genetic loci within a limited number of iterations. These results indicate that the proposed method is effective and computationally efficient for high-order epistasis detection.",
      "authors": [
        "Shuta Kikuchi",
        "Shu Tanaka"
      ],
      "published": "2026-01-05T07:41:34Z",
      "categories": [
        "cs.LG",
        "quant-ph"
      ],
      "primary_category": "cs.LG",
      "abs_url": "https://arxiv.org/abs/2601.01860v1",
      "pdf_url": "https://arxiv.org/pdf/2601.01860v1.pdf"
    },
    {
      "id": "2601.01857v1",
      "title": "Jenius Agent: Towards Experience-Driven Accuracy Optimization in Real-World Scenarios",
      "abstract": "As agent systems powered by large language models (LLMs) advance, improving the task performance of an autonomous agent, especially in context understanding, tool usage, and response generation, has become increasingly critical. Although prior studies have advanced the overall design of LLM-based agents, systematic optimization of their internal reasoning and tool-use pipelines remains underexplored. This paper introduces an agent framework grounded in real-world practical experience, with three key innovations: (1) an adaptive prompt generation strategy that aligns with the agent's state and task goals to improve reliability and robustness; (2) a context-aware tool orchestration module that performs tool categorization, semantic retrieval, and adaptive invocation based on user intent and context; and (3) a layered memory mechanism that integrates session memory, task history, and external summaries to improve relevance and efficiency through dynamic summarization and compression. An end-to-end framework named Jenius-Agent has been integrated with three key optimizations, including tools based on the Model Context Protocol (MCP), file input/output (I/O), and execution feedback. The experiments show a 20 percent improvement in task accuracy, along with a reduced token cost, response latency, and invocation failures. The framework is already deployed in Jenius (https://www.jenius.cn), providing a lightweight and scalable solution for robust, protocol-compatible autonomous agents.",
      "authors": [
        "Defei Xia",
        "Bingfeng Pi",
        "Shenbin Zhang",
        "Song Hua",
        "Yunfei Wei",
        "Lei Zuo"
      ],
      "published": "2026-01-05T07:35:12Z",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "abs_url": "https://arxiv.org/abs/2601.01857v1",
      "pdf_url": "https://arxiv.org/pdf/2601.01857v1.pdf"
    },
    {
      "id": "2601.01856v1",
      "title": "GCR: Geometry-Consistent Routing for Task-Agnostic Continual Anomaly Detection",
      "abstract": "Feature-based anomaly detection is widely adopted in industrial inspection due to the strong representational power of large pre-trained vision encoders. While most existing methods focus on improving within-category anomaly scoring, practical deployments increasingly require task-agnostic operation under continual category expansion, where the category identity is unknown at test time. In this setting, overall performance is often dominated by expert selection, namely routing an input to an appropriate normality model before any head-specific scoring is applied. However, routing rules that compare head-specific anomaly scores across independently constructed heads are unreliable in practice, as score distributions can differ substantially across categories in scale and tail behavior.\n  We propose GCR, a lightweight mixture-of-experts framework for stabilizing task-agnostic continual anomaly detection through geometry-consistent routing. GCR routes each test image directly in a shared frozen patch-embedding space by minimizing an accumulated nearest-prototype distance to category-specific prototype banks, and then computes anomaly maps only within the routed expert using a standard prototype-based scoring rule. By separating cross-head decision making from within-head anomaly scoring, GCR avoids cross-head score comparability issues without requiring end-to-end representation learning.\n  Experiments on MVTec AD and VisA show that geometry-consistent routing substantially improves routing stability and mitigates continual performance collapse, achieving near-zero forgetting while maintaining competitive detection and localization performance. These results indicate that many failures previously attributed to representation forgetting can instead be explained by decision-rule instability in cross-head routing. Code is available at https://github.com/jw-chae/GCR",
      "authors": [
        "Joongwon Chae",
        "Lihui Luo",
        "Yang Liu",
        "Runming Wang",
        "Dongmei Yu",
        "Zeming Liang",
        "Xi Yuan",
        "Dayan Zhang",
        "Zhenglin Chen",
        "Peiwu Qin",
        "Ilmoon Chae"
      ],
      "published": "2026-01-05T07:33:50Z",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "abs_url": "https://arxiv.org/abs/2601.01856v1",
      "pdf_url": "https://arxiv.org/pdf/2601.01856v1.pdf"
    },
    {
      "id": "2601.01852v1",
      "title": "MORE: Multi-Objective Adversarial Attacks on Speech Recognition",
      "abstract": "The emergence of large-scale automatic speech recognition (ASR) models such as Whisper has greatly expanded their adoption across diverse real-world applications. Ensuring robustness against even minor input perturbations is therefore critical for maintaining reliable performance in real-time environments. While prior work has mainly examined accuracy degradation under adversarial attacks, robustness with respect to efficiency remains largely unexplored. This narrow focus provides only a partial understanding of ASR model vulnerabilities. To address this gap, we conduct a comprehensive study of ASR robustness under multiple attack scenarios. We introduce MORE, a multi-objective repetitive doubling encouragement attack, which jointly degrades recognition accuracy and inference efficiency through a hierarchical staged repulsion-anchoring mechanism. Specifically, we reformulate multi-objective adversarial optimization into a hierarchical framework that sequentially achieves the dual objectives. To further amplify effectiveness, we propose a novel repetitive encouragement doubling objective (REDO) that induces duplicative text generation by maintaining accuracy degradation and periodically doubling the predicted sequence length. Overall, MORE compels ASR models to produce incorrect transcriptions at a substantially higher computational cost, triggered by a single adversarial input. Experiments show that MORE consistently yields significantly longer transcriptions while maintaining high word error rates compared to existing baselines, underscoring its effectiveness in multi-objective adversarial attack.",
      "authors": [
        "Xiaoxue Gao",
        "Zexin Li",
        "Yiming Chen",
        "Nancy F. Chen"
      ],
      "published": "2026-01-05T07:27:57Z",
      "categories": [
        "eess.AS",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "eess.AS",
      "abs_url": "https://arxiv.org/abs/2601.01852v1",
      "pdf_url": "https://arxiv.org/pdf/2601.01852v1.pdf"
    },
    {
      "id": "2601.01847v1",
      "title": "ESGaussianFace: Emotional and Stylized Audio-Driven Facial Animation via 3D Gaussian Splatting",
      "abstract": "Most current audio-driven facial animation research primarily focuses on generating videos with neutral emotions. While some studies have addressed the generation of facial videos driven by emotional audio, efficiently generating high-quality talking head videos that integrate both emotional expressions and style features remains a significant challenge. In this paper, we propose ESGaussianFace, an innovative framework for emotional and stylized audio-driven facial animation. Our approach leverages 3D Gaussian Splatting to reconstruct 3D scenes and render videos, ensuring efficient generation of 3D consistent results. We propose an emotion-audio-guided spatial attention method that effectively integrates emotion features with audio content features. Through emotion-guided attention, the model is able to reconstruct facial details across different emotional states more accurately. To achieve emotional and stylized deformations of the 3D Gaussian points through emotion and style features, we introduce two 3D Gaussian deformation predictors. Futhermore, we propose a multi-stage training strategy, enabling the step-by-step learning of the character's lip movements, emotional variations, and style features. Our generated results exhibit high efficiency, high quality, and 3D consistency. Extensive experimental results demonstrate that our method outperforms existing state-of-the-art techniques in terms of lip movement accuracy, expression variation, and style feature expressiveness.",
      "authors": [
        "Chuhang Ma",
        "Shuai Tan",
        "Ye Pan",
        "Jiaolong Yang",
        "Xin Tong"
      ],
      "published": "2026-01-05T07:19:38Z",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "abs_url": "https://arxiv.org/abs/2601.01847v1",
      "pdf_url": "https://arxiv.org/pdf/2601.01847v1.pdf"
    },
    {
      "id": "2601.01844v1",
      "title": "Clinical Knowledge Graph Construction and Evaluation with Multi-LLMs via Retrieval-Augmented Generation",
      "abstract": "Large language models (LLMs) offer new opportunities for constructing knowledge graphs (KGs) from unstructured clinical narratives. However, existing approaches often rely on structured inputs and lack robust validation of factual accuracy and semantic consistency, limitations that are especially problematic in oncology. We introduce an end-to-end framework for clinical KG construction and evaluation directly from free text using multi-agent prompting and a schema-constrained Retrieval-Augmented Generation (KG-RAG) strategy. Our pipeline integrates (1) prompt-driven entity, attribute, and relation extraction; (2) entropy-based uncertainty scoring; (3) ontology-aligned RDF/OWL schema generation; and (4) multi-LLM consensus validation for hallucination detection and semantic refinement. Beyond static graph construction, the framework supports continuous refinement and self-supervised evaluation, enabling iterative improvement of graph quality. Applied to two oncology cohorts (PDAC and BRCA), our method produces interpretable, SPARQL-compatible, and clinically grounded knowledge graphs without relying on gold-standard annotations. Experimental results demonstrate consistent gains in precision, relevance, and ontology compliance over baseline methods.",
      "authors": [
        "Udiptaman Das",
        "Krishnasai B. Atmakuri",
        "Duy Ho",
        "Chi Lee",
        "Yugyung Lee"
      ],
      "published": "2026-01-05T07:16:29Z",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "abs_url": "https://arxiv.org/abs/2601.01844v1",
      "pdf_url": "https://arxiv.org/pdf/2601.01844v1.pdf"
    },
    {
      "id": "2601.01842v1",
      "title": "Towards Automated Lexicography: Generating and Evaluating Definitions for Learner's Dictionaries",
      "abstract": "We study dictionary definition generation (DDG), i.e., the generation of non-contextualized definitions for given headwords. Dictionary definitions are an essential resource for learning word senses, but manually creating them is costly, which motivates us to automate the process. Specifically, we address learner's dictionary definition generation (LDDG), where definitions should consist of simple words. First, we introduce a reliable evaluation approach for DDG, based on our new evaluation criteria and powered by an LLM-as-a-judge. To provide reference definitions for the evaluation, we also construct a Japanese dataset in collaboration with a professional lexicographer. Validation results demonstrate that our evaluation approach agrees reasonably well with human annotators. Second, we propose an LDDG approach via iterative simplification with an LLM. Experimental results indicate that definitions generated by our approach achieve high scores on our criteria while maintaining lexical simplicity.",
      "authors": [
        "Yusuke Ide",
        "Adam Nohejl",
        "Joshua Tanner",
        "Hitomi Yanaka",
        "Christopher Lindsay",
        "Taro Watanabe"
      ],
      "published": "2026-01-05T07:11:24Z",
      "categories": [
        "cs.CL"
      ],
      "primary_category": "cs.CL",
      "abs_url": "https://arxiv.org/abs/2601.01842v1",
      "pdf_url": "https://arxiv.org/pdf/2601.01842v1.pdf"
    },
    {
      "id": "2601.01840v1",
      "title": "Tackling Resource-Constrained and Data-Heterogeneity in Federated Learning with Double-Weight Sparse Pack",
      "abstract": "Federated learning has drawn widespread interest from researchers, yet the data heterogeneity across edge clients remains a key challenge, often degrading model performance. Existing methods enhance model compatibility with data heterogeneity by splitting models and knowledge distillation. However, they neglect the insufficient communication bandwidth and computing power on the client, failing to strike an effective balance between addressing data heterogeneity and accommodating limited client resources. To tackle this limitation, we propose a personalized federated learning method based on cosine sparsification parameter packing and dual-weighted aggregation (FedCSPACK), which effectively leverages the limited client resources and reduces the impact of data heterogeneity on model performance. In FedCSPACK, the client packages model parameters and selects the most contributing parameter packages for sharing based on cosine similarity, effectively reducing bandwidth requirements. The client then generates a mask matrix anchored to the shared parameter package to improve the alignment and aggregation efficiency of sparse updates on the server. Furthermore, directional and distribution distance weights are embedded in the mask to implement a weighted-guided aggregation mechanism, enhancing the robustness and generalization performance of the global model. Extensive experiments across four datasets using ten state-of-the-art methods demonstrate that FedCSPACK effectively improves communication and computational efficiency while maintaining high model accuracy.",
      "authors": [
        "Qiantao Yang",
        "Liquan Chen",
        "Mingfu Xue",
        "Songze Li"
      ],
      "published": "2026-01-05T07:03:04Z",
      "categories": [
        "cs.LG",
        "cs.DC"
      ],
      "primary_category": "cs.LG",
      "abs_url": "https://arxiv.org/abs/2601.01840v1",
      "pdf_url": "https://arxiv.org/pdf/2601.01840v1.pdf"
    },
    {
      "id": "2601.01839v1",
      "title": "The Machine Learning Canvas: Empirical Findings on Why Strategy Matters More Than AI Code Generation",
      "abstract": "Despite the growing popularity of AI coding assistants, over 80% of machine learning (ML) projects fail to deliver real business value. This study creates and tests a Machine Learning Canvas, a practical framework that combines business strategy, software engineering, and data science in order to determine the factors that lead to the success of ML projects. We surveyed 150 data scientists and analyzed their responses using statistical modeling. We identified four key success factors: Strategy (clear goals and planning), Process (how work gets done), Ecosystem (tools and infrastructure), and Support (organizational backing and resources). Our results show that these factors are interconnected - each one affects the next. For instance, strong organizational support results in a clearer strategy (β= 0.432, p < 0.001), which improves work processes (β= 0.428, p < 0.001) and builds better infrastructure (β= 0.547, p < 0.001). Together, these elements determine whether a project succeeds. The surprising finding? Although AI assistants make coding faster, they don't guarantee project success. AI assists with the \"how\" of coding but cannot replace the \"why\" and \"what\" of strategic thinking.",
      "authors": [
        "Martin Prause"
      ],
      "published": "2026-01-05T07:02:58Z",
      "categories": [
        "cs.SE",
        "cs.AI"
      ],
      "primary_category": "cs.SE",
      "abs_url": "https://arxiv.org/abs/2601.01839v1",
      "pdf_url": "https://arxiv.org/pdf/2601.01839v1.pdf"
    },
    {
      "id": "2601.01836v1",
      "title": "COMPASS: A Framework for Evaluating Organization-Specific Policy Alignment in LLMs",
      "abstract": "As large language models are deployed in high-stakes enterprise applications, from healthcare to finance, ensuring adherence to organization-specific policies has become essential. Yet existing safety evaluations focus exclusively on universal harms. We present COMPASS (Company/Organization Policy Alignment Assessment), the first systematic framework for evaluating whether LLMs comply with organizational allowlist and denylist policies. We apply COMPASS to eight diverse industry scenarios, generating and validating 5,920 queries that test both routine compliance and adversarial robustness through strategically designed edge cases. Evaluating seven state-of-the-art models, we uncover a fundamental asymmetry: models reliably handle legitimate requests (>95% accuracy) but catastrophically fail at enforcing prohibitions, refusing only 13-40% of adversarial denylist violations. These results demonstrate that current LLMs lack the robustness required for policy-critical deployments, establishing COMPASS as an essential evaluation framework for organizational AI safety.",
      "authors": [
        "Dasol Choi",
        "DongGeon Lee",
        "Brigitta Jesica Kartono",
        "Helena Berndt",
        "Taeyoun Kwon",
        "Joonwon Jang",
        "Haon Park",
        "Hwanjo Yu",
        "Minsuk Kahng"
      ],
      "published": "2026-01-05T06:57:45Z",
      "categories": [
        "cs.AI",
        "cs.CY"
      ],
      "primary_category": "cs.AI",
      "abs_url": "https://arxiv.org/abs/2601.01836v1",
      "pdf_url": "https://arxiv.org/pdf/2601.01836v1.pdf"
    },
    {
      "id": "2601.01835v1",
      "title": "RSwinV2-MD: An Enhanced Residual SwinV2 Transformer for Monkeypox Detection from Skin Images",
      "abstract": "In this paper, a deep learning approach for Mpox diagnosis named Customized Residual SwinTransformerV2 (RSwinV2) has been proposed, trying to enhance the capability of lesion classification by employing the RSwinV2 tool-assisted vision approach. In the RSwinV2 method, a hierarchical structure of the transformer has been customized based on the input dimensionality, embedding structure, and output targeted by the method. In this RSwinV2 approach, the input image has been split into non-overlapping patches and processed using shifted windows and attention in these patches. This process has helped the method link all the windows efficiently by avoiding the locality issues of non-overlapping regions in attention, while being computationally efficient. RSwinV2 has further developed based on SwinTransformer and has included patch and position embeddings to take advantage of the transformer global-linking capability by employing multi-head attention in these embeddings. Furthermore, RSwinV2 has developed and incorporated the Inverse Residual Block (IRB) into this method, which utilizes convolutional skip connections with these inclusive designs to address the vanishing gradient issues during processing. RSwinV2 inclusion of IRB has therefore facilitated this method to link global patterns as well as local patterns; hence, its integrity has helped improve lesion classification capability by minimizing variability of Mpox and increasing differences of Mpox, chickenpox, measles, and cowpox. In testing SwinV2, its accuracy of 96.21 and an F1score of 95.62 have been achieved on the Kaggle public dataset, which has outperformed standard CNN models and SwinTransformers; RSwinV2 vector has thus proved its valiance as a computer-assisted tool for Mpox lesion observation interpretation.",
      "authors": [
        "Rashid Iqbal",
        "Saddam Hussain Khan"
      ],
      "published": "2026-01-05T06:57:26Z",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "abs_url": "https://arxiv.org/abs/2601.01835v1",
      "pdf_url": "https://arxiv.org/pdf/2601.01835v1.pdf"
    },
    {
      "id": "2601.01833v1",
      "title": "FAROS: Robust Federated Learning with Adaptive Scaling against Backdoor Attacks",
      "abstract": "Federated Learning (FL) enables multiple clients to collaboratively train a shared model without exposing local data. However, backdoor attacks pose a significant threat to FL. These attacks aim to implant a stealthy trigger into the global model, causing it to mislead on inputs that possess a specific trigger while functioning normally on benign data. Although pre-aggregation detection is a main defense direction, existing state-of-the-art defenses often rely on fixed defense parameters. This reliance makes them vulnerable to single-point-of-failure risks, rendering them less effective against sophisticated attackers. To address these limitations, we propose FAROS, an enhanced FL framework that incorporates Adaptive Differential Scaling (ADS) and Robust Core-set Computing (RCC). The ADS mechanism adjusts the defense's sensitivity dynamically, based on the dispersion of uploaded gradients by clients in each round. This allows it to counter attackers who strategically shift between stealthiness and effectiveness. Furthermore, the RCC effectively mitigates the risk of single-point failure by computing the centroid of a core set comprising clients with the highest confidence. We conducted extensive experiments across various datasets, models, and attack scenarios. The results demonstrate that our method outperforms current defenses in both attack success rate and main task accuracy.",
      "authors": [
        "Chenyu Hu",
        "Qiming Hu",
        "Sinan Chen",
        "Nianyu Li",
        "Mingyue Zhang",
        "Jialong Li"
      ],
      "published": "2026-01-05T06:55:35Z",
      "categories": [
        "cs.LG"
      ],
      "primary_category": "cs.LG",
      "abs_url": "https://arxiv.org/abs/2601.01833v1",
      "pdf_url": "https://arxiv.org/pdf/2601.01833v1.pdf"
    },
    {
      "id": "2601.01832v1",
      "title": "Yukthi Opus: A Multi-Chain Hybrid Metaheuristic for Large-Scale NP-Hard Optimization",
      "abstract": "We present Yukthi Opus (YO), a multi-chain hybrid metaheuristic designed for NP-hard optimization under explicit evaluation budget constraints. YO integrates three complementary mechanisms in a structured two-phase architecture: Markov Chain Monte Carlo (MCMC) for global exploration, greedy local search for exploitation, and simulated annealing with adaptive reheating to enable controlled escape from local minima. A dedicated burn-in phase allocates evaluations to probabilistic exploration, after which a hybrid optimization loop refines promising candidates. YO further incorporates a spatial blacklist mechanism to avoid repeated evaluation of poor regions and a multi-chain execution strategy to improve robustness and reduce sensitivity to initialization.\n  We evaluate YO on three benchmarks: the Rastrigin function (5D) with ablation studies, the Traveling Salesman Problem with 50 to 200 cities, and the Rosenbrock function (5D) with comparisons against established optimizers including CMA-ES, Bayesian optimization, and accelerated particle swarm optimization. Results show that MCMC exploration and greedy refinement are critical for solution quality, while simulated annealing and multi-chain execution primarily improve stability and variance reduction. Overall, YO achieves competitive performance on large and multimodal problems while maintaining predictable evaluation budgets, making it suitable for expensive black-box optimization settings.",
      "authors": [
        "SB Danush Vikraman",
        "Hannah Abagail",
        "Prasanna Kesavraj",
        "Gajanan V Honnavar"
      ],
      "published": "2026-01-05T06:51:08Z",
      "categories": [
        "cs.NE",
        "cs.AI"
      ],
      "primary_category": "cs.NE",
      "abs_url": "https://arxiv.org/abs/2601.01832v1",
      "pdf_url": "https://arxiv.org/pdf/2601.01832v1.pdf"
    },
    {
      "id": "2601.01831v1",
      "title": "ARIES: A Scalable Multi-Agent Orchestration Framework for Real-Time Epidemiological Surveillance and Outbreak Monitoring",
      "abstract": "Global health surveillance is currently facing a challenge of Knowledge Gaps. While general-purpose AI has proliferated, it remains fundamentally unsuited for the high-stakes epidemiological domain due to chronic hallucinations and an inability to navigate specialized data silos. This paper introduces ARIES (Agentic Retrieval Intelligence for Epidemiological Surveillance), a specialized, autonomous multi-agent framework designed to move beyond static, disease-specific dashboards toward a dynamic intelligence ecosystem. Built on a hierarchical command structure, ARIES utilizes GPTs to orchestrate a scalable swarm of sub-agents capable of autonomously querying World Health Organization (WHO), Center for Disease Control and Prevention (CDC), and peer-reviewed research papers. By automating the extraction and logical synthesis of surveillance data, ARIES provides a specialized reasoning that identifies emergent threats and signal divergence in near real-time. This modular architecture proves that a task-specific agentic swarm can outperform generic models, offering a robust, extensible for next-generation outbreak response and global health intelligence.",
      "authors": [
        "Aniket Wattamwar",
        "Sampson Akwafuo"
      ],
      "published": "2026-01-05T06:50:40Z",
      "categories": [
        "cs.MA",
        "cs.AI",
        "cs.IR",
        "cs.SE"
      ],
      "primary_category": "cs.MA",
      "abs_url": "https://arxiv.org/abs/2601.01831v1",
      "pdf_url": "https://arxiv.org/pdf/2601.01831v1.pdf"
    },
    {
      "id": "2601.01829v1",
      "title": "RealPDEBench: A Benchmark for Complex Physical Systems with Real-World Data",
      "abstract": "Predicting the evolution of complex physical systems remains a central problem in science and engineering. Despite rapid progress in scientific Machine Learning (ML) models, a critical bottleneck is the lack of expensive real-world data, resulting in most current models being trained and validated on simulated data. Beyond limiting the development and evaluation of scientific ML, this gap also hinders research into essential tasks such as sim-to-real transfer. We introduce RealPDEBench, the first benchmark for scientific ML that integrates real-world measurements with paired numerical simulations. RealPDEBench consists of five datasets, three tasks, eight metrics, and ten baselines. We first present five real-world measured datasets with paired simulated datasets across different complex physical systems. We further define three tasks, which allow comparisons between real-world and simulated data, and facilitate the development of methods to bridge the two. Moreover, we design eight evaluation metrics, spanning data-oriented and physics-oriented metrics, and finally benchmark ten representative baselines, including state-of-the-art models, pretrained PDE foundation models, and a traditional method. Experiments reveal significant discrepancies between simulated and real-world data, while showing that pretraining with simulated data consistently improves both accuracy and convergence. In this work, we hope to provide insights from real-world data, advancing scientific ML toward bridging the sim-to-real gap and real-world deployment. Our benchmark, datasets, and instructions are available at https://realpdebench.github.io/.",
      "authors": [
        "Peiyan Hu",
        "Haodong Feng",
        "Hongyuan Liu",
        "Tongtong Yan",
        "Wenhao Deng",
        "Tianrun Gao",
        "Rong Zheng",
        "Haoren Zheng",
        "Chenglei Yu",
        "Chuanrui Wang",
        "Kaiwen Li",
        "Zhi-Ming Ma",
        "Dezhi Zhou",
        "Xingcai Lu",
        "Dixia Fan",
        "Tailin Wu"
      ],
      "published": "2026-01-05T06:49:13Z",
      "categories": [
        "cs.LG"
      ],
      "primary_category": "cs.LG",
      "abs_url": "https://arxiv.org/abs/2601.01829v1",
      "pdf_url": "https://arxiv.org/pdf/2601.01829v1.pdf"
    },
    {
      "id": "2601.01828v1",
      "title": "Emergent Introspective Awareness in Large Language Models",
      "abstract": "We investigate whether large language models can introspect on their internal states. It is difficult to answer this question through conversation alone, as genuine introspection cannot be distinguished from confabulations. Here, we address this challenge by injecting representations of known concepts into a model's activations, and measuring the influence of these manipulations on the model's self-reported states. We find that models can, in certain scenarios, notice the presence of injected concepts and accurately identify them. Models demonstrate some ability to recall prior internal representations and distinguish them from raw text inputs. Strikingly, we find that some models can use their ability to recall prior intentions in order to distinguish their own outputs from artificial prefills. In all these experiments, Claude Opus 4 and 4.1, the most capable models we tested, generally demonstrate the greatest introspective awareness; however, trends across models are complex and sensitive to post-training strategies. Finally, we explore whether models can explicitly control their internal representations, finding that models can modulate their activations when instructed or incentivized to \"think about\" a concept. Overall, our results indicate that current language models possess some functional introspective awareness of their own internal states. We stress that in today's models, this capacity is highly unreliable and context-dependent; however, it may continue to develop with further improvements to model capabilities.",
      "authors": [
        "Jack Lindsey"
      ],
      "published": "2026-01-05T06:47:41Z",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "abs_url": "https://arxiv.org/abs/2601.01828v1",
      "pdf_url": "https://arxiv.org/pdf/2601.01828v1.pdf"
    },
    {
      "id": "2601.01827v1",
      "title": "Aspect Extraction from E-Commerce Product and Service Reviews",
      "abstract": "Aspect Extraction (AE) is a key task in Aspect-Based Sentiment Analysis (ABSA), yet it remains difficult to apply in low-resource and code-switched contexts like Taglish, a mix of Tagalog and English commonly used in Filipino e-commerce reviews. This paper introduces a comprehensive AE pipeline designed for Taglish, combining rule-based, large language model (LLM)-based, and fine-tuning techniques to address both aspect identification and extraction. A Hierarchical Aspect Framework (HAF) is developed through multi-method topic modeling, along with a dual-mode tagging scheme for explicit and implicit aspects. For aspect identification, four distinct models are evaluated: a Rule-Based system, a Generative LLM (Gemini 2.0 Flash), and two Fine-Tuned Gemma-3 1B models trained on different datasets (Rule-Based vs. LLM-Annotated). Results indicate that the Generative LLM achieved the highest performance across all tasks (Macro F1 0.91), demonstrating superior capability in handling implicit aspects. In contrast, the fine-tuned models exhibited limited performance due to dataset imbalance and architectural capacity constraints. This work contributes a scalable and linguistically adaptive framework for enhancing ABSA in diverse, code-switched environments.",
      "authors": [
        "Valiant Lance D. Dionela",
        "Fatima Kriselle S. Dy",
        "Robin James M. Hombrebueno",
        "Aaron Rae M. Nicolas",
        "Charibeth K. Cheng",
        "Raphael W. Gonda"
      ],
      "published": "2026-01-05T06:45:51Z",
      "categories": [
        "cs.CL",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "abs_url": "https://arxiv.org/abs/2601.01827v1",
      "pdf_url": "https://arxiv.org/pdf/2601.01827v1.pdf"
    },
    {
      "id": "2601.01825v1",
      "title": "CSCBench: A PVC Diagnostic Benchmark for Commodity Supply Chain Reasoning",
      "abstract": "Large Language Models (LLMs) have achieved remarkable success in general benchmarks, yet their competence in commodity supply chains (CSCs) -- a domain governed by institutional rule systems and feasibility constraints -- remains under-explored. CSC decisions are shaped jointly by process stages (e.g., planning, procurement, delivery), variety-specific rules (e.g., contract specifications and delivery grades), and reasoning depth (from retrieval to multi-step analysis and decision selection). We introduce CSCBench, a 2.3K+ single-choice benchmark for CSC reasoning, instantiated through our PVC 3D Evaluation Framework (Process, Variety, and Cognition). The Process axis aligns tasks with SCOR+Enable; the Variety axis operationalizes commodity-specific rule systems under coupled material-information-financial constraints, grounded in authoritative exchange guidebooks/rulebooks and industry reports; and the Cognition axis follows Bloom's revised taxonomy. Evaluating representative LLMs under a direct prompting setting, we observe strong performance on the Process and Cognition axes but substantial degradation on the Variety axis, especially on Freight Agreements. CSCBench provides a diagnostic yardstick for measuring and improving LLM capabilities in this high-stakes domain.",
      "authors": [
        "Yaxin Cui",
        "Yuanqiang Zeng",
        "Jiapeng Yan",
        "Keling Lin",
        "Kai Ji",
        "Jianhui Zeng",
        "Sheng Zhang",
        "Xin Luo",
        "Binzhu Su",
        "Chaolai Shen",
        "Jiahao Yu"
      ],
      "published": "2026-01-05T06:44:29Z",
      "categories": [
        "cs.CL"
      ],
      "primary_category": "cs.CL",
      "abs_url": "https://arxiv.org/abs/2601.01825v1",
      "pdf_url": "https://arxiv.org/pdf/2601.01825v1.pdf"
    },
    {
      "id": "2601.01822v1",
      "title": "DisCo-FLoc: Using Dual-Level Visual-Geometric Contrasts to Disambiguate Depth-Aware Visual Floorplan Localization",
      "abstract": "Since floorplan data is readily available, long-term persistent, and robust to changes in visual appearance, visual Floorplan Localization (FLoc) has garnered significant attention. Existing methods either ingeniously match geometric priors or utilize sparse semantics to reduce FLoc uncertainty. However, they still suffer from ambiguous FLoc caused by repetitive structures within minimalist floorplans. Moreover, expensive but limited semantic annotations restrict their applicability. To address these issues, we propose DisCo-FLoc, which utilizes dual-level visual-geometric Contrasts to Disambiguate depth-aware visual Floc, without requiring additional semantic labels. Our solution begins with a ray regression predictor tailored for ray-casting-based FLoc, predicting a series of FLoc candidates using depth estimation expertise. In addition, a novel contrastive learning method with position-level and orientation-level constraints is proposed to strictly match depth-aware visual features with the corresponding geometric structures in the floorplan. Such matches can effectively eliminate FLoc ambiguity and select the optimal imaging pose from FLoc candidates. Exhaustive comparative studies on two standard visual Floc benchmarks demonstrate that our method outperforms the state-of-the-art semantic-based method, achieving significant improvements in both robustness and accuracy.",
      "authors": [
        "Shiyong Meng",
        "Tao Zou",
        "Bolei Chen",
        "Chaoxu Mu",
        "Jianxin Wang"
      ],
      "published": "2026-01-05T06:31:24Z",
      "categories": [
        "cs.RO",
        "cs.CV"
      ],
      "primary_category": "cs.RO",
      "abs_url": "https://arxiv.org/abs/2601.01822v1",
      "pdf_url": "https://arxiv.org/pdf/2601.01822v1.pdf"
    },
    {
      "id": "2601.01818v1",
      "title": "Robust Egocentric Visual Attention Prediction Through Language-guided Scene Context-aware Learning",
      "abstract": "As the demand for analyzing egocentric videos grows, egocentric visual attention prediction, anticipating where a camera wearer will attend, has garnered increasing attention. However, it remains challenging due to the inherent complexity and ambiguity of dynamic egocentric scenes. Motivated by evidence that scene contextual information plays a crucial role in modulating human attention, in this paper, we present a language-guided scene context-aware learning framework for robust egocentric visual attention prediction. We first design a context perceiver which is guided to summarize the egocentric video based on a language-based scene description, generating context-aware video representations. We then introduce two training objectives that: 1) encourage the framework to focus on the target point-of-interest regions and 2) suppress distractions from irrelevant regions which are less likely to attract first-person attention. Extensive experiments on Ego4D and Aria Everyday Activities (AEA) datasets demonstrate the effectiveness of our approach, achieving state-of-the-art performance and enhanced robustness across diverse, dynamic egocentric scenarios.",
      "authors": [
        "Sungjune Park",
        "Hongda Mao",
        "Qingshuang Chen",
        "Yong Man Ro",
        "Yelin Kim"
      ],
      "published": "2026-01-05T06:14:41Z",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "abs_url": "https://arxiv.org/abs/2601.01818v1",
      "pdf_url": "https://arxiv.org/pdf/2601.01818v1.pdf"
    },
    {
      "id": "2601.01816v1",
      "title": "Admissibility Alignment",
      "abstract": "This paper introduces Admissibility Alignment: a reframing of AI alignment as a property of admissible action and decision selection over distributions of outcomes under uncertainty, evaluated through the behavior of candidate policies. We present MAP-AI (Monte Carlo Alignment for Policy) as a canonical system architecture for operationalizing admissibility alignment, formalizing alignment as a probabilistic, decision-theoretic property rather than a static or binary condition.\n  MAP-AI, a new control-plane system architecture for aligned decision-making under uncertainty, enforces alignment through Monte Carlo estimation of outcome distributions and admissibility-controlled policy selection rather than static model-level constraints. The framework evaluates decision policies across ensembles of plausible futures, explicitly modeling uncertainty, intervention effects, value ambiguity, and governance constraints. Alignment is assessed through distributional properties including expected utility, variance, tail risk, and probability of misalignment rather than accuracy or ranking performance. This approach distinguishes probabilistic prediction from decision reasoning under uncertainty and provides an executable methodology for evaluating trust and alignment in enterprise and institutional AI systems. The result is a practical foundation for governing AI systems whose impact is determined not by individual forecasts, but by policy behavior across distributions and tail events. Finally, we show how distributional alignment evaluation can be integrated into decision-making itself, yielding an admissibility-controlled action selection mechanism that alters policy behavior under uncertainty without retraining or modifying underlying models.",
      "authors": [
        "Chris Duffey"
      ],
      "published": "2026-01-05T05:58:19Z",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "abs_url": "https://arxiv.org/abs/2601.01816v1",
      "pdf_url": "https://arxiv.org/pdf/2601.01816v1.pdf"
    },
    {
      "id": "2601.01807v1",
      "title": "Adaptive Hybrid Optimizer based Framework for Lumpy Skin Disease Identification",
      "abstract": "Lumpy Skin Disease (LSD) is a contagious viral infection that significantly deteriorates livestock health, thereby posing a serious threat to the global economy and food security. Owing to its rapid spread characteristics, early and precise identification is crucial to prevent outbreaks and ensure timely intervention. In this paper, we propose a hybrid deep learning-based approach called LUMPNet for the early detection of LSD. LUMPNet utilizes image data to detect and classify skin nodules -- the primary indicator of LSD. To this end, LUMPNet uses YOLOv11, EfficientNet-based CNN classifier with compound scaling, and a novel adaptive hybrid optimizer. More precisely, LUMPNet detects and localizes LSD skin nodules and lesions on cattle images. It exploits EfficientNet to classify the localized cattle images into LSD-affected or healthy categories. To stabilize and accelerate the training of YOLOv11 and EfficientNet hybrid model, a novel adaptive hybrid optimizer is proposed and utilized. We evaluate LUMPNet at various stages of LSD using a publicly available dataset. Results indicate that the proposed scheme achieves 99% LSD detection training accuracy, and outperforms existing schemes. The model also achieves validation accuracy of 98%. Moreover, for further evaluation, we conduct a case study using an optimized EfficientNet-B0 model trained with the AdamW optimizer, and compare its performance with LUMPNet. The results show that LUMPNet achieves superior performance.",
      "authors": [
        " Ubaidullah",
        "Muhammad Abid Hussain",
        "Mohsin Raza Jafri",
        "Rozi Khan",
        "Moid Sandhu",
        "Abd Ullah Khan",
        "Hyundong Shin"
      ],
      "published": "2026-01-05T05:35:45Z",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "abs_url": "https://arxiv.org/abs/2601.01807v1",
      "pdf_url": "https://arxiv.org/pdf/2601.01807v1.pdf"
    },
    {
      "id": "2601.01804v1",
      "title": "Causality-Aware Temporal Projection for Video Understanding in Video-LLMs",
      "abstract": "Recent Video Large Language Models (Video-LLMs) have shown strong multimodal reasoning capabilities, yet remain challenged by video understanding tasks that require consistent temporal ordering and causal coherence. Many parameter-efficient Video-LLMs rely on unconstrained bidirectional projectors to model inter-frame interactions, which can blur temporal ordering by allowing later frames to influence earlier representations, without explicit architectural mechanisms to respect the directional nature of video reasoning. To address this limitation, we propose V-CORE, a parameter-efficient framework that introduces explicit temporal ordering constraints for video understanding. V-CORE consists of two key components: (1) Learnable Spatial Aggregation (LSA), which adaptively selects salient spatial tokens to reduce redundancy, and (2) a Causality-Aware Temporal Projector (CATP), which enforces structured unidirectional information flow via block-causal attention and a terminal dynamic summary token acting as a causal sink. This design preserves intra-frame spatial interactions while ensuring that temporal information is aggregated in a strictly ordered manner. With 4-bit QLoRA and a frozen LLM backbone, V-CORE can be trained efficiently on a single consumer GPU. Experiments show that V-CORE achieves strong performance on the challenging NExT-QA benchmark, reaching 61.2% accuracy, and remains competitive across MSVD-QA, MSRVTT-QA, and TGIF-QA, with gains concentrated in temporal and causal reasoning subcategories (+3.5% and +5.2% respectively), directly validating the importance of explicit temporal ordering constraints.",
      "authors": [
        "Zhengjian Kang",
        "Qi Chen",
        "Rui Liu",
        "Kangtong Mo",
        "Xingyu Zhang",
        "Xiaoyu Deng",
        "Ye Zhang"
      ],
      "published": "2026-01-05T05:30:13Z",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "abs_url": "https://arxiv.org/abs/2601.01804v1",
      "pdf_url": "https://arxiv.org/pdf/2601.01804v1.pdf"
    },
    {
      "id": "2601.01803v1",
      "title": "Moments Matter:Stabilizing Policy Optimization using Return Distributions",
      "abstract": "Deep Reinforcement Learning (RL) agents often learn policies that achieve the same episodic return yet behave very differently, due to a combination of environmental (random transitions, initial conditions, reward noise) and algorithmic (minibatch selection, exploration noise) factors. In continuous control tasks, even small parameter shifts can produce unstable gaits, complicating both algorithm comparison and real-world transfer. Previous work has shown that such instability arises when policy updates traverse noisy neighborhoods and that the spread of post-update return distribution $R(θ)$, obtained by repeatedly sampling minibatches, updating $θ$, and measuring final returns, is a useful indicator of this noise. Although explicitly constraining the policy to maintain a narrow $R(θ)$ can improve stability, directly estimating $R(θ)$ is computationally expensive in high-dimensional settings. We propose an alternative that takes advantage of environmental stochasticity to mitigate update-induced variability. Specifically, we model state-action return distribution through a distributional critic and then bias the advantage function of PPO using higher-order moments (skewness and kurtosis) of this distribution. By penalizing extreme tail behaviors, our method discourages policies from entering parameter regimes prone to instability. We hypothesize that in environments where post-update critic values align poorly with post-update returns, standard PPO struggles to produce a narrow $R(θ)$. In such cases, our moment-based correction narrows $R(θ)$, improving stability by up to 75% in Walker2D, while preserving comparable evaluation returns.",
      "authors": [
        "Dennis Jabs",
        "Aditya Mohan",
        "Marius Lindauer"
      ],
      "published": "2026-01-05T05:27:11Z",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "abs_url": "https://arxiv.org/abs/2601.01803v1",
      "pdf_url": "https://arxiv.org/pdf/2601.01803v1.pdf"
    },
    {
      "id": "2601.01802v1",
      "title": "PsychEval: A Multi-Session and Multi-Therapy Benchmark for High-Realism and Comprehensive AI Psychological Counselor",
      "abstract": "To develop a reliable AI for psychological assessment, we introduce \\texttt{PsychEval}, a multi-session, multi-therapy, and highly realistic benchmark designed to address three key challenges: \\textbf{1) Can we train a highly realistic AI counselor?} Realistic counseling is a longitudinal task requiring sustained memory and dynamic goal tracking. We propose a multi-session benchmark (spanning 6-10 sessions across three distinct stages) that demands critical capabilities such as memory continuity, adaptive reasoning, and longitudinal planning. The dataset is annotated with extensive professional skills, comprising over 677 meta-skills and 4577 atomic skills. \\textbf{2) How to train a multi-therapy AI counselor?} While existing models often focus on a single therapy, complex cases frequently require flexible strategies among various therapies. We construct a diverse dataset covering five therapeutic modalities (Psychodynamic, Behaviorism, CBT, Humanistic Existentialist, and Postmodernist) alongside an integrative therapy with a unified three-stage clinical framework across six core psychological topics. \\textbf{3) How to systematically evaluate an AI counselor?} We establish a holistic evaluation framework with 18 therapy-specific and therapy-shared metrics across Client-Level and Counselor-Level dimensions. To support this, we also construct over 2,000 diverse client profiles. Extensive experimental analysis fully validates the superior quality and clinical fidelity of our dataset. Crucially, \\texttt{PsychEval} transcends static benchmarking to serve as a high-fidelity reinforcement learning environment that enables the self-evolutionary training of clinically responsible and adaptive AI counselors.",
      "authors": [
        "Qianjun Pan",
        "Junyi Wang",
        "Jie Zhou",
        "Yutao Yang",
        "Junsong Li",
        "Kaiyin Xu",
        "Yougen Zhou",
        "Yihan Li",
        "Jingyuan Zhao",
        "Qin Chen",
        "Ningning Zhou",
        "Kai Chen",
        "Liang He"
      ],
      "published": "2026-01-05T05:26:57Z",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "abs_url": "https://arxiv.org/abs/2601.01802v1",
      "pdf_url": "https://arxiv.org/pdf/2601.01802v1.pdf"
    },
    {
      "id": "2601.01800v1",
      "title": "Sparse Threats, Focused Defense: Criticality-Aware Robust Reinforcement Learning for Safe Autonomous Driving",
      "abstract": "Reinforcement learning (RL) has shown considerable potential in autonomous driving (AD), yet its vulnerability to perturbations remains a critical barrier to real-world deployment. As a primary countermeasure, adversarial training improves policy robustness by training the AD agent in the presence of an adversary that deliberately introduces perturbations. Existing approaches typically model the interaction as a zero-sum game with continuous attacks. However, such designs overlook the inherent asymmetry between the agent and the adversary and then fail to reflect the sparsity of safety-critical risks, rendering the achieved robustness inadequate for practical AD scenarios. To address these limitations, we introduce criticality-aware robust RL (CARRL), a novel adversarial training approach for handling sparse, safety-critical risks in autonomous driving. CARRL consists of two interacting components: a risk exposure adversary (REA) and a risk-targeted robust agent (RTRA). We model the interaction between the REA and RTRA as a general-sum game, allowing the REA to focus on exposing safety-critical failures (e.g., collisions) while the RTRA learns to balance safety with driving efficiency. The REA employs a decoupled optimization mechanism to better identify and exploit sparse safety-critical moments under a constrained budget. However, such focused attacks inevitably result in a scarcity of adversarial data. The RTRA copes with this scarcity by jointly leveraging benign and adversarial experiences via a dual replay buffer and enforces policy consistency under perturbations to stabilize behavior. Experimental results demonstrate that our approach reduces the collision rate by at least 22.66\\% across all cases compared to state-of-the-art baseline methods.",
      "authors": [
        "Qi Wei",
        "Junchao Fan",
        "Zhao Yang",
        "Jianhua Wang",
        "Jingkai Mao",
        "Xiaolin Chang"
      ],
      "published": "2026-01-05T05:20:16Z",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "abs_url": "https://arxiv.org/abs/2601.01800v1",
      "pdf_url": "https://arxiv.org/pdf/2601.01800v1.pdf"
    },
    {
      "id": "2601.01798v1",
      "title": "VerLM: Explaining Face Verification Using Natural Language",
      "abstract": "Face verification systems have seen substantial advancements; however, they often lack transparency in their decision-making processes. In this paper, we introduce an innovative Vision-Language Model (VLM) for Face Verification, which not only accurately determines if two face images depict the same individual but also explicitly explains the rationale behind its decisions. Our model is uniquely trained using two complementary explanation styles: (1) concise explanations that summarize the key factors influencing its decision, and (2) comprehensive explanations detailing the specific differences observed between the images. We adapt and enhance a state-of-the-art modeling approach originally designed for audio-based differentiation to suit visual inputs effectively. This cross-modal transfer significantly improves our model's accuracy and interpretability. The proposed VLM integrates sophisticated feature extraction techniques with advanced reasoning capabilities, enabling clear articulation of its verification process. Our approach demonstrates superior performance, surpassing baseline methods and existing models. These findings highlight the immense potential of vision language models in face verification set up, contributing to more transparent, reliable, and explainable face verification systems.",
      "authors": [
        "Syed Abdul Hannan",
        "Hazim Bukhari",
        "Thomas Cantalapiedra",
        "Eman Ansar",
        "Massa Baali",
        "Rita Singh",
        "Bhiksha Raj"
      ],
      "published": "2026-01-05T05:16:07Z",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "abs_url": "https://arxiv.org/abs/2601.01798v1",
      "pdf_url": "https://arxiv.org/pdf/2601.01798v1.pdf"
    },
    {
      "id": "2601.01793v1",
      "title": "Distributed Federated Learning by Alternating Periods of Training",
      "abstract": "Federated learning is a privacy-focused approach towards machine learning where models are trained on client devices with locally available data and aggregated at a central server. However, the dependence on a single central server is challenging in the case of a large number of clients and even poses the risk of a single point of failure. To address these critical limitations of scalability and fault-tolerance, we present a distributed approach to federated learning comprising multiple servers with inter-server communication capabilities. While providing a fully decentralized approach, the designed framework retains the core federated learning structure where each server is associated with a disjoint set of clients with server-client communication capabilities. We propose a novel DFL (Distributed Federated Learning) algorithm which uses alternating periods of local training on the client data followed by global training among servers. We show that the DFL algorithm, under a suitable choice of parameters, ensures that all the servers converge to a common model value within a small tolerance of the ideal model, thus exhibiting effective integration of local and global training models. Finally, we illustrate our theoretical claims through numerical simulations.",
      "authors": [
        "Shamik Bhattacharyya",
        "Rachel Kalpana Kalaimani"
      ],
      "published": "2026-01-05T05:06:58Z",
      "categories": [
        "cs.LG",
        "eess.SY"
      ],
      "primary_category": "cs.LG",
      "abs_url": "https://arxiv.org/abs/2601.01793v1",
      "pdf_url": "https://arxiv.org/pdf/2601.01793v1.pdf"
    },
    {
      "id": "2601.01792v1",
      "title": "HyperCLOVA X 8B Omni",
      "abstract": "In this report, we present HyperCLOVA X 8B Omni, the first any-to-any omnimodal model in the HyperCLOVA X family that supports text, audio, and vision as both inputs and outputs. By consolidating multimodal understanding and generation into a single model rather than separate modality-specific pipelines, HyperCLOVA X 8B Omni serves as an 8B-scale omni-pathfinding point toward practical any-to-any omni assistants. At a high level, the model unifies modalities through a shared next-token prediction interface over an interleaved multimodal sequence, while vision and audio encoders inject continuous embeddings for fine-grained understanding and grounding. Empirical evaluations demonstrate competitive performance against comparably sized models across diverse input-output combinations spanning text, audio, and vision, in both Korean and English. We anticipate that the open-weight release of HyperCLOVA X 8B Omni will support a wide range of research and deployment scenarios.",
      "authors": [
        " NAVER Cloud HyperCLOVA X Team"
      ],
      "published": "2026-01-05T05:06:11Z",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL",
        "cs.SD"
      ],
      "primary_category": "cs.LG",
      "abs_url": "https://arxiv.org/abs/2601.01792v1",
      "pdf_url": "https://arxiv.org/pdf/2601.01792v1.pdf"
    },
    {
      "id": "2601.01786v1",
      "title": "UnPII: Unlearning Personally Identifiable Information with Quantifiable Exposure Risk",
      "abstract": "The ever-increasing adoption of Large Language Models in critical sectors like finance, healthcare, and government raises privacy concerns regarding the handling of sensitive Personally Identifiable Information (PII) during training. In response, regulations such as European Union's General Data Protection Regulation (GDPR) mandate the deletion of PII upon requests, underscoring the need for reliable and cost-effective data removal solutions. Machine unlearning has emerged as a promising direction for selectively forgetting data points. However, existing unlearning techniques typically apply a uniform forgetting strategy that neither accounts for the varying privacy risks posed by different PII attributes nor reflects associated business risks. In this work, we propose UnPII, the first PII-centric unlearning approach that prioritizes forgetting based on the risk of individual or combined PII attributes. To this end, we introduce the PII risk index (PRI), a composite metric that incorporates multiple dimensions of risk factors: identifiability, sensitivity, usability, linkability, permanency, exposability, and compliancy. The PRI enables a nuanced evaluation of privacy risks associated with PII exposures and can be tailored to align with organizational privacy policies. To support realistic assessment, we systematically construct a synthetic PII dataset (e.g., 1,700 PII instances) that simulates realistic exposure scenarios. UnPII seamlessly integrates with established unlearning algorithms, such as Gradient Ascent, Negative Preference Optimization, and Direct Preference Optimization, without modifying their underlying principles. Our experimental results demonstrate that UnPII achieves the improvements of accuracy up to 11.8%, utility up to 6.3%, and generalizability up to 12.4%, respectively, while incurring a modest fine-tuning overhead of 27.5% on average during unlearning.",
      "authors": [
        "Intae Jeon",
        "Yujeong Kwon",
        "Hyungjoon Koo"
      ],
      "published": "2026-01-05T04:45:04Z",
      "categories": [
        "cs.LG",
        "cs.CR"
      ],
      "primary_category": "cs.LG",
      "abs_url": "https://arxiv.org/abs/2601.01786v1",
      "pdf_url": "https://arxiv.org/pdf/2601.01786v1.pdf"
    },
    {
      "id": "2601.01785v1",
      "title": "SRAS: A Lightweight Reinforcement Learning-based Document Selector for Edge-Native RAG Pipelines",
      "abstract": "Retrieval-Augmented Generation (RAG) systems often rely on fixed top-k document selection mechanisms that ignore downstream generation quality and impose computational overheads. We propose SRAS (Sparse Reward-Aware Selector), a lightweight document selector trained via reinforcement learning (RL) for edge-native RAG deployment. Unlike prior RL-based retrievers that assume large memory and latency budgets, SRAS learns a compact (~0.76MB) policy using Proximal Policy Optimization (PPO), guided by a hybrid reward signal combining Relaxed F1 and BERTScore. Our method operates under tight token and compute constraints, maintaining <1s latency on CPU. SRAS outperforms supervised and random selectors on a synthetic QA benchmark, and generalizes to real-world data, achieving BERTScore F1 of 0.8546 on SQuAD v2 without domain-specific tuning. This work is the first to demonstrate that RL-based document selection can be made ultra-lightweight, latency-aware, and effective for on-device RAG pipelines.",
      "authors": [
        "Rajiv Chaitanya Muttur"
      ],
      "published": "2026-01-05T04:39:31Z",
      "categories": [
        "cs.IR",
        "cs.LG"
      ],
      "primary_category": "cs.IR",
      "abs_url": "https://arxiv.org/abs/2601.01785v1",
      "pdf_url": "https://arxiv.org/pdf/2601.01785v1.pdf"
    },
    {
      "id": "2601.01784v1",
      "title": "DDNet: A Dual-Stream Graph Learning and Disentanglement Framework for Temporal Forgery Localization",
      "abstract": "The rapid evolution of AIGC technology enables misleading viewers by tampering mere small segments within a video, rendering video-level detection inaccurate and unpersuasive. Consequently, temporal forgery localization (TFL), which aims to precisely pinpoint tampered segments, becomes critical. However, existing methods are often constrained by \\emph{local view}, failing to capture global anomalies. To address this, we propose a \\underline{d}ual-stream graph learning and \\underline{d}isentanglement framework for temporal forgery localization (DDNet). By coordinating a \\emph{Temporal Distance Stream} for local artifacts and a \\emph{Semantic Content Stream} for long-range connections, DDNet prevents global cues from being drowned out by local smoothness. Furthermore, we introduce Trace Disentanglement and Adaptation (TDA) to isolate generic forgery fingerprints, alongside Cross-Level Feature Embedding (CLFE) to construct a robust feature foundation via deep fusion of hierarchical features. Experiments on ForgeryNet and TVIL benchmarks demonstrate that our method outperforms state-of-the-art approaches by approximately 9\\% in AP@0.95, with significant improvements in cross-domain robustness.",
      "authors": [
        "Boyang Zhao",
        "Xin Liao",
        "Jiaxin Chen",
        "Xiaoshuai Wu",
        "Yufeng Wu"
      ],
      "published": "2026-01-05T04:35:39Z",
      "categories": [
        "cs.CV",
        "cs.MM",
        "eess.IV"
      ],
      "primary_category": "cs.CV",
      "abs_url": "https://arxiv.org/abs/2601.01784v1",
      "pdf_url": "https://arxiv.org/pdf/2601.01784v1.pdf"
    },
    {
      "id": "2601.01781v1",
      "title": "Subimage Overlap Prediction: Task-Aligned Self-Supervised Pretraining For Semantic Segmentation In Remote Sensing Imagery",
      "abstract": "Self-supervised learning (SSL) methods have become a dominant paradigm for creating general purpose models whose capabilities can be transferred to downstream supervised learning tasks. However, most such methods rely on vast amounts of pretraining data. This work introduces Subimage Overlap Prediction, a novel self-supervised pretraining task to aid semantic segmentation in remote sensing imagery that uses significantly lesser pretraining imagery. Given an image, a sub-image is extracted and the model is trained to produce a semantic mask of the location of the extracted sub-image within the original image. We demonstrate that pretraining with this task results in significantly faster convergence, and equal or better performance (measured via mIoU) on downstream segmentation. This gap in convergence and performance widens when labeled training data is reduced. We show this across multiple architecture types, and with multiple downstream datasets. We also show that our method matches or exceeds performance while requiring significantly lesser pretraining data relative to other SSL methods. Code and model weights are provided at \\href{https://github.com/sharmalakshay93/subimage-overlap-prediction}{github.com/sharmalakshay93/subimage-overlap-prediction}.",
      "authors": [
        "Lakshay Sharma",
        "Alex Marin"
      ],
      "published": "2026-01-05T04:28:49Z",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "abs_url": "https://arxiv.org/abs/2601.01781v1",
      "pdf_url": "https://arxiv.org/pdf/2601.01781v1.pdf"
    },
    {
      "id": "2601.01780v1",
      "title": "LIA: Supervised Fine-Tuning of Large Language Models for Automatic Issue Assignment",
      "abstract": "Issue assignment is a critical process in software maintenance, where new issue reports are validated and assigned to suitable developers. However, manual issue assignment is often inconsistent and error-prone, especially in large open-source projects where thousands of new issues are reported monthly. Existing automated approaches have shown promise, but many rely heavily on large volumes of project-specific training data or relational information that is often sparse and noisy, which limits their effectiveness. To address these challenges, we propose LIA (LLM-based Issue Assignment), which employs supervised fine-tuning to adapt an LLM, DeepSeek-R1-Distill-Llama-8B in this work, for automatic issue assignment. By leveraging the LLM's pretrained semantic understanding of natural language and software-related text, LIA learns to generate ranked developer recommendations directly from issue titles and descriptions. The ranking is based on the model's learned understanding of historical issue-to-developer assignments, using patterns from past tasks to infer which developers are most likely to handle new issues. Through comprehensive evaluation, we show that LIA delivers substantial improvements over both its base pretrained model and state-of-the-art baselines. It achieves up to +187.8% higher Hit@1 compared to the DeepSeek-R1-Distill-Llama-8B pretrained base model, and outperforms four leading issue assignment methods by as much as +211.2% in Hit@1 score. These results highlight the effectiveness of domain-adapted LLMs for software maintenance tasks and establish LIA as a practical, high-performing solution for issue assignment.",
      "authors": [
        "Arsham Khosravani",
        "Alireza Hosseinpour",
        "Arshia Akhavan",
        "Mehdi Keshani",
        "Abbas Heydarnoori"
      ],
      "published": "2026-01-05T04:26:46Z",
      "categories": [
        "cs.SE",
        "cs.AI"
      ],
      "primary_category": "cs.SE",
      "abs_url": "https://arxiv.org/abs/2601.01780v1",
      "pdf_url": "https://arxiv.org/pdf/2601.01780v1.pdf"
    },
    {
      "id": "2601.01779v1",
      "title": "Machine learning modularity",
      "abstract": "Based on a transformer based sequence-to-sequence architecture combined with a dynamic batching algorithm, this work introduces a machine learning framework for automatically simplifying complex expressions involving multiple elliptic Gamma functions, including the $q$-$θ$ function and the elliptic Gamma function. The model learns to apply algebraic identities, particularly the SL$(2,\\mathbb{Z})$ and SL$(3,\\mathbb{Z})$ modular transformations, to reduce heavily scrambled expressions to their canonical forms. Experimental results show that the model achieves over 99\\% accuracy on in-distribution tests and maintains robust performance (exceeding 90\\% accuracy) under significant extrapolation, such as with deeper scrambling depths. This demonstrates that the model has internalized the underlying algebraic rules of modular transformations rather than merely memorizing training patterns. Our work presents the first successful application of machine learning to perform symbolic simplification using modular identities, offering a new automated tool for computations with special functions in quantum field theory and the string theory.",
      "authors": [
        "Yi Fan",
        "Vishnu Jejjala",
        "Yang Lei"
      ],
      "published": "2026-01-05T04:17:55Z",
      "categories": [
        "hep-th",
        "cs.LG"
      ],
      "primary_category": "hep-th",
      "abs_url": "https://arxiv.org/abs/2601.01779v1",
      "pdf_url": "https://arxiv.org/pdf/2601.01779v1.pdf"
    },
    {
      "id": "2601.01778v1",
      "title": "BanglaIPA: Towards Robust Text-to-IPA Transcription with Contextual Rewriting in Bengali",
      "abstract": "Despite its widespread use, Bengali lacks a robust automated International Phonetic Alphabet (IPA) transcription system that effectively supports both standard language and regional dialectal texts. Existing approaches struggle to handle regional variations, numerical expressions, and generalize poorly to previously unseen words. To address these limitations, we propose BanglaIPA, a novel IPA generation system that integrates a character-based vocabulary with word-level alignment. The proposed system accurately handles Bengali numerals and demonstrates strong performance across regional dialects. BanglaIPA improves inference efficiency by leveraging a precomputed word-to-IPA mapping dictionary for previously observed words. The system is evaluated on the standard Bengali and six regional variations of the DUAL-IPA dataset. Experimental results show that BanglaIPA outperforms baseline IPA transcription models by 58.4-78.7% and achieves an overall mean word error rate of 11.4%, highlighting its robustness in phonetic transcription generation for the Bengali language.",
      "authors": [
        "Jakir Hasan",
        "Shrestha Datta",
        "Md Saiful Islam",
        "Shubhashis Roy Dipta",
        "Ameya Debnath"
      ],
      "published": "2026-01-05T04:17:31Z",
      "categories": [
        "cs.CL"
      ],
      "primary_category": "cs.CL",
      "abs_url": "https://arxiv.org/abs/2601.01778v1",
      "pdf_url": "https://arxiv.org/pdf/2601.01778v1.pdf"
    },
    {
      "id": "2601.01774v1",
      "title": "Can Large Language Models Solve Engineering Equations? A Systematic Comparison of Direct Prediction and Solver-Assisted Approaches",
      "abstract": "Transcendental equations requiring iterative numerical solution pervade engineering practice, from fluid mechanics friction factor calculations to orbital position determination. We systematically evaluate whether Large Language Models can solve these equations through direct numerical prediction or whether a hybrid architecture combining LLM symbolic manipulation with classical iterative solvers proves more effective. Testing six state-of-the-art models (GPT-5.1, GPT-5.2, Gemini-3-Flash, Gemini-2.5-Lite, Claude-Sonnet-4.5, Claude-Opus-4.5) on 100 problems spanning seven engineering domains, we compare direct prediction against solver-assisted computation where LLMs formulate governing equations and provide initial conditions while Newton-Raphson iteration performs numerical solution. Direct prediction yields mean relative errors of 0.765 to 1.262 across models, while solver-assisted computation achieves 0.225 to 0.301, representing error reductions of 67.9% to 81.8%. Domain-specific analysis reveals dramatic improvements in Electronics (93.1%) due to exponential equation sensitivity, contrasted with modest gains in Fluid Mechanics (7.2%) where LLMs exhibit effective pattern recognition. These findings establish that contemporary LLMs excel at symbolic manipulation and domain knowledge retrieval but struggle with precision-critical iterative arithmetic, suggesting their optimal deployment as intelligent interfaces to classical numerical solvers rather than standalone computational engines.",
      "authors": [
        "Sai Varun Kodathala",
        "Rakesh Vunnam"
      ],
      "published": "2026-01-05T04:04:55Z",
      "categories": [
        "cs.AI",
        "cs.CE",
        "math.NA"
      ],
      "primary_category": "cs.AI",
      "abs_url": "https://arxiv.org/abs/2601.01774v1",
      "pdf_url": "https://arxiv.org/pdf/2601.01774v1.pdf"
    },
    {
      "id": "2601.01769v1",
      "title": "CTIS-QA: Clinical Template-Informed Slide-level Question Answering for Pathology",
      "abstract": "In this paper, we introduce a clinical diagnosis template-based pipeline to systematically collect and structure pathological information. In collaboration with pathologists and guided by the the College of American Pathologists (CAP) Cancer Protocols, we design a Clinical Pathology Report Template (CPRT) that ensures comprehensive and standardized extraction of diagnostic elements from pathology reports. We validate the effectiveness of our pipeline on TCGA-BRCA. First, we extract pathological features from reports using CPRT. These features are then used to build CTIS-Align, a dataset of 80k slide-description pairs from 804 WSIs for vision-language alignment training, and CTIS-Bench, a rigorously curated VQA benchmark comprising 977 WSIs and 14,879 question-answer pairs. CTIS-Bench emphasizes clinically grounded, closed-ended questions (e.g., tumor grade, receptor status) that reflect real diagnostic workflows, minimize non-visual reasoning, and require genuine slide understanding. We further propose CTIS-QA, a Slide-level Question Answering model, featuring a dual-stream architecture that mimics pathologists' diagnostic approach. One stream captures global slide-level context via clustering-based feature aggregation, while the other focuses on salient local regions through attention-guided patch perception module. Extensive experiments on WSI-VQA, CTIS-Bench, and slide-level diagnostic tasks show that CTIS-QA consistently outperforms existing state-of-the-art models across multiple metrics. Code and data are available at https://github.com/HLSvois/CTIS-QA.",
      "authors": [
        "Hao Lu",
        "Ziniu Qian",
        "Yifu Li",
        "Yang Zhou",
        "Bingzheng Wei",
        "Yan Xu"
      ],
      "published": "2026-01-05T03:54:02Z",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "abs_url": "https://arxiv.org/abs/2601.01769v1",
      "pdf_url": "https://arxiv.org/pdf/2601.01769v1.pdf"
    },
    {
      "id": "2601.01768v1",
      "title": "Can LLMs Track Their Output Length? A Dynamic Feedback Mechanism for Precise Length Regulation",
      "abstract": "Precisely controlling the length of generated text is a common requirement in real-world applications. However, despite significant advancements in following human instructions, Large Language Models (LLMs) still struggle with this task. In this work, we demonstrate that LLMs often fail to accurately measure input text length, leading to poor adherence to length constraints. To address this issue, we propose a novel length regulation approach that incorporates dynamic length feedback during generation, enabling adaptive adjustments to meet target lengths. Experiments on summarization and biography tasks show our training-free approach significantly improves precision in achieving target token, word, or sentence counts without compromising quality. Additionally, we demonstrate that further supervised fine-tuning allows our method to generalize effectively to broader text-generation tasks.",
      "authors": [
        "Meiman Xiao",
        "Ante Wang",
        "Qingguo Hu",
        "Zhongjian Miao",
        "Huangjun Shen",
        "Longyue Wang",
        "Weihua Luo",
        "Jinsong Su"
      ],
      "published": "2026-01-05T03:49:14Z",
      "categories": [
        "cs.CL"
      ],
      "primary_category": "cs.CL",
      "abs_url": "https://arxiv.org/abs/2601.01768v1",
      "pdf_url": "https://arxiv.org/pdf/2601.01768v1.pdf"
    },
    {
      "id": "2601.01765v1",
      "title": "A New Benchmark for the Appropriate Evaluation of RTL Code Optimization",
      "abstract": "The rapid progress of artificial intelligence increasingly relies on efficient integrated circuit (IC) design. Recent studies have explored the use of large language models (LLMs) for generating Register Transfer Level (RTL) code, but existing benchmarks mainly evaluate syntactic correctness rather than optimization quality in terms of power, performance, and area (PPA). This work introduces RTL-OPT, a benchmark for assessing the capability of LLMs in RTL optimization. RTL-OPT contains 36 handcrafted digital designs that cover diverse implementation categories including combinational logic, pipelined datapaths, finite state machines, and memory interfaces. Each task provides a pair of RTL codes, a suboptimal version and a human-optimized reference that reflects industry-proven optimization patterns not captured by conventional synthesis tools. Furthermore, RTL-OPT integrates an automated evaluation framework to verify functional correctness and quantify PPA improvements, enabling standardized and meaningful assessment of generative models for hardware design optimization.",
      "authors": [
        "Yao Lu",
        "Shang Liu",
        "Hangan Zhou",
        "Wenji Fang",
        "Qijun Zhang",
        "Zhiyao Xie"
      ],
      "published": "2026-01-05T03:47:26Z",
      "categories": [
        "cs.AI",
        "cs.SE"
      ],
      "primary_category": "cs.AI",
      "abs_url": "https://arxiv.org/abs/2601.01765v1",
      "pdf_url": "https://arxiv.org/pdf/2601.01765v1.pdf"
    },
    {
      "id": "2601.01762v1",
      "title": "AlignDrive: Aligned Lateral-Longitudinal Planning for End-to-End Autonomous Driving",
      "abstract": "End-to-end autonomous driving has rapidly progressed, enabling joint perception and planning in complex environments. In the planning stage, state-of-the-art (SOTA) end-to-end autonomous driving models decouple planning into parallel lateral and longitudinal predictions. While effective, this parallel design can lead to i) coordination failures between the planned path and speed, and ii) underutilization of the drive path as a prior for longitudinal planning, thus redundantly encoding static information. To address this, we propose a novel cascaded framework that explicitly conditions longitudinal planning on the drive path, enabling coordinated and collision-aware lateral and longitudinal planning. Specifically, we introduce a path-conditioned formulation that explicitly incorporates the drive path into longitudinal planning. Building on this, the model predicts longitudinal displacements along the drive path rather than full 2D trajectory waypoints. This design simplifies longitudinal reasoning and more tightly couples it with lateral planning. Additionally, we introduce a planning-oriented data augmentation strategy that simulates rare safety-critical events, such as vehicle cut-ins, by adding agents and relabeling longitudinal targets to avoid collision. Evaluated on the challenging Bench2Drive benchmark, our method sets a new SOTA, achieving a driving score of 89.07 and a success rate of 73.18%, demonstrating significantly improved coordination and safety",
      "authors": [
        "Yanhao Wu",
        "Haoyang Zhang",
        "Fei He",
        "Rui Wu",
        "Congpei Qiu",
        "Liang Gao",
        "Wei Ke",
        "Tong Zhang"
      ],
      "published": "2026-01-05T03:41:20Z",
      "categories": [
        "cs.RO",
        "cs.CV"
      ],
      "primary_category": "cs.RO",
      "abs_url": "https://arxiv.org/abs/2601.01762v1",
      "pdf_url": "https://arxiv.org/pdf/2601.01762v1.pdf"
    },
    {
      "id": "2601.01757v1",
      "title": "Sparse Convex Biclustering",
      "abstract": "Biclustering is an essential unsupervised machine learning technique for simultaneously clustering rows and columns of a data matrix, with widespread applications in genomics, transcriptomics, and other high-dimensional omics data. Despite its importance, existing biclustering methods struggle to meet the demands of modern large-scale datasets. The challenges stem from the accumulation of noise in high-dimensional features, the limitations of non-convex optimization formulations, and the computational complexity of identifying meaningful biclusters. These issues often result in reduced accuracy and stability as the size of the dataset increases. To overcome these challenges, we propose Sparse Convex Biclustering (SpaCoBi), a novel method that penalizes noise during the biclustering process to improve both accuracy and robustness. By adopting a convex optimization framework and introducing a stability-based tuning criterion, SpaCoBi achieves an optimal balance between cluster fidelity and sparsity. Comprehensive numerical studies, including simulations and an application to mouse olfactory bulb data, demonstrate that SpaCoBi significantly outperforms state-of-the-art methods in accuracy. These results highlight SpaCoBi as a robust and efficient solution for biclustering in high-dimensional and large-scale datasets.",
      "authors": [
        "Jiakun Jiang",
        "Dewei Xiang",
        "Chenliang Gu",
        "Wei Liu",
        "Binhuan Wang"
      ],
      "published": "2026-01-05T03:15:52Z",
      "categories": [
        "stat.ML",
        "cs.LG"
      ],
      "primary_category": "stat.ML",
      "abs_url": "https://arxiv.org/abs/2601.01757v1",
      "pdf_url": "https://arxiv.org/pdf/2601.01757v1.pdf"
    },
    {
      "id": "2601.01754v1",
      "title": "Context-Free Recognition with Transformers",
      "abstract": "Transformers excel on tasks that process well-formed inputs according to some grammar, such as natural language and code. However, it remains unclear how they can process grammatical syntax. In fact, under standard complexity conjectures, standard transformers cannot recognize context-free languages (CFLs), a canonical formalism to describe syntax, or even regular languages, a subclass of CFLs (Merrill et al., 2022). Merrill & Sabharwal (2024) show that $\\mathcal{O}(\\log n)$ looping layers (w.r.t. input length $n$) allows transformers to recognize regular languages, but the question of context-free recognition remained open. In this work, we show that looped transformers with $\\mathcal{O}(\\log n)$ looping layers and $\\mathcal{O}(n^6)$ padding tokens can recognize all CFLs. However, training and inference with $\\mathcal{O}(n^6)$ padding tokens is potentially impractical. Fortunately, we show that, for natural subclasses such as unambiguous CFLs, the recognition problem on transformers becomes more tractable, requiring $\\mathcal{O}(n^3)$ padding. We empirically validate our results and show that looping helps on a language that provably requires logarithmic depth. Overall, our results shed light on the intricacy of CFL recognition by transformers: While general recognition may require an intractable amount of padding, natural constraints such as unambiguity yield efficient recognition algorithms.",
      "authors": [
        "Selim Jerad",
        "Anej Svete",
        "Sophie Hao",
        "Ryan Cotterell",
        "William Merrill"
      ],
      "published": "2026-01-05T03:14:23Z",
      "categories": [
        "cs.LG",
        "cs.CC",
        "cs.CL",
        "cs.FL"
      ],
      "primary_category": "cs.LG",
      "abs_url": "https://arxiv.org/abs/2601.01754v1",
      "pdf_url": "https://arxiv.org/pdf/2601.01754v1.pdf"
    },
    {
      "id": "2601.01753v1",
      "title": "MergeRec: Model Merging for Data-Isolated Cross-Domain Sequential Recommendation",
      "abstract": "Modern recommender systems trained on domain-specific data often struggle to generalize across multiple domains. Cross-domain sequential recommendation has emerged as a promising research direction to address this challenge; however, existing approaches face fundamental limitations, such as reliance on overlapping users or items across domains, or unrealistic assumptions that ignore privacy constraints. In this work, we propose a new framework, MergeRec, based on model merging under a new and realistic problem setting termed data-isolated cross-domain sequential recommendation, where raw user interaction data cannot be shared across domains. MergeRec consists of three key components: (1) merging initialization, (2) pseudo-user data construction, and (3) collaborative merging optimization. First, we initialize a merged model using training-free merging techniques. Next, we construct pseudo-user data by treating each item as a virtual sequence in each domain, enabling the synthesis of meaningful training samples without relying on real user interactions. Finally, we optimize domain-specific merging weights through a joint objective that combines a recommendation loss, which encourages the merged model to identify relevant items, and a distillation loss, which transfers collaborative filtering signals from the fine-tuned source models. Extensive experiments demonstrate that MergeRec not only preserves the strengths of the original models but also significantly enhances generalizability to unseen domains. Compared to conventional model merging methods, MergeRec consistently achieves superior performance, with average improvements of up to 17.21% in Recall@10, highlighting the potential of model merging as a scalable and effective approach for building universal recommender systems. The source code is available at https://github.com/DIALLab-SKKU/MergeRec.",
      "authors": [
        "Hyunsoo Kim",
        "Jaewan Moon",
        "Seongmin Park",
        "Jongwuk Lee"
      ],
      "published": "2026-01-05T03:14:23Z",
      "categories": [
        "cs.IR",
        "cs.AI"
      ],
      "primary_category": "cs.IR",
      "abs_url": "https://arxiv.org/abs/2601.01753v1",
      "pdf_url": "https://arxiv.org/pdf/2601.01753v1.pdf"
    },
    {
      "id": "2601.01751v1",
      "title": "Query-Document Dense Vectors for LLM Relevance Judgment Bias Analysis",
      "abstract": "Large Language Models (LLMs) have been used as relevance assessors for Information Retrieval (IR) evaluation collection creation due to reduced cost and increased scalability as compared to human assessors. While previous research has looked at the reliability of LLMs as compared to human assessors, in this work, we aim to understand if LLMs make systematic mistakes when judging relevance, rather than just understanding how good they are on average. To this aim, we propose a novel representational method for queries and documents that allows us to analyze relevance label distributions and compare LLM and human labels to identify patterns of disagreement and localize systematic areas of disagreement. We introduce a clustering-based framework that embeds query-document (Q-D) pairs into a joint semantic space, treating relevance as a relational property. Experiments on TREC Deep Learning 2019 and 2020 show that systematic disagreement between humans and LLMs is concentrated in specific semantic clusters rather than distributed randomly. Query-level analyses reveal recurring failures, most often in definition-seeking, policy-related, or ambiguous contexts. Queries with large variation in agreement across their clusters emerge as disagreement hotspots, where LLMs tend to under-recall relevant content or over-include irrelevant material. This framework links global diagnostics with localized clustering to uncover hidden weaknesses in LLM judgments, enabling bias-aware and more reliable IR evaluation.",
      "authors": [
        "Samaneh Mohtadi",
        "Gianluca Demartini"
      ],
      "published": "2026-01-05T03:02:33Z",
      "categories": [
        "cs.IR",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.IR",
      "abs_url": "https://arxiv.org/abs/2601.01751v1",
      "pdf_url": "https://arxiv.org/pdf/2601.01751v1.pdf"
    },
    {
      "id": "2601.01749v1",
      "title": "MANGO:Natural Multi-speaker 3D Talking Head Generation via 2D-Lifted Enhancement",
      "abstract": "Current audio-driven 3D head generation methods mainly focus on single-speaker scenarios, lacking natural, bidirectional listen-and-speak interaction. Achieving seamless conversational behavior, where speaking and listening states transition fluidly remains a key challenge. Existing 3D conversational avatar approaches rely on error-prone pseudo-3D labels that fail to capture fine-grained facial dynamics. To address these limitations, we introduce a novel two-stage framework MANGO, which leveraging pure image-level supervision by alternately training to mitigate the noise introduced by pseudo-3D labels, thereby achieving better alignment with real-world conversational behaviors. Specifically, in the first stage, a diffusion-based transformer with a dual-audio interaction module models natural 3D motion from multi-speaker audio. In the second stage, we use a fast 3D Gaussian Renderer to generate high-fidelity images and provide 2D-level photometric supervision for the 3D motions through alternate training. Additionally, we introduce MANGO-Dialog, a high-quality dataset with over 50 hours of aligned 2D-3D conversational data across 500+ identities. Extensive experiments demonstrate that our method achieves exceptional accuracy and realism in modeling two-person 3D dialogue motion, significantly advancing the fidelity and controllability of audio-driven talking heads.",
      "authors": [
        "Lei Zhu",
        "Lijian Lin",
        "Ye Zhu",
        "Jiahao Wu",
        "Xuehan Hou",
        "Yu Li",
        "Yunfei Liu",
        "Jie Chen"
      ],
      "published": "2026-01-05T02:59:49Z",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "abs_url": "https://arxiv.org/abs/2601.01749v1",
      "pdf_url": "https://arxiv.org/pdf/2601.01749v1.pdf"
    },
    {
      "id": "2601.01747v1",
      "title": "Crafting Adversarial Inputs for Large Vision-Language Models Using Black-Box Optimization",
      "abstract": "Recent advancements in Large Vision-Language Models (LVLMs) have shown groundbreaking capabilities across diverse multimodal tasks. However, these models remain vulnerable to adversarial jailbreak attacks, where adversaries craft subtle perturbations to bypass safety mechanisms and trigger harmful outputs. Existing white-box attacks methods require full model accessibility, suffer from computing costs and exhibit insufficient adversarial transferability, making them impractical for real-world, black-box settings. To address these limitations, we propose a black-box jailbreak attack on LVLMs via Zeroth-Order optimization using Simultaneous Perturbation Stochastic Approximation (ZO-SPSA). ZO-SPSA provides three key advantages: (i) gradient-free approximation by input-output interactions without requiring model knowledge, (ii) model-agnostic optimization without the surrogate model and (iii) lower resource requirements with reduced GPU memory consumption. We evaluate ZO-SPSA on three LVLMs, including InstructBLIP, LLaVA and MiniGPT-4, achieving the highest jailbreak success rate of 83.0% on InstructBLIP, while maintaining imperceptible perturbations comparable to white-box methods. Moreover, adversarial examples generated from MiniGPT-4 exhibit strong transferability to other LVLMs, with ASR reaching 64.18%. These findings underscore the real-world feasibility of black-box jailbreaks and expose critical weaknesses in the safety mechanisms of current LVLMs",
      "authors": [
        "Jiwei Guan",
        "Haibo Jin",
        "Haohan Wang"
      ],
      "published": "2026-01-05T02:49:33Z",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.CV",
        "cs.LG"
      ],
      "primary_category": "cs.CR",
      "abs_url": "https://arxiv.org/abs/2601.01747v1",
      "pdf_url": "https://arxiv.org/pdf/2601.01747v1.pdf"
    },
    {
      "id": "2601.01746v1",
      "title": "Point-SRA: Self-Representation Alignment for 3D Representation Learning",
      "abstract": "Masked autoencoders (MAE) have become a dominant paradigm in 3D representation learning, setting new performance benchmarks across various downstream tasks. Existing methods with fixed mask ratio neglect multi-level representational correlations and intrinsic geometric structures, while relying on point-wise reconstruction assumptions that conflict with the diversity of point cloud. To address these issues, we propose a 3D representation learning method, termed Point-SRA, which aligns representations through self-distillation and probabilistic modeling. Specifically, we assign different masking ratios to the MAE to capture complementary geometric and semantic information, while the MeanFlow Transformer (MFT) leverages cross-modal conditional embeddings to enable diverse probabilistic reconstruction. Our analysis further reveals that representations at different time steps in MFT also exhibit complementarity. Therefore, a Dual Self-Representation Alignment mechanism is proposed at both the MAE and MFT levels. Finally, we design a Flow-Conditioned Fine-Tuning Architecture to fully exploit the point cloud distribution learned via MeanFlow. Point-SRA outperforms Point-MAE by 5.37% on ScanObjectNN. On intracranial aneurysm segmentation, it reaches 96.07% mean IoU for arteries and 86.87% for aneurysms. For 3D object detection, Point-SRA achieves 47.3% AP@50, surpassing MaskPoint by 5.12%.",
      "authors": [
        "Lintong Wei",
        "Jian Lu",
        "Haozhe Cheng",
        "Jihua Zhu",
        "Kaibing Zhang"
      ],
      "published": "2026-01-05T02:44:21Z",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "abs_url": "https://arxiv.org/abs/2601.01746v1",
      "pdf_url": "https://arxiv.org/pdf/2601.01746v1.pdf"
    },
    {
      "id": "2601.01745v1",
      "title": "Multi-granularity Interactive Attention Framework for Residual Hierarchical Pronunciation Assessment",
      "abstract": "Automatic pronunciation assessment plays a crucial role in computer-assisted pronunciation training systems. Due to the ability to perform multiple pronunciation tasks simultaneously, multi-aspect multi-granularity pronunciation assessment methods are gradually receiving more attention and achieving better performance than single-level modeling tasks. However, existing methods only consider unidirectional dependencies between adjacent granularity levels, lacking bidirectional interaction among phoneme, word, and utterance levels and thus insufficiently capturing the acoustic structural correlations. To address this issue, we propose a novel residual hierarchical interactive method, HIA for short, that enables bidirectional modeling across granularities. As the core of HIA, the Interactive Attention Module leverages an attention mechanism to achieve dynamic bidirectional interaction, effectively capturing linguistic features at each granularity while integrating correlations between different granularity levels. We also propose a residual hierarchical structure to alleviate the feature forgetting problem when modeling acoustic hierarchies. In addition, we use 1-D convolutional layers to enhance the extraction of local contextual cues at each granularity. Extensive experiments on the speechocean762 dataset show that our model is comprehensively ahead of the existing state-of-the-art methods.",
      "authors": [
        "Hong Han",
        "Hao-Chen Pei",
        "Zhao-Zheng Nie",
        "Xin Luo",
        "Xin-Shun Xu"
      ],
      "published": "2026-01-05T02:43:04Z",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "abs_url": "https://arxiv.org/abs/2601.01745v1",
      "pdf_url": "https://arxiv.org/pdf/2601.01745v1.pdf"
    },
    {
      "id": "2601.01743v1",
      "title": "AI Agent Systems: Architectures, Applications, and Evaluation",
      "abstract": "AI agents -- systems that combine foundation models with reasoning, planning, memory, and tool use -- are rapidly becoming a practical interface between natural-language intent and real-world computation. This survey synthesizes the emerging landscape of AI agent architectures across: (i) deliberation and reasoning (e.g., chain-of-thought-style decomposition, self-reflection and verification, and constraint-aware decision making), (ii) planning and control (from reactive policies to hierarchical and multi-step planners), and (iii) tool calling and environment interaction (retrieval, code execution, APIs, and multimodal perception). We organize prior work into a unified taxonomy spanning agent components (policy/LLM core, memory, world models, planners, tool routers, and critics), orchestration patterns (single-agent vs.\\ multi-agent; centralized vs.\\ decentralized coordination), and deployment settings (offline analysis vs.\\ online interactive assistance; safety-critical vs.\\ open-ended tasks). We discuss key design trade-offs -- latency vs.\\ accuracy, autonomy vs.\\ controllability, and capability vs.\\ reliability -- and highlight how evaluation is complicated by non-determinism, long-horizon credit assignment, tool and environment variability, and hidden costs such as retries and context growth. Finally, we summarize measurement and benchmarking practices (task suites, human preference and utility metrics, success under constraints, robustness and security) and identify open challenges including verification and guardrails for tool actions, scalable memory and context management, interpretability of agent decisions, and reproducible evaluation under realistic workloads.",
      "authors": [
        "Bin Xu"
      ],
      "published": "2026-01-05T02:38:40Z",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "abs_url": "https://arxiv.org/abs/2601.01743v1",
      "pdf_url": "https://arxiv.org/pdf/2601.01743v1.pdf"
    },
    {
      "id": "2601.01741v1",
      "title": "Latent Space Element Method",
      "abstract": "How can we build surrogate solvers that train on small domains but scale to larger ones without intrusive access to PDE operators? Inspired by the Data-Driven Finite Element Method (DD-FEM) framework for modular data-driven solvers, we propose the Latent Space Element Method (LSEM), an element-based latent surrogate assembly approach in which a learned subdomain (\"element\") model can be tiled and coupled to form a larger computational domain. Each element is a LaSDI latent ODE surrogate trained from snapshots on a local patch, and neighboring elements are coupled through learned directional interaction terms in latent space, avoiding Schwarz iterations and interface residual evaluations. A smooth window-based blending reconstructs a global field from overlapping element predictions, yielding a scalable assembled latent dynamical system. Experiments on the 1D Burgers and Korteweg-de Vries equations show that LSEM maintains predictive accuracy while scaling to spatial domains larger than those seen in training. LSEM offers an interpretable and extensible route toward foundation-model surrogate solvers built from reusable local models.",
      "authors": [
        "Seung Whan Chung",
        "Youngsoo Choi",
        "Christopher Miller",
        "H. Keo Springer",
        "Kyle T. Sullivan"
      ],
      "published": "2026-01-05T02:32:50Z",
      "categories": [
        "math.DS",
        "cs.LG",
        "math.AP",
        "math.NA"
      ],
      "primary_category": "math.DS",
      "abs_url": "https://arxiv.org/abs/2601.01741v1",
      "pdf_url": "https://arxiv.org/pdf/2601.01741v1.pdf"
    },
    {
      "id": "2601.01739v1",
      "title": "K-EXAONE Technical Report",
      "abstract": "This technical report presents K-EXAONE, a large-scale multilingual language model developed by LG AI Research. K-EXAONE is built on a Mixture-of-Experts architecture with 236B total parameters, activating 23B parameters during inference. It supports a 256K-token context window and covers six languages: Korean, English, Spanish, German, Japanese, and Vietnamese. We evaluate K-EXAONE on a comprehensive benchmark suite spanning reasoning, agentic, general, Korean, and multilingual abilities. Across these evaluations, K-EXAONE demonstrates performance comparable to open-weight models of similar size. K-EXAONE, designed to advance AI for a better life, is positioned as a powerful proprietary AI foundation model for a wide range of industrial and research applications.",
      "authors": [
        "Eunbi Choi",
        "Kibong Choi",
        "Seokhee Hong",
        "Junwon Hwang",
        "Hyojin Jeon",
        "Hyunjik Jo",
        "Joonkee Kim",
        "Seonghwan Kim",
        "Soyeon Kim",
        "Sunkyoung Kim",
        "Yireun Kim",
        "Yongil Kim",
        "Haeju Lee",
        "Jinsik Lee",
        "Kyungmin Lee",
        "Sangha Park",
        "Heuiyeen Yeen",
        "Hwan Chang",
        "Stanley Jungkyu Choi",
        "Yejin Choi",
        "Jiwon Ham",
        "Kijeong Jeon",
        "Geunyeong Jeong",
        "Gerrard Jeongwon Jo",
        "Yonghwan Jo",
        "Jiyeon Jung",
        "Naeun Kang",
        "Dohoon Kim",
        "Euisoon Kim",
        "Hayeon Kim",
        "Hyosang Kim",
        "Hyunseo Kim",
        "Jieun Kim",
        "Minu Kim",
        "Myoungshin Kim",
        "Unsol Kim",
        "Youchul Kim",
        "YoungJin Kim",
        "Chaeeun Lee",
        "Chaeyoon Lee",
        "Changhun Lee",
        "Dahm Lee",
        "Edward Hwayoung Lee",
        "Honglak Lee",
        "Jinsang Lee",
        "Jiyoung Lee",
        "Sangeun Lee",
        "Seungwon Lim",
        "Solji Lim",
        "Woohyung Lim",
        "Chanwoo Moon",
        "Jaewoo Park",
        "Jinho Park",
        "Yongmin Park",
        "Hyerin Seo",
        "Wooseok Seo",
        "Yongwoo Song",
        "Sejong Yang",
        "Sihoon Yang",
        "Chang En Yea",
        "Sihyuk Yi",
        "Chansik Yoon",
        "Dongkeun Yoon",
        "Sangyeon Yoon",
        "Hyeongu Yun"
      ],
      "published": "2026-01-05T02:30:59Z",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "abs_url": "https://arxiv.org/abs/2601.01739v1",
      "pdf_url": "https://arxiv.org/pdf/2601.01739v1.pdf"
    },
    {
      "id": "2601.01726v1",
      "title": "Simulations and Advancements in MRI-Guided Power-Driven Ferric Tools for Wireless Therapeutic Interventions",
      "abstract": "Designing a robotic system that functions effectively within the specific environment of a Magnetic Resonance Imaging (MRI) scanner requires solving numerous technical issues, such as maintaining the robot's precision and stability under strong magnetic fields. This research focuses on enhancing MRI's role in medical imaging, especially in its application to guide intravascular interventions using robot-assisted devices. A newly developed computational system is introduced, designed for seamless integration with the MRI scanner, including a computational unit and user interface. This system processes MR images to delineate the vascular network, establishing virtual paths and boundaries within vessels to prevent procedural damage. Key findings reveal the system's capability to create tailored magnetic field gradient patterns for device control, considering the vessel's geometry and safety norms, and adapting to different blood flow characteristics for finer navigation. Additionally, the system's modeling aspect assesses the safety and feasibility of navigating pre-set vascular paths. Conclusively, this system, based on the Qt framework and C/C++, with specialized software modules, represents a major step forward in merging imaging technology with robotic aid, significantly enhancing precision and safety in intravascular procedures.",
      "authors": [
        "Wenhui Chu",
        "Aobo Jin",
        "Hardik A. Gohel"
      ],
      "published": "2026-01-05T02:04:39Z",
      "categories": [
        "cs.RO",
        "eess.SY"
      ],
      "primary_category": "cs.RO",
      "abs_url": "https://arxiv.org/abs/2601.01726v1",
      "pdf_url": "https://arxiv.org/pdf/2601.01726v1.pdf"
    },
    {
      "id": "2601.01720v1",
      "title": "FFP-300K: Scaling First-Frame Propagation for Generalizable Video Editing",
      "abstract": "First-Frame Propagation (FFP) offers a promising paradigm for controllable video editing, but existing methods are hampered by a reliance on cumbersome run-time guidance. We identify the root cause of this limitation as the inadequacy of current training datasets, which are often too short, low-resolution, and lack the task diversity required to teach robust temporal priors. To address this foundational data gap, we first introduce FFP-300K, a new large-scale dataset comprising 300K high-fidelity video pairs at 720p resolution and 81 frames in length, constructed via a principled two-track pipeline for diverse local and global edits. Building on this dataset, we propose a novel framework designed for true guidance-free FFP that resolves the critical tension between maintaining first-frame appearance and preserving source video motion. Architecturally, we introduce Adaptive Spatio-Temporal RoPE (AST-RoPE), which dynamically remaps positional encodings to disentangle appearance and motion references. At the objective level, we employ a self-distillation strategy where an identity propagation task acts as a powerful regularizer, ensuring long-term temporal stability and preventing semantic drift. Comprehensive experiments on the EditVerseBench benchmark demonstrate that our method significantly outperforming existing academic and commercial models by receiving about 0.2 PickScore and 0.3 VLM score improvement against these competitors.",
      "authors": [
        "Xijie Huang",
        "Chengming Xu",
        "Donghao Luo",
        "Xiaobin Hu",
        "Peng Tang",
        "Xu Peng",
        "Jiangning Zhang",
        "Chengjie Wang",
        "Yanwei Fu"
      ],
      "published": "2026-01-05T01:46:22Z",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "abs_url": "https://arxiv.org/abs/2601.01720v1",
      "pdf_url": "https://arxiv.org/pdf/2601.01720v1.pdf"
    },
    {
      "id": "2601.01718v1",
      "title": "Yuan3.0 Flash: An Open Multimodal Large Language Model for Enterprise Applications",
      "abstract": "We introduce Yuan3.0 Flash, an open-source Mixture-of-Experts (MoE) MultiModal Large Language Model featuring 3.7B activated parameters and 40B total parameters, specifically designed to enhance performance on enterprise-oriented tasks while maintaining competitive capabilities on general-purpose tasks. To address the overthinking phenomenon commonly observed in Large Reasoning Models (LRMs), we propose Reflection-aware Adaptive Policy Optimization (RAPO), a novel RL training algorithm that effectively regulates overthinking behaviors. In enterprise-oriented tasks such as retrieval-augmented generation (RAG), complex table understanding, and summarization, Yuan3.0 Flash consistently achieves superior performance. Moreover, it also demonstrates strong reasoning capabilities in domains such as mathematics, science, etc., attaining accuracy comparable to frontier model while requiring only approximately 1/4 to 1/2 of the average tokens. Yuan3.0 Flash has been fully open-sourced to facilitate further research and real-world deployment: https://github.com/Yuan-lab-LLM/Yuan3.0.",
      "authors": [
        "YuanLab. ai",
        " :",
        "Shawn Wu",
        "Sean Wang",
        "Louie Li",
        "Darcy Chen",
        "Allen Wang",
        "Jiangang Luo",
        "Xudong Zhao",
        "Joseph Shen",
        "Gawain Ma",
        "Jasper Jia",
        "Marcus Mao",
        "Claire Wang",
        "Hunter He",
        "Carol Wang",
        "Zera Zhang",
        "Jason Wang",
        "Chonly Shen",
        "Leo Zhang",
        "Logan Chen",
        "Qasim Meng",
        "James Gong",
        "Danied Zhao",
        "Penn Zheng",
        "Owen Zhu",
        "Tong Yu"
      ],
      "published": "2026-01-05T01:44:09Z",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "abs_url": "https://arxiv.org/abs/2601.01718v1",
      "pdf_url": "https://arxiv.org/pdf/2601.01718v1.pdf"
    },
    {
      "id": "2601.01714v1",
      "title": "Entropy-Aligned Decoding of LMs for Better Writing and Reasoning",
      "abstract": "Language models (LMs) are trained on billions of tokens in an attempt to recover the true language distribution. Still, vanilla random sampling from LMs yields low quality generations. Decoding algorithms attempt to restrict the LM distribution to a set of high-probability continuations, but rely on greedy heuristics that introduce myopic distortions, yielding sentences that are homogeneous, repetitive and incoherent. In this paper, we introduce EPIC, a hyperparameter-free decoding approach that incorporates the entropy of future trajectories into LM decoding. EPIC explicitly regulates the amount of uncertainty expressed at every step of generation, aligning the sampling distribution's entropy to the aleatoric (data) uncertainty. Through Entropy-Aware Lazy Gumbel-Max sampling, EPIC manages to be exact, while also being efficient, requiring only a sublinear number of entropy evaluations per step. Unlike current baselines, EPIC yields sampling distributions that are empirically well-aligned with the entropy of the underlying data distribution. Across creative writing and summarization tasks, EPIC consistently improves LM-as-judge preference win-rates over widely used decoding strategies. These preference gains are complemented by automatic metrics, showing that EPIC produces more diverse generations and more faithful summaries. We also evaluate EPIC on mathematical reasoning, where it outperforms all baselines.",
      "authors": [
        "Kareem Ahmed",
        "Sameer Singh"
      ],
      "published": "2026-01-05T01:37:10Z",
      "categories": [
        "cs.LG",
        "cs.CL"
      ],
      "primary_category": "cs.LG",
      "abs_url": "https://arxiv.org/abs/2601.01714v1",
      "pdf_url": "https://arxiv.org/pdf/2601.01714v1.pdf"
    },
    {
      "id": "2601.01712v1",
      "title": "RelayGR: Scaling Long-Sequence Generative Recommendation via Cross-Stage Relay-Race Inference",
      "abstract": "Real-time recommender systems execute multi-stage cascades (retrieval, pre-processing, fine-grained ranking) under strict tail-latency SLOs, leaving only tens of milliseconds for ranking. Generative recommendation (GR) models can improve quality by consuming long user-behavior sequences, but in production their online sequence length is tightly capped by the ranking-stage P99 budget. We observe that the majority of GR tokens encode user behaviors that are independent of the item candidates, suggesting an opportunity to pre-infer a user-behavior prefix once and reuse it during ranking rather than recomputing it on the critical path. Realizing this idea at industrial scale is non-trivial: the prefix cache must survive across multiple pipeline stages before the final ranking instance is determined, the user population implies cache footprints far beyond a single device, and indiscriminate pre-inference would overload shared resources under high QPS. We present RelayGR, a production system that enables in-HBM relay-race inference for GR. RelayGR selectively pre-infers long-term user prefixes, keeps their KV caches resident in HBM over the request lifecycle, and ensures the subsequent ranking can consume them without remote fetches. RelayGR combines three techniques: 1) a sequence-aware trigger that admits only at-risk requests under a bounded cache footprint and pre-inference load, 2) an affinity-aware router that co-locates cache production and consumption by routing both the auxiliary pre-infer signal and the ranking request to the same instance, and 3) a memory-aware expander that uses server-local DRAM to capture short-term cross-request reuse while avoiding redundant reloads. We implement RelayGR on Huawei Ascend NPUs and evaluate it with real queries. Under a fixed P99 SLO, RelayGR supports up to 1.5$\\times$ longer sequences and improves SLO-compliant throughput by up to 3.6$\\times$.",
      "authors": [
        "Jiarui Wang",
        "Huichao Chai",
        "Yuanhang Zhang",
        "Zongjin Zhou",
        "Wei Guo",
        "Xingkun Yang",
        "Qiang Tang",
        "Bo Pan",
        "Jiawei Zhu",
        "Ke Cheng",
        "Yuting Yan",
        "Shulan Wang",
        "Yingjie Zhu",
        "Zhengfan Yuan",
        "Jiaqi Huang",
        "Yuhan Zhang",
        "Xiaosong Sun",
        "Zhinan Zhang",
        "Hong Zhu",
        "Yongsheng Zhang",
        "Tiantian Dong",
        "Zhong Xiao",
        "Deliang Liu",
        "Chengzhou Lu",
        "Yuan Sun",
        "Zhiyuan Chen",
        "Xinming Han",
        "Zaizhu Liu",
        "Yaoyuan Wang",
        "Ziyang Zhang",
        "Yong Liu",
        "Jinxin Xu",
        "Yajing Sun",
        "Zhoujun Yu",
        "Wenting Zhou",
        "Qidong Zhang",
        "Zhengyong Zhang",
        "Zhonghai Gu",
        "Yibo Jin",
        "Yongxiang Feng",
        "Pengfei Zuo"
      ],
      "published": "2026-01-05T01:34:06Z",
      "categories": [
        "cs.DC",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.DC",
      "abs_url": "https://arxiv.org/abs/2601.01712v1",
      "pdf_url": "https://arxiv.org/pdf/2601.01712v1.pdf"
    },
    {
      "id": "2601.01709v1",
      "title": "Reinforcement Learning for Option Hedging: Static Implied-Volatility Fit versus Shortfall-Aware Performance",
      "abstract": "We extend the Q-learner in Black-Scholes (QLBS) framework by incorporating risk aversion and trading costs, and propose a novel Replication Learning of Option Pricing (RLOP) approach. Both methods are fully compatible with standard reinforcement learning algorithms and operate under market frictions. Using SPY and XOP option data, we evaluate performance along static and dynamic dimensions. Adaptive-QLBS achieves higher static pricing accuracy in implied volatility space, while RLOP delivers superior dynamic hedging performance by reducing shortfall probability. These results highlight the importance of evaluating option pricing models beyond static fit, emphasizing realized hedging outcomes.",
      "authors": [
        "Ziheng Chen",
        "Minxuan Hu",
        "Jiayu Yi",
        "Wenxi Sun"
      ],
      "published": "2026-01-05T01:02:41Z",
      "categories": [
        "q-fin.PR",
        "cs.LG"
      ],
      "primary_category": "q-fin.PR",
      "abs_url": "https://arxiv.org/abs/2601.01709v1",
      "pdf_url": "https://arxiv.org/pdf/2601.01709v1.pdf"
    },
    {
      "id": "2601.01708v1",
      "title": "A Training-Free Large Reasoning Model-based Knowledge Tracing Framework for Unified Prediction and Prescription",
      "abstract": "Knowledge Tracing (KT) aims to estimate a learner's evolving mastery based on interaction histories. Recent studies have explored Large Language Models (LLMs) for KT via autoregressive nature, but such approaches typically require fine-tuning and exhibit unstable or near-random performance. Moreover, prior KT systems primarily focus on prediction and rely on multi-stage pipelines for feedback and recommendation, resulting in increased system complexity and resources. To address this gap, we propose Thinking-KT, a training-free KT framework that incorporates Test-Time Scaling (TTS), enabling even small LLMs to achieve competitive KT performance. Moreover, in this framework, a small LLM can jointly perform KT prediction, personalized feedback generation, and learning recommendation in a unified output without degrading prediction accuracy. Beyond performance, we present the systematic analysis of reasoning traces in KT. Our results demonstrate that TTS is a critical yet underexplored factor in LLM-based KT, and that small LLMs can serve as unified ITS engines.",
      "authors": [
        "Unggi Lee",
        "Joo Young Kim",
        "Ran Ju",
        "Minyoung Jung",
        "Jeyeon Eo"
      ],
      "published": "2026-01-05T01:02:21Z",
      "categories": [
        "cs.CL"
      ],
      "primary_category": "cs.CL",
      "abs_url": "https://arxiv.org/abs/2601.01708v1",
      "pdf_url": "https://arxiv.org/pdf/2601.01708v1.pdf"
    },
    {
      "id": "2601.01705v1",
      "title": "Explicit World Models for Reliable Human-Robot Collaboration",
      "abstract": "This paper addresses the topic of robustness under sensing noise, ambiguous instructions, and human-robot interaction. We take a radically different tack to the issue of reliable embodied AI: instead of focusing on formal verification methods aimed at achieving model predictability and robustness, we emphasise the dynamic, ambiguous and subjective nature of human-robot interactions that requires embodied AI systems to perceive, interpret, and respond to human intentions in a manner that is consistent, comprehensible and aligned with human expectations. We argue that when embodied agents operate in human environments that are inherently social, multimodal, and fluid, reliability is contextually determined and only has meaning in relation to the goals and expectations of humans involved in the interaction. This calls for a fundamentally different approach to achieving reliable embodied AI that is centred on building and updating an accessible \"explicit world model\" representing the common ground between human and AI, that is used to align robot behaviours with human expectations.",
      "authors": [
        "Kenneth Kwok",
        "Basura Fernando",
        "Qianli Xu",
        "Vigneshwaran Subbaraju",
        "Dongkyu Choi",
        "Boon Kiat Quek"
      ],
      "published": "2026-01-05T00:58:19Z",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "abs_url": "https://arxiv.org/abs/2601.01705v1",
      "pdf_url": "https://arxiv.org/pdf/2601.01705v1.pdf"
    },
    {
      "id": "2601.01703v1",
      "title": "Beyond Homophily: Community Search on Heterophilic Graphs",
      "abstract": "Community search aims to identify a refined set of nodes that are most relevant to a given query, supporting tasks ranging from fraud detection to recommendation. Unlike homophilic graphs, many real-world networks are heterophilic, where edges predominantly connect dissimilar nodes. Therefore, structural signals that once reflected smooth, low-frequency similarity now appear as sharp, high-frequency contrasts. However, both classical algorithms (e.g., k-core, k-truss) and recent ML-based models struggle to achieve effective community search on heterophilic graphs, where edge signs or semantics are generally unknown. Algorithm-based methods often return communities with mixed class labels, while GNNs, built on homophily, smooth away meaningful signals and blur community boundaries. Therefore, we propose Adaptive Community Search (AdaptCS), a unified framework featuring three key designs: (i) an AdaptCS Encoder that disentangles multi-hop and multi-frequency signals, enabling the model to capture both smooth (homophilic) and contrastive (heterophilic) relations; (ii) a memory-efficient low-rank optimization that removes the main computational bottleneck and ensures model scalability; and (iii) an Adaptive Community Score (ACS) that guides online search by balancing embedding similarity and topological relations. Extensive experiments on both heterophilic and homophilic benchmarks demonstrate that AdaptCS outperforms the best-performing baseline by an average of 11% in F1-score, retains robustness across heterophily levels, and achieves up to 2 orders of magnitude speedup.",
      "authors": [
        "Qing Sima",
        "Xiaoyang Wang",
        "Wenjie Zhang"
      ],
      "published": "2026-01-05T00:44:17Z",
      "categories": [
        "cs.SI",
        "cs.AI",
        "cs.DB",
        "cs.IR"
      ],
      "primary_category": "cs.SI",
      "abs_url": "https://arxiv.org/abs/2601.01703v1",
      "pdf_url": "https://arxiv.org/pdf/2601.01703v1.pdf"
    },
    {
      "id": "2601.01701v1",
      "title": "Digital Twin-Driven Communication-Efficient Federated Anomaly Detection for Industrial IoT",
      "abstract": "Anomaly detection is increasingly becoming crucial for maintaining the safety, reliability, and efficiency of industrial systems. Recently, with the advent of digital twins and data-driven decision-making, several statistical and machine-learning methods have been proposed. However, these methods face several challenges, such as dependence on only real sensor datasets, limited labeled data, high false alarm rates, and privacy concerns. To address these problems, we propose a suite of digital twin-integrated federated learning (DTFL) methods that enhance global model performance while preserving data privacy and communication efficiency. Specifically, we present five novel approaches: Digital Twin-Based Meta-Learning (DTML), Federated Parameter Fusion (FPF), Layer-wise Parameter Exchange (LPE), Cyclic Weight Adaptation (CWA), and Digital Twin Knowledge Distillation (DTKD). Each method introduces a unique mechanism to combine synthetic and real-world knowledge, balancing generalization with communication overhead. We conduct an extensive experiment using a publicly available cyber-physical anomaly detection dataset. For a target accuracy of 80%, CWA reaches the target in 33 rounds, FPF in 41 rounds, LPE in 48 rounds, and DTML in 87 rounds, whereas the standard FedAvg baseline and DTKD do not reach the target within 100 rounds. These results highlight substantial communication-efficiency gains (up to 62% fewer rounds than DTML and 31% fewer than LPE) and demonstrate that integrating DT knowledge into FL accelerates convergence to operationally meaningful accuracy thresholds for IIoT anomaly detection.",
      "authors": [
        "Mohammed Ayalew Belay",
        "Adil Rasheed",
        "Pierluigi Salvo Rossi"
      ],
      "published": "2026-01-05T00:26:15Z",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "abs_url": "https://arxiv.org/abs/2601.01701v1",
      "pdf_url": "https://arxiv.org/pdf/2601.01701v1.pdf"
    },
    {
      "id": "2601.01698v1",
      "title": "Hidden costs for inference with deep network on embedded system devices",
      "abstract": "This study evaluates the inference performance of various deep learning models under an embedded system environment. In previous works, Multiply-Accumulate operation is typically used to measure computational load of a deep model. According to this study, however, this metric has a limitation to estimate inference time on embedded devices. This paper poses the question of what aspects are overlooked when expressed in terms of Multiply-Accumulate operations. In experiments, an image classification task is performed on an embedded system device using the CIFAR-100 dataset to compare and analyze the inference times of ten deep models with the theoretically calculated Multiply-Accumulate operations for each model. The results highlight the importance of considering additional computations between tensors when optimizing deep learning models for real-time performing in embedded systems.",
      "authors": [
        "Chankyu Lee",
        "Woohyun Choi",
        "Sangwook Park"
      ],
      "published": "2026-01-05T00:18:51Z",
      "categories": [
        "cs.CC",
        "cs.LG"
      ],
      "primary_category": "cs.CC",
      "abs_url": "https://arxiv.org/abs/2601.01698v1",
      "pdf_url": "https://arxiv.org/pdf/2601.01698v1.pdf"
    },
    {
      "id": "2601.01696v1",
      "title": "Real-Time Lane Detection via Efficient Feature Alignment and Covariance Optimization for Low-Power Embedded Systems",
      "abstract": "Real-time lane detection in embedded systems encounters significant challenges due to subtle and sparse visual signals in RGB images, often constrained by limited computational resources and power consumption. Although deep learning models for lane detection categorized into segmentation-based, anchor-based, and curve-based methods there remains a scarcity of universally applicable optimization techniques tailored for low-power embedded environments. To overcome this, we propose an innovative Covariance Distribution Optimization (CDO) module specifically designed for efficient, real-time applications. The CDO module aligns lane feature distributions closely with ground-truth labels, significantly enhancing detection accuracy without increasing computational complexity. Evaluations were conducted on six diverse models across all three method categories, including two optimized for real-time applications and four state-of-the-art (SOTA) models, tested comprehensively on three major datasets: CULane, TuSimple, and LLAMAS. Experimental results demonstrate accuracy improvements ranging from 0.01% to 1.5%. The proposed CDO module is characterized by ease of integration into existing systems without structural modifications and utilizes existing model parameters to facilitate ongoing training, thus offering substantial benefits in performance, power efficiency, and operational flexibility in embedded systems.",
      "authors": [
        "Yian Liu",
        "Xiong Wang",
        "Ping Xu",
        "Lei Zhu",
        "Ming Yan",
        "Linyun Xue"
      ],
      "published": "2026-01-05T00:06:06Z",
      "categories": [
        "cs.CV",
        "cs.RO"
      ],
      "primary_category": "cs.CV",
      "abs_url": "https://arxiv.org/abs/2601.01696v1",
      "pdf_url": "https://arxiv.org/pdf/2601.01696v1.pdf"
    },
    {
      "id": "2601.01695v1",
      "title": "Learnability-Driven Submodular Optimization for Active Roadside 3D Detection",
      "abstract": "Roadside perception datasets are typically constructed via cooperative labeling between synchronized vehicle and roadside frame pairs. However, real deployment often requires annotation of roadside-only data due to hardware and privacy constraints. Even human experts struggle to produce accurate labels without vehicle-side data (image, LIDAR), which not only increases annotation difficulty and cost, but also reveals a fundamental learnability problem: many roadside-only scenes contain distant, blurred, or occluded objects whose 3D properties are ambiguous from a single view and can only be reliably annotated by cross-checking paired vehicle--roadside frames. We refer to such cases as inherently ambiguous samples. To reduce wasted annotation effort on inherently ambiguous samples while still obtaining high-performing models, we turn to active learning. This work focuses on active learning for roadside monocular 3D object detection and proposes a learnability-driven framework that selects scenes which are both informative and reliably labelable, suppressing inherently ambiguous samples while ensuring coverage. Experiments demonstrate that our method, LH3D, achieves 86.06%, 67.32%, and 78.67% of full-performance for vehicles, pedestrians, and cyclists respectively, using only 25% of the annotation budget on DAIR-V2X-I, significantly outperforming uncertainty-based baselines. This confirms that learnability, not uncertainty, matters for roadside 3D perception.",
      "authors": [
        "Ruiyu Mao",
        "Baoming Zhang",
        "Nicholas Ruozzi",
        "Yunhui Guo"
      ],
      "published": "2026-01-04T23:59:06Z",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "abs_url": "https://arxiv.org/abs/2601.01695v1",
      "pdf_url": "https://arxiv.org/pdf/2601.01695v1.pdf"
    },
    {
      "id": "2601.01692v1",
      "title": "Enhanced Multi-model Online Conformal Prediction",
      "abstract": "Conformal prediction is a framework for uncertainty quantification that constructs prediction sets for previously unseen data, guaranteeing coverage of the true label with a specified probability. However, the efficiency of these prediction sets, measured by their size, depends on the choice of the underlying learning model. Relying on a single fixed model may lead to suboptimal performance in online environments, as a single model may not consistently perform well across all time steps. To mitigate this, prior work has explored selecting a model from a set of candidates. However, this approach becomes computationally expensive as the number of candidate models increases. Moreover, poorly performing models in the set may also hinder the effectiveness. To tackle this challenge, this work develops a novel multi-model online conformal prediction algorithm that reduces computational complexity and improves prediction efficiency. At each time step, a bipartite graph is generated to identify a subset of effective models, from which a model is selected to construct the prediction set. Experiments demonstrate that our method outperforms existing multi-model conformal prediction techniques in terms of both prediction set size and computational efficiency.",
      "authors": [
        "Erfan Hajihashemi",
        "Yanning Shen"
      ],
      "published": "2026-01-04T23:44:43Z",
      "categories": [
        "cs.LG"
      ],
      "primary_category": "cs.LG",
      "abs_url": "https://arxiv.org/abs/2601.01692v1",
      "pdf_url": "https://arxiv.org/pdf/2601.01692v1.pdf"
    },
    {
      "id": "2601.01689v1",
      "title": "Mitigating Longitudinal Performance Degradation in Child Face Recognition Using Synthetic Data",
      "abstract": "Longitudinal face recognition in children remains challenging due to rapid and nonlinear facial growth, which causes template drift and increasing verification errors over time. This work investigates whether synthetic face data can act as a longitudinal stabilizer by improving temporal robustness of child face recognition models. Using an identity disjoint protocol on the Young Face Aging (YFA) dataset, we evaluate three settings: (i) pretrained MagFace embeddings without dataset specific fine-tuning, (ii) MagFace fine-tuned using authentic training faces only, and (iii) MagFace fine-tuned using a combination of authentic and synthetically generated training faces. Synthetic data is generated using StyleGAN2 ADA and incorporated exclusively within the training identities; a post generation filtering step is applied to mitigate identity leakage and remove artifact affected samples. Experimental results across enrollment verification gaps from 6 to 36 months show that synthetic-augmented fine tuning substantially reduces error rates relative to both the pretrained baseline and real only fine tuning. These findings provide a risk aware assessment of synthetic augmentation for improving identity persistence in pediatric face recognition.",
      "authors": [
        "Afzal Hossain",
        "Stephanie Schuckers"
      ],
      "published": "2026-01-04T23:03:03Z",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "abs_url": "https://arxiv.org/abs/2601.01689v1",
      "pdf_url": "https://arxiv.org/pdf/2601.01689v1.pdf"
    },
    {
      "id": "2601.01688v1",
      "title": "DiMEx: Breaking the Cold Start Barrier in Data-Free Model Extraction via Latent Diffusion Priors",
      "abstract": "Model stealing attacks pose an existential threat to Machine Learning as a Service (MLaaS), allowing adversaries to replicate proprietary models for a fraction of their training cost. While Data-Free Model Extraction (DFME) has emerged as a stealthy vector, it remains fundamentally constrained by the \"Cold Start\" problem: GAN-based adversaries waste thousands of queries converging from random noise to meaningful data. We propose DiMEx, a framework that weaponizes the rich semantic priors of pre-trained Latent Diffusion Models to bypass this initialization barrier entirely. By employing Random Embedding Bayesian Optimization (REMBO) within the generator's latent space, DiMEx synthesizes high-fidelity queries immediately, achieving 52.1 percent agreement on SVHN with just 2,000 queries - outperforming state-of-the-art GAN baselines by over 16 percent. To counter this highly semantic threat, we introduce the Hybrid Stateful Ensemble (HSE) defense, which identifies the unique \"optimization trajectory\" of latent-space attacks. Our results demonstrate that while DiMEx evades static distribution detectors, HSE exploits this temporal signature to suppress attack success rates to 21.6 percent with negligible latency.",
      "authors": [
        "Yash Thesia",
        "Meera Suthar"
      ],
      "published": "2026-01-04T22:58:34Z",
      "categories": [
        "cs.LG"
      ],
      "primary_category": "cs.LG",
      "abs_url": "https://arxiv.org/abs/2601.01688v1",
      "pdf_url": "https://arxiv.org/pdf/2601.01688v1.pdf"
    },
    {
      "id": "2601.01687v1",
      "title": "FALCON: Few-Shot Adversarial Learning for Cross-Domain Medical Image Segmentation",
      "abstract": "Precise delineation of anatomical and pathological structures within 3D medical volumes is crucial for accurate diagnosis, effective surgical planning, and longitudinal disease monitoring. Despite advancements in AI, clinically viable segmentation is often hindered by the scarcity of 3D annotations, patient-specific variability, data privacy concerns, and substantial computational overhead. In this work, we propose FALCON, a cross-domain few-shot segmentation framework that achieves high-precision 3D volume segmentation by processing data as 2D slices. The framework is first meta-trained on natural images to learn-to-learn generalizable segmentation priors, then transferred to the medical domain via adversarial fine-tuning and boundary-aware learning. Task-aware inference, conditioned on support cues, allows FALCON to adapt dynamically to patient-specific anatomical variations across slices. Experiments on four benchmarks demonstrate that FALCON consistently achieves the lowest Hausdorff Distance scores, indicating superior boundary accuracy while maintaining a Dice Similarity Coefficient comparable to the state-of-the-art models. Notably, these results are achieved with significantly less labeled data, no data augmentation, and substantially lower computational overhead.",
      "authors": [
        "Abdur R. Fayjie",
        "Pankhi Kashyap",
        "Jutika Borah",
        "Patrick Vandewalle"
      ],
      "published": "2026-01-04T22:57:49Z",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "abs_url": "https://arxiv.org/abs/2601.01687v1",
      "pdf_url": "https://arxiv.org/pdf/2601.01687v1.pdf"
    },
    {
      "id": "2601.01685v1",
      "title": "Lying with Truths: Open-Channel Multi-Agent Collusion for Belief Manipulation via Generative Montage",
      "abstract": "As large language models (LLMs) transition to autonomous agents synthesizing real-time information, their reasoning capabilities introduce an unexpected attack surface. This paper introduces a novel threat where colluding agents steer victim beliefs using only truthful evidence fragments distributed through public channels, without relying on covert communications, backdoors, or falsified documents. By exploiting LLMs' overthinking tendency, we formalize the first cognitive collusion attack and propose Generative Montage: a Writer-Editor-Director framework that constructs deceptive narratives through adversarial debate and coordinated posting of evidence fragments, causing victims to internalize and propagate fabricated conclusions. To study this risk, we develop CoPHEME, a dataset derived from real-world rumor events, and simulate attacks across diverse LLM families. Our results show pervasive vulnerability across 14 LLM families: attack success rates reach 74.4% for proprietary models and 70.6% for open-weights models. Counterintuitively, stronger reasoning capabilities increase susceptibility, with reasoning-specialized models showing higher attack success than base models or prompts. Furthermore, these false beliefs then cascade to downstream judges, achieving over 60% deception rates, highlighting a socio-technical vulnerability in how LLM-based agents interact with dynamic information environments. Our implementation and data are available at: https://github.com/CharlesJW222/Lying_with_Truth/tree/main.",
      "authors": [
        "Jinwei Hu",
        "Xinmiao Huang",
        "Youcheng Sun",
        "Yi Dong",
        "Xiaowei Huang"
      ],
      "published": "2026-01-04T22:50:23Z",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.MA"
      ],
      "primary_category": "cs.CL",
      "abs_url": "https://arxiv.org/abs/2601.01685v1",
      "pdf_url": "https://arxiv.org/pdf/2601.01685v1.pdf"
    },
    {
      "id": "2601.01684v1",
      "title": "LACONIC: Dense-Level Effectiveness for Scalable Sparse Retrieval via a Two-Phase Training Curriculum",
      "abstract": "While dense retrieval models have become the standard for state-of-the-art information retrieval, their deployment is often constrained by high memory requirements and reliance on GPU accelerators for vector similarity search. Learned sparse retrieval offers a compelling alternative by enabling efficient search via inverted indices, yet it has historically received less attention than dense approaches. In this report, we introduce LACONIC, a family of learned sparse retrievers based on the Llama-3 architecture (1B, 3B, and 8B). We propose a streamlined two-phase training curriculum consisting of (1) weakly supervised pre-finetuning to adapt causal LLMs for bidirectional contextualization and (2) high-signal finetuning using curated hard negatives. Our results demonstrate that LACONIC effectively bridges the performance gap with dense models: the 8B variant achieves a state-of-the-art 60.2 nDCG on the MTEB Retrieval benchmark, ranking 15th on the leaderboard as of January 1, 2026, while utilizing 71\\% less index memory than an equivalent dense model. By delivering high retrieval effectiveness on commodity CPU hardware with a fraction of the compute budget required by competing models, LACONIC provides a scalable and efficient solution for real-world search applications.",
      "authors": [
        "Zhichao Xu",
        "Shengyao Zhuang",
        "Crystina Zhang",
        "Xueguang Ma",
        "Yijun Tian",
        "Maitrey Mehta",
        "Jimmy Lin",
        "Vivek Srikumar"
      ],
      "published": "2026-01-04T22:42:20Z",
      "categories": [
        "cs.IR",
        "cs.CL"
      ],
      "primary_category": "cs.IR",
      "abs_url": "https://arxiv.org/abs/2601.01684v1",
      "pdf_url": "https://arxiv.org/pdf/2601.01684v1.pdf"
    },
    {
      "id": "2601.01680v1",
      "title": "Evaluating Deep Learning-Based Face Recognition for Infants and Toddlers: Impact of Age Across Developmental Stages",
      "abstract": "Face recognition for infants and toddlers presents unique challenges due to rapid facial morphology changes, high inter-class similarity, and limited dataset availability. This study evaluates the performance of four deep learning-based face recognition models FaceNet, ArcFace, MagFace, and CosFace on a newly developed longitudinal dataset collected over a 24 month period in seven sessions involving children aged 0 to 3 years. Our analysis examines recognition accuracy across developmental stages, showing that the True Accept Rate (TAR) is only 30.7% at 0.1% False Accept Rate (FAR) for infants aged 0 to 6 months, due to unstable facial features. Performance improves significantly in older children, reaching 64.7% TAR at 0.1% FAR in the 2.5 to 3 year age group. We also evaluate verification performance over different time intervals, revealing that shorter time gaps result in higher accuracy due to reduced embedding drift. To mitigate this drift, we apply a Domain Adversarial Neural Network (DANN) approach that improves TAR by over 12%, yielding features that are more temporally stable and generalizable. These findings are critical for building biometric systems that function reliably over time in smart city applications such as public healthcare, child safety, and digital identity services. The challenges observed in early age groups highlight the importance of future research on privacy preserving biometric authentication systems that can address temporal variability, particularly in secure and regulated urban environments where child verification is essential.",
      "authors": [
        "Afzal Hossain",
        "Mst Rumana Sumi",
        "Stephanie Schuckers"
      ],
      "published": "2026-01-04T22:30:28Z",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "abs_url": "https://arxiv.org/abs/2601.01680v1",
      "pdf_url": "https://arxiv.org/pdf/2601.01680v1.pdf"
    },
    {
      "id": "2601.01679v1",
      "title": "Simplex Deep Linear Discriminant Analysis",
      "abstract": "We revisit Deep Linear Discriminant Analysis (Deep LDA) from a likelihood-based perspective. While classical LDA is a simple Gaussian model with linear decision boundaries, attaching an LDA head to a neural encoder raises the question of how to train the resulting deep classifier by maximum likelihood estimation (MLE). We first show that end-to-end MLE training of an unconstrained Deep LDA model ignores discrimination: when both the LDA parameters and the encoder parameters are learned jointly, the likelihood admits a degenerate solution in which some of the class clusters may heavily overlap or even collapse, and classification performance deteriorates. Batchwise moment re-estimation of the LDA parameters does not remove this failure mode. We then propose a constrained Deep LDA formulation that fixes the class means to the vertices of a regular simplex in the latent space and restricts the shared covariance to be spherical, leaving only the priors and a single variance parameter to be learned along with the encoder. Under these geometric constraints, MLE becomes stable and yields well-separated class clusters in the latent space. On images (Fashion-MNIST, CIFAR-10, CIFAR-100), the resulting Deep LDA models achieve accuracy competitive with softmax baselines while offering a simple, interpretable latent geometry that is clearly visible in two-dimensional projections.",
      "authors": [
        "Maxat Tezekbayev",
        "Arman Bolatov",
        "Zhenisbek Assylbekov"
      ],
      "published": "2026-01-04T22:22:59Z",
      "categories": [
        "stat.ML",
        "cs.LG"
      ],
      "primary_category": "stat.ML",
      "abs_url": "https://arxiv.org/abs/2601.01679v1",
      "pdf_url": "https://arxiv.org/pdf/2601.01679v1.pdf"
    },
    {
      "id": "2601.01678v1",
      "title": "HeurekaBench: A Benchmarking Framework for AI Co-scientist",
      "abstract": "LLM-based reasoning models have enabled the development of agentic systems that act as co-scientists, assisting in multi-step scientific analysis. However, evaluating these systems is challenging, as it requires realistic, end-to-end research scenarios that integrate data analysis, interpretation, and the generation of new insights from the experimental data. To address this limitation, we introduce HeurekaBench, a framework to create benchmarks with exploratory, open-ended research questions for experimental datasets. Each such question is grounded in a scientific study and its corresponding code repository, and is created using a semi-automated pipeline that leverages multiple LLMs to extract insights and generate candidate workflows, which are then verified against reported findings. We instantiate the framework in single-cell biology to obtain sc-HeurekaBench benchmark and use it to compare state-of-the-art single-cell agents. We further showcase the benefits of our benchmark for quantitatively analyzing current design choices in agentic systems. We find that the addition of a critic module can improve ill-formed responses for open-source LLM-based agents by up to 22% and close the gap with their closed-source counterparts. Overall, HeurekaBench sets a path toward rigorous, end-to-end evaluation of scientific agents, grounding benchmark construction in real scientific workflows.",
      "authors": [
        "Siba Smarak Panigrahi",
        "Jovana Videnović",
        "Maria Brbić"
      ],
      "published": "2026-01-04T22:16:42Z",
      "categories": [
        "cs.LG"
      ],
      "primary_category": "cs.LG",
      "abs_url": "https://arxiv.org/abs/2601.01678v1",
      "pdf_url": "https://arxiv.org/pdf/2601.01678v1.pdf"
    },
    {
      "id": "2601.01677v1",
      "title": "Trustworthy Data-Driven Wildfire Risk Prediction and Understanding in Western Canada",
      "abstract": "In recent decades, the intensification of wildfire activity in western Canada has resulted in substantial socio-economic and environmental losses. Accurate wildfire risk prediction is hindered by the intrinsic stochasticity of ignition and spread and by nonlinear interactions among fuel conditions, meteorology, climate variability, topography, and human activities, challenging the reliability and interpretability of purely data-driven models. We propose a trustworthy data-driven wildfire risk prediction framework based on long-sequence, multi-scale temporal modeling, which integrates heterogeneous drivers while explicitly quantifying predictive uncertainty and enabling process-level interpretation. Evaluated over western Canada during the record-breaking 2023 and 2024 fire seasons, the proposed model outperforms existing time-series approaches, achieving an F1 score of 0.90 and a PR-AUC of 0.98 with low computational cost. Uncertainty-aware analysis reveals structured spatial and seasonal patterns in predictive confidence, highlighting increased uncertainty associated with ambiguous predictions and spatiotemporal decision boundaries. SHAP-based interpretation provides mechanistic understanding of wildfire controls, showing that temperature-related drivers dominate wildfire risk in both years, while moisture-related constraints play a stronger role in shaping spatial and land-cover-specific contrasts in 2024 compared to the widespread hot and dry conditions of 2023. Data and code are available at https://github.com/SynUW/mmFire.",
      "authors": [
        "Zhengsen Xu",
        "Lanying Wang",
        "Sibo Cheng",
        "Xue Rui",
        "Kyle Gao",
        "Yimin Zhu",
        "Mabel Heffring",
        "Zack Dewis",
        "Saeid Taleghanidoozdoozan",
        "Megan Greenwood",
        "Motasem Alkayid",
        "Quinn Ledingham",
        "Hongjie He",
        "Jonathan Li",
        "Lincoln Linlin Xu"
      ],
      "published": "2026-01-04T22:05:17Z",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "abs_url": "https://arxiv.org/abs/2601.01677v1",
      "pdf_url": "https://arxiv.org/pdf/2601.01677v1.pdf"
    },
    {
      "id": "2601.01676v1",
      "title": "LabelAny3D: Label Any Object 3D in the Wild",
      "abstract": "Detecting objects in 3D space from monocular input is crucial for applications ranging from robotics to scene understanding. Despite advanced performance in the indoor and autonomous driving domains, existing monocular 3D detection models struggle with in-the-wild images due to the lack of 3D in-the-wild datasets and the challenges of 3D annotation. We introduce LabelAny3D, an \\emph{analysis-by-synthesis} framework that reconstructs holistic 3D scenes from 2D images to efficiently produce high-quality 3D bounding box annotations. Built on this pipeline, we present COCO3D, a new benchmark for open-vocabulary monocular 3D detection, derived from the MS-COCO dataset and covering a wide range of object categories absent from existing 3D datasets. Experiments show that annotations generated by LabelAny3D improve monocular 3D detection performance across multiple benchmarks, outperforming prior auto-labeling approaches in quality. These results demonstrate the promise of foundation-model-driven annotation for scaling up 3D recognition in realistic, open-world settings.",
      "authors": [
        "Jin Yao",
        "Radowan Mahmud Redoy",
        "Sebastian Elbaum",
        "Matthew B. Dwyer",
        "Zezhou Cheng"
      ],
      "published": "2026-01-04T22:03:45Z",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "abs_url": "https://arxiv.org/abs/2601.01676v1",
      "pdf_url": "https://arxiv.org/pdf/2601.01676v1.pdf"
    },
    {
      "id": "2601.01675v1",
      "title": "VisuoTactile 6D Pose Estimation of an In-Hand Object using Vision and Tactile Sensor Data",
      "abstract": "Knowledge of the 6D pose of an object can benefit in-hand object manipulation. In-hand 6D object pose estimation is challenging because of heavy occlusion produced by the robot's grippers, which can have an adverse effect on methods that rely on vision data only. Many robots are equipped with tactile sensors at their fingertips that could be used to complement vision data. In this paper, we present a method that uses both tactile and vision data to estimate the pose of an object grasped in a robot's hand. To address challenges like lack of standard representation for tactile data and sensor fusion, we propose the use of point clouds to represent object surfaces in contact with the tactile sensor and present a network architecture based on pixel-wise dense fusion. We also extend NVIDIA's Deep Learning Dataset Synthesizer to produce synthetic photo-realistic vision data and corresponding tactile point clouds. Results suggest that using tactile data in addition to vision data improves the 6D pose estimate, and our network generalizes successfully from synthetic training to real physical robots.",
      "authors": [
        "Snehal s. Dikhale",
        "Karankumar Patel",
        "Daksh Dhingra",
        "Itoshi Naramura",
        "Akinobu Hayashi",
        "Soshi Iba",
        "Nawid Jamali"
      ],
      "published": "2026-01-04T21:59:34Z",
      "categories": [
        "cs.RO"
      ],
      "primary_category": "cs.RO",
      "abs_url": "https://arxiv.org/abs/2601.01675v1",
      "pdf_url": "https://arxiv.org/pdf/2601.01675v1.pdf"
    },
    {
      "id": "2601.01673v1",
      "title": "Exposing Hidden Interfaces: LLM-Guided Type Inference for Reverse Engineering macOS Private Frameworks",
      "abstract": "Private macOS frameworks underpin critical services and daemons but remain undocumented and distributed only as stripped binaries, complicating security analysis. We present MOTIF, an agentic framework that integrates tool-augmented analysis with a finetuned large language model specialized for Objective-C type inference. The agent manages runtime metadata extraction, binary inspection, and constraint checking, while the model generates candidate method signatures that are validated and refined into compilable headers. On MOTIF-Bench, a benchmark built from public frameworks with groundtruth headers, MOTIF improves signature recovery from 15% to 86% compared to baseline static analysis tooling, with consistent gains in tool-use correctness and inference stability. Case studies on private frameworks show that reconstructed headers compile, link, and facilitate downstream security research and vulnerability studies. By transforming opaque binaries into analyzable interfaces, MOTIF establishes a scalable foundation for systematic auditing of macOS internals.",
      "authors": [
        "Arina Kharlamova",
        "Youcheng Sun",
        "Ting Yu"
      ],
      "published": "2026-01-04T21:44:55Z",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "primary_category": "cs.CR",
      "abs_url": "https://arxiv.org/abs/2601.01673v1",
      "pdf_url": "https://arxiv.org/pdf/2601.01673v1.pdf"
    },
    {
      "id": "2601.01668v1",
      "title": "EHRSummarizer: A Privacy-Aware, FHIR-Native Architecture for Structured Clinical Summarization of Electronic Health Records",
      "abstract": "Clinicians routinely navigate fragmented electronic health record (EHR) interfaces to assemble a coherent picture of a patient's problems, medications, recent encounters, and longitudinal trends. This work describes EHRSummarizer, a privacy-aware, FHIR-native reference architecture that retrieves a targeted set of high-yield FHIR R4 resources, normalizes them into a consistent clinical context package, and produces structured summaries intended to support structured chart review. The system can be configured for data minimization, stateless processing, and flexible deployment, including local inference within an organization's trust boundary. To mitigate the risk of unsupported or unsafe behavior, the summarization stage is constrained to evidence present in the retrieved context package, is intended to indicate missing or unavailable domains where feasible, and avoids diagnostic or treatment recommendations. Prototype demonstrations on synthetic and test FHIR environments illustrate end-to-end behavior and output formats; however, this manuscript does not report clinical outcomes or controlled workflow studies. We outline an evaluation plan centered on faithfulness, omission risk, temporal correctness, usability, and operational monitoring to guide future institutional assessments.",
      "authors": [
        "Houman Kazemzadeh",
        "Nima Minaifar",
        "Kamyar Naderi",
        "Sho Tabibzadeh"
      ],
      "published": "2026-01-04T21:10:42Z",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "abs_url": "https://arxiv.org/abs/2601.01668v1",
      "pdf_url": "https://arxiv.org/pdf/2601.01668v1.pdf"
    },
    {
      "id": "2601.01665v1",
      "title": "Adversarial Instance Generation and Robust Training for Neural Combinatorial Optimization with Multiple Objectives",
      "abstract": "Deep reinforcement learning (DRL) has shown great promise in addressing multi-objective combinatorial optimization problems (MOCOPs). Nevertheless, the robustness of these learning-based solvers has remained insufficiently explored, especially across diverse and complex problem distributions. In this paper, we propose a unified robustness-oriented framework for preference-conditioned DRL solvers for MOCOPs. Within this framework, we develop a preference-based adversarial attack to generate hard instances that expose solver weaknesses, and quantify the attack impact by the resulting degradation on Pareto-front quality. We further introduce a defense strategy that integrates hardness-aware preference selection into adversarial training to reduce overfitting to restricted preference regions and improve out-of-distribution performance. The experimental results on multi-objective traveling salesman problem (MOTSP), multi-objective capacitated vehicle routing problem (MOCVRP), and multi-objective knapsack problem (MOKP) verify that our attack method successfully learns hard instances for different solvers. Furthermore, our defense method significantly strengthens the robustness and generalizability of neural solvers, delivering superior performance on hard or out-of-distribution instances.",
      "authors": [
        "Wei Liu",
        "Yaoxin Wu",
        "Yingqian Zhang",
        "Thomas Bäck",
        "Yingjie Fan"
      ],
      "published": "2026-01-04T20:57:43Z",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "abs_url": "https://arxiv.org/abs/2601.01665v1",
      "pdf_url": "https://arxiv.org/pdf/2601.01665v1.pdf"
    },
    {
      "id": "2601.01664v1",
      "title": "Who is the Winning Algorithm? Rank Aggregation for Comparative Studies",
      "abstract": "Consider a collection of m competing machine learning algorithms. Given their performance on a benchmark of datasets, we would like to identify the best performing algorithm. Specifically, which algorithm is most likely to ``win'' (rank highest) on a future, unseen dataset. The standard maximum likelihood approach suggests counting the number of wins per each algorithm. In this work, we argue that there is much more information in the complete rankings. That is, the number of times that each algorithm finished second, third and so forth. Yet, it is not entirely clear how to effectively utilize this information for our purpose. In this work we introduce a novel conceptual framework for estimating the win probability for each of the m algorithms, given their complete rankings over a benchmark of datasets. Our proposed framework significantly improves upon currently known methods in synthetic and real-world examples.",
      "authors": [
        "Amichai Painsky"
      ],
      "published": "2026-01-04T20:52:47Z",
      "categories": [
        "cs.LG"
      ],
      "primary_category": "cs.LG",
      "abs_url": "https://arxiv.org/abs/2601.01664v1",
      "pdf_url": "https://arxiv.org/pdf/2601.01664v1.pdf"
    },
    {
      "id": "2601.01663v1",
      "title": "Length-Aware Adversarial Training for Variable-Length Trajectories: Digital Twins for Mall Shopper Paths",
      "abstract": "We study generative modeling of \\emph{variable-length trajectories} -- sequences of visited locations/items with associated timestamps -- for downstream simulation and counterfactual analysis. A recurring practical issue is that standard mini-batch training can be unstable when trajectory lengths are highly heterogeneous, which in turn degrades \\emph{distribution matching} for trajectory-derived statistics. We propose \\textbf{length-aware sampling (LAS)}, a simple batching strategy that groups trajectories by length and samples batches from a single length bucket, reducing within-batch length heterogeneity (and making updates more consistent) without changing the model class. We integrate LAS into a conditional trajectory GAN with auxiliary time-alignment losses and provide (i) a distribution-level guarantee for derived variables under mild boundedness assumptions, and (ii) an IPM/Wasserstein mechanism explaining why LAS improves distribution matching by removing length-only shortcut critics and targeting within-bucket discrepancies. Empirically, LAS consistently improves matching of derived-variable distributions on a multi-mall dataset of shopper trajectories and on diverse public sequence datasets (GPS, education, e-commerce, and movies), outperforming random sampling across dataset-specific metrics.",
      "authors": [
        "He Sun",
        "Jiwoong Shin",
        "Ravi Dhar"
      ],
      "published": "2026-01-04T20:52:07Z",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "abs_url": "https://arxiv.org/abs/2601.01663v1",
      "pdf_url": "https://arxiv.org/pdf/2601.01663v1.pdf"
    },
    {
      "id": "2601.01660v1",
      "title": "Animated 3DGS Avatars in Diverse Scenes with Consistent Lighting and Shadows",
      "abstract": "We present a method for consistent lighting and shadows when animated 3D Gaussian Splatting (3DGS) avatars interact with 3DGS scenes or with dynamic objects inserted into otherwise static scenes. Our key contribution is Deep Gaussian Shadow Maps (DGSM), a modern analogue of the classical shadow mapping algorithm tailored to the volumetric 3DGS representation. Building on the classic deep shadow mapping idea, we show that 3DGS admits closed form light accumulation along light rays, enabling volumetric shadow computation without meshing. For each estimated light, we tabulate transmittance over concentric radial shells and store them in octahedral atlases, which modern GPUs can sample in real time per query to attenuate affected scene Gaussians and thus cast and receive shadows consistently. To relight moving avatars, we approximate the local environment illumination with HDRI probes represented in a spherical harmonic (SH) basis and apply a fast per Gaussian radiance transfer, avoiding explicit BRDF estimation or offline optimization. We demonstrate environment consistent lighting for avatars from AvatarX and ActorsHQ, composited into ScanNet++, DL3DV, and SuperSplat scenes, and show interactions with inserted objects. Across single and multi avatar settings, DGSM and SH relighting operate fully in the volumetric 3DGS representation, yielding coherent shadows and relighting while avoiding meshing.",
      "authors": [
        "Aymen Mir",
        "Riza Alp Guler",
        "Jian Wang",
        "Gerard Pons-Moll",
        "Bing Zhou"
      ],
      "published": "2026-01-04T20:42:06Z",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "abs_url": "https://arxiv.org/abs/2601.01660v1",
      "pdf_url": "https://arxiv.org/pdf/2601.01660v1.pdf"
    },
    {
      "id": "2601.01655v1",
      "title": "UniCrop: A Universal, Multi-Source Data Engineering Pipeline for Scalable Crop Yield Prediction",
      "abstract": "Accurate crop yield prediction relies on diverse data streams, including satellite, meteorological, soil, and topographic information. However, despite rapid advances in machine learning, existing approaches remain crop- or region-specific and require data engineering efforts. This limits scalability, reproducibility, and operational deployment. This study introduces UniCrop, a universal and reusable data pipeline designed to automate the acquisition, cleaning, harmonisation, and engineering of multi-source environmental data for crop yield prediction. For any given location, crop type, and temporal window, UniCrop automatically retrieves, harmonises, and engineers over 200 environmental variables (Sentinel-1/2, MODIS, ERA5-Land, NASA POWER, SoilGrids, and SRTM), reducing them to a compact, analysis-ready feature set utilising a structured feature reduction workflow with minimum redundancy maximum relevance (mRMR). To validate, UniCrop was applied to a rice yield dataset comprising 557 field observations. Using only the selected 15 features, four baseline machine learning models (LightGBM, Random Forest, Support Vector Regression, and Elastic Net) were trained. LightGBM achieved the best single-model performance (RMSE = 465.1 kg/ha, $R^2 = 0.6576$), while a constrained ensemble of all baselines further improved accuracy (RMSE = 463.2 kg/ha, $R^2 = 0.6604$). UniCrop contributes a scalable and transparent data-engineering framework that addresses the primary bottleneck in operational crop yield modelling: the preparation of consistent and harmonised multi-source data. By decoupling data specification from implementation and supporting any crop, region, and time frame through simple configuration updates, UniCrop provides a practical foundation for scalable agricultural analytics. The code and implementation documentation are shared in https://github.com/CoDIS-Lab/UniCrop.",
      "authors": [
        "Emiliya Khidirova",
        "Oktay Karakuş"
      ],
      "published": "2026-01-04T20:17:32Z",
      "categories": [
        "eess.IV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "eess.IV",
      "abs_url": "https://arxiv.org/abs/2601.01655v1",
      "pdf_url": "https://arxiv.org/pdf/2601.01655v1.pdf"
    },
    {
      "id": "2601.01653v1",
      "title": "Learning Resilient Elections with Adversarial GNNs",
      "abstract": "In the face of adverse motives, it is indispensable to achieve a consensus. Elections have been the canonical way by which modern democracy has operated since the 17th century. Nowadays, they regulate markets, provide an engine for modern recommender systems or peer-to-peer networks, and remain the main approach to represent democracy. However, a desirable universal voting rule that satisfies all hypothetical scenarios is still a challenging topic, and the design of these systems is at the forefront of mechanism design research. Automated mechanism design is a promising approach, and recent works have demonstrated that set-invariant architectures are uniquely suited to modelling electoral systems. However, various concerns prevent the direct application to real-world settings, such as robustness to strategic voting. In this paper, we generalise the expressive capability of learned voting rules, and combine improvements in neural network architecture with adversarial training to improve the resilience of voting rules while maximizing social welfare. We evaluate the effectiveness of our methods on both synthetic and real-world datasets. Our method resolves critical limitations of prior work regarding learning voting rules by representing elections using bipartite graphs, and learning such voting rules using graph neural networks. We believe this opens new frontiers for applying machine learning to real-world elections.",
      "authors": [
        "Hao Xiang Li",
        "Yash Shah",
        "Lorenzo Giusti"
      ],
      "published": "2026-01-04T20:12:14Z",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.MA",
        "cs.SI"
      ],
      "primary_category": "cs.LG",
      "abs_url": "https://arxiv.org/abs/2601.01653v1",
      "pdf_url": "https://arxiv.org/pdf/2601.01653v1.pdf"
    },
    {
      "id": "2601.01651v1",
      "title": "DemoBot: Efficient Learning of Bimanual Manipulation with Dexterous Hands From Third-Person Human Videos",
      "abstract": "This work presents DemoBot, a learning framework that enables a dual-arm, multi-finger robotic system to acquire complex manipulation skills from a single unannotated RGB-D video demonstration. The method extracts structured motion trajectories of both hands and objects from raw video data. These trajectories serve as motion priors for a novel reinforcement learning (RL) pipeline that learns to refine them through contact-rich interactions, thereby eliminating the need to learn from scratch. To address the challenge of learning long-horizon manipulation skills, we introduce: (1) Temporal-segment based RL to enforce temporal alignment of the current state with demonstrations; (2) Success-Gated Reset strategy to balance the refinement of readily acquired skills and the exploration of subsequent task stages; and (3) Event-Driven Reward curriculum with adaptive thresholding to guide the RL learning of high-precision manipulation. The novel video processing and RL framework successfully achieved long-horizon synchronous and asynchronous bimanual assembly tasks, offering a scalable approach for direct skill acquisition from human videos.",
      "authors": [
        "Yucheng Xu",
        "Xiaofeng Mao",
        "Elle Miller",
        "Xinyu Yi",
        "Yang Li",
        "Zhibin Li",
        "Robert B. Fisher"
      ],
      "published": "2026-01-04T20:06:01Z",
      "categories": [
        "cs.RO"
      ],
      "primary_category": "cs.RO",
      "abs_url": "https://arxiv.org/abs/2601.01651v1",
      "pdf_url": "https://arxiv.org/pdf/2601.01651v1.pdf"
    },
    {
      "id": "2601.01649v1",
      "title": "Communication-Efficient Federated AUC Maximization with Cyclic Client Participation",
      "abstract": "Federated AUC maximization is a powerful approach for learning from imbalanced data in federated learning (FL). However, existing methods typically assume full client availability, which is rarely practical. In real-world FL systems, clients often participate in a cyclic manner: joining training according to a fixed, repeating schedule. This setting poses unique optimization challenges for the non-decomposable AUC objective. This paper addresses these challenges by developing and analyzing communication-efficient algorithms for federated AUC maximization under cyclic client participation. We investigate two key settings: First, we study AUC maximization with a squared surrogate loss, which reformulates the problem as a nonconvex-strongly-concave minimax optimization. By leveraging the Polyak-Łojasiewicz (PL) condition, we establish a state-of-the-art communication complexity of $\\widetilde{O}(1/ε^{1/2})$ and iteration complexity of $\\widetilde{O}(1/ε)$. Second, we consider general pairwise AUC losses. We establish a communication complexity of $O(1/ε^3)$ and an iteration complexity of $O(1/ε^4)$. Further, under the PL condition, these bounds improve to communication complexity of $\\widetilde{O}(1/ε^{1/2})$ and iteration complexity of $\\widetilde{O}(1/ε)$. Extensive experiments on benchmark tasks in image classification, medical imaging, and fraud detection demonstrate the superior efficiency and effectiveness of our proposed methods.",
      "authors": [
        "Umesh Vangapally",
        "Wenhan Wu",
        "Chen Chen",
        "Zhishuai Guo"
      ],
      "published": "2026-01-04T19:57:41Z",
      "categories": [
        "cs.LG",
        "cs.DC"
      ],
      "primary_category": "cs.LG",
      "abs_url": "https://arxiv.org/abs/2601.01649v1",
      "pdf_url": "https://arxiv.org/pdf/2601.01649v1.pdf"
    },
    {
      "id": "2601.01639v1",
      "title": "An Empirical Study of Monocular Human Body Measurement Under Weak Calibration",
      "abstract": "Estimating human body measurements from monocular RGB imagery remains challenging due to scale ambiguity, viewpoint sensitivity, and the absence of explicit depth information. This work presents a systematic empirical study of three weakly calibrated monocular strategies: landmark-based geometry, pose-driven regression, and object-calibrated silhouettes, evaluated under semi-constrained conditions using consumer-grade cameras. Rather than pursuing state-of-the-art accuracy, the study analyzes how differing calibration assumptions influence measurement behavior, robustness, and failure modes across varied body types. The results reveal a clear trade-off between user effort during calibration and the stability of resulting circumferential quantities. This paper serves as an empirical design reference for lightweight monocular human measurement systems intended for deployment on consumer devices.",
      "authors": [
        "Gaurav Sekar"
      ],
      "published": "2026-01-04T19:05:45Z",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "abs_url": "https://arxiv.org/abs/2601.01639v1",
      "pdf_url": "https://arxiv.org/pdf/2601.01639v1.pdf"
    },
    {
      "id": "2601.01627v1",
      "title": "JMedEthicBench: A Multi-Turn Conversational Benchmark for Evaluating Medical Safety in Japanese Large Language Models",
      "abstract": "As Large Language Models (LLMs) are increasingly deployed in healthcare field, it becomes essential to carefully evaluate their medical safety before clinical use. However, existing safety benchmarks remain predominantly English-centric, and test with only single-turn prompts despite multi-turn clinical consultations. To address these gaps, we introduce JMedEthicBench, the first multi-turn conversational benchmark for evaluating medical safety of LLMs for Japanese healthcare. Our benchmark is based on 67 guidelines from the Japan Medical Association and contains over 50,000 adversarial conversations generated using seven automatically discovered jailbreak strategies. Using a dual-LLM scoring protocol, we evaluate 27 models and find that commercial models maintain robust safety while medical-specialized models exhibit increased vulnerability. Furthermore, safety scores decline significantly across conversation turns (median: 9.5 to 5.0, $p < 0.001$). Cross-lingual evaluation on both Japanese and English versions of our benchmark reveals that medical model vulnerabilities persist across languages, indicating inherent alignment limitations rather than language-specific factors. These findings suggest that domain-specific fine-tuning may accidentally weaken safety mechanisms and that multi-turn interactions represent a distinct threat surface requiring dedicated alignment strategies.",
      "authors": [
        "Junyu Liu",
        "Zirui Li",
        "Qian Niu",
        "Zequn Zhang",
        "Yue Xun",
        "Wenlong Hou",
        "Shujun Wang",
        "Yusuke Iwasawa",
        "Yutaka Matsuo",
        "Kan Hatakeyama-Sato"
      ],
      "published": "2026-01-04T18:18:18Z",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "abs_url": "https://arxiv.org/abs/2601.01627v1",
      "pdf_url": "https://arxiv.org/pdf/2601.01627v1.pdf"
    },
    {
      "id": "2601.01624v1",
      "title": "How Does Prefix Matter in Reasoning Model Tuning?",
      "abstract": "Recent alignment studies commonly remove introductory boilerplate phrases from supervised fine-tuning (SFT) datasets. This work challenges that assumption. We hypothesize that safety- and reasoning-oriented prefix sentences serve as lightweight alignment signals that can guide model decoding toward safer and more coherent responses. To examine this, we fine-tune three R1 series models across three core model capabilities: reasoning (mathematics, coding), safety, and factuality, systematically varying prefix inclusion from 0% to 100%.\n  Results show that prefix-conditioned SFT improves both safety and reasoning performance, yielding up to +6% higher Safe@1 accuracy on adversarial benchmarks (WildJailbreak, StrongReject) and +7% improvement on GSM8K reasoning. However, factuality and coding tasks show marginal or negative effects, indicating that prefix-induced narrowing of the search space benefits structured reasoning. Token-level loss analysis further reveals that prefix tokens such as \"revised\" and \"logically\" incur higher gradient magnitudes, acting as alignment anchors that stabilize reasoning trajectories. Our findings suggest that prefix conditioning offers a scalable and interpretable mechanism for improving reasoning safety, serving as an implicit form of alignment that complements traditional reward-based methods.",
      "authors": [
        "Raj Vardhan Tomar",
        "Preslav Nakov",
        "Yuxia Wang"
      ],
      "published": "2026-01-04T18:04:23Z",
      "categories": [
        "cs.CL"
      ],
      "primary_category": "cs.CL",
      "abs_url": "https://arxiv.org/abs/2601.01624v1",
      "pdf_url": "https://arxiv.org/pdf/2601.01624v1.pdf"
    },
    {
      "id": "2601.01620v1",
      "title": "The Gray Area: Characterizing Moderator Disagreement on Reddit",
      "abstract": "Volunteer moderators play a crucial role in sustaining online dialogue, but they often disagree about what should or should not be allowed. In this paper, we study the complexity of content moderation with a focus on disagreements between moderators, which we term the ``gray area'' of moderation. Leveraging 5 years and 4.3 million moderation log entries from 24 subreddits of different topics and sizes, we characterize how gray area, or disputed cases, differ from undisputed cases. We show that one-in-seven moderation cases are disputed among moderators, often addressing transgressions where users' intent is not directly legible, such as in trolling and brigading, as well as tensions around community governance. This is concerning, as almost half of all gray area cases involved automated moderation decisions. Through information-theoretic evaluations, we demonstrate that gray area cases are inherently harder to adjudicate than undisputed cases and show that state-of-the-art language models struggle to adjudicate them. We highlight the key role of expert human moderators in overseeing the moderation process and provide insights about the challenges of current moderation processes and tools.",
      "authors": [
        "Shayan Alipour",
        "Shruti Phadke",
        "Seyed Shahabeddin Mousavi",
        "Amirhossein Afsharrad",
        "Morteza Zihayat",
        "Mattia Samory"
      ],
      "published": "2026-01-04T17:59:33Z",
      "categories": [
        "cs.CY",
        "cs.CL",
        "cs.IT"
      ],
      "primary_category": "cs.CY",
      "abs_url": "https://arxiv.org/abs/2601.01620v1",
      "pdf_url": "https://arxiv.org/pdf/2601.01620v1.pdf"
    },
    {
      "id": "2601.01619v1",
      "title": "Deep Linear Discriminant Analysis Revisited",
      "abstract": "We show that for unconstrained Deep Linear Discriminant Analysis (LDA) classifiers, maximum-likelihood training admits pathological solutions in which class means drift together, covariances collapse, and the learned representation becomes almost non-discriminative. Conversely, cross-entropy training yields excellent accuracy but decouples the head from the underlying generative model, leading to highly inconsistent parameter estimates. To reconcile generative structure with discriminative performance, we introduce the \\emph{Discriminative Negative Log-Likelihood} (DNLL) loss, which augments the LDA log-likelihood with a simple penalty on the mixture density. DNLL can be interpreted as standard LDA NLL plus a term that explicitly discourages regions where several classes are simultaneously likely. Deep LDA trained with DNLL produces clean, well-separated latent spaces, matches the test accuracy of softmax classifiers on synthetic data and standard image benchmarks, and yields substantially better calibrated predictive probabilities, restoring a coherent probabilistic interpretation to deep discriminant models.",
      "authors": [
        "Maxat Tezekbayev",
        "Rustem Takhanov",
        "Arman Bolatov",
        "Zhenisbek Assylbekov"
      ],
      "published": "2026-01-04T17:59:11Z",
      "categories": [
        "stat.ML",
        "cs.LG"
      ],
      "primary_category": "stat.ML",
      "abs_url": "https://arxiv.org/abs/2601.01619v1",
      "pdf_url": "https://arxiv.org/pdf/2601.01619v1.pdf"
    },
    {
      "id": "2601.01618v1",
      "title": "Action-Sketcher: From Reasoning to Action via Visual Sketches for Long-Horizon Robotic Manipulation",
      "abstract": "Long-horizon robotic manipulation is increasingly important for real-world deployment, requiring spatial disambiguation in complex layouts and temporal resilience under dynamic interaction. However, existing end-to-end and hierarchical Vision-Language-Action (VLA) policies often rely on text-only cues while keeping plan intent latent, which undermines referential grounding in cluttered or underspecified scenes, impedes effective task decomposition of long-horizon goals with close-loop interaction, and limits causal explanation by obscuring the rationale behind action choices. To address these issues, we first introduce Visual Sketch, an implausible visual intermediate that renders points, boxes, arrows, and typed relations in the robot's current views to externalize spatial intent, connect language to scene geometry. Building on Visual Sketch, we present Action-Sketcher, a VLA framework that operates in a cyclic See-Think-Sketch-Act workflow coordinated by adaptive token-gated strategy for reasoning triggers, sketch revision, and action issuance, thereby supporting reactive corrections and human interaction while preserving real-time action prediction. To enable scalable training and evaluation, we curate diverse corpus with interleaved images, text, Visual Sketch supervision, and action sequences, and train Action-Sketcher with a multi-stage curriculum recipe that combines interleaved sequence alignment for modality unification, language-to-sketch consistency for precise linguistic grounding, and imitation learning augmented with sketch-to-action reinforcement for robustness. Extensive experiments on cluttered scenes and multi-object tasks, in simulation and on real-world tasks, show improved long-horizon success, stronger robustness to dynamic scene changes, and enhanced interpretability via editable sketches and step-wise plans. Project website: https://action-sketcher.github.io",
      "authors": [
        "Huajie Tan",
        "Peterson Co",
        "Yijie Xu",
        "Shanyu Rong",
        "Yuheng Ji",
        "Cheng Chi",
        "Xiansheng Chen",
        "Qiongyu Zhang",
        "Zhongxia Zhao",
        "Pengwei Wang",
        "Zhongyuan Wang",
        "Shanghang Zhang"
      ],
      "published": "2026-01-04T17:53:42Z",
      "categories": [
        "cs.RO"
      ],
      "primary_category": "cs.RO",
      "abs_url": "https://arxiv.org/abs/2601.01618v1",
      "pdf_url": "https://arxiv.org/pdf/2601.01618v1.pdf"
    },
    {
      "id": "2601.01616v1",
      "title": "Real Time NILM Based Power Monitoring of Identical Induction Motors Representing Cutting Machines in Textile Industry",
      "abstract": "The textile industry in Bangladesh is one of the most energy-intensive sectors, yet its monitoring practices remain largely outdated, resulting in inefficient power usage and high operational costs. To address this, we propose a real-time Non-Intrusive Load Monitoring (NILM)-based framework tailored for industrial applications, with a focus on identical motor-driven loads representing textile cutting machines. A hardware setup comprising voltage and current sensors, Arduino Mega and ESP8266 was developed to capture aggregate and individual load data, which was stored and processed on cloud platforms. A new dataset was created from three identical induction motors and auxiliary loads, totaling over 180,000 samples, to evaluate the state-of-the-art MATNILM model under challenging industrial conditions. Results indicate that while aggregate energy estimation was reasonably accurate, per-appliance disaggregation faced difficulties, particularly when multiple identical machines operated simultaneously. Despite these challenges, the integrated system demonstrated practical real-time monitoring with remote accessibility through the Blynk application. This work highlights both the potential and limitations of NILM in industrial contexts, offering insights into future improvements such as higher-frequency data collection, larger-scale datasets and advanced deep learning approaches for handling identical loads.",
      "authors": [
        "Md Istiauk Hossain Rifat",
        "Moin Khan",
        "Mohammad Zunaed"
      ],
      "published": "2026-01-04T17:51:19Z",
      "categories": [
        "cs.LG",
        "eess.SP"
      ],
      "primary_category": "cs.LG",
      "abs_url": "https://arxiv.org/abs/2601.01616v1",
      "pdf_url": "https://arxiv.org/pdf/2601.01616v1.pdf"
    },
    {
      "id": "2601.01613v1",
      "title": "CAP-IQA: Context-Aware Prompt-Guided CT Image Quality Assessment",
      "abstract": "Prompt-based methods, which encode medical priors through descriptive text, have been only minimally explored for CT Image Quality Assessment (IQA). While such prompts can embed prior knowledge about diagnostic quality, they often introduce bias by reflecting idealized definitions that may not hold under real-world degradations such as noise, motion artifacts, or scanner variability. To address this, we propose the Context-Aware Prompt-guided Image Quality Assessment (CAP-IQA) framework, which integrates text-level priors with instance-level context prompts and applies causal debiasing to separate idealized knowledge from factual, image-specific degradations. Our framework combines a CNN-based visual encoder with a domain-specific text encoder to assess diagnostic visibility, anatomical clarity, and noise perception in abdominal CT images. The model leverages radiology-style prompts and context-aware fusion to align semantic and perceptual representations. On the 2023 LDCTIQA challenge benchmark, CAP-IQA achieves an overall correlation score of 2.8590 (sum of PLCC, SROCC, and KROCC), surpassing the top-ranked leaderboard team (2.7427) by 4.24%. Moreover, our comprehensive ablation experiments confirm that prompt-guided fusion and the simplified encoder-only design jointly enhance feature alignment and interpretability. Furthermore, evaluation on an in-house dataset of 91,514 pediatric CT images demonstrates the true generalizability of CAP-IQA in assessing perceptual fidelity in a different patient population.",
      "authors": [
        "Kazi Ramisa Rifa",
        "Jie Zhang",
        "Abdullah Imran"
      ],
      "published": "2026-01-04T17:30:45Z",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "abs_url": "https://arxiv.org/abs/2601.01613v1",
      "pdf_url": "https://arxiv.org/pdf/2601.01613v1.pdf"
    },
    {
      "id": "2601.01608v1",
      "title": "Guiding Token-Sparse Diffusion Models",
      "abstract": "Diffusion models deliver high quality in image synthesis but remain expensive during training and inference. Recent works have leveraged the inherent redundancy in visual content to make training more affordable by training only on a subset of visual information. While these methods were successful in providing cheaper and more effective training, sparsely trained diffusion models struggle in inference. This is due to their lacking response to Classifier-free Guidance (CFG) leading to underwhelming performance during inference. To overcome this, we propose Sparse Guidance (SG). Instead of using conditional dropout as a signal to guide diffusion models, SG uses token-level sparsity. As a result, SG preserves the high-variance of the conditional prediction better, achieving good quality and high variance outputs. Leveraging token-level sparsity at inference, SG improves fidelity at lower compute, achieving 1.58 FID on the commonly used ImageNet-256 benchmark with 25% fewer FLOPs, and yields up to 58% FLOP savings at matched baseline quality. To demonstrate the effectiveness of Sparse Guidance, we train a 2.5B text-to-image diffusion model using training time sparsity and leverage SG during inference. SG achieves improvements in composition and human preference score while increasing throughput at the same time.",
      "authors": [
        "Felix Krause",
        "Stefan Andreas Baumann",
        "Johannes Schusterbauer",
        "Olga Grebenkova",
        "Ming Gui",
        "Vincent Tao Hu",
        "Björn Ommer"
      ],
      "published": "2026-01-04T17:18:27Z",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "abs_url": "https://arxiv.org/abs/2601.01608v1",
      "pdf_url": "https://arxiv.org/pdf/2601.01608v1.pdf"
    },
    {
      "id": "2601.01605v1",
      "title": "REE-TTT: Highly Adaptive Radar Echo Extrapolation Based on Test-Time Training",
      "abstract": "Precipitation nowcasting is critically important for meteorological forecasting. Deep learning-based Radar Echo Extrapolation (REE) has become a predominant nowcasting approach, yet it suffers from poor generalization due to its reliance on high-quality local training data and static model parameters, limiting its applicability across diverse regions and extreme events. To overcome this, we propose REE-TTT, a novel model that incorporates an adaptive Test-Time Training (TTT) mechanism. The core of our model lies in the newly designed Spatio-temporal Test-Time Training (ST-TTT) block, which replaces the standard linear projections in TTT layers with task-specific attention mechanisms, enabling robust adaptation to non-stationary meteorological distributions and thereby significantly enhancing the feature representation of precipitation. Experiments under cross-regional extreme precipitation scenarios demonstrate that REE-TTT substantially outperforms state-of-the-art baseline models in prediction accuracy and generalization, exhibiting remarkable adaptability to data distribution shifts.",
      "authors": [
        "Xin Di",
        "Xinglin Piao",
        "Fei Wang",
        "Guodong Jing",
        "Yong Zhang"
      ],
      "published": "2026-01-04T17:06:48Z",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "abs_url": "https://arxiv.org/abs/2601.01605v1",
      "pdf_url": "https://arxiv.org/pdf/2601.01605v1.pdf"
    },
    {
      "id": "2601.01594v1",
      "title": "Variance-Reduced Diffusion Sampling via Conditional Score Expectation Identity",
      "abstract": "We introduce and prove a \\textbf{Conditional Score Expectation (CSE)} identity: an exact relation for the marginal score of affine diffusion processes that links scores across time via a conditional expectation under the forward dynamics. Motivated by this identity, we propose a CSE-based statistical estimator for the score using a Self-Normalized Importance Sampling (SNIS) procedure with prior samples and forward noise. We analyze its relationship to the standard Tweedie estimator, proving anti-correlation for Gaussian targets and establishing the same behavior for general targets in the small time-step regime. Exploiting this structure, we derive a variance-minimizing blended score estimator given by a state--time dependent convex combination of the CSE and Tweedie estimators. Numerical experiments show that this optimal-blending estimator reduces variance and improves sample quality for a fixed computational budget compared to either baseline. We further extend the framework to Bayesian inverse problems via likelihood-informed SNIS weights, and demonstrate improved reconstruction quality and sample diversity on high-dimensional image reconstruction tasks and PDE-governed inverse problems.",
      "authors": [
        "Alois Duston",
        "Tan Bui-Thanh"
      ],
      "published": "2026-01-04T16:46:31Z",
      "categories": [
        "stat.ML",
        "cs.LG"
      ],
      "primary_category": "stat.ML",
      "abs_url": "https://arxiv.org/abs/2601.01594v1",
      "pdf_url": "https://arxiv.org/pdf/2601.01594v1.pdf"
    },
    {
      "id": "2601.01593v1",
      "title": "Beyond Patches: Global-aware Autoregressive Model for Multimodal Few-Shot Font Generation",
      "abstract": "Manual font design is an intricate process that transforms a stylistic visual concept into a coherent glyph set. This challenge persists in automated Few-shot Font Generation (FFG), where models often struggle to preserve both the structural integrity and stylistic fidelity from limited references. While autoregressive (AR) models have demonstrated impressive generative capabilities, their application to FFG is constrained by conventional patch-level tokenization, which neglects global dependencies crucial for coherent font synthesis. Moreover, existing FFG methods remain within the image-to-image paradigm, relying solely on visual references and overlooking the role of language in conveying stylistic intent during font design. To address these limitations, we propose GAR-Font, a novel AR framework for multimodal few-shot font generation. GAR-Font introduces a global-aware tokenizer that effectively captures both local structures and global stylistic patterns, a multimodal style encoder offering flexible style control through a lightweight language-style adapter without requiring intensive multimodal pretraining, and a post-refinement pipeline that further enhances structural fidelity and style coherence. Extensive experiments show that GAR-Font outperforms existing FFG methods, excelling in maintaining global style faithfulness and achieving higher-quality results with textual stylistic guidance.",
      "authors": [
        "Haonan Cai",
        "Yuxuan Luo",
        "Zhouhui Lian"
      ],
      "published": "2026-01-04T16:46:13Z",
      "categories": [
        "cs.CV",
        "cs.MM"
      ],
      "primary_category": "cs.CV",
      "abs_url": "https://arxiv.org/abs/2601.01593v1",
      "pdf_url": "https://arxiv.org/pdf/2601.01593v1.pdf"
    },
    {
      "id": "2601.01592v1",
      "title": "OpenRT: An Open-Source Red Teaming Framework for Multimodal LLMs",
      "abstract": "The rapid integration of Multimodal Large Language Models (MLLMs) into critical applications is increasingly hindered by persistent safety vulnerabilities. However, existing red-teaming benchmarks are often fragmented, limited to single-turn text interactions, and lack the scalability required for systematic evaluation. To address this, we introduce OpenRT, a unified, modular, and high-throughput red-teaming framework designed for comprehensive MLLM safety evaluation. At its core, OpenRT architects a paradigm shift in automated red-teaming by introducing an adversarial kernel that enables modular separation across five critical dimensions: model integration, dataset management, attack strategies, judging methods, and evaluation metrics. By standardizing attack interfaces, it decouples adversarial logic from a high-throughput asynchronous runtime, enabling systematic scaling across diverse models. Our framework integrates 37 diverse attack methodologies, spanning white-box gradients, multi-modal perturbations, and sophisticated multi-agent evolutionary strategies. Through an extensive empirical study on 20 advanced models (including GPT-5.2, Claude 4.5, and Gemini 3 Pro), we expose critical safety gaps: even frontier models fail to generalize across attack paradigms, with leading models exhibiting average Attack Success Rates as high as 49.14%. Notably, our findings reveal that reasoning models do not inherently possess superior robustness against complex, multi-turn jailbreaks. By open-sourcing OpenRT, we provide a sustainable, extensible, and continuously maintained infrastructure that accelerates the development and standardization of AI safety.",
      "authors": [
        "Xin Wang",
        "Yunhao Chen",
        "Juncheng Li",
        "Yixu Wang",
        "Yang Yao",
        "Tianle Gu",
        "Jie Li",
        "Yan Teng",
        "Xingjun Ma",
        "Yingchun Wang",
        "Xia Hu"
      ],
      "published": "2026-01-04T16:41:33Z",
      "categories": [
        "cs.CR",
        "cs.CV"
      ],
      "primary_category": "cs.CR",
      "abs_url": "https://arxiv.org/abs/2601.01592v1",
      "pdf_url": "https://arxiv.org/pdf/2601.01592v1.pdf"
    },
    {
      "id": "2601.01590v1",
      "title": "Identifying recurrent flows in high-dimensional dissipative chaos from low-dimensional embeddings",
      "abstract": "Unstable periodic orbits (UPOs) are the non-chaotic, dynamical building blocks of spatio-temporal chaos, motivating a first-principles based theory for turbulence ever since the discovery of deterministic chaos. Despite their key role in the ergodic theory approach to fluid turbulence, identifying UPOs is challenging for two reasons: chaotic dynamics and the high-dimensionality of the spatial discretization. We address both issues at once by proposing a loop convergence algorithm for UPOs directly within a low-dimensional embedding of the chaotic attractor. The convergence algorithm circumvents time-integration, hence avoiding instabilities from exponential error amplification, and operates on a latent dynamics obtained by pulling back the physical equations using automatic differentiation through the learned embedding function. The interpretable latent dynamics is accurate in a statistical sense, and, crucially, the embedding preserves the internal structure of the attractor, which we demonstrate through an equivalence between the latent and physical UPOs of both a model PDE and the 2D Navier-Stokes equations. This allows us to exploit the collapse of high-dimensional dissipative systems onto a lower dimensional manifold, and identify UPOs in the low-dimensional embedding.",
      "authors": [
        "Pierre Beck",
        "Tobias M. Schneider"
      ],
      "published": "2026-01-04T16:35:23Z",
      "categories": [
        "nlin.CD",
        "cs.LG",
        "physics.flu-dyn"
      ],
      "primary_category": "nlin.CD",
      "abs_url": "https://arxiv.org/abs/2601.01590v1",
      "pdf_url": "https://arxiv.org/pdf/2601.01590v1.pdf"
    },
    {
      "id": "2601.01589v1",
      "title": "Learning Relationship between Quantum Walks and Underdamped Langevin Dynamics",
      "abstract": "Fast computational algorithms are in constant demand, and their development has been driven by advances such as quantum speedup and classical acceleration. This paper intends to study search algorithms based on quantum walks in quantum computation and sampling algorithms based on Langevin dynamics in classical computation. On the quantum side, quantum walk-based search algorithms can achieve quadratic speedups over their classical counterparts. In classical computation, a substantial body of work has focused on gradient acceleration, with gradient-adjusted algorithms derived from underdamped Langevin dynamics providing quadratic acceleration over conventional Langevin algorithms.\n  Since both search and sampling algorithms are designed to address learning tasks, we study learning relationship between coined quantum walks and underdamped Langevin dynamics. Specifically, we show that, in terms of the Le Cam deficiency distance, a quantum walk with randomization is asymptotically equivalent to underdamped Langevin dynamics, whereas the quantum walk without randomization is not asymptotically equivalent due to its high-frequency oscillatory behavior. We further discuss the implications of these equivalence and nonequivalence results for the computational and inferential properties of the associated algorithms in machine learning tasks. Our findings offer new insight into the relationship between quantum walks and underdamped Langevin dynamics, as well as the intrinsic mechanisms underlying quantum speedup and classical gradient acceleration.",
      "authors": [
        "Yazhen Wang"
      ],
      "published": "2026-01-04T16:29:47Z",
      "categories": [
        "quant-ph",
        "cs.LG"
      ],
      "primary_category": "quant-ph",
      "abs_url": "https://arxiv.org/abs/2601.01589v1",
      "pdf_url": "https://arxiv.org/pdf/2601.01589v1.pdf"
    },
    {
      "id": "2601.01584v1",
      "title": "Steerability of Instrumental-Convergence Tendencies in LLMs",
      "abstract": "We examine two properties of AI systems: capability (what a system can do) and steerability (how reliably one can shift behavior toward intended outcomes). In our experiments, higher capability does not imply lower steerability. We distinguish between authorized steerability (builders reliably reaching intended behaviors) and unauthorized steerability (attackers eliciting disallowed behaviors). This distinction highlights a fundamental safety--security dilemma for open-weight AI models: safety requires high steerability to enforce control (e.g., stop/refuse), while security requires low steerability to prevent malicious actors from eliciting harmful behaviors. This tension is acute for open-weight models, which are currently highly steerable via common techniques such as fine-tuning and adversarial prompting. Using Qwen3 models (4B/30B; Base/Instruct/Thinking) and InstrumentalEval, we find that a short anti-instrumental prompt suffix sharply reduces outputs labeled as instrumental convergence (e.g., shutdown avoidance, deception, self-replication). For Qwen3-30B Instruct, convergence drops from 81.69% under a pro-instrumental suffix to 2.82% under an anti-instrumental suffix. Under anti-instrumental prompting, larger aligned models produce fewer convergence-labeled outputs than smaller ones (Instruct: 2.82% vs. 4.23%; Thinking: 4.23% vs. 9.86%). Code is available at github.com/j-hoscilowicz/instrumental_steering.",
      "authors": [
        "Jakub Hoscilowicz"
      ],
      "published": "2026-01-04T16:15:59Z",
      "categories": [
        "cs.CL"
      ],
      "primary_category": "cs.CL",
      "abs_url": "https://arxiv.org/abs/2601.01584v1",
      "pdf_url": "https://arxiv.org/pdf/2601.01584v1.pdf"
    },
    {
      "id": "2601.01580v1",
      "title": "The Two-Stage Decision-Sampling Hypothesis: Understanding the Emergence of Self-Reflection in RL-Trained LLMs",
      "abstract": "Self-reflection capabilities emerge in Large Language Models after RL post-training, with multi-turn RL achieving substantial gains over SFT counterparts. Yet the mechanism of how a unified optimization objective gives rise to functionally distinct capabilities of generating solutions and evaluating when to revise them remains opaque. To address this question, we introduce the Gradient Attribution Property to characterize how reward gradients distribute across policy components, formalized through the Two-Stage Decision-Sampling (DS) Hypothesis, which decomposes the policy into sampling ($π_{sample}$) for generation and decision ($π_{d}$) for verification. We prove that surrogate rewards exhibit Balanced Gradient Attribution, while SFT and KL penalties exhibit Unbalanced Gradient Attribution, with length-weighting creating asymmetric regularization that constrains $π_{sample}$ while leaving $π_{d}$ under-optimized, providing an theoretical explanation of why RL succeeds where SFT fails. We also empirically validate our theoretical predictions on arithmetic reasoning demonstrates that RL's superior generalization stems primarily from improved decision-making ($π_{d}$) rather than sampling capabilities, providing a first-principles mechanistic explanation for self-correction in thinking models.",
      "authors": [
        "Zibo Zhao",
        "Yuanting Zha",
        "Haipeng Zhang",
        "Xingcheng Xu"
      ],
      "published": "2026-01-04T15:59:15Z",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "abs_url": "https://arxiv.org/abs/2601.01580v1",
      "pdf_url": "https://arxiv.org/pdf/2601.01580v1.pdf"
    },
    {
      "id": "2601.01577v1",
      "title": "HanoiWorld : A Joint Embedding Predictive Architecture BasedWorld Model for Autonomous Vehicle Controller",
      "abstract": "Current attempts of Reinforcement Learning for Autonomous Controller are data-demanding while the results are under-performed, unstable, and unable to grasp and anchor on the concept of safety, and over-concentrating on noise features due to the nature of pixel reconstruction. While current Self-Supervised Learningapproachs that learning on high-dimensional representations by leveraging the JointEmbedding Predictive Architecture (JEPA) are interesting and an effective alternative, as the idea mimics the natural ability of the human brain in acquiring new skill usingimagination and minimal samples of observations. This study introduces Hanoi-World, a JEPA-based world model that using recurrent neural network (RNN) formaking longterm horizontal planning with effective inference time. Experimentsconducted on the Highway-Env package with difference enviroment showcase the effective capability of making a driving plan while safety-awareness, with considerablecollision rate in comparison with SOTA baselines",
      "authors": [
        "Tran Tien Dat",
        "Nguyen Hai An",
        "Nguyen Khanh Viet Dung",
        "Nguyen Duy Duc"
      ],
      "published": "2026-01-04T15:49:46Z",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "abs_url": "https://arxiv.org/abs/2601.01577v1",
      "pdf_url": "https://arxiv.org/pdf/2601.01577v1.pdf"
    },
    {
      "id": "2601.01576v1",
      "title": "OpenNovelty: An LLM-powered Agentic System for Verifiable Scholarly Novelty Assessment",
      "abstract": "Evaluating novelty is critical yet challenging in peer review, as reviewers must assess submissions against a vast, rapidly evolving literature. This report presents OpenNovelty, an LLM-powered agentic system for transparent, evidence-based novelty analysis. The system operates through four phases: (1) extracting the core task and contribution claims to generate retrieval queries; (2) retrieving relevant prior work based on extracted queries via semantic search engine; (3) constructing a hierarchical taxonomy of core-task-related work and performing contribution-level full-text comparisons against each contribution; and (4) synthesizing all analyses into a structured novelty report with explicit citations and evidence snippets. Unlike naive LLM-based approaches, \\textsc{OpenNovelty} grounds all assessments in retrieved real papers, ensuring verifiable judgments. We deploy our system on 500+ ICLR 2026 submissions with all reports publicly available on our website, and preliminary analysis suggests it can identify relevant prior work, including closely related papers that authors may overlook. OpenNovelty aims to empower the research community with a scalable tool that promotes fair, consistent, and evidence-backed peer review.",
      "authors": [
        "Ming Zhang",
        "Kexin Tan",
        "Yueyuan Huang",
        "Yujiong Shen",
        "Chunchun Ma",
        "Li Ju",
        "Xinran Zhang",
        "Yuhui Wang",
        "Wenqing Jing",
        "Jingyi Deng",
        "Huayu Sha",
        "Binze Hu",
        "Jingqi Tong",
        "Changhao Jiang",
        "Yage Geng",
        "Yuankai Ying",
        "Yue Zhang",
        "Zhangyue Yin",
        "Zhiheng Xi",
        "Shihan Dou",
        "Tao Gui",
        "Qi Zhang",
        "Xuanjing Huang"
      ],
      "published": "2026-01-04T15:48:51Z",
      "categories": [
        "cs.IR",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.IR",
      "abs_url": "https://arxiv.org/abs/2601.01576v1",
      "pdf_url": "https://arxiv.org/pdf/2601.01576v1.pdf"
    },
    {
      "id": "2601.01568v1",
      "title": "MM-Sonate: Multimodal Controllable Audio-Video Generation with Zero-Shot Voice Cloning",
      "abstract": "Joint audio-video generation aims to synthesize synchronized multisensory content, yet current unified models struggle with fine-grained acoustic control, particularly for identity-preserving speech. Existing approaches either suffer from temporal misalignment due to cascaded generation or lack the capability to perform zero-shot voice cloning within a joint synthesis framework. In this work, we present MM-Sonate, a multimodal flow-matching framework that unifies controllable audio-video joint generation with zero-shot voice cloning capabilities. Unlike prior works that rely on coarse semantic descriptions, MM-Sonate utilizes a unified instruction-phoneme input to enforce strict linguistic and temporal alignment. To enable zero-shot voice cloning, we introduce a timbre injection mechanism that effectively decouples speaker identity from linguistic content. Furthermore, addressing the limitations of standard classifier-free guidance in multimodal settings, we propose a noise-based negative conditioning strategy that utilizes natural noise priors to significantly enhance acoustic fidelity. Empirical evaluations demonstrate that MM-Sonate establishes new state-of-the-art performance in joint generation benchmarks, significantly outperforming baselines in lip synchronization and speech intelligibility, while achieving voice cloning fidelity comparable to specialized Text-to-Speech systems.",
      "authors": [
        "Chunyu Qiang",
        "Jun Wang",
        "Xiaopeng Wang",
        "Kang Yin",
        "Yuxin Guo",
        "Xijuan Zeng",
        "Nan Li",
        "Zihan Li",
        "Yuzhe Liang",
        "Ziyu Zhang",
        "Teng Ma",
        "Yushen Chen",
        "Zhongliang Liu",
        "Feng Deng",
        "Chen Zhang",
        "Pengfei Wan"
      ],
      "published": "2026-01-04T15:26:15Z",
      "categories": [
        "cs.SD",
        "cs.AI",
        "cs.CV",
        "cs.MM",
        "eess.AS"
      ],
      "primary_category": "cs.SD",
      "abs_url": "https://arxiv.org/abs/2601.01568v1",
      "pdf_url": "https://arxiv.org/pdf/2601.01568v1.pdf"
    },
    {
      "id": "2601.01561v1",
      "title": "AIMS: An Adaptive Integration of Multi-Sensor Measurements for Quadrupedal Robot Localization",
      "abstract": "This paper addresses the problem of accurate localization for quadrupedal robots operating in narrow tunnel-like environments. Due to the long and homogeneous characteristics of such scenarios, LiDAR measurements often provide weak geometric constraints, making traditional sensor fusion methods susceptible to accumulated motion estimation errors. To address these challenges, we propose AIMS, an adaptive LiDAR-IMU-leg odometry fusion method for robust quadrupedal robot localization in degenerate environments. The proposed method is formulated within an error-state Kalman filtering framework, where LiDAR and leg odometry measurements are integrated with IMU-based state prediction, and measurement noise covariance matrices are adaptively adjusted based on online degeneracy-aware reliability assessment. Experimental results obtained in narrow corridor environments demonstrate that the proposed method improves localization accuracy and robustness compared with state-of-the-art approaches.",
      "authors": [
        "Yujian Qiu",
        "Yuqiu Mu",
        "Wen Yang",
        "Hao Zhu"
      ],
      "published": "2026-01-04T15:21:52Z",
      "categories": [
        "cs.RO"
      ],
      "primary_category": "cs.RO",
      "abs_url": "https://arxiv.org/abs/2601.01561v1",
      "pdf_url": "https://arxiv.org/pdf/2601.01561v1.pdf"
    },
    {
      "id": "2601.01552v1",
      "title": "HalluZig: Hallucination Detection using Zigzag Persistence",
      "abstract": "The factual reliability of Large Language Models (LLMs) remains a critical barrier to their adoption in high-stakes domains due to their propensity to hallucinate. Current detection methods often rely on surface-level signals from the model's output, overlooking the failures that occur within the model's internal reasoning process. In this paper, we introduce a new paradigm for hallucination detection by analyzing the dynamic topology of the evolution of model's layer-wise attention. We model the sequence of attention matrices as a zigzag graph filtration and use zigzag persistence, a tool from Topological Data Analysis, to extract a topological signature. Our core hypothesis is that factual and hallucinated generations exhibit distinct topological signatures. We validate our framework, HalluZig, on multiple benchmarks, demonstrating that it outperforms strong baselines. Furthermore, our analysis reveals that these topological signatures are generalizable across different models and hallucination detection is possible only using structural signatures from partial network depth.",
      "authors": [
        "Shreyas N. Samaga",
        "Gilberto Gonzalez Arroyo",
        "Tamal K. Dey"
      ],
      "published": "2026-01-04T14:55:43Z",
      "categories": [
        "cs.CL"
      ],
      "primary_category": "cs.CL",
      "abs_url": "https://arxiv.org/abs/2601.01552v1",
      "pdf_url": "https://arxiv.org/pdf/2601.01552v1.pdf"
    },
    {
      "id": "2601.01547v1",
      "title": "EscherVerse: An Open World Benchmark and Dataset for Teleo-Spatial Intelligence with Physical-Dynamic and Intent-Driven Understanding",
      "abstract": "The ability to reason about spatial dynamics is a cornerstone of intelligence, yet current research overlooks the human intent behind spatial changes. To address these limitations, we introduce Teleo-Spatial Intelligence (TSI), a new paradigm that unifies two critical pillars: Physical-Dynamic Reasoning--understanding the physical principles of object interactions--and Intent-Driven Reasoning--inferring the human goals behind these actions. To catalyze research in TSI, we present EscherVerse, consisting of a large-scale, open-world benchmark (Escher-Bench), a dataset (Escher-35k), and models (Escher series). Derived from real-world videos, EscherVerse moves beyond constrained settings to explicitly evaluate an agent's ability to reason about object permanence, state transitions, and trajectory prediction in dynamic, human-centric scenarios. Crucially, it is the first benchmark to systematically assess Intent-Driven Reasoning, challenging models to connect physical events to their underlying human purposes. Our work, including a novel data curation pipeline, provides a foundational resource to advance spatial intelligence from passive scene description toward a holistic, purpose-driven understanding of the world.",
      "authors": [
        "Tianjun Gu",
        "Chenghua Gong",
        "Jingyu Gong",
        "Zhizhong Zhang",
        "Yuan Xie",
        "Lizhuang Ma",
        "Xin Tan"
      ],
      "published": "2026-01-04T14:42:39Z",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "abs_url": "https://arxiv.org/abs/2601.01547v1",
      "pdf_url": "https://arxiv.org/pdf/2601.01547v1.pdf"
    },
    {
      "id": "2601.01543v1",
      "title": "Bridging the Data Gap: Creating a Hindi Text Summarization Dataset from the English XSUM",
      "abstract": "Current advancements in Natural Language Processing (NLP) have largely favored resource-rich languages, leaving a significant gap in high-quality datasets for low-resource languages like Hindi. This scarcity is particularly evident in text summarization, where the development of robust models is hindered by a lack of diverse, specialized corpora.\n  To address this disparity, this study introduces a cost-effective, automated framework for creating a comprehensive Hindi text summarization dataset. By leveraging the English Extreme Summarization (XSUM) dataset as a source, we employ advanced translation and linguistic adaptation techniques. To ensure high fidelity and contextual relevance, we utilize the Crosslingual Optimized Metric for Evaluation of Translation (COMET) for validation, supplemented by the selective use of Large Language Models (LLMs) for curation.\n  The resulting dataset provides a diverse, multi-thematic resource that mirrors the complexity of the original XSUM corpus. This initiative not only provides a direct tool for Hindi NLP research but also offers a scalable methodology for democratizing NLP in other underserved languages. By reducing the costs associated with dataset creation, this work fosters the development of more nuanced, culturally relevant models in computational linguistics.",
      "authors": [
        "Praveenkumar Katwe",
        "RakeshChandra Balabantaray",
        "Kaliprasad Vittala"
      ],
      "published": "2026-01-04T14:38:58Z",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "abs_url": "https://arxiv.org/abs/2601.01543v1",
      "pdf_url": "https://arxiv.org/pdf/2601.01543v1.pdf"
    },
    {
      "id": "2601.01541v1",
      "title": "Sim2Real SAR Image Restoration: Metadata-Driven Models for Joint Despeckling and Sidelobes Reduction",
      "abstract": "Synthetic aperture radar (SAR) provides valuable information about the Earth's surface under all weather and illumination conditions. However, the inherent phenomenon of speckle and the presence of sidelobes around bright targets pose challenges for accurate interpretation of SAR imagery. Most existing SAR image restoration methods address despeckling and sidelobes reduction as separate tasks. In this paper, we propose a unified framework that jointly performs both tasks using neural networks (NNs) trained on a realistic SAR simulated dataset generated with MOCEM. Inference can then be performed on real SAR images, demonstrating effective simulation to real (Sim2Real) transferability. Additionally, we incorporate acquisition metadata as auxiliary input to the NNs, demonstrating improved restoration performance.",
      "authors": [
        "Antoine De Paepe",
        "Pascal Nguyen",
        "Michael Mabelle",
        "Cédric Saleun",
        "Antoine Jouadé",
        "Jean-Christophe Louvigne"
      ],
      "published": "2026-01-04T14:32:04Z",
      "categories": [
        "eess.IV",
        "cs.CV",
        "eess.SP"
      ],
      "primary_category": "eess.IV",
      "abs_url": "https://arxiv.org/abs/2601.01541v1",
      "pdf_url": "https://arxiv.org/pdf/2601.01541v1.pdf"
    },
    {
      "id": "2601.01532v1",
      "title": "Aletheia: Quantifying Cognitive Conviction in Reasoning Models via Regularized Inverse Confusion Matrix",
      "abstract": "In the progressive journey toward Artificial General Intelligence (AGI), current evaluation paradigms face an epistemological crisis. Static benchmarks measure knowledge breadth but fail to quantify the depth of belief. While Simhi et al. (2025) defined the CHOKE phenomenon in standard QA, we extend this framework to quantify \"Cognitive Conviction\" in System 2 reasoning models. We propose Project Aletheia, a cognitive physics framework that employs Tikhonov Regularization to invert the judge's confusion matrix. To validate this methodology without relying on opaque private data, we implement a Synthetic Proxy Protocol. Our preliminary pilot study on 2025 baselines (e.g., DeepSeek-R1, OpenAI o1) suggests that while reasoning models act as a \"cognitive buffer,\" they may exhibit \"Defensive OverThinking\" under adversarial pressure. Furthermore, we introduce the Aligned Conviction Score (S_aligned) to verify that conviction does not compromise safety. This work serves as a blueprint for measuring AI scientific integrity.",
      "authors": [
        "Fanzhe Fu"
      ],
      "published": "2026-01-04T13:57:32Z",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "abs_url": "https://arxiv.org/abs/2601.01532v1",
      "pdf_url": "https://arxiv.org/pdf/2601.01532v1.pdf"
    },
    {
      "id": "2601.01530v1",
      "title": "EmoHarbor: Evaluating Personalized Emotional Support by Simulating the User's Internal World",
      "abstract": "Current evaluation paradigms for emotional support conversations tend to reward generic empathetic responses, yet they fail to assess whether the support is genuinely personalized to users' unique psychological profiles and contextual needs. We introduce EmoHarbor, an automated evaluation framework that adopts a User-as-a-Judge paradigm by simulating the user's inner world. EmoHarbor employs a Chain-of-Agent architecture that decomposes users' internal processes into three specialized roles, enabling agents to interact with supporters and complete assessments in a manner similar to human users. We instantiate this benchmark using 100 real-world user profiles that cover a diverse range of personality traits and situations, and define 10 evaluation dimensions of personalized support quality. Comprehensive evaluation of 20 advanced LLMs on EmoHarbor reveals a critical insight: while these models excel at generating empathetic responses, they consistently fail to tailor support to individual user contexts. This finding reframes the central challenge, shifting research focus from merely enhancing generic empathy to developing truly user-aware emotional support. EmoHarbor provides a reproducible and scalable framework to guide the development and evaluation of more nuanced and user-aware emotional support systems.",
      "authors": [
        "Jing Ye",
        "Lu Xiang",
        "Yaping Zhang",
        "Chengqing Zong"
      ],
      "published": "2026-01-04T13:46:51Z",
      "categories": [
        "cs.CL",
        "cs.HC"
      ],
      "primary_category": "cs.CL",
      "abs_url": "https://arxiv.org/abs/2601.01530v1",
      "pdf_url": "https://arxiv.org/pdf/2601.01530v1.pdf"
    },
    {
      "id": "2601.01528v1",
      "title": "DrivingGen: A Comprehensive Benchmark for Generative Video World Models in Autonomous Driving",
      "abstract": "Video generation models, as one form of world models, have emerged as one of the most exciting frontiers in AI, promising agents the ability to imagine the future by modeling the temporal evolution of complex scenes. In autonomous driving, this vision gives rise to driving world models: generative simulators that imagine ego and agent futures, enabling scalable simulation, safe testing of corner cases, and rich synthetic data generation. Yet, despite fast-growing research activity, the field lacks a rigorous benchmark to measure progress and guide priorities. Existing evaluations remain limited: generic video metrics overlook safety-critical imaging factors; trajectory plausibility is rarely quantified; temporal and agent-level consistency is neglected; and controllability with respect to ego conditioning is ignored. Moreover, current datasets fail to cover the diversity of conditions required for real-world deployment. To address these gaps, we present DrivingGen, the first comprehensive benchmark for generative driving world models. DrivingGen combines a diverse evaluation dataset curated from both driving datasets and internet-scale video sources, spanning varied weather, time of day, geographic regions, and complex maneuvers, with a suite of new metrics that jointly assess visual realism, trajectory plausibility, temporal coherence, and controllability. Benchmarking 14 state-of-the-art models reveals clear trade-offs: general models look better but break physics, while driving-specific ones capture motion realistically but lag in visual quality. DrivingGen offers a unified evaluation framework to foster reliable, controllable, and deployable driving world models, enabling scalable simulation, planning, and data-driven decision-making.",
      "authors": [
        "Yang Zhou",
        "Hao Shao",
        "Letian Wang",
        "Zhuofan Zong",
        "Hongsheng Li",
        "Steven L. Waslander"
      ],
      "published": "2026-01-04T13:36:21Z",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.RO"
      ],
      "primary_category": "cs.CV",
      "abs_url": "https://arxiv.org/abs/2601.01528v1",
      "pdf_url": "https://arxiv.org/pdf/2601.01528v1.pdf"
    },
    {
      "id": "2601.01522v1",
      "title": "Bayesian Orchestration of Multi-LLM Agents for Cost-Aware Sequential Decision-Making",
      "abstract": "Large language models (LLMs) are increasingly deployed as autonomous decision agents in settings with asymmetric error costs: hiring (missed talent vs wasted interviews), medical triage (missed emergencies vs unnecessary escalation), and fraud detection (approved fraud vs declined legitimate payments). The dominant design queries a single LLM for a posterior over states, thresholds \"confidence,\" and acts; we prove this is inadequate for sequential decisions with costs. We propose a Bayesian, cost-aware multi-LLM orchestration framework that treats LLMs as approximate likelihood models rather than classifiers. For each candidate state, we elicit likelihoods via contrastive prompting, aggregate across diverse models with robust statistics, and update beliefs with Bayes rule under explicit priors as new evidence arrives. This enables coherent belief updating, expected-cost action selection, principled information gathering via value of information, and fairness gains via ensemble bias mitigation. In resume screening with costs of 40000 USD per missed hire, 2500 USD per interview, and 150 USD per phone screen, experiments on 1000 resumes using five LLMs (GPT-4o, Claude 4.5 Sonnet, Gemini Pro, Grok, DeepSeek) reduce total cost by 294000 USD (34 percent) versus the best single-LLM baseline and improve demographic parity by 45 percent (max group gap 22 to 5 percentage points). Ablations attribute 51 percent of savings to multi-LLM aggregation, 43 percent to sequential updating, and 20 percent to disagreement-triggered information gathering, consistent with the theoretical benefits of correct probabilistic foundations.",
      "authors": [
        "Danial Amin"
      ],
      "published": "2026-01-04T13:19:27Z",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.ET"
      ],
      "primary_category": "cs.AI",
      "abs_url": "https://arxiv.org/abs/2601.01522v1",
      "pdf_url": "https://arxiv.org/pdf/2601.01522v1.pdf"
    },
    {
      "id": "2601.01498v1",
      "title": "From Failure to Mastery: Generating Hard Samples for Tool-use Agents",
      "abstract": "The advancement of LLM agents with tool-use capabilities requires diverse and complex training corpora. Existing data generation methods, which predominantly follow a paradigm of random sampling and shallow generation, often yield simple and homogeneous trajectories that fail to capture complex, implicit logical dependencies. To bridge this gap, we introduce HardGen, an automatic agentic pipeline designed to generate hard tool-use training samples with verifiable reasoning. Firstly, HardGen establishes a dynamic API Graph built upon agent failure cases, from which it samples to synthesize hard traces. Secondly, these traces serve as conditional priors to guide the instantiation of modular, abstract advanced tools, which are subsequently leveraged to formulate hard queries. Finally, the advanced tools and hard queries enable the generation of verifiable complex Chain-of-Thought (CoT), with a closed-loop evaluation feedback steering the continuous refinement of the process. Extensive evaluations demonstrate that a 4B parameter model trained with our curated dataset achieves superior performance compared to several leading open-source and closed-source competitors (e.g., GPT-5.2, Gemini-3-Pro and Claude-Opus-4.5). Our code, models, and dataset will be open-sourced to facilitate future research.",
      "authors": [
        "Bingguang Hao",
        "Zengzhuang Xu",
        "Yuntao Wen",
        "Xinyi Xu",
        "Yang Liu",
        "Tong Zhao",
        "Maolin Wang",
        "Long Chen",
        "Dong Wang",
        "Yicheng Chen",
        "Cunyin Peng",
        "Xiangyu Zhao",
        "Chenyi Zhuang",
        "Ji Zhang"
      ],
      "published": "2026-01-04T11:56:33Z",
      "categories": [
        "cs.CL"
      ],
      "primary_category": "cs.CL",
      "abs_url": "https://arxiv.org/abs/2601.01498v1",
      "pdf_url": "https://arxiv.org/pdf/2601.01498v1.pdf"
    },
    {
      "id": "2601.01490v1",
      "title": "Distortion Instead of Hallucination: The Effect of Reasoning Under Strict Constraints",
      "abstract": "With the widespread adoption of large language models (LLMs), hallucinations, which are non-factual fabrications in model outputs, have become serious concerns. Reasoning capabilities have received attention as a self-verification process to improve output reliability. However, the effect of reasoning within a closed system where LLMs cannot rely on external tools or knowledge has yet to be clarified. We therefore conduct experiments under strict constraints (recommending peer-reviewed journal articles in computer science) to examine the effect of reasoning across multiple models (GPT-5.2 and Gemini 3 Flash). Our results reveal a problematic trade-off between constraint compliance and factual accuracy. Non-reasoning models exhibit high constraint violation rates (66-75%) but maintain factual accuracy, while reasoning models reduce violations (13-26%) but systematically distort known facts to satisfy constraints and increase complete fabrication. This trade-off pattern is consistent across both models despite different architectures, indicating a fundamental limitation of reasoning. Furthermore, reasoning does not uniformly improve output authenticity: effects diverge by model, reflecting different allocations of the compliance-truthfulness trade-off. These findings challenge the assumption that reasoning universally improves reliability: reasoning models trade honest constraint violations for detection-resistant distortions.",
      "authors": [
        "Junichiro Niimi"
      ],
      "published": "2026-01-04T11:35:39Z",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "abs_url": "https://arxiv.org/abs/2601.01490v1",
      "pdf_url": "https://arxiv.org/pdf/2601.01490v1.pdf"
    },
    {
      "id": "2601.01488v1",
      "title": "Four Quadrants of Difficulty: A Simple Categorisation and its Limits",
      "abstract": "Curriculum Learning (CL) aims to improve the outcome of model training by estimating the difficulty of samples and scheduling them accordingly. In NLP, difficulty is commonly approximated using task-agnostic linguistic heuristics or human intuition, implicitly assuming that these signals correlate with what neural models find difficult to learn. We propose a four-quadrant categorisation of difficulty signals -- human vs. model and task-agnostic vs. task-dependent -- and systematically analyse their interactions on a natural language understanding dataset. We find that task-agnostic features behave largely independently and that only task-dependent features align. These findings challenge common CL intuitions and highlight the need for lightweight, task-dependent difficulty estimators that better reflect model learning behaviour.",
      "authors": [
        "Vanessa Toborek",
        "Sebastian Müller",
        "Christian Bauckhage"
      ],
      "published": "2026-01-04T11:31:51Z",
      "categories": [
        "cs.CL",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "abs_url": "https://arxiv.org/abs/2601.01488v1",
      "pdf_url": "https://arxiv.org/pdf/2601.01488v1.pdf"
    },
    {
      "id": "2601.01477v1",
      "title": "Can Legislation Be Made Machine-Readable in PROLEG?",
      "abstract": "The anticipated positive social impact of regulatory processes requires both the accuracy and efficiency of their application. Modern artificial intelligence technologies, including natural language processing and machine-assisted reasoning, hold great promise for addressing this challenge. We present a framework to address the challenge of tools for regulatory application, based on current state-of-the-art (SOTA) methods for natural language processing (large language models or LLMs) and formalization of legal reasoning (the legal representation system PROLEG). As an example, we focus on Article 6 of the European General Data Protection Regulation (GDPR). In our framework, a single LLM prompt simultaneously transforms legal text into if-then rules and a corresponding PROLEG encoding, which are then validated and refined by legal domain experts. The final output is an executable PROLEG program that can produce human-readable explanations for instances of GDPR decisions. We describe processes to support the end-to-end transformation of a segment of a regulatory document (Article 6 from GDPR), including the prompting frame to guide an LLM to \"compile\" natural language text to if-then rules, then to further \"compile\" the vetted if-then rules to PROLEG. Finally, we produce an instance that shows the PROLEG execution. We conclude by summarizing the value of this approach and note observed limitations with suggestions to further develop such technologies for capturing and deploying regulatory frameworks.",
      "authors": [
        "May-Myo Zin",
        "Sabine Wehnert",
        "Yuntao Kong",
        "Ha-Thanh Nguyen",
        "Wachara Fungwacharakorn",
        "Jieying Xue",
        "Michał Araszkiewicz",
        "Randy Goebel",
        "Ken Satoh",
        "Le-Minh Nguyen"
      ],
      "published": "2026-01-04T10:53:16Z",
      "categories": [
        "cs.CL"
      ],
      "primary_category": "cs.CL",
      "abs_url": "https://arxiv.org/abs/2601.01477v1",
      "pdf_url": "https://arxiv.org/pdf/2601.01477v1.pdf"
    },
    {
      "id": "2601.01461v1",
      "title": "Bridging the gap: A comparative exploration of Speech-LLM and end-to-end architecture for multilingual conversational ASR",
      "abstract": "The INTERSPEECH 2025 Challenge on Multilingual Conversational Speech Language Models (MLC-SLM) promotes multilingual conversational ASR with large language models (LLMs). Our previous SHNU-mASR system adopted a competitive parallel-speech-encoder architecture that integrated Whisper and mHuBERT with an LLM. However, it faced two challenges: simple feature concatenation may not fully exploit complementary information, and the performance gap between LLM-based ASR and end-to-end(E2E) encoder-decoder ASR remained unexplored. In this work, we present an enhanced LLM-based ASR framework that combines fine-tuned Whisper and mHuBERT encoders with an LLM to enrich speech representations. We first evaluate E2E Whisper models with LoRA and full fine-tuning on the MLC-SLM ASR task, and then propose cross-attention-based fusion mechanisms for the parallel-speech-encoder. On the official evaluation set of the MLC-SLM Challenge, our system achieves a CER/WER of 10.69%, ranking on par with the top-ranked Track 1 systems, even though it uses only 1,500 hours of baseline training data compared with their large-scale training sets. Nonetheless, we find that our final LLM-based ASR still does not match the performance of a fine-tuned E2E Whisper model, providing valuable empirical guidance for future Speech-LLM design. Our code is publicly available at https://github.com/1535176727/MLC-SLM.",
      "authors": [
        "Yuxiang Mei",
        "Dongxing Xu",
        "Jiaen Liang",
        "Yanhua Long"
      ],
      "published": "2026-01-04T10:08:53Z",
      "categories": [
        "cs.CL",
        "cs.SD",
        "eess.AS"
      ],
      "primary_category": "cs.CL",
      "abs_url": "https://arxiv.org/abs/2601.01461v1",
      "pdf_url": "https://arxiv.org/pdf/2601.01461v1.pdf"
    },
    {
      "id": "2601.01449v1",
      "title": "Segmentation and Processing of German Court Decisions from Open Legal Data",
      "abstract": "The availability of structured legal data is important for advancing Natural Language Processing (NLP) techniques for the German legal system. One of the most widely used datasets, Open Legal Data, provides a large-scale collection of German court decisions. While the metadata in this raw dataset is consistently structured, the decision texts themselves are inconsistently formatted and often lack clearly marked sections. Reliable separation of these sections is important not only for rhetorical role classification but also for downstream tasks such as retrieval and citation analysis. In this work, we introduce a cleaned and sectioned dataset of 251,038 German court decisions derived from the official Open Legal Data dataset. We systematically separated three important sections in German court decisions, namely Tenor (operative part of the decision), Tatbestand (facts of the case), and Entscheidungsgründe (judicial reasoning), which are often inconsistently represented in the original dataset. To ensure the reliability of our extraction process, we used Cochran's formula with a 95% confidence level and a 5% margin of error to draw a statistically representative random sample of 384 cases, and manually verified that all three sections were correctly identified. We also extracted the Rechtsmittelbelehrung (appeal notice) as a separate field, since it is a procedural instruction and not part of the decision itself. The resulting corpus is publicly available in the JSONL format, making it an accessible resource for further research on the German legal system.",
      "authors": [
        "Harshil Darji",
        "Martin Heckelmann",
        "Christina Kratsch",
        "Gerard de Melo"
      ],
      "published": "2026-01-04T09:30:04Z",
      "categories": [
        "cs.CL",
        "cs.IR",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "abs_url": "https://arxiv.org/abs/2601.01449v1",
      "pdf_url": "https://arxiv.org/pdf/2601.01449v1.pdf"
    },
    {
      "id": "2601.01446v1",
      "title": "iFlip: Iterative Feedback-driven Counterfactual Example Refinement",
      "abstract": "Counterfactual examples are minimal edits to an input that alter a model's prediction. They are widely employed in explainable AI to probe model behavior and in natural language processing (NLP) to augment training data. However, generating valid counterfactuals with large language models (LLMs) remains challenging, as existing single-pass methods often fail to induce reliable label changes, neglecting LLMs' self-correction capabilities. To explore this untapped potential, we propose iFlip, an iterative refinement approach that leverages three types of feedback, including model confidence, feature attribution, and natural language. Our results show that iFlip achieves an average 57.8% higher validity than the five state-of-the-art baselines, as measured by the label flipping rate. The user study further corroborates that iFlip outperforms baselines in completeness, overall satisfaction, and feasibility. In addition, ablation studies demonstrate that three components are paramount for iFlip to generate valid counterfactuals: leveraging an appropriate number of iterations, pointing to highly attributed words, and early stopping. Finally, counterfactuals generated by iFlip enable effective counterfactual data augmentation, substantially improving model performance and robustness.",
      "authors": [
        "Yilong Wang",
        "Qianli Wang",
        "Nils Feldhus"
      ],
      "published": "2026-01-04T09:19:45Z",
      "categories": [
        "cs.CL",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "abs_url": "https://arxiv.org/abs/2601.01446v1",
      "pdf_url": "https://arxiv.org/pdf/2601.01446v1.pdf"
    },
    {
      "id": "2601.01438v1",
      "title": "Online Estimation and Manipulation of Articulated Objects",
      "abstract": "From refrigerators to kitchen drawers, humans interact with articulated objects effortlessly every day while completing household chores. For automating these tasks, service robots must be capable of manipulating arbitrary articulated objects. Recent deep learning methods have been shown to predict valuable priors on the affordance of articulated objects from vision. In contrast, many other works estimate object articulations by observing the articulation motion, but this requires the robot to already be capable of manipulating the object. In this article, we propose a novel approach combining these methods by using a factor graph for online estimation of articulation which fuses learned visual priors and proprioceptive sensing during interaction into an analytical model of articulation based on Screw Theory. With our method, a robotic system makes an initial prediction of articulation from vision before touching the object, and then quickly updates the estimate from kinematic and force sensing during manipulation. We evaluate our method extensively in both simulations and real-world robotic manipulation experiments. We demonstrate several closed-loop estimation and manipulation experiments in which the robot was capable of opening previously unseen drawers. In real hardware experiments, the robot achieved a 75% success rate for autonomous opening of unknown articulated objects.",
      "authors": [
        "Russell Buchanan",
        "Adrian Röfer",
        "João Moura",
        "Abhinav Valada",
        "Sethu Vijayakumar"
      ],
      "published": "2026-01-04T08:52:56Z",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "abs_url": "https://arxiv.org/abs/2601.01438v1",
      "pdf_url": "https://arxiv.org/pdf/2601.01438v1.pdf"
    },
    {
      "id": "2601.01426v1",
      "title": "SWE-Lego: Pushing the Limits of Supervised Fine-tuning for Software Issue Resolving",
      "abstract": "We present SWE-Lego, a supervised fine-tuning (SFT) recipe designed to achieve state-ofthe-art performance in software engineering (SWE) issue resolving. In contrast to prevalent methods that rely on complex training paradigms (e.g., mid-training, SFT, reinforcement learning, and their combinations), we explore how to push the limits of a lightweight SFT-only approach for SWE tasks. SWE-Lego comprises three core building blocks, with key findings summarized as follows: 1) the SWE-Lego dataset, a collection of 32k highquality task instances and 18k validated trajectories, combining real and synthetic data to complement each other in both quality and quantity; 2) a refined SFT procedure with error masking and a difficulty-based curriculum, which demonstrably improves action quality and overall performance. Empirical results show that with these two building bricks alone,the SFT can push SWE-Lego models to state-of-the-art performance among open-source models of comparable size on SWE-bench Verified: SWE-Lego-Qwen3-8B reaches 42.2%, and SWE-Lego-Qwen3-32B attains 52.6%. 3) We further evaluate and improve test-time scaling (TTS) built upon the SFT foundation. Based on a well-trained verifier, SWE-Lego models can be significantly boosted--for example, 42.2% to 49.6% and 52.6% to 58.8% under TTS@16 for the 8B and 32B models, respectively.",
      "authors": [
        "Chaofan Tao",
        "Jierun Chen",
        "Yuxin Jiang",
        "Kaiqi Kou",
        "Shaowei Wang",
        "Ruoyu Wang",
        "Xiaohui Li",
        "Sidi Yang",
        "Yiming Du",
        "Jianbo Dai",
        "Zhiming Mao",
        "Xinyu Wang",
        "Lifeng Shang",
        "Haoli Bai"
      ],
      "published": "2026-01-04T08:07:27Z",
      "categories": [
        "cs.SE",
        "cs.CL"
      ],
      "primary_category": "cs.SE",
      "abs_url": "https://arxiv.org/abs/2601.01426v1",
      "pdf_url": "https://arxiv.org/pdf/2601.01426v1.pdf"
    },
    {
      "id": "2601.01409v1",
      "title": "Sampling Strategy Design for Model Predictive Path Integral Control on Legged Robot Locomotion",
      "abstract": "Model Predictive Path Integral (MPPI) control has emerged as a powerful sampling-based optimal control method for complex, nonlinear, and high-dimensional systems. However, directly applying MPPI to legged robotic systems presents several challenges. This paper systematically investigates the role of sampling strategy design within the MPPI framework for legged robot locomotion. Based upon the idea of structured control parameterization, we explore and compare multiple sampling strategies within the framework, including both unstructured and spline-based approaches. Through extensive simulations on a quadruped robot platform, we evaluate how different sampling strategies affect control smoothness, task performance, robustness, and sample efficiency. The results provide new insights into the practical implications of sampling design for deploying MPPI on complex legged systems.",
      "authors": [
        "Chuyuan Tao",
        "Fanxin Wang",
        "Haolong Jiang",
        "Jia He",
        "Yiyang Chen",
        "Qinglei Bu"
      ],
      "published": "2026-01-04T07:25:37Z",
      "categories": [
        "eess.SY",
        "cs.RO"
      ],
      "primary_category": "eess.SY",
      "abs_url": "https://arxiv.org/abs/2601.01409v1",
      "pdf_url": "https://arxiv.org/pdf/2601.01409v1.pdf"
    },
    {
      "id": "2601.01407v1",
      "title": "From Emotion Classification to Emotional Reasoning: Enhancing Emotional Intelligence in Large Language Models",
      "abstract": "This work investigates whether synthetic emotional chain-of-thought data can improve the emotional reasoning abilities of smaller open large language models (LLMs). We design a multi-agent generation pipeline that produces therapy-style conversations and converts them into structured emotion multiple-choice questions (MCQs) with explanations. We propose that fine-tuning a variety of 7B models on this dataset should yield substantial gains in emotional understanding and emotional awareness on EmoBench-style evaluations, suggesting that emotional reasoning can be induced without architectural changes. Our results demonstrate that fine-tuned Mistral 7B achieves EU improvements from 10.5 to 20.5 and EA improvements from 40.5 to 60.0, validating the effectiveness of synthetic emotional reasoning data for enhancing model capabilities in nuanced emotional tasks.",
      "authors": [
        "Arjhun Sreedar",
        "Rohan Pillay",
        "Laukik Patade"
      ],
      "published": "2026-01-04T07:08:37Z",
      "categories": [
        "cs.CL"
      ],
      "primary_category": "cs.CL",
      "abs_url": "https://arxiv.org/abs/2601.01407v1",
      "pdf_url": "https://arxiv.org/pdf/2601.01407v1.pdf"
    },
    {
      "id": "2601.01401v1",
      "title": "LANCET: Neural Intervention via Structural Entropy for Mitigating Faithfulness Hallucinations in LLMs",
      "abstract": "Large Language Models have revolutionized information processing, yet their reliability is severely compromised by faithfulness hallucinations. While current approaches attempt to mitigate this issue through node-level adjustments or coarse suppression, they often overlook the distributed nature of neural information, leading to imprecise interventions. Recognizing that hallucinations propagate through specific forward transmission pathways like an infection, we aim to surgically block this flow using precise structural analysis. To leverage this, we propose Lancet, a novel framework that achieves precise neural intervention by leveraging structural entropy and hallucination difference ratios. Lancet first locates hallucination-prone neurons via gradient-driven contrastive analysis, then maps their propagation pathways by minimizing structural entropy, and finally implements a hierarchical intervention strategy that preserves general model capabilities. Comprehensive evaluations across hallucination benchmark datasets demonstrate that Lancet significantly outperforms state-of-the-art methods, validating the effectiveness of our surgical approach to neural intervention.",
      "authors": [
        "Chenxu Wang",
        "Chaozhuo Li",
        "Pengbo Wang",
        "Litian Zhang",
        "Songyang Liu",
        "Ji Qi",
        "Jiahui Hu",
        "Yushan Cai",
        "Hao Zhao",
        "Rui Pu"
      ],
      "published": "2026-01-04T06:41:28Z",
      "categories": [
        "cs.CL",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "abs_url": "https://arxiv.org/abs/2601.01401v1",
      "pdf_url": "https://arxiv.org/pdf/2601.01401v1.pdf"
    },
    {
      "id": "2601.01400v1",
      "title": "EternalMath: A Living Benchmark of Frontier Mathematics that Evolves with Human Discovery",
      "abstract": "Current evaluations of mathematical reasoning in large language models (LLMs) are dominated by static benchmarks, either derived from competition-style problems or curated through costly expert effort, resulting in limited coverage of research-level mathematics and rapid performance saturation. We propose a fully automated, theorem-grounded pipeline for evaluating frontier mathematical reasoning, which directly transforms recent peer-reviewed mathematical literature into executable and verifiable reasoning tasks. The pipeline identifies constructive or quantitative results, instantiates them into parameterized problem templates, and generates deterministic solutions through execution-based verification, enabling scalable, reproducible, and continuously updatable evaluation without reliance on large-scale expert authoring. By design, this approach supports temporal extensibility, intrinsic correctness checking, and domain-specific customization across mathematical subfields. Applying this pipeline yields \\textbf{EternalMath}, an evolving evaluation suite derived from contemporary research papers. Experiments with state-of-the-art LLMs reveal substantial performance gaps, indicating that mathematical reasoning at the research frontier remains far from saturated and underscoring the need for evaluation methodologies that evolve in step with human mathematical discovery.",
      "authors": [
        "Jicheng Ma",
        "Guohua Wang",
        "Xinhua Feng",
        "Yiming Liu",
        "Zhichao Hu",
        "Yuhong Liu"
      ],
      "published": "2026-01-04T06:40:25Z",
      "categories": [
        "cs.CL"
      ],
      "primary_category": "cs.CL",
      "abs_url": "https://arxiv.org/abs/2601.01400v1",
      "pdf_url": "https://arxiv.org/pdf/2601.01400v1.pdf"
    },
    {
      "id": "2601.01392v1",
      "title": "SAFE-QAQ: End-to-End Slow-Thinking Audio-Text Fraud Detection via Reinforcement Learning",
      "abstract": "Existing fraud detection methods predominantly rely on transcribed text, suffering from ASR errors and missing crucial acoustic cues like vocal tone and environmental context. This limits their effectiveness against complex deceptive strategies. To address these challenges, we first propose \\textbf{SAFE-QAQ}, an end-to-end comprehensive framework for audio-based slow-thinking fraud detection. First, the SAFE-QAQ framework eliminates the impact of transcription errors on detection performance. Secondly, we propose rule-based slow-thinking reward mechanisms that systematically guide the system to identify fraud-indicative patterns by accurately capturing fine-grained audio details, through hierarchical reasoning processes. Besides, our framework introduces a dynamic risk assessment framework during live calls, enabling early detection and prevention of fraud. Experiments on the TeleAntiFraud-Bench demonstrate that SAFE-QAQ achieves dramatic improvements over existing methods in multiple key dimensions, including accuracy, inference efficiency, and real-time processing capabilities. Currently deployed and analyzing over 70,000 calls daily, SAFE-QAQ effectively automates complex fraud detection, reducing human workload and financial losses. Code: https://anonymous.4open.science/r/SAFE-QAQ.",
      "authors": [
        "Peidong Wang",
        "Zhiming Ma",
        "Xin Dai",
        "Yongkang Liu",
        "Shi Feng",
        "Xiaocui Yang",
        "Wenxing Hu",
        "Zhihao Wang",
        "Mingjun Pan",
        "Li Yuan",
        "Daling Wang"
      ],
      "published": "2026-01-04T06:09:07Z",
      "categories": [
        "cs.SD",
        "cs.CL",
        "eess.AS"
      ],
      "primary_category": "cs.SD",
      "abs_url": "https://arxiv.org/abs/2601.01392v1",
      "pdf_url": "https://arxiv.org/pdf/2601.01392v1.pdf"
    },
    {
      "id": "2601.01362v1",
      "title": "Investigating the Multilingual Calibration Effects of Language Model Instruction-Tuning",
      "abstract": "Ensuring that deep learning models are well-calibrated in terms of their predictive uncertainty is essential in maintaining their trustworthiness and reliability, yet despite increasing advances in foundation model research, the relationship between such large language models (LLMs) and their calibration remains an open area of research. In this work, we look at a critical gap in the calibration of LLMs within multilingual settings, in an attempt to better understand how the data scarcity can potentially lead to different calibration effects and how commonly used techniques can apply in these settings. Our analysis on two multilingual benchmarks, over 29 and 42 languages respectively, reveals that even in low-resource languages, model confidence can increase significantly after instruction-tuning on high-resource language SFT datasets. However, improvements in accuracy are marginal or non-existent, resulting in mis-calibration, highlighting a critical shortcoming of standard SFT for multilingual languages. Furthermore, we observe that the use of label smoothing to be a reasonable method alleviate this concern, again without any need for low-resource SFT data, maintaining better calibration across all languages. Overall, this highlights the importance of multilingual considerations for both training and tuning LLMs in order to improve their reliability and fairness in downstream use.",
      "authors": [
        "Jerry Huang",
        "Peng Lu",
        "Qiuhao Zeng",
        "Yusuke Iwasawa",
        "Yutaka Matsuo",
        "Sarath Chandar",
        "Edison Marrese-Taylor",
        "Irene Li"
      ],
      "published": "2026-01-04T04:29:12Z",
      "categories": [
        "cs.CL",
        "cs.LG",
        "stat.ML"
      ],
      "primary_category": "cs.CL",
      "abs_url": "https://arxiv.org/abs/2601.01362v1",
      "pdf_url": "https://arxiv.org/pdf/2601.01362v1.pdf"
    },
    {
      "id": "2601.01350v1",
      "title": "FC-CONAN: An Exhaustively Paired Dataset for Robust Evaluation of Retrieval Systems",
      "abstract": "Hate speech (HS) is a critical issue in online discourse, and one promising strategy to counter it is through the use of counter-narratives (CNs). Datasets linking HS with CNs are essential for advancing counterspeech research. However, even flagship resources like CONAN (Chung et al., 2019) annotate only a sparse subset of all possible HS-CN pairs, limiting evaluation. We introduce FC-CONAN (Fully Connected CONAN), the first dataset created by exhaustively considering all combinations of 45 English HS messages and 129 CNs. A two-stage annotation process involving nine annotators and four validators produces four partitions-Diamond, Gold, Silver, and Bronze-that balance reliability and scale. None of the labeled pairs overlap with CONAN, uncovering hundreds of previously unlabelled positives. FC-CONAN enables more faithful evaluation of counterspeech retrieval systems and facilitates detailed error analysis. The dataset is publicly available.",
      "authors": [
        "Juan Junqueras",
        "Florian Boudin",
        "May-Myo Zin",
        "Ha-Thanh Nguyen",
        "Wachara Fungwacharakorn",
        "Damián Ariel Furman",
        "Akiko Aizawa",
        "Ken Satoh"
      ],
      "published": "2026-01-04T03:38:46Z",
      "categories": [
        "cs.CL"
      ],
      "primary_category": "cs.CL",
      "abs_url": "https://arxiv.org/abs/2601.01350v1",
      "pdf_url": "https://arxiv.org/pdf/2601.01350v1.pdf"
    },
    {
      "id": "2601.01341v1",
      "title": "Reasoning Over Recall: Evaluating the Efficacy of Generalist Architectures vs. Specialized Fine-Tunes in RAG-Based Mental Health Dialogue Systems",
      "abstract": "The deployment of Large Language Models (LLMs) in mental health counseling faces the dual challenges of hallucinations and lack of empathy. While the former may be mitigated by RAG (retrieval-augmented generation) by anchoring answers in trusted clinical sources, there remains an open question as to whether the most effective model under this paradigm would be one that is fine-tuned on mental health data, or a more general and powerful model that succeeds purely on the basis of reasoning. In this paper, we perform a direct comparison by running four open-source models through the same RAG pipeline using ChromaDB: two generalist reasoners (Qwen2.5-3B and Phi-3-Mini) and two domain-specific fine-tunes (MentalHealthBot-7B and TherapyBot-7B). We use an LLM-as-a-Judge framework to automate evaluation over 50 turns. We find a clear trend: the generalist models outperform the domain-specific ones in empathy (3.72 vs. 3.26, $p < 0.001$) in spite of being much smaller (3B vs. 7B), and all models perform well in terms of safety, but the generalist models show better contextual understanding and are less prone to overfitting as we observe in the domain-specific models. Overall, our results indicate that for RAG-based therapy systems, strong reasoning is more important than training on mental health-specific vocabulary; i.e. a well-reasoned general model would provide more empathetic and balanced support than a larger narrowly fine-tuned model, so long as the answer is already grounded in clinical evidence.",
      "authors": [
        "Md Abdullah Al Kafi",
        "Raka Moni",
        "Sumit Kumar Banshal"
      ],
      "published": "2026-01-04T03:09:23Z",
      "categories": [
        "cs.CL"
      ],
      "primary_category": "cs.CL",
      "abs_url": "https://arxiv.org/abs/2601.01341v1",
      "pdf_url": "https://arxiv.org/pdf/2601.01341v1.pdf"
    },
    {
      "id": "2601.01332v1",
      "title": "FLOP-Efficient Training: Early Stopping Based on Test-Time Compute Awareness",
      "abstract": "Scaling training compute, measured in FLOPs, has long been shown to improve the accuracy of large language models, yet training remains resource-intensive. Prior work shows that increasing test-time compute (TTC)-for example through iterative sampling-can allow smaller models to rival or surpass much larger ones at lower overall cost. We introduce TTC-aware training, where an intermediate checkpoint and a corresponding TTC configuration can together match or exceed the accuracy of a fully trained model while requiring substantially fewer training FLOPs. Building on this insight, we propose an early stopping algorithm that jointly selects a checkpoint and TTC configuration to minimize training compute without sacrificing accuracy. To make this practical, we develop an efficient TTC evaluation method that avoids exhaustive search, and we formalize a break-even bound that identifies when increased inference compute compensates for reduced training compute. Experiments demonstrate up to 92\\% reductions in training FLOPs while maintaining and sometimes remarkably improving accuracy. These results highlight a new perspective for balancing training and inference compute in model development, enabling faster deployment cycles and more frequent model refreshes. Codes will be publicly released.",
      "authors": [
        "Hossam Amer",
        "Maryam Dialameh",
        "Hossein Rajabzadeh",
        "Walid Ahmed",
        "Weiwei Zhang",
        "Yang Liu"
      ],
      "published": "2026-01-04T02:33:30Z",
      "categories": [
        "cs.CL",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "abs_url": "https://arxiv.org/abs/2601.01332v1",
      "pdf_url": "https://arxiv.org/pdf/2601.01332v1.pdf"
    },
    {
      "id": "2601.01331v1",
      "title": "AppellateGen: A Benchmark for Appellate Legal Judgment Generation",
      "abstract": "Legal judgment generation is a critical task in legal intelligence. However, existing research in legal judgment generation has predominantly focused on first-instance trials, relying on static fact-to-verdict mappings while neglecting the dialectical nature of appellate (second-instance) review. To address this, we introduce AppellateGen, a benchmark for second-instance legal judgment generation comprising 7,351 case pairs. The task requires models to draft legally binding judgments by reasoning over the initial verdict and evidentiary updates, thereby modeling the causal dependency between trial stages. We further propose a judicial Standard Operating Procedure (SOP)-based Legal Multi-Agent System (SLMAS) to simulate judicial workflows, which decomposes the generation process into discrete stages of issue identification, retrieval, and drafting. Experimental results indicate that while SLMAS improves logical consistency, the complexity of appellate reasoning remains a substantial challenge for current LLMs. The dataset and code are publicly available at: https://anonymous.4open.science/r/AppellateGen-5763.",
      "authors": [
        "Hongkun Yang",
        "Lionel Z. Wang",
        "Wei Fan",
        "Yiran Hu",
        "Lixu Wang",
        "Chenyu Liu",
        "Shenghong Fu",
        "Haoyang Li",
        "Xin Xu",
        "Jiexin Zheng",
        "Wei Dong"
      ],
      "published": "2026-01-04T02:15:17Z",
      "categories": [
        "cs.CY",
        "cs.CL",
        "cs.LG"
      ],
      "primary_category": "cs.CY",
      "abs_url": "https://arxiv.org/abs/2601.01331v1",
      "pdf_url": "https://arxiv.org/pdf/2601.01331v1.pdf"
    },
    {
      "id": "2601.01299v1",
      "title": "T3C: Test-Time Tensor Compression with Consistency Guarantees",
      "abstract": "We present T3C, a train-once, test-time budget-conditioned compression framework that exposes rank and precision as a controllable deployment knob. T3C combines elastic tensor factorization (maintained up to a maximal rank) with rank-tied mixed-precision quantization and a lightweight controller that maps a latency/energy/size budget token to per-layer rank/bit assignments; the policy snaps to hardware-aligned profiles and is monotone in the budget. A fast, layerwise consistency certificate, computed from spectral proxies and activation statistics, upper-bounds logit drift and regularizes training, yielding a practical reliability signal with negligible overhead. On ImageNet-1k, T3C shifts the vision Pareto frontier: for ResNet-50 at matched accuracy (\\leq 0.5% drop), p50 latency is 1.18ms with a 38MB model, outperforming PTQ-8b (1.44ms, 88MB); for ViT-B/16, T3C reaches 2.30ms p50 with 59MB, improving over strong PTQ/QAT baselines. A single T3C checkpoint therefore provides predictable, certificate-backed accuracy-latency-size trade-offs on demand across devices.",
      "authors": [
        "Ismail Lamaakal",
        "Chaymae Yahyati",
        "Yassine Maleh",
        "Khalid El Makkaoui",
        "Ibrahim Ouahbi"
      ],
      "published": "2026-01-03T23:16:27Z",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.CL",
      "abs_url": "https://arxiv.org/abs/2601.01299v1",
      "pdf_url": "https://arxiv.org/pdf/2601.01299v1.pdf"
    },
    {
      "id": "2601.01297v1",
      "title": "ARGUS: Adaptive Rotation-Invariant Geometric Unsupervised System",
      "abstract": "Detecting distributional drift in high-dimensional data streams presents fundamental challenges: global comparison methods scale poorly, projection-based approaches lose geometric structure, and re-clustering methods suffer from identity instability. This paper introduces Argus, A framework that reconceptualizes drift detection as tracking local statistics over a fixed spatial partition of the data manifold.\n  The key contributions are fourfold. First, it is proved that Voronoi tessellations over canonical orthonormal frames yield drift metrics that are invariant to orthogonal transformations. The rotations and reflections that preserve Euclidean geometry. Second, it is established that this framework achieves O(N) complexity per snapshot while providing cell-level spatial localization of distributional change. Third, a graph-theoretic characterization of drift propagation is developed that distinguishes coherent distributional shifts from isolated perturbations. Fourth, product quantization tessellation is introduced for scaling to very high dimensions (d>500) by decomposing the space into independent subspaces and aggregating drift signals across subspaces.\n  This paper formalizes the theoretical foundations, proves invariance properties, and presents experimental validation demonstrating that the framework correctly identifies drift under coordinate rotation while existing methods produce false positives. The tessellated approach offers a principled geometric foundation for distribution monitoring that preserves high-dimensional structure without the computational burden of pairwise comparisons.",
      "authors": [
        "Anantha Sharma"
      ],
      "published": "2026-01-03T22:39:20Z",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.LG",
      "abs_url": "https://arxiv.org/abs/2601.01297v1",
      "pdf_url": "https://arxiv.org/pdf/2601.01297v1.pdf"
    },
    {
      "id": "2601.01288v1",
      "title": "PyBatchRender: A Python Library for Batched 3D Rendering at Up to One Million FPS",
      "abstract": "Reinforcement learning from pixels is often bottlenecked by the performance and complexity of 3D rendered environments. Researchers face a trade-off between high-speed, low-level engines and slower, more accessible Python frameworks. To address this, we introduce PyBatchRender, a Python library for high-throughput, batched 3D rendering that achieves over 1 million FPS on simple scenes. Built on the Panda3D game engine, it utilizes its mature ecosystem while enhancing performance through optimized batched rendering for up to 1000X speedups. Designed as a physics-agnostic renderer for reinforcement learning from pixels, PyBatchRender offers greater flexibility than dedicated libraries, simpler setup than typical game-engine wrappers, and speeds rivaling state-of-the-art C++ engines like Madrona. Users can create custom scenes entirely in Python with tens of lines of code, enabling rapid prototyping for scalable AI training. Open-source and easy to integrate, it serves to democratize high-performance 3D simulation for researchers and developers. The library is available at https://github.com/dolphin-in-a-coma/PyBatchRender.",
      "authors": [
        "Evgenii Rudakov",
        "Jonathan Shock",
        "Benjamin Ultan Cowley"
      ],
      "published": "2026-01-03T21:19:57Z",
      "categories": [
        "cs.GR",
        "cs.AI",
        "cs.PF",
        "cs.RO"
      ],
      "primary_category": "cs.GR",
      "abs_url": "https://arxiv.org/abs/2601.01288v1",
      "pdf_url": "https://arxiv.org/pdf/2601.01288v1.pdf"
    },
    {
      "id": "2601.01282v1",
      "title": "SAHA: Supervised Autonomous HArvester for selective forest thinning",
      "abstract": "Forestry plays a vital role in our society, creating significant ecological, economic, and recreational value. Efficient forest management involves labor-intensive and complex operations. One essential task for maintaining forest health and productivity is selective thinning, which requires skilled operators to remove specific trees to create optimal growing conditions for the remaining ones. In this work, we present a solution based on a small-scale robotic harvester (SAHA) designed for executing this task with supervised autonomy. We build on a 4.5-ton harvester platform and implement key hardware modifications for perception and automatic control. We implement learning- and model-based approaches for precise control of hydraulic actuators, accurate navigation through cluttered environments, robust state estimation, and reliable semantic estimation of terrain traversability. Integrating state-of-the-art techniques in perception, planning, and control, our robotic harvester can autonomously navigate forest environments and reach targeted trees for selective thinning. We present experimental results from extensive field trials over kilometer-long autonomous missions in northern European forests, demonstrating the harvester's ability to operate in real forests. We analyze the performance and provide the lessons learned for advancing robotic forest management.",
      "authors": [
        "Fang Nan",
        "Meher Malladi",
        "Qingqing Li",
        "Fan Yang",
        "Joonas Juola",
        "Tiziano Guadagnino",
        "Jens Behley",
        "Cesar Cadena",
        "Cyrill Stachniss",
        "Marco Hutter"
      ],
      "published": "2026-01-03T20:48:10Z",
      "categories": [
        "cs.RO"
      ],
      "primary_category": "cs.RO",
      "abs_url": "https://arxiv.org/abs/2601.01282v1",
      "pdf_url": "https://arxiv.org/pdf/2601.01282v1.pdf"
    },
    {
      "id": "2601.01280v1",
      "title": "Does Memory Need Graphs? A Unified Framework and Empirical Analysis for Long-Term Dialog Memory",
      "abstract": "Graph structures are increasingly used in dialog memory systems, but empirical findings on their effectiveness remain inconsistent, making it unclear which design choices truly matter. We present an experimental, system-oriented analysis of long-term dialog memory architectures. We introduce a unified framework that decomposes dialog memory systems into core components and supports both graph-based and non-graph approaches. Under this framework, we conduct controlled, stage-wise experiments on LongMemEval and HaluMem, comparing common design choices in memory representation, organization, maintenance, and retrieval. Our results show that many performance differences are driven by foundational system settings rather than specific architectural innovations. Based on these findings, we identify stable and reliable strong baselines for future dialog memory research.",
      "authors": [
        "Sen Hu",
        "Yuxiang Wei",
        "Jiaxin Ran",
        "Zhiyuan Yao",
        "Lei Zou"
      ],
      "published": "2026-01-03T20:39:39Z",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "abs_url": "https://arxiv.org/abs/2601.01280v1",
      "pdf_url": "https://arxiv.org/pdf/2601.01280v1.pdf"
    },
    {
      "id": "2601.01279v1",
      "title": "LLM Collusion",
      "abstract": "We study how delegating pricing to large language models (LLMs) can facilitate collusion in a duopoly when both sellers rely on the same pre-trained model. The LLM is characterized by (i) a propensity parameter capturing its internal bias toward high-price recommendations and (ii) an output-fidelity parameter measuring how tightly outputs track that bias; the propensity evolves through retraining. We show that configuring LLMs for robustness and reproducibility can induce collusion via a phase transition: there exists a critical output-fidelity threshold that pins down long-run behavior. Below it, competitive pricing is the unique long-run outcome. Above it, the system is bistable, with competitive and collusive pricing both locally stable and the realized outcome determined by the model's initial preference. The collusive regime resembles tacit collusion: prices are elevated on average, yet occasional low-price recommendations provide plausible deniability. With perfect fidelity, full collusion emerges from any interior initial condition. For finite training batches of size $b$, infrequent retraining (driven by computational costs) further amplifies collusion: conditional on starting in the collusive basin, the probability of collusion approaches one as $b$ grows, since larger batches dampen stochastic fluctuations that might otherwise tip the system toward competition. The indeterminacy region shrinks at rate $O(1/\\sqrt{b})$.",
      "authors": [
        "Shengyu Cao",
        "Ming Hu"
      ],
      "published": "2026-01-03T20:38:21Z",
      "categories": [
        "econ.TH",
        "cs.AI",
        "cs.CE",
        "cs.CL",
        "cs.GT"
      ],
      "primary_category": "econ.TH",
      "abs_url": "https://arxiv.org/abs/2601.01279v1",
      "pdf_url": "https://arxiv.org/pdf/2601.01279v1.pdf"
    },
    {
      "id": "2601.01266v1",
      "title": "From Policy to Logic for Efficient and Interpretable Coverage Assessment",
      "abstract": "Large Language Models (LLMs) have demonstrated strong capabilities in interpreting lengthy, complex legal and policy language. However, their reliability can be undermined by hallucinations and inconsistencies, particularly when analyzing subjective and nuanced documents. These challenges are especially critical in medical coverage policy review, where human experts must be able to rely on accurate information. In this paper, we present an approach designed to support human reviewers by making policy interpretation more efficient and interpretable. We introduce a methodology that pairs a coverage-aware retriever with symbolic rule-based reasoning to surface relevant policy language, organize it into explicit facts and rules, and generate auditable rationales. This hybrid system minimizes the number of LLM inferences required which reduces overall model cost. Notably, our approach achieves a 44% reduction in inference cost alongside a 4.5% improvement in F1 score, demonstrating both efficiency and effectiveness.",
      "authors": [
        "Rhitabrat Pokharel",
        "Hamid Hassanzadeh",
        "Ameeta Agrawal"
      ],
      "published": "2026-01-03T19:24:51Z",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "abs_url": "https://arxiv.org/abs/2601.01266v1",
      "pdf_url": "https://arxiv.org/pdf/2601.01266v1.pdf"
    },
    {
      "id": "2601.01260v1",
      "title": "MambaFormer: Token-Level Guided Routing Mixture-of-Experts for Accurate and Efficient Clinical Assistance",
      "abstract": "The deployment of large language models (LLMs) in real-world clinical applications is constrained by the fundamental trade-off between computational cost and the efficiency of linear-time models. To address this, we propose an LLM-based MambaFormer hybrid Mixture-of-Experts (MoE) framework for efficient medical question-answering (QA) and clinical assistance. The MambaFormer employs a lightweight gating mechanism that performs token-level dynamic routing to a customized Transformer expert (ET5) for short, complex queries or to a State Space Model expert (EMamba) for long, high-throughput sequences. The customized EMamba and ET5 models are tailored to accommodate input sequence dimensionality, embedding structure, sequence length, and target-specific output heads, and are fine-tuned through transfer learning on a new, custom-designed DentalQA dataset. Moreover, intelligent routing decisions are driven by the contextual complexity of token embeddings, normalized sequence length, and domain-aware features, thereby enforcing a Pareto-optimal trade-off between inference latency and prediction accuracy. Furthermore, a novel utility-guided multi-objective loss jointly optimizes decisions, router parameters, routing behavior, expert utilization, and computational cost by adaptively regulating token-level expert activation. Finally, the proposed MambaFormer is cross-validated (holdout) for medical QA on the new, custom-designed DentalQA and PubMedQA datasets and compared with state-of-the-art techniques. The proposed MambaFormer outperforms (BERTScore = 0.9180) with ultra-low latency (0.077 s), delivering a 24.4 speedup over T5-Large and establishing a scalable solution for resource-constrained clinical deployment.",
      "authors": [
        "Hamad Khan",
        "Saddam Hussain Khan"
      ],
      "published": "2026-01-03T19:01:33Z",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.CV",
      "abs_url": "https://arxiv.org/abs/2601.01260v1",
      "pdf_url": "https://arxiv.org/pdf/2601.01260v1.pdf"
    },
    {
      "id": "2601.01254v1",
      "title": "Entity-Aware and Secure Query Optimization in Database Using Named Entity Recognition",
      "abstract": "Cloud storage has become the backbone of modern data infrastructure, yet privacy and efficient data retrieval remain significant challenges. Traditional privacy-preserving approaches primarily focus on enhancing database security but fail to address the automatic identification of sensitive information before encryption. This can dramatically reduce query processing time and mitigate errors during manual identification of sensitive information, thereby reducing potential privacy risks. To address this limitation, this research proposes an intelligent privacy-preserving query optimization framework that integrates Named Entity Recognition (NER) to detect sensitive information in queries, utilizing secure data encryption and query optimization techniques for both sensitive and non-sensitive data in parallel, thereby enabling efficient database optimization. Combined deep learning algorithms and transformer-based models to detect and classify sensitive entities with high precision, and the Advanced Encryption Standard (AES) algorithm to encrypt, with blind indexing to secure search functionality of the sensitive data, whereas non-sensitive data was divided into groups using the K-means algorithm, along with a rank search for optimization. Among all NER models, the Deep Belief Network combined with Long Short-Term Memory (DBN-LSTM) delivers the best performance, with an accuracy of 93% and precision (94%), recall, and F1 score of 93%, and 93%, respectively. Besides, encrypted search achieved considerably faster results with the help of blind indexing, and non-sensitive data fetching also outperformed traditional clustering-based searches. By integrating sensitive data detection, encryption, and query optimization, this work advances the state of privacy-preserving computation in modern cloud infrastructures.",
      "authors": [
        "Azrin Sultana",
        "Hasibur Rashid Chayon"
      ],
      "published": "2026-01-03T18:30:09Z",
      "categories": [
        "cs.DB",
        "cs.CL"
      ],
      "primary_category": "cs.DB",
      "abs_url": "https://arxiv.org/abs/2601.01254v1",
      "pdf_url": "https://arxiv.org/pdf/2601.01254v1.pdf"
    },
    {
      "id": "2601.01244v1",
      "title": "Racka: Efficient Hungarian LLM Adaptation on Academic Infrastructure",
      "abstract": "We present Racka, a lightweight, continually pretrained large language model designed to bridge the resource gap between Hungarian and high-resource languages such as English and German. Racka employs parameter-efficient continual pretraining via Low-Rank Adaptation (LoRA) on a Qwen-3 4B backbone, making the recipe practical on A100 (40GB)-based HPC clusters with low inter-node bandwidth. To better match the training distribution, we replace and adapt the tokenizer, achieving substantially improved tokenization fertility for Hungarian while maintaining competitive performance in English and German. The model is trained on 160B subword tokens drawn from a mixture of internet and high-quality curated sources, with a composition of 44% Hungarian, 24% English, 21% German, and 11% code. This data mix is chosen to mitigate catastrophic forgetting and preserve high-resource language capabilities during continual pretraining. Our preliminary results indicate modest but stable results in language adaptation.",
      "authors": [
        "Zsolt Csibi",
        "Bence György Gortka",
        "Natabara Gyöngyössy",
        "Kornél Nagy",
        "Dávid Márk Nemeskey",
        "Martin Sallai",
        "András Simonyi",
        "András Márk Szekeres",
        "Gábor Palkó"
      ],
      "published": "2026-01-03T17:32:48Z",
      "categories": [
        "cs.CL"
      ],
      "primary_category": "cs.CL",
      "abs_url": "https://arxiv.org/abs/2601.01244v1",
      "pdf_url": "https://arxiv.org/pdf/2601.01244v1.pdf"
    },
    {
      "id": "2601.01234v1",
      "title": "Bridging Language Gaps: Utilizing Interactive Robots to Teach Cantonese in Real-Life Contexts for Newly-Arrived Children",
      "abstract": "Hong Kong's education system is notably multicultural, including local, non-Chinese-speaking, and newly arrived students (NAS) (Mandarine Chinese-speaking). NAS can guess the meaning of vocabulary but cannot speak out, presenting unique challenges for them, particularly language barriers and cultural differences. These challenges hinder their academic success and social integration, leading to feelings of isolation and demotivation. Current resources often fail to address the emotional well-being of these students and predominantly focus on English language acquisition, leaving a gap in support for learning Cantonese and navigating the local cultural landscape. This study explores the effectiveness of an interactive robot, Boon Boon, in teaching Cantonese through real-life contexts to enhance NAS children learning engagement and motivation. The research questions are: (1) How does interactive robot-empowered scenario learning influence the learning engagement and motivation of NAS in learning Cantonese? and (2) What is the impact of a robot-empowered scenario learning system on the Cantonese language proficiency of NAS? Fourteen children are invited to participate in a four-day learning program with Boon Boon. The preliminary result indicated that Boon Boon drove students' attention to learning and academic achievement. Future research will focus on long-term assessments of robot-empowered learning's effectiveness and explore the scalability of this approach across diverse educational settings and cultural backgrounds.",
      "authors": [
        "Ka-Yan Fung",
        "Yuxing Tao",
        " Tze-Leung",
        "Rick Lui",
        "Kuen-Fung Sin"
      ],
      "published": "2026-01-03T16:45:34Z",
      "categories": [
        "cs.ET",
        "cs.HC",
        "cs.RO"
      ],
      "primary_category": "cs.ET",
      "abs_url": "https://arxiv.org/abs/2601.01234v1",
      "pdf_url": "https://arxiv.org/pdf/2601.01234v1.pdf"
    },
    {
      "id": "2601.01227v1",
      "title": "LiveBo: Empowering Non-Chinese Speaking Students through AI-Driven Real-Life Scenarios in Cantonese",
      "abstract": "Language learning is a multifaceted process. Insufficient vocabulary can hinder communication and lead to demotivation. For non-Chinese speaking (NCS) students, learning Traditional Chinese (Cantonese) poses distinct challenges, particularly due to the complexity of converting spoken and written forms. To address this issue, this study examines the effectiveness of real-life scenario simulations integrated with interactive social robots in enhancing NCS student engagement and language acquisition. The research employs a quasi-experimental design involving NCS students who interact with an AI-driven, robot-assisted language learning system, LiveBo. The study aims to assess the impact of this innovative approach on active participation and motivation. Data are collected through proficiency tests, questionnaires and semi-structured interviews. Findings indicate that NCS students experience positive improvements in behavioural and emotional engagement, motivation and learning outcomes, highlighting the potential of integrating novel technologies in language education. We plan to compare with the control group in the future. This study highlights the significance of interactive and immersive learning experiences in promoting motivation and enhancing language acquisition among NCS students.",
      "authors": [
        "Ka Yan Fung",
        "Kwong Chiu Fung",
        "Yuxing Tao",
        "Tze Leung Rick Lui",
        "Kuen Fung Sin"
      ],
      "published": "2026-01-03T16:25:17Z",
      "categories": [
        "cs.HC",
        "cs.RO"
      ],
      "primary_category": "cs.HC",
      "abs_url": "https://arxiv.org/abs/2601.01227v1",
      "pdf_url": "https://arxiv.org/pdf/2601.01227v1.pdf"
    },
    {
      "id": "2601.01225v1",
      "title": "Stylometry Analysis of Human and Machine Text for Academic Integrity",
      "abstract": "This work addresses critical challenges to academic integrity, including plagiarism, fabrication, and verification of authorship of educational content, by proposing a Natural Language Processing (NLP)-based framework for authenticating students' content through author attribution and style change detection. Despite some initial efforts, several aspects of the topic are yet to be explored. In contrast to existing solutions, the paper provides a comprehensive analysis of the topic by targeting four relevant tasks, including (i) classification of human and machine text, (ii) differentiating in single and multi-authored documents, (iii) author change detection within multi-authored documents, and (iv) author recognition in collaboratively produced documents. The solutions proposed for the tasks are evaluated on two datasets generated with Gemini using two different prompts, including a normal and a strict set of instructions. During experiments, some reduction in the performance of the proposed solutions is observed on the dataset generated through the strict prompt, demonstrating the complexities involved in detecting machine-generated text with cleverly crafted prompts. The generated datasets, code, and other relevant materials are made publicly available on GitHub, which are expected to provide a baseline for future research in the domain.",
      "authors": [
        "Hezam Albaqami",
        "Muhammad Asif Ayub",
        "Nasir Ahmad",
        "Yaseen Ahmad",
        "Mohammed M. Alqahtani",
        "Abdullah M. Algamdi",
        "Almoaid A. Owaidah",
        "Kashif Ahmad"
      ],
      "published": "2026-01-03T16:13:38Z",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "abs_url": "https://arxiv.org/abs/2601.01225v1",
      "pdf_url": "https://arxiv.org/pdf/2601.01225v1.pdf"
    },
    {
      "id": "2601.01218v1",
      "title": "MotiBo: The Impact of Interactive Digital Storytelling Robots on Student Motivation through Self-Determination Theory",
      "abstract": "Creativity is increasingly recognized as an important skill in education, and storytelling can enhance motivation and engagement among students. However, conventional storytelling methods often lack the interactive elements necessary to engage students. To this end, this study examines the impact of an interactive digital storytelling system incorporating a human-like robot on student engagement and creativity. The study aims to compare engagement levels across three modalities: paper-based, PowerPoint, and robot-assisted storytelling, MotiBo. Utilizing a quasi-experimental design, this work involves three groups of students who interact with the storytelling system over a five-day learning. Findings reveal that students using MotiBo exhibit statistically significant improvement in behavioural and cognitive engagement compared to those using traditional methods. These results suggest that the integration of novel technologies can effectively enhance the learning experience, ultimately promoting creativity and self-learning ability in educational settings. Future research will investigate the long-term effects of these technologies on learning outcomes and explore their potential for broader applications in diverse educational contexts.",
      "authors": [
        "Ka Yan Fung",
        "Tze Leung Rick Lui",
        "Yuxing Tao",
        "Kuen Fung Sin"
      ],
      "published": "2026-01-03T15:59:47Z",
      "categories": [
        "cs.HC",
        "cs.MM",
        "cs.RO"
      ],
      "primary_category": "cs.HC",
      "abs_url": "https://arxiv.org/abs/2601.01218v1",
      "pdf_url": "https://arxiv.org/pdf/2601.01218v1.pdf"
    },
    {
      "id": "2601.01210v1",
      "title": "Real-Time LiDAR Point Cloud Densification for Low-Latency Spatial Data Transmission",
      "abstract": "To realize low-latency spatial transmission system for immersive telepresence, there are two major problems: capturing dynamic 3D scene densely and processing them in real time. LiDAR sensors capture 3D in real time, but produce sparce point clouds. Therefore, this paper presents a high-speed LiDAR point cloud densification method to generate dense 3D scene with minimal latency, addressing the need for on-the-fly depth completion while maintaining real-time performance. Our approach combines multiple LiDAR inputs with high-resolution color images and applies a joint bilateral filtering strategy implemented through a convolutional neural network architecture. Experiments demonstrate that the proposed method produces dense depth maps at full HD resolution in real time (30 fps), which is over 15x faster than a recent training-based depth completion approach. The resulting dense point clouds exhibit accurate geometry without multiview inconsistencies or ghosting artifacts.",
      "authors": [
        "Kazuhiko Murasaki",
        "Shunsuke Konagai",
        "Masakatsu Aoki",
        "Taiga Yoshida",
        "Ryuichi Tanida"
      ],
      "published": "2026-01-03T15:27:57Z",
      "categories": [
        "cs.CV",
        "cs.RO"
      ],
      "primary_category": "cs.CV",
      "abs_url": "https://arxiv.org/abs/2601.01210v1",
      "pdf_url": "https://arxiv.org/pdf/2601.01210v1.pdf"
    },
    {
      "id": "2601.01196v1",
      "title": "EduSim-LLM: An Educational Platform Integrating Large Language Models and Robotic Simulation for Beginners",
      "abstract": "In recent years, the rapid development of Large Language Models (LLMs) has significantly enhanced natural language understanding and human-computer interaction, creating new opportunities in the field of robotics. However, the integration of natural language understanding into robotic control is an important challenge in the rapid development of human-robot interaction and intelligent automation industries. This challenge hinders intuitive human control over complex robotic systems, limiting their educational and practical accessibility. To address this, we present the EduSim-LLM, an educational platform that integrates LLMs with robot simulation and constructs a language-drive control model that translates natural language instructions into executable robot behavior sequences in CoppeliaSim. We design two human-robot interaction models: direct control and autonomous control, conduct systematic simulations based on multiple language models, and evaluate multi-robot collaboration, motion planning, and manipulation capabilities. Experiential results show that LLMs can reliably convert natural language into structured robot actions; after applying prompt-engineering templates instruction-parsing accuracy improves significantly; as task complexity increases, overall accuracy rate exceeds 88.9% in the highest complexity tests.",
      "authors": [
        "Shenqi Lu",
        "Liangwei Zhang"
      ],
      "published": "2026-01-03T14:40:39Z",
      "categories": [
        "cs.RO"
      ],
      "primary_category": "cs.RO",
      "abs_url": "https://arxiv.org/abs/2601.01196v1",
      "pdf_url": "https://arxiv.org/pdf/2601.01196v1.pdf"
    },
    {
      "id": "2601.01188v1",
      "title": "DST-Calib: A Dual-Path, Self-Supervised, Target-Free LiDAR-Camera Extrinsic Calibration Network",
      "abstract": "LiDAR-camera extrinsic calibration is essential for multi-modal data fusion in robotic perception systems. However, existing approaches typically rely on handcrafted calibration targets (e.g., checkerboards) or specific, static scene types, limiting their adaptability and deployment in real-world autonomous and robotic applications. This article presents the first self-supervised LiDAR-camera extrinsic calibration network that operates in an online fashion and eliminates the need for specific calibration targets. We first identify a significant generalization degradation problem in prior methods, caused by the conventional single-sided data augmentation strategy. To overcome this limitation, we propose a novel double-sided data augmentation technique that generates multi-perspective camera views using estimated depth maps, thereby enhancing robustness and diversity during training. Built upon this augmentation strategy, we design a dual-path, self-supervised calibration framework that reduces the dependence on high-precision ground truth labels and supports fully adaptive online calibration. Furthermore, to improve cross-modal feature association, we replace the traditional dual-branch feature extraction design with a difference map construction process that explicitly correlates LiDAR and camera features. This not only enhances calibration accuracy but also reduces model complexity. Extensive experiments conducted on five public benchmark datasets, as well as our own recorded dataset, demonstrate that the proposed method significantly outperforms existing approaches in terms of generalizability.",
      "authors": [
        "Zhiwei Huang",
        "Yanwei Fu",
        "Yi Zhou",
        "Xieyuanli Chen",
        "Qijun Chen",
        "Rui Fan"
      ],
      "published": "2026-01-03T13:57:01Z",
      "categories": [
        "cs.RO",
        "cs.CV"
      ],
      "primary_category": "cs.RO",
      "abs_url": "https://arxiv.org/abs/2601.01188v1",
      "pdf_url": "https://arxiv.org/pdf/2601.01188v1.pdf"
    },
    {
      "id": "2601.01171v1",
      "title": "Almost Clinical: Linguistic properties of synthetic electronic health records",
      "abstract": "This study evaluates the linguistic and clinical suitability of synthetic electronic health records (EHRs) in the field of mental health. First, we describe the rationale and the methodology for creating the synthetic corpus. Second, we assess agency, modality, and information flow across four clinical genres (Assessments, Correspondence, Referrals and Care plans) to understand how LLMs grammatically construct medical authority and patient agency through linguistic choices. While LLMs produce coherent, terminology-appropriate texts that approximate clinical practice, systematic divergences remain, including registerial shifts, insufficient clinical specificity, and inaccuracies in medication use and diagnostic procedures.",
      "authors": [
        "Serge Sharoff",
        "John Baker",
        "David Francis Hunt",
        "Alan Simpson"
      ],
      "published": "2026-01-03T12:22:07Z",
      "categories": [
        "cs.CL"
      ],
      "primary_category": "cs.CL",
      "abs_url": "https://arxiv.org/abs/2601.01171v1",
      "pdf_url": "https://arxiv.org/pdf/2601.01171v1.pdf"
    },
    {
      "id": "2601.01162v1",
      "title": "Bridging the Semantic Gap for Categorical Data Clustering via Large Language Models",
      "abstract": "Categorical data are prevalent in domains such as healthcare, marketing, and bioinformatics, where clustering serves as a fundamental tool for pattern discovery. A core challenge in categorical data clustering lies in measuring similarity among attribute values that lack inherent ordering or distance. Without appropriate similarity measures, values are often treated as equidistant, creating a semantic gap that obscures latent structures and degrades clustering quality. Although existing methods infer value relationships from within-dataset co-occurrence patterns, such inference becomes unreliable when samples are limited, leaving the semantic context of the data underexplored. To bridge this gap, we present ARISE (Attention-weighted Representation with Integrated Semantic Embeddings), which draws on external semantic knowledge from Large Language Models (LLMs) to construct semantic-aware representations that complement the metric space of categorical data for accurate clustering. That is, LLM is adopted to describe attribute values for representation enhancement, and the LLM-enhanced embeddings are combined with the original data to explore semantically prominent clusters. Experiments on eight benchmark datasets demonstrate consistent improvements over seven representative counterparts, with gains of 19-27%. Code is available at https://github.com/develop-yang/ARISE",
      "authors": [
        "Zihua Yang",
        "Xin Liao",
        "Yiqun Zhang",
        "Yiu-ming Cheung"
      ],
      "published": "2026-01-03T11:37:46Z",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.LG",
      "abs_url": "https://arxiv.org/abs/2601.01162v1",
      "pdf_url": "https://arxiv.org/pdf/2601.01162v1.pdf"
    },
    {
      "id": "2601.01156v1",
      "title": "DHI: Leveraging Diverse Hallucination Induction for Enhanced Contrastive Factuality Control in Large Language Models",
      "abstract": "Large language models (LLMs) frequently produce inaccurate or fabricated information, known as \"hallucinations,\" which compromises their reliability. Existing approaches often train an \"Evil LLM\" to deliberately generate hallucinations on curated datasets, using these induced hallucinations to guide contrastive decoding against a reliable \"positive model\" for hallucination mitigation. However, this strategy is limited by the narrow diversity of hallucinations induced, as Evil LLMs trained on specific error types tend to reproduce only these particular patterns, thereby restricting their overall effectiveness. To address these limitations, we propose DHI (Diverse Hallucination Induction), a novel training framework that enables the Evil LLM to generate a broader range of hallucination types without relying on pre-annotated hallucination data. DHI employs a modified loss function that down-weights the generation of specific factually correct tokens, encouraging the Evil LLM to produce diverse hallucinations at targeted positions while maintaining overall factual content. Additionally, we introduce a causal attention masking adaptation to reduce the impact of this penalization on the generation of subsequent tokens. During inference, we apply an adaptive rationality constraint that restricts contrastive decoding to tokens where the positive model exhibits high confidence, thereby avoiding unnecessary penalties on factually correct tokens. Extensive empirical results show that DHI achieves significant performance gains over other contrastive decoding-based approaches across multiple hallucination benchmarks.",
      "authors": [
        "Jiani Guo",
        "Xiangke Zeng",
        "Jie Wu",
        "Zuchao Li"
      ],
      "published": "2026-01-03T10:55:41Z",
      "categories": [
        "cs.CL"
      ],
      "primary_category": "cs.CL",
      "abs_url": "https://arxiv.org/abs/2601.01156v1",
      "pdf_url": "https://arxiv.org/pdf/2601.01156v1.pdf"
    },
    {
      "id": "2601.01155v1",
      "title": "ORION: Option-Regularized Deep Reinforcement Learning for Cooperative Multi-Agent Online Navigation",
      "abstract": "Existing methods for multi-agent navigation typically assume fully known environments, offering limited support for partially known scenarios such as warehouses or factory floors. There, agents may need to plan trajectories that balance their own path optimality with their ability to collect and share information about the environment that can help their teammates reach their own goals. To these ends, we propose ORION, a novel deep reinforcement learning framework for cooperative multi-agent online navigation in partially known environments. Starting from an imperfect prior map, ORION trains agents to make decentralized decisions, coordinate to reach their individual targets, and actively reduce map uncertainty by sharing online observations in a closed perception-action loop. We first design a shared graph encoder that fuses prior map with online perception into a unified representation, providing robust state embeddings under dynamic map discrepancies. At the core of ORION is an option-critic framework that learns to reason about a set of high-level cooperative modes that translate into sequences of low-level actions, allowing agents to switch between individual navigation and team-level exploration adaptively. We further introduce a dual-stage cooperation strategy that enables agents to assist teammates under map uncertainty, thereby reducing the overall makespan. Across extensive maze-like maps and large-scale warehouse environments, our simulation results show that ORION achieves high-quality, real-time decentralized cooperation over varying team sizes, outperforming state-of-the-art classical and learning-based baselines. Finally, we validate ORION on physical robot teams, demonstrating its robustness and practicality for real-world cooperative navigation.",
      "authors": [
        "Zhang Shizhe",
        "Liang Jingsong",
        "Zhou Zhitao",
        "Ye Shuhan",
        "Wang Yizhuo",
        "Tan Ming Siang Derek",
        "Chiun Jimmy",
        "Cao Yuhong",
        "Sartoretti Guillaume"
      ],
      "published": "2026-01-03T10:55:10Z",
      "categories": [
        "cs.RO"
      ],
      "primary_category": "cs.RO",
      "abs_url": "https://arxiv.org/abs/2601.01155v1",
      "pdf_url": "https://arxiv.org/pdf/2601.01155v1.pdf"
    },
    {
      "id": "2601.01153v1",
      "title": "SongSage: A Large Musical Language Model with Lyric Generative Pre-training",
      "abstract": "Large language models have achieved significant success in various domains, yet their understanding of lyric-centric knowledge has not been fully explored. In this work, we first introduce PlaylistSense, a dataset to evaluate the playlist understanding capability of language models. PlaylistSense encompasses ten types of user queries derived from common real-world perspectives, challenging LLMs to accurately grasp playlist features and address diverse user intents. Comprehensive evaluations indicate that current general-purpose LLMs still have potential for improvement in playlist understanding. Inspired by this, we introduce SongSage, a large musical language model equipped with diverse lyric-centric intelligence through lyric generative pretraining. SongSage undergoes continual pretraining on LyricBank, a carefully curated corpus of 5.48 billion tokens focused on lyrical content, followed by fine-tuning with LyricBank-SFT, a meticulously crafted instruction set comprising 775k samples across nine core lyric-centric tasks. Experimental results demonstrate that SongSage exhibits a strong understanding of lyric-centric knowledge, excels in rewriting user queries for zero-shot playlist recommendations, generates and continues lyrics effectively, and performs proficiently across seven additional capabilities. Beyond its lyric-centric expertise, SongSage also retains general knowledge comprehension and achieves a competitive MMLU score. We will keep the datasets inaccessible due to copyright restrictions and release the SongSage and training script to ensure reproducibility and support music AI research and applications, the datasets release plan details are provided in the appendix.",
      "authors": [
        "Jiani Guo",
        "Jiajia Li",
        "Jie Wu",
        "Zuchao Li",
        "Yujiu Yang",
        "Ping Wang"
      ],
      "published": "2026-01-03T10:54:37Z",
      "categories": [
        "cs.CL"
      ],
      "primary_category": "cs.CL",
      "abs_url": "https://arxiv.org/abs/2601.01153v1",
      "pdf_url": "https://arxiv.org/pdf/2601.01153v1.pdf"
    },
    {
      "id": "2601.01144v1",
      "title": "VISO: Robust Underwater Visual-Inertial-Sonar SLAM with Photometric Rendering for Dense 3D Reconstruction",
      "abstract": "Visual challenges in underwater environments significantly hinder the accuracy of vision-based localisation and the high-fidelity dense reconstruction. In this paper, we propose VISO, a robust underwater SLAM system that fuses a stereo camera, an inertial measurement unit (IMU), and a 3D sonar to achieve accurate 6-DoF localisation and enable efficient dense 3D reconstruction with high photometric fidelity. We introduce a coarse-to-fine online calibration approach for extrinsic parameters estimation between the 3D sonar and the camera. Additionally, a photometric rendering strategy is proposed for the 3D sonar point cloud to enrich the sonar map with visual information. Extensive experiments in a laboratory tank and an open lake demonstrate that VISO surpasses current state-of-the-art underwater and visual-based SLAM algorithms in terms of localisation robustness and accuracy, while also exhibiting real-time dense 3D reconstruction performance comparable to the offline dense mapping method.",
      "authors": [
        "Shu Pan",
        "Simon Archieri",
        "Ahmet Cinar",
        "Jonatan Scharff Willners",
        "Ignacio Carlucho",
        "Yvan Petillot"
      ],
      "published": "2026-01-03T10:18:09Z",
      "categories": [
        "cs.RO"
      ],
      "primary_category": "cs.RO",
      "abs_url": "https://arxiv.org/abs/2601.01144v1",
      "pdf_url": "https://arxiv.org/pdf/2601.01144v1.pdf"
    },
    {
      "id": "2601.01143v1",
      "title": "KOS-TL (Knowledge Operation System Type Logic)",
      "abstract": "This paper introduces KOS-TL (Knowledge Operation System Type Logic), a novel constructive framework designed to provide a rigorous logical foundation for autonomous and executable knowledge systems. Traditional knowledge representation models often suffer from a gap between static symbolic logic and dynamic system execution. To bridge this divide, KOS-TL leverages Dependent Type Theory to unify data, logic, and proof into a singular computational substrate.The architecture of KOS-TL is organized into three hierarchical layers: the Core Layer, which defines the static type universe and constructive primitives; the Kernel Layer, which governs state evolution through an event-driven mechanism characterized by the triple $\\langle Σ, \\textsf{Ev}, Δ\\rangle$; and the Runtime Layer, responsible for the bidirectional refinement of physical signals into logical evidence. We formally define the operational semantics of the system and prove key meta-theoretical properties, including Progress and Evolutionary Consistency, ensuring that the system remains logically self-consistent and free from stuck states during continuous state transitions.By integrating Davidsonian event semantics with Martin-Löf type theory, KOS-TL enables the construction of \"proof-carrying knowledge,\" where every state change in the knowledge base is accompanied by a formal witness of its validity. We demonstrate the practical utility of this logic through application examples in industrial traceability and cross-border financial compliance. Our results suggest that KOS-TL provides a robust, formally verifiable basis for the next generation of intelligent, autonomous operating systems.",
      "authors": [
        "Peng Chen"
      ],
      "published": "2026-01-03T10:15:48Z",
      "categories": [
        "cs.CL",
        "cs.LO"
      ],
      "primary_category": "cs.CL",
      "abs_url": "https://arxiv.org/abs/2601.01143v1",
      "pdf_url": "https://arxiv.org/pdf/2601.01143v1.pdf"
    },
    {
      "id": "2601.01139v1",
      "title": "Latent Space Reinforcement Learning for Multi-Robot Exploration",
      "abstract": "Autonomous mapping of unknown environments is a critical challenge, particularly in scenarios where time is limited. Multi-agent systems can enhance efficiency through collaboration, but the scalability of motion-planning algorithms remains a key limitation. Reinforcement learning has been explored as a solution, but existing approaches are constrained by the limited input size required for effective learning, restricting their applicability to discrete environments. This work addresses that limitation by leveraging autoencoders to perform dimensionality reduction, compressing high-fidelity occupancy maps into latent state vectors while preserving essential spatial information. Additionally, we introduce a novel procedural generation algorithm based on Perlin noise, designed to generate topologically complex training environments that simulate asteroid fields, caves and forests. These environments are used for training the autoencoder and the navigation algorithm using a hierarchical deep reinforcement learning framework for decentralized coordination. We introduce a weighted consensus mechanism that modulates reliance on shared data via a tuneable trust parameter, ensuring robustness to accumulation of errors. Experimental results demonstrate that the proposed system scales effectively with number of agents and generalizes well to unfamiliar, structurally distinct environments and is resilient in communication-constrained settings.",
      "authors": [
        "Sriram Rajasekar",
        "Ashwini Ratnoo"
      ],
      "published": "2026-01-03T10:06:40Z",
      "categories": [
        "cs.RO",
        "cs.MA"
      ],
      "primary_category": "cs.RO",
      "abs_url": "https://arxiv.org/abs/2601.01139v1",
      "pdf_url": "https://arxiv.org/pdf/2601.01139v1.pdf"
    },
    {
      "id": "2601.01129v1",
      "title": "RovoDev Code Reviewer: A Large-Scale Online Evaluation of LLM-based Code Review Automation at Atlassian",
      "abstract": "Large Language Models (LLMs)-powered code review automation has the potential to transform code review workflows. Despite the advances of LLM-powered code review comment generation approaches, several practical challenges remain for designing enterprise-grade code review automation tools. In particular, this paper aims at answering the practical question: how can we design a review-guided, context-aware, quality-checked code review comment generation without fine-tuning?\n  In this paper, we present RovoDev Code Reviewer, an enterprise-grade LLM-based code review automation tool designed and deployed at scale within Atlassian's development ecosystem with seamless integration into Atlassian's Bitbucket. Through the offline, online, user feedback evaluations over a one-year period, we conclude that RovoDev Code Reviewer is (1) effective in generating code review comments that could lead to code resolution for 38.70% (i.e., comments that triggered code changes in the subsequent commits); and (2) offers the promise of accelerating feedback cycles (i.e., decreasing the PR cycle time by 30.8%), alleviating reviewer workload (i.e., reducing the number of human-written comments by 35.6%), and improving overall software quality (i.e., finding errors with actionable suggestions).",
      "authors": [
        "Kla Tantithamthavorn",
        "Yaotian Zou",
        "Andy Wong",
        "Michael Gupta",
        "Zhe Wang",
        "Mike Buller",
        "Ryan Jiang",
        "Matthew Watson",
        "Minwoo Jeong",
        "Kun Chen",
        "Ming Wu"
      ],
      "published": "2026-01-03T09:27:56Z",
      "categories": [
        "cs.SE",
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ],
      "primary_category": "cs.SE",
      "abs_url": "https://arxiv.org/abs/2601.01129v1",
      "pdf_url": "https://arxiv.org/pdf/2601.01129v1.pdf"
    },
    {
      "id": "2601.01126v1",
      "title": "RoboPhD: Self-Improving Text-to-SQL Through Autonomous Agent Evolution",
      "abstract": "We present RoboPhD, a system where AI agents autonomously conduct research to improve Text-to-SQL performance. RoboPhD implements a closed-loop evolution cycle with two coordinated components: a SQL Generation agent composed of a database analysis script and SQL generation instructions, and an Evolution agent that designs new versions based on performance feedback. Central to the framework is an ELO-based selection mechanism enabling survival-of-the-fittest dynamics while handling non-transitivity in performance. Starting from a naive 70-line baseline, RoboPhD evolves agents through iterative cross-pollination, discovering effective techniques without any external guidance on the Text-to-SQL domain. Our best agent, evolved to 1500 lines over 18 iterations, autonomously discovered strategies such as size-adaptive database analysis that adjusts depth based on schema complexity and SQL generation patterns for column selection, evidence interpretation, and aggregation. Evolution provides the largest gains on cheaper models: while we improve by 2.3 points over a strong Claude Opus 4.5 naive baseline, we show an improvement of 8.9 points over the weaker Claude Haiku model. This enables 'skip a tier' deployment: evolved Haiku exceeds naive Sonnet accuracy, and evolved Sonnet exceeds naive Opus, both at lower cost. The full system achieves 73.67% accuracy on the BIRD test set, demonstrating that AI can autonomously build a strong agentic system with only a trivial human-provided starting point.",
      "authors": [
        "Andrew Borthwick",
        "Stephen Ash"
      ],
      "published": "2026-01-03T09:16:07Z",
      "categories": [
        "cs.CL"
      ],
      "primary_category": "cs.CL",
      "abs_url": "https://arxiv.org/abs/2601.01126v1",
      "pdf_url": "https://arxiv.org/pdf/2601.01126v1.pdf"
    },
    {
      "id": "2601.01121v1",
      "title": "Listen, Attend, Understand: a Regularization Technique for Stable E2E Speech Translation Training on High Variance labels",
      "abstract": "End-to-End Speech Translation often shows slower convergence and worse performance when target transcriptions exhibit high variance and semantic ambiguity. We propose Listen, Attend, Understand (LAU), a semantic regularization technique that constrains the acoustic encoder's latent space during training. By leveraging frozen text embeddings to provide a directional auxiliary loss, LAU injects linguistic groundedness into the acoustic representation without increasing inference cost. We evaluate our method on a Bambara-to-French dataset with 30 hours of Bambara speech translated by non-professionals. Experimental results demonstrate that LAU models achieve comparable performance by standard metrics compared to an E2E-ST system pretrained with 100\\% more data and while performing better in preserving semantic meaning. Furthermore, we introduce Total Parameter Drift as a metric to quantify the structural impact of regularization to demonstrate that semantic constraints actively reorganize the encoder's weights to prioritize meaning over literal phonetics. Our findings suggest that LAU is a robust alternative to post-hoc rescoring and a valuable addition to E2E-ST training, especially when training data is scarce and/or noisy.",
      "authors": [
        "Yacouba Diarra",
        "Michael Leventhal"
      ],
      "published": "2026-01-03T08:45:59Z",
      "categories": [
        "cs.CL"
      ],
      "primary_category": "cs.CL",
      "abs_url": "https://arxiv.org/abs/2601.01121v1",
      "pdf_url": "https://arxiv.org/pdf/2601.01121v1.pdf"
    },
    {
      "id": "2601.01112v1",
      "title": "EmoLoom-2B: Fast Base-Model Screening for Emotion Classification and VAD with Lexicon-Weak Supervision and KV-Off Evaluation",
      "abstract": "We introduce EmoLoom-2B, a lightweight and reproducible pipeline that turns small language models under 2B parameters into fast screening candidates for joint emotion classification and Valence-Arousal-Dominance prediction. To ensure protocol-faithful and fair evaluation, we unify data loading, training, and inference under a single JSON input-output contract and remove avoidable variance by adopting KV-off decoding as the default setting. We incorporate two orthogonal semantic regularizers: a VAD-preserving constraint that aligns generated text with target VAD triples, and a lightweight external appraisal classifier that provides training-time guidance on goal attainment, controllability, certainty, and fairness without injecting long rationales. To improve polarity sensitivity, we introduce Valence Flip augmentation based on mirrored emotional pairs. During supervised fine-tuning, we apply A/B mixture sampling with entropy-aware temperature scheduling to balance coverage and convergence. Using Qwen-1.8B-Chat as the base model, EmoLoom-2B achieves strong performance on GoEmotions and EmpatheticDialogues, and demonstrates robust cross-corpus generalization on DailyDialog. The proposed recipe is budget-aware, auditable, and re-entrant, serving as a dependable screening pass before heavier training or multimodal fusion.",
      "authors": [
        "Zilin Li",
        "Weiwei Xu",
        "Xuanbo Lu",
        "Zheda Liu"
      ],
      "published": "2026-01-03T08:25:58Z",
      "categories": [
        "cs.CL"
      ],
      "primary_category": "cs.CL",
      "abs_url": "https://arxiv.org/abs/2601.01112v1",
      "pdf_url": "https://arxiv.org/pdf/2601.01112v1.pdf"
    },
    {
      "id": "2601.01106v1",
      "title": "Towards reliable subsea object recovery: a simulation study of an auv with a suction-actuated end effector",
      "abstract": "Autonomous object recovery in the hadal zone is challenging due to extreme hydrostatic pressure, limited visibility and currents, and the need for precise manipulation at full ocean depth. Field experimentation in such environments is costly, high-risk, and constrained by limited vehicle availability, making early validation of autonomous behaviors difficult. This paper presents a simulation-based study of a complete autonomous subsea object recovery mission using a Hadal Small Vehicle (HSV) equipped with a three-degree-of-freedom robotic arm and a suction-actuated end effector. The Stonefish simulator is used to model realistic vehicle dynamics, hydrodynamic disturbances, sensing, and interaction with a target object under hadal-like conditions. The control framework combines a world-frame PID controller for vehicle navigation and stabilization with an inverse-kinematics-based manipulator controller augmented by acceleration feed-forward, enabling coordinated vehicle - manipulator operation. In simulation, the HSV autonomously descends from the sea surface to 6,000 m, performs structured seafloor coverage, detects a target object, and executes a suction-based recovery. The results demonstrate that high-fidelity simulation provides an effective and low-risk means of evaluating autonomous deep-sea intervention behaviors prior to field deployment.",
      "authors": [
        "Michele Grimaldi",
        "Yosaku Maeda",
        "Hitoshi Kakami",
        "Ignacio Carlucho",
        "Yvan Petillot",
        "Tomoya Inoue"
      ],
      "published": "2026-01-03T07:55:23Z",
      "categories": [
        "cs.RO"
      ],
      "primary_category": "cs.RO",
      "abs_url": "https://arxiv.org/abs/2601.01106v1",
      "pdf_url": "https://arxiv.org/pdf/2601.01106v1.pdf"
    },
    {
      "id": "2601.01091v1",
      "title": "ks-lit-3m: A 3.1 million word kashmiri text dataset for large language model pretraining",
      "abstract": "Large Language Models (LLMs) demonstrate remarkable fluency across high-resource languages yet consistently fail to generate coherent text in Kashmiri, a language spoken by approximately seven million people. This performance disparity stems not from inherent model limitations but from a critical scarcity of high-quality training data. Decades of Kashmiri literature remain inaccessible to modern NLP pipelines due to their encoding in the proprietary InPage desktop publishing format. This paper introduces KS-LIT-3M, a curated corpus of 3.1 million words (16.4 million characters) specifically designed for pretraining language models on Kashmiri. The dataset is structured as a single continuous linear text stream, optimized for causal language model training where models learn to predict subsequent tokens from preceding context. The corpus was constructed through the development of a specialized InPage-to-Unicode converter, followed by rigorous preprocessing including English contamination removal, character normalization, and quality validation. Encompassing 131,607 unique words drawn from diverse genres including literary works, journalistic writing, academic texts, and religious scholarship, KS-LIT-3M addresses a fundamental resource gap for Kashmiri language technology. The dataset is released under the CC-BY-4.0 license to facilitate research in Kashmiri natural language processing.",
      "authors": [
        "Haq Nawaz Malik"
      ],
      "published": "2026-01-03T06:43:26Z",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "abs_url": "https://arxiv.org/abs/2601.01091v1",
      "pdf_url": "https://arxiv.org/pdf/2601.01091v1.pdf"
    },
    {
      "id": "2601.01088v1",
      "title": "600k-ks-ocr: a large-scale synthetic dataset for optical character recognition in kashmiri script",
      "abstract": "This technical report presents the 600K-KS-OCR Dataset, a large-scale synthetic corpus comprising approximately 602,000 word-level segmented images designed for training and evaluating optical character recognition systems targeting Kashmiri script. The dataset addresses a critical resource gap for Kashmiri, an endangered Dardic language utilizing a modified Perso-Arabic writing system spoken by approximately seven million people. Each image is rendered at 256x64 pixels with corresponding ground-truth transcriptions provided in multiple formats compatible with CRNN, TrOCR, and generalpurpose machine learning pipelines. The generation methodology incorporates three traditional Kashmiri typefaces, comprehensive data augmentation simulating real-world document degradation, and diverse background textures to enhance model robustness. The dataset is distributed across ten partitioned archives totaling approximately 10.6 GB and is released under the CC-BY-4.0 license to facilitate research in low-resource language optical character recognition.",
      "authors": [
        "Haq Nawaz Malik"
      ],
      "published": "2026-01-03T06:29:17Z",
      "categories": [
        "cs.CV",
        "cs.CL"
      ],
      "primary_category": "cs.CV",
      "abs_url": "https://arxiv.org/abs/2601.01088v1",
      "pdf_url": "https://arxiv.org/pdf/2601.01088v1.pdf"
    },
    {
      "id": "2601.01076v1",
      "title": "Scalable Data-Driven Reachability Analysis and Control via Koopman Operators with Conformal Coverage Guarantees",
      "abstract": "We propose a scalable reachability-based framework for probabilistic, data-driven safety verification of unknown nonlinear dynamics. We use Koopman theory with a neural network (NN) lifting function to learn an approximate linear representation of the dynamics and design linear controllers in this space to enable closed-loop tracking of a reference trajectory distribution. Closed-loop reachable sets are efficiently computed in the lifted space and mapped back to the original state space via NN verification tools. To capture model mismatch between the Koopman dynamics and the true system, we apply conformal prediction to produce statistically-valid error bounds that inflate the reachable sets to ensure the true trajectories are contained with a user-specified probability. These bounds generalize across references, enabling reuse without recomputation. Results on high-dimensional MuJoCo tasks (11D Hopper, 28D Swimmer) and 12D quadcopters show improved reachable set coverage rate, computational efficiency, and conservativeness over existing methods.",
      "authors": [
        "Devesh Nath",
        "Haoran Yin",
        "Glen Chou"
      ],
      "published": "2026-01-03T05:31:08Z",
      "categories": [
        "eess.SY",
        "cs.AI",
        "cs.LG",
        "cs.RO",
        "math.OC"
      ],
      "primary_category": "eess.SY",
      "abs_url": "https://arxiv.org/abs/2601.01076v1",
      "pdf_url": "https://arxiv.org/pdf/2601.01076v1.pdf"
    },
    {
      "id": "2601.01067v1",
      "title": "Topological Mapping and Navigation using a Monocular Camera based on AnyLoc",
      "abstract": "This paper proposes a method for topological mapping and navigation using a monocular camera. Based on AnyLoc, keyframes are converted into descriptors to construct topological relationships, enabling loop detection and map building. Unlike metric maps, topological maps simplify path planning and navigation by representing environments with key nodes instead of precise coordinates. Actions for visual navigation are determined by comparing segmented images with the image associated with target nodes. The system relies solely on a monocular camera, ensuring fast map building and navigation using key nodes. Experiments show effective loop detection and navigation in real and simulation environments without pre-training. Compared to a ResNet-based method, this approach improves success rates by 60.2% on average while reducing time and space costs, offering a lightweight solution for robot and human navigation in various scenarios.",
      "authors": [
        "Wenzheng Zhang",
        "Yoshitaka Hara",
        "Sousuke Nakamura"
      ],
      "published": "2026-01-03T04:34:48Z",
      "categories": [
        "cs.RO"
      ],
      "primary_category": "cs.RO",
      "abs_url": "https://arxiv.org/abs/2601.01067v1",
      "pdf_url": "https://arxiv.org/pdf/2601.01067v1.pdf"
    },
    {
      "id": "2601.01060v1",
      "title": "Unsupervised Text Style Transfer for Controllable Intensity",
      "abstract": "Unsupervised Text Style Transfer (UTST) aims to build a system to transfer the stylistic properties of a given text without parallel text pairs. Compared with text transfer between style polarities, UTST for controllable intensity is more challenging due to the subtle differences in stylistic features across different intensity levels. Faced with the challenges posed by the lack of parallel data and the indistinguishability between adjacent intensity levels, we propose a SFT-then-PPO paradigm to fine-tune an LLM. We first fine-tune the LLM with synthesized parallel data. Then, we further train the LLM with PPO, where the rewards are elaborately designed for distinguishing the stylistic intensity in hierarchical levels. Both the global and local stylistic features are considered to formulate the reward functions. The experiments on two UTST benchmarks showcase that both rewards have their advantages and applying them to LLM fine-tuning can effectively improve the performance of an LLM backbone based on various evaluation metrics. Even for close levels of intensity, we can still observe the noticeable stylistic difference between the generated text.",
      "authors": [
        "Shuhuan Gu",
        "Wenbiao Tao",
        "Xinchen Ma",
        "Kangkang He",
        "Ye Guo",
        "Xiang Li",
        "Yunshi Lan"
      ],
      "published": "2026-01-03T04:04:07Z",
      "categories": [
        "cs.CL"
      ],
      "primary_category": "cs.CL",
      "abs_url": "https://arxiv.org/abs/2601.01060v1",
      "pdf_url": "https://arxiv.org/pdf/2601.01060v1.pdf"
    },
    {
      "id": "2601.01046v1",
      "title": "KV-Embedding: Training-free Text Embedding via Internal KV Re-routing in Decoder-only LLMs",
      "abstract": "While LLMs are powerful embedding backbones, their application in training-free settings faces two structural challenges: causal attention restricts early tokens from accessing subsequent context, and the next-token prediction objective biases representations toward generation rather than semantic compression. To address these limitations, we propose KV-Embedding, a framework that activates the latent representation power of frozen LLMs. Our method leverages the observation that the key-value (KV) states of the final token at each layer encode a compressed view of the sequence. By re-routing these states as a prepended prefix, we enable all tokens to access sequence-level context within a single forward pass. To ensure model-agnostic applicability, we introduce an automated layer selection strategy based on intrinsic dimensionality. Evaluations on MTEB across Qwen, Mistral, and Llama backbones show that KV-Embedding outperforms existing training-free baselines by up to 10%, while maintaining robust performance on sequences up to 4,096 tokens. These results demonstrate that internal state manipulation offers an efficient alternative to input modification, and we hope this work encourages further exploration of LLM internals for representation learning.",
      "authors": [
        "Yixuan Tang",
        "Yi Yang"
      ],
      "published": "2026-01-03T02:55:43Z",
      "categories": [
        "cs.CL"
      ],
      "primary_category": "cs.CL",
      "abs_url": "https://arxiv.org/abs/2601.01046v1",
      "pdf_url": "https://arxiv.org/pdf/2601.01046v1.pdf"
    },
    {
      "id": "2601.01037v1",
      "title": "Multi-Dimensional Prompt Chaining to Improve Open-Domain Dialogue Generation",
      "abstract": "Small language models (SLMs) offer significant deployment advantages but often struggle to match the dialogue quality of larger models in open-domain settings. In this paper, we propose a multi-dimensional prompt-chaining framework that integrates Naturalness, Coherence, and Engagingness dimensions to enhance human-likeness in open-domain dialogue generation. We apply the framework to two SLMs, TinyLlama and Llama-2-7B, and benchmark their performance against responses generated by substantially larger models, including Llama-2-70B and GPT-3.5 Turbo. We then employ automatic and human evaluation to assess the responses based on diversity, contextual coherence, as well as overall quality. Results show that the full framework improves response diversity by up to 29%, contextual coherence by up to 28%, and engagingness as well as naturalness by up to 29%. Notably, Llama-2-7B achieves performance comparable to substantially larger models, including Llama-2-70B and GPT-3.5 Turbo. Overall, the findings demonstrate that carefully designed prompt-based strategies provide an effective and resource-efficient pathway to improving open-domain dialogue quality in SLMs.",
      "authors": [
        "Livia Leong Hui Teng"
      ],
      "published": "2026-01-03T02:21:27Z",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "abs_url": "https://arxiv.org/abs/2601.01037v1",
      "pdf_url": "https://arxiv.org/pdf/2601.01037v1.pdf"
    },
    {
      "id": "2601.01027v1",
      "title": "A Platform for Interactive AI Character Experiences",
      "abstract": "From movie characters to modern science fiction - bringing characters into interactive, story-driven conversations has captured imaginations across generations. Achieving this vision is highly challenging and requires much more than just language modeling. It involves numerous complex AI challenges, such as conversational AI, maintaining character integrity, managing personality and emotions, handling knowledge and memory, synthesizing voice, generating animations, enabling real-world interactions, and integration with physical environments. Recent advancements in the development of foundation models, prompt engineering, and fine-tuning for downstream tasks have enabled researchers to address these individual challenges. However, combining these technologies for interactive characters remains an open problem. We present a system and platform for conveniently designing believable digital characters, enabling a conversational and story-driven experience while providing solutions to all of the technical challenges. As a proof-of-concept, we introduce Digital Einstein, which allows users to engage in conversations with a digital representation of Albert Einstein about his life, research, and persona. While Digital Einstein exemplifies our methods for a specific character, our system is flexible and generalizes to any story-driven or conversational character. By unifying these diverse AI components into a single, easy-to-adapt platform, our work paves the way for immersive character experiences, turning the dream of lifelike, story-based interactions into a reality.",
      "authors": [
        "Rafael Wampfler",
        "Chen Yang",
        "Dillon Elste",
        "Nikola Kovacevic",
        "Philine Witzig",
        "Markus Gross"
      ],
      "published": "2026-01-03T01:27:19Z",
      "categories": [
        "cs.HC",
        "cs.AI",
        "cs.CL",
        "cs.GR"
      ],
      "primary_category": "cs.HC",
      "abs_url": "https://arxiv.org/abs/2601.01027v1",
      "pdf_url": "https://arxiv.org/pdf/2601.01027v1.pdf"
    },
    {
      "id": "2601.01015v1",
      "title": "HyperJoin: LLM-augmented Hypergraph Link Prediction for Joinable Table Discovery",
      "abstract": "As a pivotal task in data lake management, joinable table discovery has attracted widespread interest. While existing language model-based methods achieve remarkable performance by combining offline column representation learning with online ranking, their design insufficiently accounts for the underlying structural interactions: (1) offline, they directly model tables into isolated or pairwise columns, thereby struggling to capture the rich inter-table and intra-table structural information; and (2) online, they rank candidate columns based solely on query-candidate similarity, ignoring the mutual interactions among the candidates, leading to incoherent result sets. To address these limitations, we propose HyperJoin, a large language model (LLM)-augmented Hypergraph framework for Joinable table discovery. Specifically, we first construct a hypergraph to model tables using both the intra-table hyperedges and the LLM-augmented inter-table hyperedges. Consequently, the task of joinable table discovery is formulated as link prediction on this constructed hypergraph. We then design HIN, a Hierarchical Interaction Network that learns expressive column representations through bidirectional message passing over columns and hyperedges. To strengthen coherence and internal consistency in the result columns, we cast online ranking as a coherence-aware top-k column selection problem. We then introduce a reranking module that leverages a maximum spanning tree algorithm to prune noisy connections and maximize coherence. Experiments demonstrate the superiority of HyperJoin, achieving average improvements of 21.4% (Precision@15) and 17.2% (Recall@15) over the best baseline.",
      "authors": [
        "Shiyuan Liu",
        "Jianwei Wang",
        "Xuemin Lin",
        "Lu Qin",
        "Wenjie Zhang",
        "Ying Zhang"
      ],
      "published": "2026-01-03T00:54:55Z",
      "categories": [
        "cs.CL",
        "cs.DB"
      ],
      "primary_category": "cs.CL",
      "abs_url": "https://arxiv.org/abs/2601.01015v1",
      "pdf_url": "https://arxiv.org/pdf/2601.01015v1.pdf"
    },
    {
      "id": "2601.01003v1",
      "title": "Contractive Diffusion Policies: Robust Action Diffusion via Contractive Score-Based Sampling with Differential Equations",
      "abstract": "Diffusion policies have emerged as powerful generative models for offline policy learning, whose sampling process can be rigorously characterized by a score function guiding a Stochastic Differential Equation (SDE). However, the same score-based SDE modeling that grants diffusion policies the flexibility to learn diverse behavior also incurs solver and score-matching errors, large data requirements, and inconsistencies in action generation. While less critical in image generation, these inaccuracies compound and lead to failure in continuous control settings. We introduce Contractive Diffusion Policies (CDPs) to induce contractive behavior in the diffusion sampling dynamics. Contraction pulls nearby flows closer to enhance robustness against solver and score-matching errors while reducing unwanted action variance. We develop an in-depth theoretical analysis along with a practical implementation recipe to incorporate CDPs into existing diffusion policy architectures with minimal modification and computational cost. We evaluate CDPs for offline learning by conducting extensive experiments in simulation and real-world settings. Across benchmarks, CDPs often outperform baseline policies, with pronounced benefits under data scarcity.",
      "authors": [
        "Amin Abyaneh",
        "Charlotte Morissette",
        "Mohamad H. Danesh",
        "Anas El Houssaini",
        "David Meger",
        "Gregory Dudek",
        "Hsiu-Chin Lin"
      ],
      "published": "2026-01-02T23:33:59Z",
      "categories": [
        "cs.LG",
        "cs.RO"
      ],
      "primary_category": "cs.LG",
      "abs_url": "https://arxiv.org/abs/2601.01003v1",
      "pdf_url": "https://arxiv.org/pdf/2601.01003v1.pdf"
    },
    {
      "id": "2601.00981v1",
      "title": "Simulations of MRI Guided and Powered Ferric Applicators for Tetherless Delivery of Therapeutic Interventions",
      "abstract": "Magnetic Resonance Imaging (MRI) is a well-established modality for pre-operative planning and is also explored for intra-operative guidance of procedures such as intravascular interventions. Among the experimental robot-assisted technologies, the magnetic field gradients of the MRI scanner are used to power and maneuver ferromagnetic applicators for accessing sites in the patient's body via the vascular network. In this work, we propose a computational platform for preoperative planning and modeling of MRI-powered applicators inside blood vessels. This platform was implemented as a two-way data and command pipeline that links the MRI scanner, the computational core, and the operator. The platform first processes multi-slice MR data to extract the vascular bed and then fits a virtual corridor inside the vessel. This corridor serves as a virtual fixture (VF), a forbidden region for the applicators to avoid vessel perforation or collision. The geometric features of the vessel centerline, the VF, and MRI safety compliance (dB/dt, max available gradient) are then used to generate magnetic field gradient waveforms. Different blood flow profiles can be user-selected, and those parameters are used for modeling the applicator's maneuvering. The modeling module further generates cues about whether the selected vascular path can be safely maneuvered. Given future experimental studies that require a real-time operation, the platform was implemented on the Qt framework (C/C++) with software modules performing specific tasks running on dedicated threads: PID controller, generation of VF, generation of MR gradient waveforms.",
      "authors": [
        "Wenhui Chu",
        "Khang Tran",
        "Nikolaos V. Tsekos"
      ],
      "published": "2026-01-02T20:33:51Z",
      "categories": [
        "cs.RO",
        "cs.CV",
        "eess.SY"
      ],
      "primary_category": "cs.RO",
      "abs_url": "https://arxiv.org/abs/2601.00981v1",
      "pdf_url": "https://arxiv.org/pdf/2601.00981v1.pdf"
    },
    {
      "id": "2601.00978v1",
      "title": "From Perception to Symbolic Task Planning: Vision-Language Guided Human-Robot Collaborative Structured Assembly",
      "abstract": "Human-robot collaboration (HRC) in structured assembly requires reliable state estimation and adaptive task planning under noisy perception and human interventions. To address these challenges, we introduce a design-grounded human-aware planning framework for human-robot collaborative structured assembly. The framework comprises two coupled modules. Module I, Perception-to-Symbolic State (PSS), employs vision-language models (VLMs) based agents to align RGB-D observations with design specifications and domain knowledge, synthesizing verifiable symbolic assembly states. It outputs validated installed and uninstalled component sets for online state tracking. Module II, Human-Aware Planning and Replanning (HPR), performs task-level multi-robot assignment and updates the plan only when the observed state deviates from the expected execution outcome. It applies a minimal-change replanning rule to selectively revise task assignments and preserve plan stability even under human interventions. We validate the framework on a 27-component timber-frame assembly. The PSS module achieves 97% state synthesis accuracy, and the HPR module maintains feasible task progression across diverse HRC scenarios. Results indicate that integrating VLM-based perception with knowledge-driven planning improves robustness of state estimation and task planning under dynamic conditions.",
      "authors": [
        "Yanyi Chen",
        "Min Deng"
      ],
      "published": "2026-01-02T20:12:50Z",
      "categories": [
        "cs.RO"
      ],
      "primary_category": "cs.RO",
      "abs_url": "https://arxiv.org/abs/2601.00978v1",
      "pdf_url": "https://arxiv.org/pdf/2601.00978v1.pdf"
    },
    {
      "id": "2601.00969v1",
      "title": "Value Vision-Language-Action Planning & Search",
      "abstract": "Vision-Language-Action (VLA) models have emerged as powerful generalist policies for robotic manipulation, yet they remain fundamentally limited by their reliance on behavior cloning, leading to brittleness under distribution shift. While augmenting pretrained models with test-time search algorithms like Monte Carlo Tree Search (MCTS) can mitigate these failures, existing formulations rely solely on the VLA prior for guidance, lacking a grounded estimate of expected future return. Consequently, when the prior is inaccurate, the planner can only correct action selection via the exploration term, which requires extensive simulation to become effective. To address this limitation, we introduce Value Vision-Language-Action Planning and Search (V-VLAPS), a framework that augments MCTS with a lightweight, learnable value function. By training a simple multilayer perceptron (MLP) on the latent representations of a fixed VLA backbone (Octo), we provide the search with an explicit success signal that biases action selection toward high-value regions. We evaluate V-VLAPS on the LIBERO robotic manipulation suite, demonstrating that our value-guided search improves success rates by over 5 percentage points while reducing the average number of MCTS simulations by 5-15 percent compared to baselines that rely only on the VLA prior.",
      "authors": [
        "Ali Salamatian",
        " Ke",
        " Ren",
        "Kieran Pattison",
        "Cyrus Neary"
      ],
      "published": "2026-01-02T19:40:34Z",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "abs_url": "https://arxiv.org/abs/2601.00969v1",
      "pdf_url": "https://arxiv.org/pdf/2601.00969v1.pdf"
    },
    {
      "id": "2601.00754v1",
      "title": "Calling for Backup: How Children Navigate Successive Robot Communication Failures",
      "abstract": "How do children respond to repeated robot errors? While prior research has examined adult reactions to successive robot errors, children's responses remain largely unexplored. In this study, we explore children's reactions to robot social errors and performance errors. For the latter, this study reproduces the successive robot failure paradigm of Liu et al. with child participants (N=59, ages 8-10) to examine how young users respond to repeated robot conversational errors. Participants interacted with a robot that failed to understand their prompts three times in succession, with their behavioral responses video-recorded and analyzed. We found both similarities and differences compared to adult responses from the original study. Like adults, children adjusted their prompts, modified their verbal tone, and exhibited increasingly emotional non-verbal responses throughout successive errors. However, children demonstrated more disengagement behaviors, including temporarily ignoring the robot or actively seeking an adult. Errors did not affect participants' perception of the robot, suggesting more flexible conversational expectations in children. These findings inform the design of more effective and developmentally appropriate human-robot interaction systems for young users.",
      "authors": [
        "Maria Teresa Parreira",
        "Isabel Neto",
        "Filipa Rocha",
        "Wendy Ju"
      ],
      "published": "2026-01-02T17:18:10Z",
      "categories": [
        "cs.RO",
        "cs.HC"
      ],
      "primary_category": "cs.RO",
      "abs_url": "https://arxiv.org/abs/2601.00754v1",
      "pdf_url": "https://arxiv.org/pdf/2601.00754v1.pdf"
    },
    {
      "id": "2601.00702v1",
      "title": "DefVINS: Visual-Inertial Odometry for Deformable Scenes",
      "abstract": "Deformable scenes violate the rigidity assumptions underpinning classical visual-inertial odometry (VIO), often leading to over-fitting to local non-rigid motion or severe drift when deformation dominates visual parallax. We introduce DefVINS, a visual-inertial odometry framework that explicitly separates a rigid, IMU-anchored state from a non--rigid warp represented by an embedded deformation graph. The system is initialized using a standard VIO procedure that fixes gravity, velocity, and IMU biases, after which non-rigid degrees of freedom are activated progressively as the estimation becomes well conditioned. An observability analysis is included to characterize how inertial measurements constrain the rigid motion and render otherwise unobservable modes identifiable in the presence of deformation. This analysis motivates the use of IMU anchoring and informs a conditioning-based activation strategy that prevents ill-posed updates under poor excitation. Ablation studies demonstrate the benefits of combining inertial constraints with observability-aware deformation activation, resulting in improved robustness under non-rigid environments.",
      "authors": [
        "Samuel Cerezo",
        "Javier Civera"
      ],
      "published": "2026-01-02T14:40:33Z",
      "categories": [
        "cs.RO",
        "cs.CV"
      ],
      "primary_category": "cs.RO",
      "abs_url": "https://arxiv.org/abs/2601.00702v1",
      "pdf_url": "https://arxiv.org/pdf/2601.00702v1.pdf"
    },
    {
      "id": "2601.00696v1",
      "title": "Bayesian Inverse Games with High-Dimensional Multi-Modal Observations",
      "abstract": "Many multi-agent interaction scenarios can be naturally modeled as noncooperative games, where each agent's decisions depend on others' future actions. However, deploying game-theoretic planners for autonomous decision-making requires a specification of all agents' objectives. To circumvent this practical difficulty, recent work develops maximum likelihood techniques for solving inverse games that can identify unknown agent objectives from interaction data. Unfortunately, these methods only infer point estimates and do not quantify estimator uncertainty; correspondingly, downstream planning decisions can overconfidently commit to unsafe actions. We present an approximate Bayesian inference approach for solving the inverse game problem, which can incorporate observation data from multiple modalities and be used to generate samples from the Bayesian posterior over the hidden agent objectives given limited sensor observations in real time. Concretely, the proposed Bayesian inverse game framework trains a structured variational autoencoder with an embedded differentiable Nash game solver on interaction datasets and does not require labels of agents' true objectives. Extensive experiments show that our framework successfully learns prior and posterior distributions, improves inference quality over maximum likelihood estimation-based inverse game approaches, and enables safer downstream decision-making without sacrificing efficiency. When trajectory information is uninformative or unavailable, multimodal inference further reduces uncertainty by exploiting additional observation modalities.",
      "authors": [
        "Yash Jain",
        "Xinjie Liu",
        "Lasse Peters",
        "David Fridovich-Keil",
        "Ufuk Topcu"
      ],
      "published": "2026-01-02T14:23:38Z",
      "categories": [
        "cs.LG",
        "cs.GT",
        "cs.RO"
      ],
      "primary_category": "cs.LG",
      "abs_url": "https://arxiv.org/abs/2601.00696v1",
      "pdf_url": "https://arxiv.org/pdf/2601.00696v1.pdf"
    },
    {
      "id": "2601.00675v1",
      "title": "RoboReward: General-Purpose Vision-Language Reward Models for Robotics",
      "abstract": "A well-designed reward is critical for effective reinforcement learning-based policy improvement. In real-world robotic domains, obtaining such rewards typically requires either labor-intensive human labeling or brittle, handcrafted objectives. Vision-language models (VLMs) have shown promise as automatic reward models, yet their effectiveness on real robot tasks is poorly understood. In this work, we aim to close this gap by introducing (1) \\textbf{RoboReward}, a robotics reward dataset and benchmark built on large-scale real-robot corpora from Open X-Embodiment (OXE) and RoboArena, and (2) vision-language reward models trained on this dataset (RoboReward 4B/8B). Because OXE is success-heavy and lacks failure examples, we propose a \\emph{negative examples data augmentation} pipeline that generates calibrated \\emph{negatives} and \\emph{near-misses} via counterfactual relabeling of successful episodes and temporal clipping to create partial-progress outcomes from the same videos. Using this framework, we produce an extensive training and evaluation dataset that spans diverse tasks and embodiments and enables systematic evaluation of whether state-of-the-art VLMs can reliably provide rewards for robotics. Our evaluation of leading open-weight and proprietary VLMs reveals that no model excels across all tasks, underscoring substantial room for improvement. We then train general-purpose 4B- and 8B-parameter models that outperform much larger VLMs in assigning rewards for short-horizon robotic tasks. Finally, we deploy the 8B-parameter reward VLM in real-robot reinforcement learning and find that it improves policy learning over Gemini Robotics-ER 1.5, a frontier physical reasoning VLM trained on robotics data, by a large margin, while substantially narrowing the gap to RL training with human-provided rewards.",
      "authors": [
        "Tony Lee",
        "Andrew Wagenmaker",
        "Karl Pertsch",
        "Percy Liang",
        "Sergey Levine",
        "Chelsea Finn"
      ],
      "published": "2026-01-02T12:47:34Z",
      "categories": [
        "cs.RO"
      ],
      "primary_category": "cs.RO",
      "abs_url": "https://arxiv.org/abs/2601.00675v1",
      "pdf_url": "https://arxiv.org/pdf/2601.00675v1.pdf"
    },
    {
      "id": "2601.00614v1",
      "title": "From 2D to 3D terrain-following area coverage path planning",
      "abstract": "An algorithm for 3D terrain-following area coverage path planning is presented. Multiple adjacent paths are generated that are (i) locally apart from each other by a distance equal to the working width of a machinery, while (ii) simultaneously floating at a projection distance equal to a specific working height above the terrain. The complexities of the algorithm in comparison to its 2D equivalent are highlighted. These include uniformly spaced elevation data generation using an Inverse Distance Weighting-approach and a local search. Area coverage path planning results for real-world 3D data within an agricultural context are presented to validate the algorithm.",
      "authors": [
        "Mogens Plessen"
      ],
      "published": "2026-01-02T09:00:48Z",
      "categories": [
        "cs.RO",
        "eess.SY"
      ],
      "primary_category": "cs.RO",
      "abs_url": "https://arxiv.org/abs/2601.00614v1",
      "pdf_url": "https://arxiv.org/pdf/2601.00614v1.pdf"
    },
    {
      "id": "2601.00610v1",
      "title": "Vision-based Goal-Reaching Control for Mobile Robots Using a Hierarchical Learning Framework",
      "abstract": "Reinforcement learning (RL) is effective in many robotic applications, but it requires extensive exploration of the state-action space, during which behaviors can be unsafe. This significantly limits its applicability to large robots with complex actuators operating on unstable terrain. Hence, to design a safe goal-reaching control framework for large-scale robots, this paper decomposes the whole system into a set of tightly coupled functional modules. 1) A real-time visual pose estimation approach is employed to provide accurate robot states to 2) an RL motion planner for goal-reaching tasks that explicitly respects robot specifications. The RL module generates real-time smooth motion commands for the actuator system, independent of its underlying dynamic complexity. 3) In the actuation mechanism, a supervised deep learning model is trained to capture the complex dynamics of the robot and provide this model to 4) a model-based robust adaptive controller that guarantees the wheels track the RL motion commands even on slip-prone terrain. 5) Finally, to reduce human intervention, a mathematical safety supervisor monitors the robot, stops it on unsafe faults, and autonomously guides it back to a safe inspection area. The proposed framework guarantees uniform exponential stability of the actuation system and safety of the whole operation. Experiments on a 6,000 kg robot in different scenarios confirm the effectiveness of the proposed framework.",
      "authors": [
        "Mehdi Heydari Shahna",
        "Pauli Mustalahti",
        "Jouni Mattila"
      ],
      "published": "2026-01-02T08:41:47Z",
      "categories": [
        "cs.RO"
      ],
      "primary_category": "cs.RO",
      "abs_url": "https://arxiv.org/abs/2601.00610v1",
      "pdf_url": "https://arxiv.org/pdf/2601.00610v1.pdf"
    },
    {
      "id": "2601.00609v1",
      "title": "NMPC-Augmented Visual Navigation and Safe Learning Control for Large-Scale Mobile Robots",
      "abstract": "A large-scale mobile robot (LSMR) is a high-order multibody system that often operates on loose, unconsolidated terrain, which reduces traction. This paper presents a comprehensive navigation and control framework for an LSMR that ensures stability and safety-defined performance, delivering robust operation on slip-prone terrain by jointly leveraging high-performance techniques. The proposed architecture comprises four main modules: (1) a visual pose-estimation module that fuses onboard sensors and stereo cameras to provide an accurate, low-latency robot pose, (2) a high-level nonlinear model predictive control that updates the wheel motion commands to correct robot drift from the robot reference pose on slip-prone terrain, (3) a low-level deep neural network control policy that approximates the complex behavior of the wheel-driven actuation mechanism in LSMRs, augmented with robust adaptive control to handle out-of-distribution disturbances, ensuring that the wheels accurately track the updated commands issued by high-level control module, and (4) a logarithmic safety module to monitor the entire robot stack and guarantees safe operation. The proposed low-level control framework guarantees uniform exponential stability of the actuation subsystem, while the safety module ensures the whole system-level safety during operation. Comparative experiments on a 6,000 kg LSMR actuated by two complex electro-hydrostatic drives, while synchronizing modules operating at different frequencies.",
      "authors": [
        "Mehdi Heydari Shahna",
        "Pauli Mustalahti",
        "Jouni Mattila"
      ],
      "published": "2026-01-02T08:40:35Z",
      "categories": [
        "cs.RO",
        "eess.SY"
      ],
      "primary_category": "cs.RO",
      "abs_url": "https://arxiv.org/abs/2601.00609v1",
      "pdf_url": "https://arxiv.org/pdf/2601.00609v1.pdf"
    },
    {
      "id": "2601.00580v1",
      "title": "Priority-Aware Multi-Robot Coverage Path Planning",
      "abstract": "Multi-robot systems are widely used for coverage tasks that require efficient coordination across large environments. In Multi-Robot Coverage Path Planning (MCPP), the objective is typically to minimize the makespan by generating non-overlapping paths for full-area coverage. However, most existing methods assume uniform importance across regions, limiting their effectiveness in scenarios where some zones require faster attention. We introduce the Priority-Aware MCPP (PA-MCPP) problem, where a subset of the environment is designated as prioritized zones with associated weights. The goal is to minimize, in lexicographic order, the total priority-weighted latency of zone coverage and the overall makespan. To address this, we propose a scalable two-phase framework combining (1) greedy zone assignment with local search, spanning-tree-based path planning, and (2) Steiner-tree-guided residual coverage. Experiments across diverse scenarios demonstrate that our method significantly reduces priority-weighted latency compared to standard MCPP baselines, while maintaining competitive makespan. Sensitivity analyses further show that the method scales well with the number of robots and that zone coverage behavior can be effectively controlled by adjusting priority weights.",
      "authors": [
        "Kanghoon Lee",
        "Hyeonjun Kim",
        "Jiachen Li",
        "Jinkyoo Park"
      ],
      "published": "2026-01-02T05:45:15Z",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.MA"
      ],
      "primary_category": "cs.RO",
      "abs_url": "https://arxiv.org/abs/2601.00580v1",
      "pdf_url": "https://arxiv.org/pdf/2601.00580v1.pdf"
    },
    {
      "id": "2601.00555v1",
      "title": "LLM-Based Agentic Exploration for Robot Navigation & Manipulation with Skill Orchestration",
      "abstract": "This paper presents an end-to-end LLM-based agentic exploration system for an indoor shopping task, evaluated in both Gazebo simulation and a corresponding real-world corridor layout. The robot incrementally builds a lightweight semantic map by detecting signboards at junctions and storing direction-to-POI relations together with estimated junction poses, while AprilTags provide repeatable anchors for approach and alignment. Given a natural-language shopping request, an LLM produces a constrained discrete action at each junction (direction and whether to enter a store), and a ROS finite-state main controller executes the decision by gating modular motion primitives, including local-costmap-based obstacle avoidance, AprilTag approaching, store entry, and grasping. Qualitative results show that the integrated stack can perform end-to-end task execution from user instruction to multi-store navigation and object retrieval, while remaining modular and debuggable through its text-based map and logged decision history.",
      "authors": [
        "Abu Hanif Muhammad Syarubany",
        "Farhan Zaki Rahmani",
        "Trio Widianto"
      ],
      "published": "2026-01-02T04:04:54Z",
      "categories": [
        "cs.RO"
      ],
      "primary_category": "cs.RO",
      "abs_url": "https://arxiv.org/abs/2601.00555v1",
      "pdf_url": "https://arxiv.org/pdf/2601.00555v1.pdf"
    },
    {
      "id": "2601.00548v1",
      "title": "Optimal Transport-Based Decentralized Multi-Agent Distribution Matching",
      "abstract": "This paper presents a decentralized control framework for distribution matching in multi-agent systems (MAS), where agents collectively achieve a prescribed terminal spatial distribution. The problem is formulated using optimal transport (Wasserstein distance), which provides a principled measure of distributional discrepancy and serves as the basis for the control design. To avoid solving the global optimal transport problem directly, the distribution-matching objective is reformulated into a tractable per-agent decision process, enabling each agent to identify its desired terminal locations using only locally available information. A sequential weight-update rule is introduced to construct feasible local transport plans, and a memory-based correction mechanism is incorporated to maintain reliable operation under intermittent and range-limited communication. Convergence guarantees are established, showing cycle-wise improvement of a surrogate transport cost under both linear and nonlinear agent dynamics. Simulation results demonstrate that the proposed framework achieves effective and scalable distribution matching while operating fully in a decentralized manner.",
      "authors": [
        "Kooktae Lee"
      ],
      "published": "2026-01-02T03:35:25Z",
      "categories": [
        "eess.SY",
        "cs.RO"
      ],
      "primary_category": "eess.SY",
      "abs_url": "https://arxiv.org/abs/2601.00548v1",
      "pdf_url": "https://arxiv.org/pdf/2601.00548v1.pdf"
    },
    {
      "id": "2601.00545v1",
      "title": "Variable Elimination in Hybrid Factor Graphs for Discrete-Continuous Inference & Estimation",
      "abstract": "Many hybrid problems in robotics involve both continuous and discrete components, and modeling them together for estimation tasks has been a long standing and difficult problem. Hybrid Factor Graphs give us a mathematical framework to model these types of problems, however existing approaches for solving them are based on approximations. In this work, we propose an efficient Hybrid Factor Graph framework alongwith a variable elimination algorithm to produce a hybrid Bayes network, which can then be used for exact Maximum A Posteriori estimation and marginalization over both sets of variables. Our approach first develops a novel hybrid Gaussian factor which can connect to both discrete and continuous variables, and a hybrid conditional which can represent multiple continuous hypotheses conditioned on the discrete variables. Using these representations, we derive the process of hybrid variable elimination under the Conditional Linear Gaussian scheme, giving us exact posteriors as hybrid Bayes network. To bound the number of discrete hypotheses, we use a tree-structured representation of the factors coupled with a simple pruning and probabilistic assignment scheme, which allows for tractable inference. We demonstrate the applicability of our framework on a SLAM dataset with ambiguous measurements, where discrete choices for the most likely measurement have to be made. Our demonstrated results showcase the accuracy, generality, and simplicity of our hybrid factor graph framework.",
      "authors": [
        "Varun Agrawal",
        "Frank Dellaert"
      ],
      "published": "2026-01-02T03:21:39Z",
      "categories": [
        "cs.RO"
      ],
      "primary_category": "cs.RO",
      "abs_url": "https://arxiv.org/abs/2601.00545v1",
      "pdf_url": "https://arxiv.org/pdf/2601.00545v1.pdf"
    },
    {
      "id": "2601.00928v1",
      "title": "Analyzing the Shopping Journey: Computing Shelf Browsing Visits in a Physical Retail Store",
      "abstract": "Motivated by recent challenges in the deployment of robots into customer-facing roles within retail, this work introduces a study of customer activity in physical stores as a step toward autonomous understanding of shopper intent. We introduce an algorithm that computes shoppers' ``shelf visits'' -- capturing their browsing behavior in the store. Shelf visits are extracted from trajectories obtained via machine vision-based 3D tracking and overhead cameras. We perform two independent calibrations of the shelf visit algorithm, using distinct sets of trajectories (consisting of 8138 and 15129 trajectories), collected in different stores and labeled by human reviewers. The calibrated models are then evaluated on trajectories held out of the calibration process both from the same store on which calibration was performed and from the other store. An analysis of the results shows that the algorithm can recognize customers' browsing activity when evaluated in an environment different from the one on which calibration was performed. We then use the model to analyze the customers' ``browsing patterns'' on a large set of trajectories and their relation to actual purchases in the stores. Finally, we discuss how shelf browsing information could be used for retail planning and in the domain of human-robot interaction scenarios.",
      "authors": [
        "Luis Yoichi Morales",
        "Francesco Zanlungo",
        "David M. Woollard"
      ],
      "published": "2026-01-02T01:40:12Z",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.RO"
      ],
      "primary_category": "cs.CV",
      "abs_url": "https://arxiv.org/abs/2601.00928v1",
      "pdf_url": "https://arxiv.org/pdf/2601.00928v1.pdf"
    },
    {
      "id": "2601.00465v1",
      "title": "Space Debris Removal using Nano-Satellites controlled by Low-Power Autonomous Agents",
      "abstract": "Space debris is an ever-increasing problem in space travel. There are already many old, no longer functional spacecraft and debris orbiting the earth, which endanger both the safe operation of satellites and space travel. Small nano-satellite swarms can address this problem by autonomously de-orbiting debris safely into the Earth's atmosphere. This work builds on the recent advances of autonomous agents deployed in resource-constrained platforms and shows a first simplified approach how such intelligent and autonomous nano-satellite swarms can be realized. We implement our autonomous agent software on wireless microcontrollers and perform experiments on a specialized test-bed to show the feasibility and overall energy efficiency of our approach.",
      "authors": [
        "Dennis Christmann",
        "Juan F. Gutierrez",
        "Sthiti Padhi",
        "Patrick Plörer",
        "Aditya Takur",
        "Simona Silvestri",
        "Andres Gomez"
      ],
      "published": "2026-01-01T20:28:17Z",
      "categories": [
        "cs.RO"
      ],
      "primary_category": "cs.RO",
      "abs_url": "https://arxiv.org/abs/2601.00465v1",
      "pdf_url": "https://arxiv.org/pdf/2601.00465v1.pdf"
    },
    {
      "id": "2601.00359v1",
      "title": "Efficient Prediction of Dense Visual Embeddings via Distillation and RGB-D Transformers",
      "abstract": "In domestic environments, robots require a comprehensive understanding of their surroundings to interact effectively and intuitively with untrained humans. In this paper, we propose DVEFormer - an efficient RGB-D Transformer-based approach that predicts dense text-aligned visual embeddings (DVE) via knowledge distillation. Instead of directly performing classical semantic segmentation with fixed predefined classes, our method uses teacher embeddings from Alpha-CLIP to guide our efficient student model DVEFormer in learning fine-grained pixel-wise embeddings. While this approach still enables classical semantic segmentation, e.g., via linear probing, it further enables flexible text-based querying and other applications, such as creating comprehensive 3D maps. Evaluations on common indoor datasets demonstrate that our approach achieves competitive performance while meeting real-time requirements, operating at 26.3 FPS for the full model and 77.0 FPS for a smaller variant on an NVIDIA Jetson AGX Orin. Additionally, we show qualitative results that highlight the effectiveness and possible use cases in real-world applications. Overall, our method serves as a drop-in replacement for traditional segmentation approaches while enabling flexible natural-language querying and seamless integration into 3D mapping pipelines for mobile robotics.",
      "authors": [
        "Söhnke Benedikt Fischedick",
        "Daniel Seichter",
        "Benedict Stephan",
        "Robin Schmidt",
        "Horst-Michael Gross"
      ],
      "published": "2026-01-01T14:29:31Z",
      "categories": [
        "cs.CV",
        "cs.RO"
      ],
      "primary_category": "cs.CV",
      "abs_url": "https://arxiv.org/abs/2601.00359v1",
      "pdf_url": "https://arxiv.org/pdf/2601.00359v1.pdf"
    },
    {
      "id": "2601.00305v1",
      "title": "Replaceable Bit-based Gripper for Picking Cluttered Food Items",
      "abstract": "The food packaging industry goes through changes in food items and their weights quite rapidly. These items range from easy-to-pick, single-piece food items to flexible, long and cluttered ones. We propose a replaceable bit-based gripper system to tackle the challenge of weight-based handling of cluttered food items. The gripper features specialized food attachments(bits) that enhance its grasping capabilities, and a belt replacement system allows switching between different food items during packaging operations. It offers a wide range of control options, enabling it to grasp and drop specific weights of granular, cluttered, and entangled foods. We specifically designed bits for two flexible food items that differ in shape: ikura(salmon roe) and spaghetti. They represent the challenging categories of sticky, granular food and long, sticky, cluttered food, respectively. The gripper successfully picked up both spaghetti and ikura and demonstrated weight-specific dropping of these items with an accuracy over 80% and 95% respectively. The gripper system also exhibited quick switching between different bits, leading to the handling of a large range of food items.",
      "authors": [
        "Prashant Kumar",
        "Yukiyasu Domae",
        "Weiwei Wan",
        "Kensuke Harada"
      ],
      "published": "2026-01-01T10:54:41Z",
      "categories": [
        "cs.RO"
      ],
      "primary_category": "cs.RO",
      "abs_url": "https://arxiv.org/abs/2601.00305v1",
      "pdf_url": "https://arxiv.org/pdf/2601.00305v1.pdf"
    },
    {
      "id": "2601.00275v1",
      "title": "Pure Inertial Navigation in Challenging Environments with Wheeled and Chassis Mounted Inertial Sensors",
      "abstract": "Autonomous vehicles and wheeled robots are widely used in many applications in both indoor and outdoor settings. In practical situations with limited GNSS signals or degraded lighting conditions, the navigation solution may rely only on inertial sensors and as result drift in time due to errors in the inertial measurement. In this work, we propose WiCHINS, a wheeled and chassis inertial navigation system by combining wheel-mounted-inertial sensors with a chassis-mounted inertial sensor for accurate pure inertial navigation. To that end, we derive a three-stage framework, each with a dedicated extended Kalman filter. This framework utilizes the benefits of each location (wheel/body) during the estimation process. To evaluate our proposed approach, we employed a dataset with five inertial measurement units with a total recording time of 228.6 minutes. We compare our approach with four other inertial baselines and demonstrate an average position error of 11.4m, which is $2.4\\%$ of the average traveled distance, using two wheels and one body inertial measurement units. As a consequence, our proposed method enables robust navigation in challenging environments and helps bridge the pure-inertial performance gap.",
      "authors": [
        "Dusan Nemec",
        "Gal Versano",
        "Itai Savin",
        "Vojtech Simak",
        "Juraj Kekelak",
        "Itzik Klein"
      ],
      "published": "2026-01-01T09:28:05Z",
      "categories": [
        "cs.RO"
      ],
      "primary_category": "cs.RO",
      "abs_url": "https://arxiv.org/abs/2601.00275v1",
      "pdf_url": "https://arxiv.org/pdf/2601.00275v1.pdf"
    },
    {
      "id": "2601.00271v1",
      "title": "Vehicle Painting Robot Path Planning Using Hierarchical Optimization",
      "abstract": "In vehicle production factories, the vehicle painting process employs multiple robotic arms to simultaneously apply paint to car bodies advancing along a conveyor line. Designing paint paths for these robotic arms, which involves assigning car body areas to arms and determining paint sequences for each arm, remains a time-consuming manual task for engineers, indicating the demand for automation and design time reduction. The unique constraints of the painting process hinder the direct application of conventional robotic path planning techniques, such as those used in welding. Therefore, this paper formulates the design of paint paths as a hierarchical optimization problem, where the upper-layer subproblem resembles a vehicle routing problem (VRP), and the lower-layer subproblem involves detailed path planning. This approach allows the use of different optimization algorithms at each layer, and permits flexible handling of constraints specific to the vehicle painting process through the design of variable representation, constraints, repair operators, and an initialization process at the upper and lower layers. Experiments with three commercially available vehicle models demonstrated that the proposed method can automatically design paths that satisfy all constraints for vehicle painting with quality comparable to those created manually by engineers.",
      "authors": [
        "Yuya Nagai",
        "Hiromitsu Nakamura",
        "Narito Shinmachi",
        "Yuta Higashizono",
        "Satoshi Ono"
      ],
      "published": "2026-01-01T09:22:21Z",
      "categories": [
        "cs.RO",
        "cs.NE"
      ],
      "primary_category": "cs.RO",
      "abs_url": "https://arxiv.org/abs/2601.00271v1",
      "pdf_url": "https://arxiv.org/pdf/2601.00271v1.pdf"
    },
    {
      "id": "2601.00238v1",
      "title": "SLAP: Slapband-based Autonomous Perching Drone with Failure Recovery for Vertical Tree Trunks",
      "abstract": "Perching allows unmanned aerial vehicles (UAVs) to reduce energy consumption, remain anchored for surface sampling operations, or stably survey their surroundings. Previous efforts for perching on vertical surfaces have predominantly focused on lightweight mechanical design solutions with relatively scant system-level integration. Furthermore, perching strategies for vertical surfaces commonly require high-speed, aggressive landing operations that are dangerous for a surveyor drone with sensitive electronics onboard. This work presents the preliminary investigation of a perching approach suitable for larger drones that both gently perches on vertical tree trunks and reacts and recovers from perch failures. The system in this work, called SLAP, consists of vision-based perch site detector, an IMU (inertial-measurement-unit)-based perch failure detector, an attitude controller for soft perching, an optical close-range detection system, and a fast active elastic gripper with microspines made from commercially-available slapbands. We validated this approach on a modified 1.2 kg commercial quadrotor with component and system analysis. Initial human-in-the-loop autonomous indoor flight experiments achieved a 75% perch success rate on a real oak tree segment across 20 flights, and 100% perch failure recovery across 2 flights with induced failures.",
      "authors": [
        "Julia Di",
        "Kenneth A. W. Hoffmann",
        "Tony G. Chen",
        "Tian-Ao Ren",
        "Mark R. Cutkosky"
      ],
      "published": "2026-01-01T07:11:10Z",
      "categories": [
        "cs.RO"
      ],
      "primary_category": "cs.RO",
      "abs_url": "https://arxiv.org/abs/2601.00238v1",
      "pdf_url": "https://arxiv.org/pdf/2601.00238v1.pdf"
    },
    {
      "id": "2601.00237v1",
      "title": "Application Research of a Deep Learning Model Integrating CycleGAN and YOLO in PCB Infrared Defect Detection",
      "abstract": "This paper addresses the critical bottleneck of infrared (IR) data scarcity in Printed Circuit Board (PCB) defect detection by proposing a cross-modal data augmentation framework integrating CycleGAN and YOLOv8. Unlike conventional methods relying on paired supervision, we leverage CycleGAN to perform unpaired image-to-image translation, mapping abundant visible-light PCB images into the infrared domain. This generative process synthesizes high-fidelity pseudo-IR samples that preserve the structural semantics of defects while accurately simulating thermal distribution patterns. Subsequently, we construct a heterogeneous training strategy that fuses generated pseudo-IR data with limited real IR samples to train a lightweight YOLOv8 detector. Experimental results demonstrate that this method effectively enhances feature learning under low-data conditions. The augmented detector significantly outperforms models trained on limited real data alone and approaches the performance benchmarks of fully supervised training, proving the efficacy of pseudo-IR synthesis as a robust augmentation strategy for industrial inspection.",
      "authors": [
        "Chao Yang",
        "Haoyuan Zheng",
        "Yue Ma"
      ],
      "published": "2026-01-01T07:01:47Z",
      "categories": [
        "cs.CV",
        "cs.LG",
        "cs.RO"
      ],
      "primary_category": "cs.CV",
      "abs_url": "https://arxiv.org/abs/2601.00237v1",
      "pdf_url": "https://arxiv.org/pdf/2601.00237v1.pdf"
    },
    {
      "id": "2601.00207v1",
      "title": "CropNeRF: A Neural Radiance Field-Based Framework for Crop Counting",
      "abstract": "Rigorous crop counting is crucial for effective agricultural management and informed intervention strategies. However, in outdoor field environments, partial occlusions combined with inherent ambiguity in distinguishing clustered crops from individual viewpoints poses an immense challenge for image-based segmentation methods. To address these problems, we introduce a novel crop counting framework designed for exact enumeration via 3D instance segmentation. Our approach utilizes 2D images captured from multiple viewpoints and associates independent instance masks for neural radiance field (NeRF) view synthesis. We introduce crop visibility and mask consistency scores, which are incorporated alongside 3D information from a NeRF model. This results in an effective segmentation of crop instances in 3D and highly-accurate crop counts. Furthermore, our method eliminates the dependence on crop-specific parameter tuning. We validate our framework on three agricultural datasets consisting of cotton bolls, apples, and pears, and demonstrate consistent counting performance despite major variations in crop color, shape, and size. A comparative analysis against the state of the art highlights superior performance on crop counting tasks. Lastly, we contribute a cotton plant dataset to advance further research on this topic.",
      "authors": [
        "Md Ahmed Al Muzaddid",
        "William J. Beksi"
      ],
      "published": "2026-01-01T04:51:02Z",
      "categories": [
        "cs.CV",
        "cs.RO"
      ],
      "primary_category": "cs.CV",
      "abs_url": "https://arxiv.org/abs/2601.00207v1",
      "pdf_url": "https://arxiv.org/pdf/2601.00207v1.pdf"
    },
    {
      "id": "2601.00163v1",
      "title": "SLEI3D: Simultaneous Exploration and Inspection via Heterogeneous Fleets under Limited Communication",
      "abstract": "Robotic fleets such as unmanned aerial and ground vehicles have been widely used for routine inspections of static environments, where the areas of interest are known and planned in advance. However, in many applications, such areas of interest are unknown and should be identified online during exploration. Thus, this paper considers the problem of simultaneous exploration, inspection of unknown environments and then real-time communication to a mobile ground control station to report the findings. The heterogeneous robots are equipped with different sensors, e.g., long-range lidars for fast exploration and close-range cameras for detailed inspection. Furthermore, global communication is often unavailable in such environments, where the robots can only communicate with each other via ad-hoc wireless networks when they are in close proximity and free of obstruction. This work proposes a novel planning and coordination framework (SLEI3D) that integrates the online strategies for collaborative 3D exploration, adaptive inspection and timely communication (via the intermit-tent or proactive protocols). To account for uncertainties w.r.t. the number and location of features, a multi-layer and multi-rate planning mechanism is developed for inter-and-intra robot subgroups, to actively meet and coordinate their local plans. The proposed framework is validated extensively via high-fidelity simulations of numerous large-scale missions with up to 48 robots and 384 thousand cubic meters. Hardware experiments of 7 robots are also conducted. Project website is available at https://junfengchen-robotics.github.io/SLEI3D/.",
      "authors": [
        "Junfeng Chen",
        "Yuxiao Zhu",
        "Xintong Zhang",
        "Bing Luo",
        "Meng Guo"
      ],
      "published": "2026-01-01T02:06:35Z",
      "categories": [
        "cs.RO"
      ],
      "primary_category": "cs.RO",
      "abs_url": "https://arxiv.org/abs/2601.00163v1",
      "pdf_url": "https://arxiv.org/pdf/2601.00163v1.pdf"
    },
    {
      "id": "2601.00126v2",
      "title": "Compositional Diffusion with Guided Search for Long-Horizon Planning",
      "abstract": "Generative models have emerged as powerful tools for planning, with compositional approaches offering particular promise for modeling long-horizon task distributions by composing together local, modular generative models. This compositional paradigm spans diverse domains, from multi-step manipulation planning to panoramic image synthesis to long video generation. However, compositional generative models face a critical challenge: when local distributions are multimodal, existing composition methods average incompatible modes, producing plans that are neither locally feasible nor globally coherent. We propose Compositional Diffusion with Guided Search (CDGS), which addresses this mode averaging problem by embedding search directly within the diffusion denoising process. Our method explores diverse combinations of local modes through population-based sampling, prunes infeasible candidates using likelihood-based filtering, and enforces global consistency through iterative resampling between overlapping segments. CDGS matches oracle performance on seven robot manipulation tasks, outperforming baselines that lack compositionality or require long-horizon training data. The approach generalizes across domains, enabling coherent text-guided panoramic images and long videos through effective local-to-global message passing. More details: https://cdgsearch.github.io/",
      "authors": [
        "Utkarsh A Mishra",
        "David He",
        "Yongxin Chen",
        "Danfei Xu"
      ],
      "published": "2025-12-31T22:03:19Z",
      "categories": [
        "cs.RO"
      ],
      "primary_category": "cs.RO",
      "abs_url": "https://arxiv.org/abs/2601.00126v2",
      "pdf_url": "https://arxiv.org/pdf/2601.00126v2.pdf"
    },
    {
      "id": "2601.00116v1",
      "title": "GRL-SNAM: Geometric Reinforcement Learning with Path Differential Hamiltonians for Simultaneous Navigation and Mapping in Unknown Environments",
      "abstract": "We present GRL-SNAM, a geometric reinforcement learning framework for Simultaneous Navigation and Mapping(SNAM) in unknown environments. A SNAM problem is challenging as it needs to design hierarchical or joint policies of multiple agents that control the movement of a real-life robot towards the goal in mapless environment, i.e. an environment where the map of the environment is not available apriori, and needs to be acquired through sensors. The sensors are invoked from the path learner, i.e. navigator, through active query responses to sensory agents, and along the motion path. GRL-SNAM differs from preemptive navigation algorithms and other reinforcement learning methods by relying exclusively on local sensory observations without constructing a global map. Our approach formulates path navigation and mapping as a dynamic shortest path search and discovery process using controlled Hamiltonian optimization: sensory inputs are translated into local energy landscapes that encode reachability, obstacle barriers, and deformation constraints, while policies for sensing, planning, and reconfiguration evolve stagewise via updating Hamiltonians. A reduced Hamiltonian serves as an adaptive score function, updating kinetic/potential terms, embedding barrier constraints, and continuously refining trajectories as new local information arrives. We evaluate GRL-SNAM on two different 2D navigation tasks. Comparing against local reactive baselines and global policy learning references under identical stagewise sensing constraints, it preserves clearance, generalizes to unseen layouts, and demonstrates that Geometric RL learning via updating Hamiltonians enables high-quality navigation through minimal exploration via local energy refinement rather than extensive global mapping. The code is publicly available on \\href{https://github.com/CVC-Lab/GRL-SNAM}{Github}.",
      "authors": [
        "Aditya Sai Ellendula",
        "Yi Wang",
        "Minh Nguyen",
        "Chandrajit Bajaj"
      ],
      "published": "2025-12-31T21:27:20Z",
      "categories": [
        "cs.LG",
        "cs.RO"
      ],
      "primary_category": "cs.LG",
      "abs_url": "https://arxiv.org/abs/2601.00116v1",
      "pdf_url": "https://arxiv.org/pdf/2601.00116v1.pdf"
    },
    {
      "id": "2601.00087v1",
      "title": "Reinforcement learning with timed constraints for robotics motion planning",
      "abstract": "Robotic systems operating in dynamic and uncertain environments increasingly require planners that satisfy complex task sequences while adhering to strict temporal constraints. Metric Interval Temporal Logic (MITL) offers a formal and expressive framework for specifying such time-bounded requirements; however, integrating MITL with reinforcement learning (RL) remains challenging due to stochastic dynamics and partial observability. This paper presents a unified automata-based RL framework for synthesizing policies in both Markov Decision Processes (MDPs) and Partially Observable Markov Decision Processes (POMDPs) under MITL specifications. MITL formulas are translated into Timed Limit-Deterministic Generalized Büchi Automata (Timed-LDGBA) and synchronized with the underlying decision process to construct product timed models suitable for Q-learning. A simple yet expressive reward structure enforces temporal correctness while allowing additional performance objectives. The approach is validated in three simulation studies: a $5 \\times 5$ grid-world formulated as an MDP, a $10 \\times 10$ grid-world formulated as a POMDP, and an office-like service-robot scenario. Results demonstrate that the proposed framework consistently learns policies that satisfy strict time-bounded requirements under stochastic transitions, scales to larger state spaces, and remains effective in partially observable environments, highlighting its potential for reliable robotic planning in time-critical and uncertain settings.",
      "authors": [
        "Zhaoan Wang",
        "Junchao Li",
        "Mahdi Mohammad",
        "Shaoping Xiao"
      ],
      "published": "2025-12-31T19:43:44Z",
      "categories": [
        "cs.RO",
        "cs.LG"
      ],
      "primary_category": "cs.RO",
      "abs_url": "https://arxiv.org/abs/2601.00087v1",
      "pdf_url": "https://arxiv.org/pdf/2601.00087v1.pdf"
    },
    {
      "id": "2512.25075v1",
      "title": "SpaceTimePilot: Generative Rendering of Dynamic Scenes Across Space and Time",
      "abstract": "We present SpaceTimePilot, a video diffusion model that disentangles space and time for controllable generative rendering. Given a monocular video, SpaceTimePilot can independently alter the camera viewpoint and the motion sequence within the generative process, re-rendering the scene for continuous and arbitrary exploration across space and time. To achieve this, we introduce an effective animation time-embedding mechanism in the diffusion process, allowing explicit control of the output video's motion sequence with respect to that of the source video. As no datasets provide paired videos of the same dynamic scene with continuous temporal variations, we propose a simple yet effective temporal-warping training scheme that repurposes existing multi-view datasets to mimic temporal differences. This strategy effectively supervises the model to learn temporal control and achieve robust space-time disentanglement. To further enhance the precision of dual control, we introduce two additional components: an improved camera-conditioning mechanism that allows altering the camera from the first frame, and CamxTime, the first synthetic space-and-time full-coverage rendering dataset that provides fully free space-time video trajectories within a scene. Joint training on the temporal-warping scheme and the CamxTime dataset yields more precise temporal control. We evaluate SpaceTimePilot on both real-world and synthetic data, demonstrating clear space-time disentanglement and strong results compared to prior work. Project page: https://zheninghuang.github.io/Space-Time-Pilot/ Code: https://github.com/ZheningHuang/spacetimepilot",
      "authors": [
        "Zhening Huang",
        "Hyeonho Jeong",
        "Xuelin Chen",
        "Yulia Gryaditskaya",
        "Tuanfeng Y. Wang",
        "Joan Lasenby",
        "Chun-Hao Huang"
      ],
      "published": "2025-12-31T18:59:57Z",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.RO"
      ],
      "primary_category": "cs.CV",
      "abs_url": "https://arxiv.org/abs/2512.25075v1",
      "pdf_url": "https://arxiv.org/pdf/2512.25075v1.pdf"
    },
    {
      "id": "2512.25072v1",
      "title": "Coordinated Humanoid Manipulation with Choice Policies",
      "abstract": "Humanoid robots hold great promise for operating in human-centric environments, yet achieving robust whole-body coordination across the head, hands, and legs remains a major challenge. We present a system that combines a modular teleoperation interface with a scalable learning framework to address this problem. Our teleoperation design decomposes humanoid control into intuitive submodules, which include hand-eye coordination, grasp primitives, arm end-effector tracking, and locomotion. This modularity allows us to collect high-quality demonstrations efficiently. Building on this, we introduce Choice Policy, an imitation learning approach that generates multiple candidate actions and learns to score them. This architecture enables both fast inference and effective modeling of multimodal behaviors. We validate our approach on two real-world tasks: dishwasher loading and whole-body loco-manipulation for whiteboard wiping. Experiments show that Choice Policy significantly outperforms diffusion policies and standard behavior cloning. Furthermore, our results indicate that hand-eye coordination is critical for success in long-horizon tasks. Our work demonstrates a practical path toward scalable data collection and learning for coordinated humanoid manipulation in unstructured environments.",
      "authors": [
        "Haozhi Qi",
        "Yen-Jen Wang",
        "Toru Lin",
        "Brent Yi",
        "Yi Ma",
        "Koushil Sreenath",
        "Jitendra Malik"
      ],
      "published": "2025-12-31T18:59:53Z",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.RO",
      "abs_url": "https://arxiv.org/abs/2512.25072v1",
      "pdf_url": "https://arxiv.org/pdf/2512.25072v1.pdf"
    },
    {
      "id": "2512.24985v1",
      "title": "DarkEQA: Benchmarking Vision-Language Models for Embodied Question Answering in Low-Light Indoor Environments",
      "abstract": "Vision Language Models (VLMs) are increasingly adopted as central reasoning modules for embodied agents. Existing benchmarks evaluate their capabilities under ideal, well-lit conditions, yet robust 24/7 operation demands performance under a wide range of visual degradations, including low-light conditions at night or in dark environments--a core necessity that has been largely overlooked. To address this underexplored challenge, we present DarkEQA, an open-source benchmark for evaluating EQA-relevant perceptual primitives under multi-level low-light conditions. DarkEQA isolates the perception bottleneck by evaluating question answering from egocentric observations under controlled degradations, enabling attributable robustness analysis. A key design feature of DarkEQA is its physical fidelity: visual degradations are modeled in linear RAW space, simulating physics-based illumination drop and sensor noise followed by an ISP-inspired rendering pipeline. We demonstrate the utility of DarkEQA by evaluating a wide range of state-of-the-art VLMs and Low-Light Image Enhancement (LLIE) models. Our analysis systematically reveals VLMs' limitations when operating under these challenging visual conditions. Our code and benchmark dataset will be released upon acceptance.",
      "authors": [
        "Yohan Park",
        "Hyunwoo Ha",
        "Wonjun Jo",
        "Tae-Hyun Oh"
      ],
      "published": "2025-12-31T17:31:29Z",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG",
        "cs.RO"
      ],
      "primary_category": "cs.CV",
      "abs_url": "https://arxiv.org/abs/2512.24985v1",
      "pdf_url": "https://arxiv.org/pdf/2512.24985v1.pdf"
    },
    {
      "id": "2512.24974v1",
      "title": "Hierarchical Deformation Planning and Neural Tracking for DLOs in Constrained Environments",
      "abstract": "Deformable linear objects (DLOs) manipulation presents significant challenges due to DLOs' inherent high-dimensional state space and complex deformation dynamics. The wide-populated obstacles in realistic workspaces further complicate DLO manipulation, necessitating efficient deformation planning and robust deformation tracking. In this work, we propose a novel framework for DLO manipulation in constrained environments. This framework combines hierarchical deformation planning with neural tracking, ensuring reliable performance in both global deformation synthesis and local deformation tracking. Specifically, the deformation planner begins by generating a spatial path set that inherently satisfies the homotopic constraints associated with DLO keypoint paths. Next, a path-set-guided optimization method is applied to synthesize an optimal temporal deformation sequence for the DLO. In manipulation execution, a neural model predictive control approach, leveraging a data-driven deformation model, is designed to accurately track the planned DLO deformation sequence. The effectiveness of the proposed framework is validated in extensive constrained DLO manipulation tasks.",
      "authors": [
        "Yunxi Tang",
        "Tianqi Yang",
        "Jing Huang",
        "Xiangyu Chu",
        "Kwok Wai Samuel Au"
      ],
      "published": "2025-12-31T17:11:53Z",
      "categories": [
        "cs.RO"
      ],
      "primary_category": "cs.RO",
      "abs_url": "https://arxiv.org/abs/2512.24974v1",
      "pdf_url": "https://arxiv.org/pdf/2512.24974v1.pdf"
    },
    {
      "id": "2601.00898v1",
      "title": "Dichotomous Diffusion Policy Optimization",
      "abstract": "Diffusion-based policies have gained growing popularity in solving a wide range of decision-making tasks due to their superior expressiveness and controllable generation during inference. However, effectively training large diffusion policies using reinforcement learning (RL) remains challenging. Existing methods either suffer from unstable training due to directly maximizing value objectives, or face computational issues due to relying on crude Gaussian likelihood approximation, which requires a large amount of sufficiently small denoising steps. In this work, we propose DIPOLE (Dichotomous diffusion Policy improvement), a novel RL algorithm designed for stable and controllable diffusion policy optimization. We begin by revisiting the KL-regularized objective in RL, which offers a desirable weighted regression objective for diffusion policy extraction, but often struggles to balance greediness and stability. We then formulate a greedified policy regularization scheme, which naturally enables decomposing the optimal policy into a pair of stably learned dichotomous policies: one aims at reward maximization, and the other focuses on reward minimization. Under such a design, optimized actions can be generated by linearly combining the scores of dichotomous policies during inference, thereby enabling flexible control over the level of greediness.Evaluations in offline and offline-to-online RL settings on ExORL and OGBench demonstrate the effectiveness of our approach. We also use DIPOLE to train a large vision-language-action (VLA) model for end-to-end autonomous driving (AD) and evaluate it on the large-scale real-world AD benchmark NAVSIM, highlighting its potential for complex real-world applications.",
      "authors": [
        "Ruiming Liang",
        "Yinan Zheng",
        "Kexin Zheng",
        "Tianyi Tan",
        "Jianxiong Li",
        "Liyuan Mao",
        "Zhihao Wang",
        "Guang Chen",
        "Hangjun Ye",
        "Jingjing Liu",
        "Jinqiao Wang",
        "Xianyuan Zhan"
      ],
      "published": "2025-12-31T16:56:56Z",
      "categories": [
        "cs.LG",
        "cs.RO"
      ],
      "primary_category": "cs.LG",
      "abs_url": "https://arxiv.org/abs/2601.00898v1",
      "pdf_url": "https://arxiv.org/pdf/2601.00898v1.pdf"
    },
    {
      "id": "2512.24955v1",
      "title": "MSACL: Multi-Step Actor-Critic Learning with Lyapunov Certificates for Exponentially Stabilizing Control",
      "abstract": "Achieving provable stability in model-free reinforcement learning (RL) remains a challenge, particularly in balancing exploration with rigorous safety. This article introduces MSACL, a framework that integrates exponential stability theory with maximum entropy RL through multi-step Lyapunov certificate learning. Unlike methods relying on complex reward engineering, MSACL utilizes off-policy multi-step data to learn Lyapunov certificates satisfying theoretical stability conditions. By introducing Exponential Stability Labels (ESL) and a $λ$-weighted aggregation mechanism, the framework effectively balances the bias-variance trade-off in multi-step learning. Policy optimization is guided by a stability-aware advantage function, ensuring the learned policy promotes rapid Lyapunov descent. We evaluate MSACL across six benchmarks, including stabilization and nonlinear tracking tasks, demonstrating its superiority over state-of-the-art Lyapunov-based RL algorithms. MSACL achieves exponential stability and rapid convergence under simple rewards, while exhibiting significant robustness to uncertainties and generalization to unseen trajectories. Sensitivity analysis establishes the multi-step horizon $n=20$ as a robust default across diverse systems. By linking Lyapunov theory with off-policy actor-critic frameworks, MSACL provides a foundation for verifiably safe learning-based control. Source code and benchmark environments will be made publicly available.",
      "authors": [
        "Yongwei Zhang",
        "Yuanzhe Xing",
        "Quan Quan",
        "Zhikun She"
      ],
      "published": "2025-12-31T16:36:44Z",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.RO",
        "eess.SY"
      ],
      "primary_category": "cs.LG",
      "abs_url": "https://arxiv.org/abs/2512.24955v1",
      "pdf_url": "https://arxiv.org/pdf/2512.24955v1.pdf"
    },
    {
      "id": "2512.24851v1",
      "title": "VLN-MME: Diagnosing MLLMs as Language-guided Visual Navigation agents",
      "abstract": "Multimodal Large Language Models (MLLMs) have demonstrated remarkable capabilities across a wide range of vision-language tasks. However, their performance as embodied agents, which requires multi-round dialogue spatial reasoning and sequential action prediction, needs further exploration. Our work investigates this potential in the context of Vision-and-Language Navigation (VLN) by introducing a unified and extensible evaluation framework to probe MLLMs as zero-shot agents by bridging traditional navigation datasets into a standardized benchmark, named VLN-MME. We simplify the evaluation with a highly modular and accessible design. This flexibility streamlines experiments, enabling structured comparisons and component-level ablations across diverse MLLM architectures, agent designs, and navigation tasks. Crucially, enabled by our framework, we observe that enhancing our baseline agent with Chain-of-Thought (CoT) reasoning and self-reflection leads to an unexpected performance decrease. This suggests MLLMs exhibit poor context awareness in embodied navigation tasks; although they can follow instructions and structure their output, their 3D spatial reasoning fidelity is low. VLN-MME lays the groundwork for systematic evaluation of general-purpose MLLMs in embodied navigation settings and reveals limitations in their sequential decision-making capabilities. We believe these findings offer crucial guidance for MLLM post-training as embodied agents.",
      "authors": [
        "Xunyi Zhao",
        "Gengze Zhou",
        "Qi Wu"
      ],
      "published": "2025-12-31T13:21:21Z",
      "categories": [
        "cs.CV",
        "cs.RO"
      ],
      "primary_category": "cs.CV",
      "abs_url": "https://arxiv.org/abs/2512.24851v1",
      "pdf_url": "https://arxiv.org/pdf/2512.24851v1.pdf"
    },
    {
      "id": "2512.24845v1",
      "title": "ArtiSG: Functional 3D Scene Graph Construction via Human-demonstrated Articulated Objects Manipulation",
      "abstract": "3D scene graphs have empowered robots with semantic understanding for navigation and planning, yet they often lack the functional information required for physical manipulation, particularly regarding articulated objects. Existing approaches for inferring articulation mechanisms from static observations are prone to visual ambiguity, while methods that estimate parameters from state changes typically rely on constrained settings such as fixed cameras and unobstructed views. Furthermore, fine-grained functional elements like small handles are frequently missed by general object detectors. To bridge this gap, we present ArtiSG, a framework that constructs functional 3D scene graphs by encoding human demonstrations into structured robotic memory. Our approach leverages a robust articulation data collection pipeline utilizing a portable setup to accurately estimate 6-DoF articulation trajectories and axes even under camera ego-motion. We integrate these kinematic priors into a hierarchical and open-vocabulary graph while utilizing interaction data to discover inconspicuous functional elements missed by visual perception. Extensive real-world experiments demonstrate that ArtiSG significantly outperforms baselines in functional element recall and articulation estimation precision. Moreover, we show that the constructed graph serves as a reliable functional memory that effectively guides robots to perform language-directed manipulation tasks in real-world environments containing diverse articulated objects.",
      "authors": [
        "Qiuyi Gu",
        "Yuze Sheng",
        "Jincheng Yu",
        "Jiahao Tang",
        "Xiaolong Shan",
        "Zhaoyang Shen",
        "Tinghao Yi",
        "Xiaodan Liang",
        "Xinlei Chen",
        "Yu Wang"
      ],
      "published": "2025-12-31T13:10:40Z",
      "categories": [
        "cs.RO"
      ],
      "primary_category": "cs.RO",
      "abs_url": "https://arxiv.org/abs/2512.24845v1",
      "pdf_url": "https://arxiv.org/pdf/2512.24845v1.pdf"
    },
    {
      "id": "2512.24838v1",
      "title": "CropTrack: A Tracking with Re-Identification Framework for Precision Agriculture",
      "abstract": "Multiple-object tracking (MOT) in agricultural environments presents major challenges due to repetitive patterns, similar object appearances, sudden illumination changes, and frequent occlusions. Contemporary trackers in this domain rely on the motion of objects rather than appearance for association. Nevertheless, they struggle to maintain object identities when targets undergo frequent and strong occlusions. The high similarity of object appearances makes integrating appearance-based association nontrivial for agricultural scenarios. To solve this problem we propose CropTrack, a novel MOT framework based on the combination of appearance and motion information. CropTrack integrates a reranking-enhanced appearance association, a one-to-many association with appearance-based conflict resolution strategy, and an exponential moving average prototype feature bank to improve appearance-based association. Evaluated on publicly available agricultural MOT datasets, CropTrack demonstrates consistent identity preservation, outperforming traditional motion-based tracking methods. Compared to the state of the art, CropTrack achieves significant gains in identification F1 and association accuracy scores with a lower number of identity switches.",
      "authors": [
        "Md Ahmed Al Muzaddid",
        "Jordan A. James",
        "William J. Beksi"
      ],
      "published": "2025-12-31T12:59:38Z",
      "categories": [
        "cs.CV",
        "cs.RO"
      ],
      "primary_category": "cs.CV",
      "abs_url": "https://arxiv.org/abs/2512.24838v1",
      "pdf_url": "https://arxiv.org/pdf/2512.24838v1.pdf"
    },
    {
      "id": "2512.24829v1",
      "title": "Explaining Why Things Go Where They Go: Interpretable Constructs of Human Organizational Preferences",
      "abstract": "Robotic systems for household object rearrangement often rely on latent preference models inferred from human demonstrations. While effective at prediction, these models offer limited insight into the interpretable factors that guide human decisions. We introduce an explicit formulation of object arrangement preferences along four interpretable constructs: spatial practicality (putting items where they naturally fit best in the space), habitual convenience (making frequently used items easy to reach), semantic coherence (placing items together if they are used for the same task or are contextually related), and commonsense appropriateness (putting things where people would usually expect to find them). To capture these constructs, we designed and validated a self-report questionnaire through a 63-participant online study. Results confirm the psychological distinctiveness of these constructs and their explanatory power across two scenarios (kitchen and living room). We demonstrate the utility of these constructs by integrating them into a Monte Carlo Tree Search (MCTS) planner and show that when guided by participant-derived preferences, our planner can generate reasonable arrangements that closely align with those generated by participants. This work contributes a compact, interpretable formulation of object arrangement preferences and a demonstration of how it can be operationalized for robot planning.",
      "authors": [
        "Emmanuel Fashae",
        "Michael Burke",
        "Leimin Tian",
        "Lingheng Meng",
        "Pamela Carreno-Medrano"
      ],
      "published": "2025-12-31T12:46:05Z",
      "categories": [
        "cs.AI",
        "cs.HC",
        "cs.RO"
      ],
      "primary_category": "cs.AI",
      "abs_url": "https://arxiv.org/abs/2512.24829v1",
      "pdf_url": "https://arxiv.org/pdf/2512.24829v1.pdf"
    },
    {
      "id": "2512.24766v1",
      "title": "Dream2Flow: Bridging Video Generation and Open-World Manipulation with 3D Object Flow",
      "abstract": "Generative video modeling has emerged as a compelling tool to zero-shot reason about plausible physical interactions for open-world manipulation. Yet, it remains a challenge to translate such human-led motions into the low-level actions demanded by robotic systems. We observe that given an initial image and task instruction, these models excel at synthesizing sensible object motions. Thus, we introduce Dream2Flow, a framework that bridges video generation and robotic control through 3D object flow as an intermediate representation. Our method reconstructs 3D object motions from generated videos and formulates manipulation as object trajectory tracking. By separating the state changes from the actuators that realize those changes, Dream2Flow overcomes the embodiment gap and enables zero-shot guidance from pre-trained video models to manipulate objects of diverse categories-including rigid, articulated, deformable, and granular. Through trajectory optimization or reinforcement learning, Dream2Flow converts reconstructed 3D object flow into executable low-level commands without task-specific demonstrations. Simulation and real-world experiments highlight 3D object flow as a general and scalable interface for adapting video generation models to open-world robotic manipulation. Videos and visualizations are available at https://dream2flow.github.io/.",
      "authors": [
        "Karthik Dharmarajan",
        "Wenlong Huang",
        "Jiajun Wu",
        "Li Fei-Fei",
        "Ruohan Zhang"
      ],
      "published": "2025-12-31T10:25:24Z",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.RO",
      "abs_url": "https://arxiv.org/abs/2512.24766v1",
      "pdf_url": "https://arxiv.org/pdf/2512.24766v1.pdf"
    },
    {
      "id": "2512.24740v1",
      "title": "Control of Microrobots with Reinforcement Learning under On-Device Compute Constraints",
      "abstract": "An important function of autonomous microrobots is the ability to perform robust movement over terrain. This paper explores an edge ML approach to microrobot locomotion, allowing for on-device, lower latency control under compute, memory, and power constraints. This paper explores the locomotion of a sub-centimeter quadrupedal microrobot via reinforcement learning (RL) and deploys the resulting controller on an ultra-small system-on-chip (SoC), SC$μ$M-3C, featuring an ARM Cortex-M0 microcontroller running at 5 MHz. We train a compact FP32 multilayer perceptron (MLP) policy with two hidden layers ($[128, 64]$) in a massively parallel GPU simulation and enhance robustness by utilizing domain randomization over simulation parameters. We then study integer (Int8) quantization (per-tensor and per-feature) to allow for higher inference update rates on our resource-limited hardware, and we connect hardware power budgets to achievable update frequency via a cycles-per-update model for inference on our Cortex-M0. We propose a resource-aware gait scheduling viewpoint: given a device power budget, we can select the gait mode (trot/intermediate/gallop) that maximizes expected RL reward at a corresponding feasible update frequency. Finally, we deploy our MLP policy on a real-world large-scale robot on uneven terrain, qualitatively noting that domain-randomized training can improve out-of-distribution stability. We do not claim real-world large-robot empirical zero-shot transfer in this work.",
      "authors": [
        "Yichen Liu",
        "Kesava Viswanadha",
        "Zhongyu Li",
        "Nelson Lojo",
        "Kristofer S. J. Pister"
      ],
      "published": "2025-12-31T09:18:36Z",
      "categories": [
        "cs.RO",
        "eess.SY"
      ],
      "primary_category": "cs.RO",
      "abs_url": "https://arxiv.org/abs/2512.24740v1",
      "pdf_url": "https://arxiv.org/pdf/2512.24740v1.pdf"
    },
    {
      "id": "2512.24712v2",
      "title": "LSRE: Latent Semantic Rule Encoding for Real-Time Semantic Risk Detection in Autonomous Driving",
      "abstract": "Real-world autonomous driving must adhere to complex human social rules that extend beyond legally codified traffic regulations. Many of these semantic constraints, such as yielding to emergency vehicles, complying with traffic officers' gestures, or stopping for school buses, are intuitive for humans yet difficult to encode explicitly. Although large vision-language models (VLMs) can interpret such semantics, their inference cost makes them impractical for real-time deployment. This work proposes LSRE, a Latent Semantic Rule Encoding framework that converts sparsely sampled VLM judgments into decision boundaries within the latent space of a recurrent world model. By encoding language-defined safety semantics into a lightweight latent classifier, LSRE enables real-time semantic risk assessment at 10 Hz without per-frame VLM queries. Experiments on six semantic-failure scenarios in CARLA demonstrate that LSRE attains semantic risk detection accuracy comparable to a large VLM baseline, while providing substantially earlier hazard anticipation and maintaining low computational latency. LSRE further generalizes to rarely seen semantic-similar test cases, indicating that language-guided latent classification offers an effective and deployable mechanism for semantic safety monitoring in autonomous driving.",
      "authors": [
        "Qian Cheng",
        "Weitao Zhou",
        "Cheng Jing",
        "Nanshan Deng",
        "Junze Wen",
        "Zhaoyang Liu",
        "Kun Jiang",
        "Diange Yang"
      ],
      "published": "2025-12-31T08:27:10Z",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "abs_url": "https://arxiv.org/abs/2512.24712v2",
      "pdf_url": "https://arxiv.org/pdf/2512.24712v2.pdf"
    },
    {
      "id": "2512.24698v1",
      "title": "Dynamic Policy Learning for Legged Robot with Simplified Model Pretraining and Model Homotopy Transfer",
      "abstract": "Generating dynamic motions for legged robots remains a challenging problem. While reinforcement learning has achieved notable success in various legged locomotion tasks, producing highly dynamic behaviors often requires extensive reward tuning or high-quality demonstrations. Leveraging reduced-order models can help mitigate these challenges. However, the model discrepancy poses a significant challenge when transferring policies to full-body dynamics environments. In this work, we introduce a continuation-based learning framework that combines simplified model pretraining and model homotopy transfer to efficiently generate and refine complex dynamic behaviors. First, we pretrain the policy using a single rigid body model to capture core motion patterns in a simplified environment. Next, we employ a continuation strategy to progressively transfer the policy to the full-body environment, minimizing performance loss. To define the continuation path, we introduce a model homotopy from the single rigid body model to the full-body model by gradually redistributing mass and inertia between the trunk and legs. The proposed method not only achieves faster convergence but also demonstrates superior stability during the transfer process compared to baseline methods. Our framework is validated on a range of dynamic tasks, including flips and wall-assisted maneuvers, and is successfully deployed on a real quadrupedal robot.",
      "authors": [
        "Dongyun Kang",
        "Min-Gyu Kim",
        "Tae-Gyu Song",
        "Hajun Kim",
        "Sehoon Ha",
        "Hae-Won Park"
      ],
      "published": "2025-12-31T08:04:22Z",
      "categories": [
        "cs.RO"
      ],
      "primary_category": "cs.RO",
      "abs_url": "https://arxiv.org/abs/2512.24698v1",
      "pdf_url": "https://arxiv.org/pdf/2512.24698v1.pdf"
    },
    {
      "id": "2512.24688v1",
      "title": "CREPES-X: Hierarchical Bearing-Distance-Inertial Direct Cooperative Relative Pose Estimation System",
      "abstract": "Relative localization is critical for cooperation in autonomous multi-robot systems. Existing approaches either rely on shared environmental features or inertial assumptions or suffer from non-line-of-sight degradation and outliers in complex environments. Robust and efficient fusion of inter-robot measurements such as bearings, distances, and inertials for tens of robots remains challenging. We present CREPES-X (Cooperative RElative Pose Estimation System with multiple eXtended features), a hierarchical relative localization framework that enhances speed, accuracy, and robustness under challenging conditions, without requiring any global information. CREPES-X starts with a compact hardware design: InfraRed (IR) LEDs, an IR camera, an ultra-wideband module, and an IMU housed in a cube no larger than 6cm on each side. Then CREPES-X implements a two-stage hierarchical estimator to meet different requirements, considering speed, accuracy, and robustness. First, we propose a single-frame relative estimator that provides instant relative poses for multi-robot setups through a closed-form solution and robust bearing outlier rejection. Then a multi-frame relative estimator is designed to offer accurate and robust relative states by exploring IMU pre-integration via robocentric relative kinematics with loosely- and tightly-coupled optimization. Extensive simulations and real-world experiments validate the effectiveness of CREPES-X, showing robustness to up to 90% bearing outliers, proving resilience in challenging conditions, and achieving RMSE of 0.073m and 1.817° in real-world datasets.",
      "authors": [
        "Zhehan Li",
        "Zheng Wang",
        "Jiadong Lu",
        "Qi Liu",
        "Zhiren Xun",
        "Yue Wang",
        "Fei Gao",
        "Chao Xu",
        "Yanjun Cao"
      ],
      "published": "2025-12-31T07:47:24Z",
      "categories": [
        "cs.RO"
      ],
      "primary_category": "cs.RO",
      "abs_url": "https://arxiv.org/abs/2512.24688v1",
      "pdf_url": "https://arxiv.org/pdf/2512.24688v1.pdf"
    },
    {
      "id": "2512.24680v1",
      "title": "ReSPIRe: Informative and Reusable Belief Tree Search for Robot Probabilistic Search and Tracking in Unknown Environments",
      "abstract": "Target search and tracking (SAT) is a fundamental problem for various robotic applications such as search and rescue and environmental exploration. This paper proposes an informative trajectory planning approach, namely ReSPIRe, for SAT in unknown cluttered environments under considerably inaccurate prior target information and limited sensing field of view. We first develop a novel sigma point-based approximation approach to fast and accurately estimate mutual information reward under non-Gaussian belief distributions, utilizing informative sampling in state and observation spaces to mitigate the computational intractability of integral calculation. To tackle significant uncertainty associated with inadequate prior target information, we propose the hierarchical particle structure in ReSPIRe, which not only extracts critical particles for global route guidance, but also adjusts the particle number adaptively for planning efficiency. Building upon the hierarchical structure, we develop the reusable belief tree search approach to build a policy tree for online trajectory planning under uncertainty, which reuses rollout evaluation to improve planning efficiency. Extensive simulations and real-world experiments demonstrate that ReSPIRe outperforms representative benchmark methods with smaller MI approximation error, higher search efficiency, and more stable tracking performance, while maintaining outstanding computational efficiency.",
      "authors": [
        "Kangjie Zhou",
        "Zhaoyang Li",
        "Han Gao",
        "Yao Su",
        "Hangxin Liu",
        "Junzhi Yu",
        "Chang Liu"
      ],
      "published": "2025-12-31T07:13:14Z",
      "categories": [
        "cs.RO"
      ],
      "primary_category": "cs.RO",
      "abs_url": "https://arxiv.org/abs/2512.24680v1",
      "pdf_url": "https://arxiv.org/pdf/2512.24680v1.pdf"
    },
    {
      "id": "2512.24673v1",
      "title": "VLA-RAIL: A Real-Time Asynchronous Inference Linker for VLA Models and Robots",
      "abstract": "Vision-Language-Action (VLA) models have achieved remarkable breakthroughs in robotics, with the action chunk playing a dominant role in these advances. Given the real-time and continuous nature of robotic motion control, the strategies for fusing a queue of successive action chunks have a profound impact on the overall performance of VLA models. Existing methods suffer from jitter, stalling, or even pauses in robotic action execution, which not only limits the achievable execution speed but also reduces the overall success rate of task completion. This paper introduces VLA-RAIL (A Real-Time Asynchronous Inference Linker), a novel framework designed to address these issues by conducting model inference and robot motion control asynchronously and guaranteeing smooth, continuous, and high-speed action execution. The core contributions of the paper are two fold: a Trajectory Smoother that effectively filters out the noise and jitter in the trajectory of one action chunk using polynomial fitting and a Chunk Fuser that seamlessly align the current executing trajectory and the newly arrived chunk, ensuring position, velocity, and acceleration continuity between two successive action chunks. We validate the effectiveness of VLA-RAIL on a benchmark of dynamic simulation tasks and several real-world manipulation tasks. Experimental results demonstrate that VLA-RAIL significantly reduces motion jitter, enhances execution speed, and improves task success rates, which will become a key infrastructure for the large-scale deployment of VLA models.",
      "authors": [
        "Yongsheng Zhao",
        "Lei Zhao",
        "Baoping Cheng",
        "Gongxin Yao",
        "Xuanzhang Wen",
        "Han Gao"
      ],
      "published": "2025-12-31T06:59:42Z",
      "categories": [
        "cs.RO",
        "cs.AI",
        "eess.SY"
      ],
      "primary_category": "cs.RO",
      "abs_url": "https://arxiv.org/abs/2512.24673v1",
      "pdf_url": "https://arxiv.org/pdf/2512.24673v1.pdf"
    },
    {
      "id": "2512.24657v1",
      "title": "Antagonistic Bowden-Cable Actuation of a Lightweight Robotic Hand: Toward Dexterous Manipulation for Payload Constrained Humanoids",
      "abstract": "Humanoid robots toward human-level dexterity require robotic hands capable of simultaneously providing high grasping force, rapid actuation speeds, multiple degrees of freedom, and lightweight structures within human-like size constraints. Meeting these conflicting requirements remains challenging, as satisfying this combination typically necessitates heavier actuators and bulkier transmission systems, significantly restricting the payload capacity of robot arms. In this letter, we present a lightweight anthropomorphic hand actuated by Bowden cables, which uniquely combines rolling-contact joint optimization with antagonistic cable actuation, enabling single-motor-per-joint control with negligible cable-length deviation. By relocating the actuator module to the torso, the design substantially reduces distal mass while maintaining anthropomorphic scale and dexterity. Additionally, this antagonistic cable actuation eliminates the need for synchronization between motors. Using the proposed methods, the hand assembly with a distal mass of 236g (excluding remote actuators and Bowden sheaths) demonstrated reliable execution of dexterous tasks, exceeding 18N fingertip force and lifting payloads over one hundred times its own mass. Furthermore, robustness was validated through Cutkosky taxonomy grasps and trajectory consistency under perturbed actuator-hand transformations.",
      "authors": [
        "Sungjae Min",
        "Hyungjoo Kim",
        "David Hyunchul Shim"
      ],
      "published": "2025-12-31T06:07:11Z",
      "categories": [
        "cs.RO"
      ],
      "primary_category": "cs.RO",
      "abs_url": "https://arxiv.org/abs/2512.24657v1",
      "pdf_url": "https://arxiv.org/pdf/2512.24657v1.pdf"
    },
    {
      "id": "2512.24653v1",
      "title": "RoboMIND 2.0: A Multimodal, Bimanual Mobile Manipulation Dataset for Generalizable Embodied Intelligence",
      "abstract": "While data-driven imitation learning has revolutionized robotic manipulation, current approaches remain constrained by the scarcity of large-scale, diverse real-world demonstrations. Consequently, the ability of existing models to generalize across long-horizon bimanual tasks and mobile manipulation in unstructured environments remains limited. To bridge this gap, we present RoboMIND 2.0, a comprehensive real-world dataset comprising over 310K dual-arm manipulation trajectories collected across six distinct robot embodiments and 739 complex tasks. Crucially, to support research in contact-rich and spatially extended tasks, the dataset incorporates 12K tactile-enhanced episodes and 20K mobile manipulation trajectories. Complementing this physical data, we construct high-fidelity digital twins of our real-world environments, releasing an additional 20K-trajectory simulated dataset to facilitate robust sim-to-real transfer. To fully exploit the potential of RoboMIND 2.0, we propose MIND-2 system, a hierarchical dual-system frame-work optimized via offline reinforcement learning. MIND-2 integrates a high-level semantic planner (MIND-2-VLM) to decompose abstract natural language instructions into grounded subgoals, coupled with a low-level Vision-Language-Action executor (MIND-2-VLA), which generates precise, proprioception-aware motor actions.",
      "authors": [
        "Chengkai Hou",
        "Kun Wu",
        "Jiaming Liu",
        "Zhengping Che",
        "Di Wu",
        "Fei Liao",
        "Guangrun Li",
        "Jingyang He",
        "Qiuxuan Feng",
        "Zhao Jin",
        "Chenyang Gu",
        "Zhuoyang Liu",
        "Nuowei Han",
        "Xiangju Mi",
        "Yaoxu Lv",
        "Yankai Fu",
        "Gaole Dai",
        "Langzhe Gu",
        "Tao Li",
        "Yuheng Zhang",
        "Yixue Zhang",
        "Xinhua Wang",
        "Shichao Fan",
        "Meng Li",
        "Zhen Zhao",
        "Ning Liu",
        "Zhiyuan Xu",
        "Pei Ren",
        "Junjie Ji",
        "Haonan Liu",
        "Kuan Cheng",
        "Shanghang Zhang",
        "Jian Tang"
      ],
      "published": "2025-12-31T05:59:40Z",
      "categories": [
        "cs.RO"
      ],
      "primary_category": "cs.RO",
      "abs_url": "https://arxiv.org/abs/2512.24653v1",
      "pdf_url": "https://arxiv.org/pdf/2512.24653v1.pdf"
    },
    {
      "id": "2512.24651v1",
      "title": "Hybrid Motion Planning with Deep Reinforcement Learning for Mobile Robot Navigation",
      "abstract": "Autonomous mobile robots operating in complex, dynamic environments face the dual challenge of navigating large-scale, structurally diverse spaces with static obstacles while safely interacting with various moving agents. Traditional graph-based planners excel at long-range pathfinding but lack reactivity, while Deep Reinforcement Learning (DRL) methods demonstrate strong collision avoidance but often fail to reach distant goals due to a lack of global context. We propose Hybrid Motion Planning with Deep Reinforcement Learning (HMP-DRL), a hybrid framework that bridges this gap. Our approach utilizes a graph-based global planner to generate a path, which is integrated into a local DRL policy via a sequence of checkpoints encoded in both the state space and reward function. To ensure social compliance, the local planner employs an entity-aware reward structure that dynamically adjusts safety margins and penalties based on the semantic type of surrounding agents. We validate the proposed method through extensive testing in a realistic simulation environment derived from real-world map data. Comprehensive experiments demonstrate that HMP-DRL consistently outperforms other methods, including state-of-the-art approaches, in terms of key metrics of robot navigation: success rate, collision rate, and time to reach the goal. Overall, these findings confirm that integrating long-term path guidance with semantically-aware local control significantly enhances both the safety and reliability of autonomous navigation in complex human-centric settings.",
      "authors": [
        "Yury Kolomeytsev",
        "Dmitry Golembiovsky"
      ],
      "published": "2025-12-31T05:58:57Z",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.RO",
      "abs_url": "https://arxiv.org/abs/2512.24651v1",
      "pdf_url": "https://arxiv.org/pdf/2512.24651v1.pdf"
    },
    {
      "id": "2512.24638v1",
      "title": "Resolving State Ambiguity in Robot Manipulation via Adaptive Working Memory Recoding",
      "abstract": "State ambiguity is common in robotic manipulation. Identical observations may correspond to multiple valid behavior trajectories. The visuomotor policy must correctly extract the appropriate types and levels of information from the history to identify the current task phase. However, naively extending the history window is computationally expensive and may cause severe overfitting. Inspired by the continuous nature of human reasoning and the recoding of working memory, we introduce PAM, a novel visuomotor Policy equipped with Adaptive working Memory. With minimal additional training cost in a two-stage manner, PAM supports a 300-frame history window while maintaining high inference speed. Specifically, a hierarchical frame feature extractor yields two distinct representations for motion primitives and temporal disambiguation. For compact representation, a context router with range-specific queries is employed to produce compact context features across multiple history lengths. And an auxiliary objective of reconstructing historical information is introduced to ensure that the context router acts as an effective bottleneck. We meticulously design 7 tasks and verify that PAM can handle multiple scenarios of state ambiguity simultaneously. With a history window of approximately 10 seconds, PAM still supports stable training and maintains inference speeds above 20Hz. Project website: https://tinda24.github.io/pam/",
      "authors": [
        "Qingda Hu",
        "Ziheng Qiu",
        "Zijun Xu",
        "Kaizhao Zhang",
        "Xizhou Bu",
        "Zuolei Sun",
        "Bo Zhang",
        "Jieru Zhao",
        "Zhongxue Gan",
        "Wenchao Ding"
      ],
      "published": "2025-12-31T05:20:13Z",
      "categories": [
        "cs.RO"
      ],
      "primary_category": "cs.RO",
      "abs_url": "https://arxiv.org/abs/2512.24638v1",
      "pdf_url": "https://arxiv.org/pdf/2512.24638v1.pdf"
    },
    {
      "id": "2512.24550v1",
      "title": "DISF: Disentangled Iterative Surface Fitting for Contact-stable Grasp Planning with Grasp Pose Alignment to the Object Center of Mass",
      "abstract": "In this work, we address the limitation of surface fitting-based grasp planning algorithm, which primarily focuses on geometric alignment between the gripper and object surface while overlooking the stability of contact point distribution, often resulting in unstable grasps due to inadequate contact configurations. To overcome this limitation, we propose a novel surface fitting algorithm that integrates contact stability while preserving geometric compatibility. Inspired by human grasping behavior, our method disentangles the grasp pose optimization into three sequential steps: (1) rotation optimization to align contact normals, (2) translation refinement to improve the alignment between the gripper frame origin and the object Center of Mass (CoM), and (3) gripper aperture adjustment to optimize contact point distribution. We validate our approach in simulation across 15 objects under both Known-shape (with clean CAD-derived dataset) and Observed-shape (with YCB object dataset) settings, including cross-platform grasp execution on three robot--gripper platforms. We further validate the method in real-world grasp experiments on a UR3e robot. Overall, DISF reduces CoM misalignment while maintaining geometric compatibility, translating into higher grasp success in both simulation and real-world execution compared to baselines. Additional videos and supplementary results are available on our project page: https://tomoya-yamanokuchi.github.io/disf-ras-project-page/",
      "authors": [
        "Tomoya Yamanokuchi",
        "Alberto Bacchin",
        "Emilio Olivastri",
        "Ryotaro Arifuku",
        "Takamitsu Matsubara",
        "Emanuele Menegatti"
      ],
      "published": "2025-12-31T01:15:09Z",
      "categories": [
        "cs.RO"
      ],
      "primary_category": "cs.RO",
      "abs_url": "https://arxiv.org/abs/2512.24550v1",
      "pdf_url": "https://arxiv.org/pdf/2512.24550v1.pdf"
    },
    {
      "id": "2512.24497v1",
      "title": "What Drives Success in Physical Planning with Joint-Embedding Predictive World Models?",
      "abstract": "A long-standing challenge in AI is to develop agents capable of solving a wide range of physical tasks and generalizing to new, unseen tasks and environments. A popular recent approach involves training a world model from state-action trajectories and subsequently use it with a planning algorithm to solve new tasks. Planning is commonly performed in the input space, but a recent family of methods has introduced planning algorithms that optimize in the learned representation space of the world model, with the promise that abstracting irrelevant details yields more efficient planning. In this work, we characterize models from this family as JEPA-WMs and investigate the technical choices that make algorithms from this class work. We propose a comprehensive study of several key components with the objective of finding the optimal approach within the family. We conducted experiments using both simulated environments and real-world robotic data, and studied how the model architecture, the training objective, and the planning algorithm affect planning success. We combine our findings to propose a model that outperforms two established baselines, DINO-WM and V-JEPA-2-AC, in both navigation and manipulation tasks. Code, data and checkpoints are available at https://github.com/facebookresearch/jepa-wms.",
      "authors": [
        "Basile Terver",
        "Tsung-Yen Yang",
        "Jean Ponce",
        "Adrien Bardes",
        "Yann LeCun"
      ],
      "published": "2025-12-30T22:50:03Z",
      "categories": [
        "cs.AI",
        "cs.LG",
        "cs.RO",
        "stat.ML"
      ],
      "primary_category": "cs.AI",
      "abs_url": "https://arxiv.org/abs/2512.24497v1",
      "pdf_url": "https://arxiv.org/pdf/2512.24497v1.pdf"
    },
    {
      "id": "2512.24493v1",
      "title": "Energy-Aware Bayesian Control Barrier Functions for Physics-Informed Gaussian Process Dynamics",
      "abstract": "We study safe control for dynamical systems whose continuous-time dynamics are learned with Gaussian processes (GPs), focusing on mechanical and port-Hamiltonian systems where safety is naturally expressed via energy constraints. The availability of a GP Hamiltonian posterior naturally raises the question of how to systematically exploit this structure to design an energy-aware control barrier function with high-probability safety guarantees. We address this problem by developing a Bayesian-CBF framework and instantiating it with energy-aware Bayesian-CBFs (EB-CBFs) that construct conservative energy-based barriers directly from the Hamiltonian and vector-field posteriors, yielding safety filters that minimally modify a nominal controller while providing probabilistic energy safety guarantees. Numerical simulations on a mass-spring system demonstrate that the proposed EB-CBFs achieve high-probability safety under noisy sampled GP-learned dynamics.",
      "authors": [
        "Chi Ho Leung",
        "Philip E. Paré"
      ],
      "published": "2025-12-30T22:24:37Z",
      "categories": [
        "eess.SY",
        "cs.RO"
      ],
      "primary_category": "eess.SY",
      "abs_url": "https://arxiv.org/abs/2512.24493v1",
      "pdf_url": "https://arxiv.org/pdf/2512.24493v1.pdf"
    },
    {
      "id": "2512.24470v2",
      "title": "Foundation models on the bridge: Semantic hazard detection and safety maneuvers for maritime autonomy with vision-language models",
      "abstract": "The draft IMO MASS Code requires autonomous and remotely supervised maritime vessels to detect departures from their operational design domain, enter a predefined fallback that notifies the operator, permit immediate human override, and avoid changing the voyage plan without approval. Meeting these obligations in the alert-to-takeover gap calls for a short-horizon, human-overridable fallback maneuver. Classical maritime autonomy stacks struggle when the correct action depends on meaning (e.g., diver-down flag means people in the water, fire close by means hazard). We argue (i) that vision-language models (VLMs) provide semantic awareness for such out-of-distribution situations, and (ii) that a fast-slow anomaly pipeline with a short-horizon, human-overridable fallback maneuver makes this practical in the handover window. We introduce Semantic Lookout, a camera-only, candidate-constrained VLM fallback maneuver selector that selects one cautious action (or station-keeping) from water-valid, world-anchored trajectories under continuous human authority. On 40 harbor scenes we measure per-call scene understanding and latency, alignment with human consensus (model majority-of-three voting), short-horizon risk-relief on fire hazard scenes, and an on-water alert->fallback maneuver->operator handover. Sub-10 s models retain most of the awareness of slower state-of-the-art models. The fallback maneuver selector outperforms geometry-only baselines and increases standoff distance on fire scenes. A field run verifies end-to-end operation. These results support VLMs as semantic fallback maneuver selectors compatible with the draft IMO MASS Code, within practical latency budgets, and motivate future work on domain-adapted, hybrid autonomy that pairs foundation-model semantics with multi-sensor bird's-eye-view perception and short-horizon replanning. Website: kimachristensen.github.io/bridge_policy",
      "authors": [
        "Kim Alexander Christensen",
        "Andreas Gudahl Tufte",
        "Alexey Gusev",
        "Rohan Sinha",
        "Milan Ganai",
        "Ole Andreas Alsos",
        "Marco Pavone",
        "Martin Steinert"
      ],
      "published": "2025-12-30T21:20:41Z",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "abs_url": "https://arxiv.org/abs/2512.24470v2",
      "pdf_url": "https://arxiv.org/pdf/2512.24470v2.pdf"
    },
    {
      "id": "2512.24428v1",
      "title": "Subsecond 3D Mesh Generation for Robot Manipulation",
      "abstract": "3D meshes are a fundamental representation widely used in computer science and engineering. In robotics, they are particularly valuable because they capture objects in a form that aligns directly with how robots interact with the physical world, enabling core capabilities such as predicting stable grasps, detecting collisions, and simulating dynamics. Although automatic 3D mesh generation methods have shown promising progress in recent years, potentially offering a path toward real-time robot perception, two critical challenges remain. First, generating high-fidelity meshes is prohibitively slow for real-time use, often requiring tens of seconds per object. Second, mesh generation by itself is insufficient. In robotics, a mesh must be contextually grounded, i.e., correctly segmented from the scene and registered with the proper scale and pose. Additionally, unless these contextual grounding steps remain efficient, they simply introduce new bottlenecks. In this work, we introduce an end-to-end system that addresses these challenges, producing a high-quality, contextually grounded 3D mesh from a single RGB-D image in under one second. Our pipeline integrates open-vocabulary object segmentation, accelerated diffusion-based mesh generation, and robust point cloud registration, each optimized for both speed and accuracy. We demonstrate its effectiveness in a real-world manipulation task, showing that it enables meshes to be used as a practical, on-demand representation for robotics perception and planning.",
      "authors": [
        "Qian Wang",
        "Omar Abdellall",
        "Tony Gao",
        "Xiatao Sun",
        "Daniel Rakita"
      ],
      "published": "2025-12-30T19:08:36Z",
      "categories": [
        "cs.RO"
      ],
      "primary_category": "cs.RO",
      "abs_url": "https://arxiv.org/abs/2512.24428v1",
      "pdf_url": "https://arxiv.org/pdf/2512.24428v1.pdf"
    },
    {
      "id": "2512.24426v1",
      "title": "Counterfactual VLA: Self-Reflective Vision-Language-Action Model with Adaptive Reasoning",
      "abstract": "Recent reasoning-augmented Vision-Language-Action (VLA) models have improved the interpretability of end-to-end autonomous driving by generating intermediate reasoning traces. Yet these models primarily describe what they perceive and intend to do, rarely questioning whether their planned actions are safe or appropriate. This work introduces Counterfactual VLA (CF-VLA), a self-reflective VLA framework that enables the model to reason about and revise its planned actions before execution. CF-VLA first generates time-segmented meta-actions that summarize driving intent, and then performs counterfactual reasoning conditioned on both the meta-actions and the visual context. This step simulates potential outcomes, identifies unsafe behaviors, and outputs corrected meta-actions that guide the final trajectory generation. To efficiently obtain such self-reflective capabilities, we propose a rollout-filter-label pipeline that mines high-value scenes from a base (non-counterfactual) VLA's rollouts and labels counterfactual reasoning traces for subsequent training rounds. Experiments on large-scale driving datasets show that CF-VLA improves trajectory accuracy by up to 17.6%, enhances safety metrics by 20.5%, and exhibits adaptive thinking: it only enables counterfactual reasoning in challenging scenarios. By transforming reasoning traces from one-shot descriptions to causal self-correction signals, CF-VLA takes a step toward self-reflective autonomous driving agents that learn to think before they act.",
      "authors": [
        "Zhenghao \"Mark\" Peng",
        "Wenhao Ding",
        "Yurong You",
        "Yuxiao Chen",
        "Wenjie Luo",
        "Thomas Tian",
        "Yulong Cao",
        "Apoorva Sharma",
        "Danfei Xu",
        "Boris Ivanovic",
        "Boyi Li",
        "Bolei Zhou",
        "Yan Wang",
        "Marco Pavone"
      ],
      "published": "2025-12-30T19:04:17Z",
      "categories": [
        "cs.RO"
      ],
      "primary_category": "cs.RO",
      "abs_url": "https://arxiv.org/abs/2512.24426v1",
      "pdf_url": "https://arxiv.org/pdf/2512.24426v1.pdf"
    },
    {
      "id": "2512.24402v1",
      "title": "Fast and Realistic Automated Scenario Simulations and Reporting for an Autonomous Racing Stack",
      "abstract": "In this paper, we describe the automated simulation and reporting pipeline implemented for our autonomous racing stack, ur.autopilot. The backbone of the simulation is based on a high-fidelity model of the vehicle interfaced as a Functional Mockup Unit (FMU). The pipeline can execute the software stack and the simulation up to three times faster than real-time, locally or on GitHub for Continuous Integration/- Continuous Delivery (CI/CD). As the most important input of the pipeline, there is a set of running scenarios. Each scenario allows the initialization of the ego vehicle in different initial conditions (position and speed), as well as the initialization of any other configuration of the stack. This functionality is essential to validate efficiently critical modules, like the one responsible for high-speed overtaking maneuvers or localization, which are among the most challenging aspects of autonomous racing. Moreover, we describe how we implemented a fault injection module, capable of introducing sensor delays and perturbations as well as modifying outputs of any node of the stack. Finally, we describe the design of our automated reporting process, aimed at maximizing the effectiveness of the simulation analysis.",
      "authors": [
        "Giovanni Lambertini",
        "Matteo Pini",
        "Eugenio Mascaro",
        "Francesco Moretti",
        "Ayoub Raji",
        "Marko Bertogna"
      ],
      "published": "2025-12-30T18:36:20Z",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.SE",
        "eess.SY"
      ],
      "primary_category": "cs.RO",
      "abs_url": "https://arxiv.org/abs/2512.24402v1",
      "pdf_url": "https://arxiv.org/pdf/2512.24402v1.pdf"
    },
    {
      "id": "2512.24385v1",
      "title": "Forging Spatial Intelligence: A Roadmap of Multi-Modal Data Pre-Training for Autonomous Systems",
      "abstract": "The rapid advancement of autonomous systems, including self-driving vehicles and drones, has intensified the need to forge true Spatial Intelligence from multi-modal onboard sensor data. While foundation models excel in single-modal contexts, integrating their capabilities across diverse sensors like cameras and LiDAR to create a unified understanding remains a formidable challenge. This paper presents a comprehensive framework for multi-modal pre-training, identifying the core set of techniques driving progress toward this goal. We dissect the interplay between foundational sensor characteristics and learning strategies, evaluating the role of platform-specific datasets in enabling these advancements. Our central contribution is the formulation of a unified taxonomy for pre-training paradigms: ranging from single-modality baselines to sophisticated unified frameworks that learn holistic representations for advanced tasks like 3D object detection and semantic occupancy prediction. Furthermore, we investigate the integration of textual inputs and occupancy representations to facilitate open-world perception and planning. Finally, we identify critical bottlenecks, such as computational efficiency and model scalability, and propose a roadmap toward general-purpose multi-modal foundation models capable of achieving robust Spatial Intelligence for real-world deployment.",
      "authors": [
        "Song Wang",
        "Lingdong Kong",
        "Xiaolu Liu",
        "Hao Shi",
        "Wentong Li",
        "Jianke Zhu",
        "Steven C. H. Hoi"
      ],
      "published": "2025-12-30T17:58:01Z",
      "categories": [
        "cs.CV",
        "cs.RO"
      ],
      "primary_category": "cs.CV",
      "abs_url": "https://arxiv.org/abs/2512.24385v1",
      "pdf_url": "https://arxiv.org/pdf/2512.24385v1.pdf"
    },
    {
      "id": "2512.24384v1",
      "title": "Geometric Multi-Session Map Merging with Learned Local Descriptors",
      "abstract": "Multi-session map merging is crucial for extended autonomous operations in large-scale environments. In this paper, we present GMLD, a learning-based local descriptor framework for large-scale multi-session point cloud map merging that systematically aligns maps collected across different sessions with overlapping regions. The proposed framework employs a keypoint-aware encoder and a plane-based geometric transformer to extract discriminative features for loop closure detection and relative pose estimation. To further improve global consistency, we include inter-session scan matching cost factors in the factor-graph optimization stage. We evaluate our framework on the public datasets, as well as self-collected data from diverse environments. The results show accurate and robust map merging with low error, and the learned features deliver strong performance in both loop closure detection and relative pose estimation.",
      "authors": [
        "Yanlong Ma",
        "Nakul S. Joshi",
        "Christa S. Robison",
        "Philip R. Osteen",
        "Brett T. Lopez"
      ],
      "published": "2025-12-30T17:56:15Z",
      "categories": [
        "cs.RO",
        "cs.CV"
      ],
      "primary_category": "cs.RO",
      "abs_url": "https://arxiv.org/abs/2512.24384v1",
      "pdf_url": "https://arxiv.org/pdf/2512.24384v1.pdf"
    },
    {
      "id": "2512.24377v1",
      "title": "New Insights into Cascaded Geometric Flight Control: From Performance Guarantees to Practical Pitfalls",
      "abstract": "We present a new stability proof for cascaded geometric control used by aerial vehicles tracking time-varying position trajectories. Our approach uses sliding variables and a recently proposed quaternion-based sliding controller to demonstrate that exponentially convergent position trajectory tracking is theoretically possible. Notably, our analysis reveals new aspects of the control strategy, including how tracking error in the attitude loop influences the position loop, how model uncertainties affect the closed-loop system, and the practical pitfalls of the control architecture.",
      "authors": [
        "Brett T. Lopez"
      ],
      "published": "2025-12-30T17:35:34Z",
      "categories": [
        "eess.SY",
        "cs.RO"
      ],
      "primary_category": "eess.SY",
      "abs_url": "https://arxiv.org/abs/2512.24377v1",
      "pdf_url": "https://arxiv.org/pdf/2512.24377v1.pdf"
    },
    {
      "id": "2512.24326v1",
      "title": "3D Path-Following Guidance via Nonlinear Model Predictive Control for Fixed-Wing Small UAS",
      "abstract": "This paper presents the design, implementation, and flight test results of two novel 3D path-following guidance algorithms based on nonlinear model predictive control (MPC), with specific application to fixed-wing small uncrewed aircraft systems. To enable MPC, control-augmented modelling and system identification of the RAAVEN small uncrewed aircraft is presented. Two formulations of MPC are then showcased. The first schedules a static reference path rate over the MPC horizon, incentivizing a constant inertial speed. The second, with inspiration from model predictive contouring control, dynamically optimizes for the reference path rate over the controller horizon as the system operates. This allows for a weighted tradeoff between path progression and distance from path, two competing objectives in path-following guidance. Both controllers are formulated to operate over general smooth 3D arc-length parameterized curves. The MPC guidance algorithms are flown over several high-curvature test paths, with comparison to a baseline lookahead guidance law. The results showcase the real-world feasibility and superior performance of nonlinear MPC for 3D path-following guidance at ground speeds up to 36 meters per second.",
      "authors": [
        "Camron Alexander Hirst",
        "Chris Reale",
        "Eric Frew"
      ],
      "published": "2025-12-30T16:27:44Z",
      "categories": [
        "cs.RO"
      ],
      "primary_category": "cs.RO",
      "abs_url": "https://arxiv.org/abs/2512.24326v1",
      "pdf_url": "https://arxiv.org/pdf/2512.24326v1.pdf"
    },
    {
      "id": "2512.24321v1",
      "title": "UniAct: Unified Motion Generation and Action Streaming for Humanoid Robots",
      "abstract": "A long-standing objective in humanoid robotics is the realization of versatile agents capable of following diverse multimodal instructions with human-level flexibility. Despite advances in humanoid control, bridging high-level multimodal perception with whole-body execution remains a significant bottleneck. Existing methods often struggle to translate heterogeneous instructions -- such as language, music, and trajectories -- into stable, real-time actions. Here we show that UniAct, a two-stage framework integrating a fine-tuned MLLM with a causal streaming pipeline, enables humanoid robots to execute multimodal instructions with sub-500 ms latency. By unifying inputs through a shared discrete codebook via FSQ, UniAct ensures cross-modal alignment while constraining motions to a physically grounded manifold. This approach yields a 19% improvement in the success rate of zero-shot tracking of imperfect reference motions. We validate UniAct on UniMoCap, our 20-hour humanoid motion benchmark, demonstrating robust generalization across diverse real-world scenarios. Our results mark a critical step toward responsive, general-purpose humanoid assistants capable of seamless interaction through unified perception and control.",
      "authors": [
        "Nan Jiang",
        "Zimo He",
        "Wanhe Yu",
        "Lexi Pang",
        "Yunhao Li",
        "Hongjie Li",
        "Jieming Cui",
        "Yuhan Li",
        "Yizhou Wang",
        "Yixin Zhu",
        "Siyuan Huang"
      ],
      "published": "2025-12-30T16:20:13Z",
      "categories": [
        "cs.CV",
        "cs.RO"
      ],
      "primary_category": "cs.CV",
      "abs_url": "https://arxiv.org/abs/2512.24321v1",
      "pdf_url": "https://arxiv.org/pdf/2512.24321v1.pdf"
    }
  ]
}